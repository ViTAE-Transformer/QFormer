[2022-11-05 19:35:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 442): INFO Full config saved to output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/config.json
[2022-11-05 19:35:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 445): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
  SCALE: null
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EMA:
  EMA_DECAY: 0.9999200015999999
  EMA_FORCE_CPU: false
  ENABLE_EMA: true
ENABLE_WANDB: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  EMSWIN:
    EM_FACTOR: 0.9
    EM_ITERS: 3
    INSTANCE_TOKENS:
    - 10
    - 10
    - 10
    - 0
  LABEL_SMOOTHING: 0.1
  NAME: QFormer_transformer_tiny_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    LAYER_RATIO:
    - 1
    - 2
    - 3
    - 4
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    RELATIVE_POS_EMBEDDING: true
    SHIFT: true
    WINDOW_SIZE: 7
  TYPE: qformer
OUTPUT: output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1
PRINT_FREQ: 50
RESUME_OPTIMIZER: true
SAVE_FREQ: 1
SEED: 0
TAG: 1024-dpr20-coords_lambda1e-1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.001
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05
  coords_lambda: 0.1

[2022-11-05 19:35:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 99): INFO Creating model:qformer/QFormer_transformer_tiny_patch4_window7_224
[2022-11-05 19:35:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 102): INFO QFormer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(96, 27, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(96, 27, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(192, 54, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(192, 54, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(768, 216, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(768, 216, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-11-05 19:35:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 115): INFO enable EMA model
[2022-11-05 19:35:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 129): INFO number of params: 29116924
[2022-11-05 19:35:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 132): INFO number of GFLOPs: 0.015670272
[2022-11-05 19:35:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 175): INFO no checkpoint found in output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1, ignoring auto resume
[2022-11-05 19:35:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 201): INFO Start training
[2022-11-05 19:35:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][0/1251]	eta 2:25:42 lr 0.000001	time 6.9886 (6.9886)	loss 6.9953 (6.9953)	grad_norm 1.0697 (1.0697)	mem 14468MB
[2022-11-05 19:36:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][50/1251]	eta 0:12:18 lr 0.000003	time 0.4694 (0.6147)	loss 6.9353 (6.9489)	grad_norm 0.9182 (0.9936)	mem 14812MB
[2022-11-05 19:36:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][100/1251]	eta 0:10:28 lr 0.000005	time 0.4628 (0.5457)	loss 6.9510 (6.9398)	grad_norm 0.8976 (0.9794)	mem 14812MB
[2022-11-05 19:36:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][150/1251]	eta 0:09:32 lr 0.000007	time 0.4620 (0.5201)	loss 6.9296 (6.9330)	grad_norm 0.9305 (0.9697)	mem 14812MB
[2022-11-05 19:37:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][200/1251]	eta 0:08:53 lr 0.000009	time 0.4683 (0.5073)	loss 6.8921 (6.9272)	grad_norm 0.9823 (0.9586)	mem 14813MB
[2022-11-05 19:37:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][250/1251]	eta 0:08:20 lr 0.000011	time 0.4640 (0.5004)	loss 6.9260 (6.9224)	grad_norm 0.9113 (0.9498)	mem 14813MB
[2022-11-05 19:38:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][300/1251]	eta 0:07:51 lr 0.000013	time 0.4698 (0.4953)	loss 6.8964 (6.9180)	grad_norm 0.8602 (0.9371)	mem 14813MB
[2022-11-05 19:38:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][350/1251]	eta 0:07:23 lr 0.000015	time 0.4635 (0.4920)	loss 6.8815 (6.9143)	grad_norm 0.8313 (0.9283)	mem 14813MB
[2022-11-05 19:38:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][400/1251]	eta 0:06:56 lr 0.000017	time 0.4589 (0.4895)	loss 6.8817 (6.9103)	grad_norm 0.9917 (0.9232)	mem 14813MB
[2022-11-05 19:39:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][450/1251]	eta 0:06:30 lr 0.000019	time 0.4722 (0.4872)	loss 6.8524 (6.9072)	grad_norm 0.9138 (0.9206)	mem 14813MB
[2022-11-05 19:39:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][500/1251]	eta 0:06:04 lr 0.000021	time 0.4582 (0.4856)	loss 6.9079 (6.9026)	grad_norm 0.9394 (0.9163)	mem 14813MB
[2022-11-05 19:39:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][550/1251]	eta 0:05:39 lr 0.000023	time 0.4708 (0.4842)	loss 6.8520 (6.8987)	grad_norm 0.7529 (0.9110)	mem 14813MB
[2022-11-05 19:40:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][600/1251]	eta 0:05:14 lr 0.000025	time 0.4741 (0.4829)	loss 6.8089 (6.8952)	grad_norm 0.9028 (0.9076)	mem 14813MB
[2022-11-05 19:40:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][650/1251]	eta 0:04:49 lr 0.000027	time 0.4727 (0.4820)	loss 6.9025 (6.8915)	grad_norm 0.7802 (0.9053)	mem 14813MB
[2022-11-05 19:41:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][700/1251]	eta 0:04:25 lr 0.000029	time 0.4741 (0.4810)	loss 6.8731 (6.8885)	grad_norm 0.7661 (0.9039)	mem 14813MB
[2022-11-05 19:41:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][750/1251]	eta 0:04:00 lr 0.000031	time 0.4705 (0.4804)	loss 6.8678 (6.8861)	grad_norm 0.9675 (0.9019)	mem 14813MB
[2022-11-05 19:41:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][800/1251]	eta 0:03:36 lr 0.000033	time 0.4654 (0.4797)	loss 6.8696 (6.8837)	grad_norm 0.9446 (0.8984)	mem 14813MB
[2022-11-05 19:42:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][850/1251]	eta 0:03:12 lr 0.000035	time 0.4783 (0.4791)	loss 6.7850 (6.8798)	grad_norm 1.1867 (0.9007)	mem 14813MB
[2022-11-05 19:42:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][900/1251]	eta 0:02:48 lr 0.000037	time 0.4610 (0.4787)	loss 6.8279 (6.8766)	grad_norm 0.8232 (0.9069)	mem 14813MB
[2022-11-05 19:43:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][950/1251]	eta 0:02:23 lr 0.000039	time 0.4665 (0.4781)	loss 6.7212 (6.8724)	grad_norm 1.2733 (0.9154)	mem 14814MB
[2022-11-05 19:43:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][1000/1251]	eta 0:01:59 lr 0.000041	time 0.4697 (0.4775)	loss 6.7682 (6.8694)	grad_norm 1.1110 (0.9296)	mem 14814MB
[2022-11-05 19:43:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][1050/1251]	eta 0:01:35 lr 0.000043	time 0.4747 (0.4770)	loss 6.6807 (6.8654)	grad_norm 1.0045 (0.9471)	mem 14814MB
[2022-11-05 19:44:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][1100/1251]	eta 0:01:11 lr 0.000045	time 0.4615 (0.4766)	loss 6.7327 (6.8609)	grad_norm 1.5719 (0.9714)	mem 14814MB
[2022-11-05 19:44:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][1150/1251]	eta 0:00:48 lr 0.000047	time 0.4704 (0.4764)	loss 6.8560 (6.8566)	grad_norm 1.1348 (1.0002)	mem 14814MB
[2022-11-05 19:45:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][1200/1251]	eta 0:00:24 lr 0.000049	time 0.4725 (0.4761)	loss 6.7103 (6.8523)	grad_norm 1.1245 (1.0204)	mem 14814MB
[2022-11-05 19:45:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [0/300][1250/1251]	eta 0:00:00 lr 0.000051	time 0.4558 (0.4757)	loss 6.6429 (6.8473)	grad_norm 1.9734 (1.0419)	mem 14814MB
[2022-11-05 19:45:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 0 training takes 0:09:55
[2022-11-05 19:45:27 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_0.pth saving......
[2022-11-05 19:45:27 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_0.pth saved !!!
[2022-11-05 19:45:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.655 (1.655)	Loss 6.3647 (6.3647)	Acc@1 2.344 (2.344)	Acc@5 7.422 (7.422)	Mem 14814MB
[2022-11-05 19:45:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 1.682 Acc@5 5.984
[2022-11-05 19:45:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 1.7%
[2022-11-05 19:45:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.757 (1.757)	Loss 6.9474 (6.9474)	Acc@1 0.098 (0.098)	Acc@5 0.977 (0.977)	Mem 14814MB
[2022-11-05 19:45:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.094 Acc@5 0.492
[2022-11-05 19:45:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.1%
[2022-11-05 19:45:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 1.68% at 0 epoch
[2022-11-05 19:45:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][0/1251]	eta 0:49:28 lr 0.000051	time 2.3730 (2.3730)	loss 6.7243 (6.7243)	grad_norm 1.0715 (1.0715)	mem 14838MB
[2022-11-05 19:46:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][50/1251]	eta 0:10:10 lr 0.000053	time 0.4640 (0.5087)	loss 6.5593 (6.7208)	grad_norm 1.9900 (1.4494)	mem 14838MB
[2022-11-05 19:46:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][100/1251]	eta 0:09:23 lr 0.000055	time 0.4689 (0.4898)	loss 6.6925 (6.7099)	grad_norm 2.5314 (1.4777)	mem 14838MB
[2022-11-05 19:47:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][150/1251]	eta 0:08:50 lr 0.000057	time 0.4704 (0.4821)	loss 6.7919 (6.7090)	grad_norm 1.8600 (1.5586)	mem 14838MB
[2022-11-05 19:47:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][200/1251]	eta 0:08:22 lr 0.000059	time 0.4779 (0.4777)	loss 6.6601 (6.7025)	grad_norm 2.8853 (1.6375)	mem 14838MB
[2022-11-05 19:47:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][250/1251]	eta 0:07:55 lr 0.000061	time 0.4653 (0.4754)	loss 6.7704 (6.6903)	grad_norm 1.2032 (1.6613)	mem 14838MB
[2022-11-05 19:48:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][300/1251]	eta 0:07:30 lr 0.000063	time 0.4663 (0.4740)	loss 6.5713 (6.6847)	grad_norm 2.3540 (1.6900)	mem 14838MB
[2022-11-05 19:48:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][350/1251]	eta 0:07:06 lr 0.000065	time 0.4567 (0.4730)	loss 6.6780 (6.6786)	grad_norm 2.1845 (1.7152)	mem 14838MB
[2022-11-05 19:48:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][400/1251]	eta 0:06:41 lr 0.000067	time 0.4634 (0.4723)	loss 6.4119 (6.6693)	grad_norm 1.5029 (1.7383)	mem 14838MB
[2022-11-05 19:49:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][450/1251]	eta 0:06:17 lr 0.000069	time 0.4687 (0.4717)	loss 6.7901 (6.6629)	grad_norm 1.4947 (1.7633)	mem 14838MB
[2022-11-05 19:49:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][500/1251]	eta 0:05:54 lr 0.000071	time 0.4647 (0.4715)	loss 6.7606 (6.6564)	grad_norm 1.6509 (1.7745)	mem 14838MB
[2022-11-05 19:50:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][550/1251]	eta 0:05:30 lr 0.000073	time 0.4618 (0.4710)	loss 6.5321 (6.6515)	grad_norm 1.7319 (1.7995)	mem 14838MB
[2022-11-05 19:50:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][600/1251]	eta 0:05:06 lr 0.000075	time 0.4663 (0.4706)	loss 6.4799 (6.6438)	grad_norm 1.5610 (1.7980)	mem 14838MB
[2022-11-05 19:50:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][650/1251]	eta 0:04:42 lr 0.000077	time 0.4583 (0.4704)	loss 6.6276 (6.6366)	grad_norm 1.5219 (1.8006)	mem 14838MB
[2022-11-05 19:51:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][700/1251]	eta 0:04:18 lr 0.000079	time 0.4679 (0.4700)	loss 6.6344 (6.6288)	grad_norm 2.1741 (1.8107)	mem 14838MB
[2022-11-05 19:51:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][750/1251]	eta 0:03:55 lr 0.000081	time 0.4666 (0.4699)	loss 6.3542 (6.6207)	grad_norm 1.8628 (1.8042)	mem 14838MB
[2022-11-05 19:52:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][800/1251]	eta 0:03:31 lr 0.000083	time 0.4593 (0.4698)	loss 6.6666 (6.6180)	grad_norm 1.4147 (1.8147)	mem 14838MB
[2022-11-05 19:52:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][850/1251]	eta 0:03:08 lr 0.000085	time 0.4627 (0.4696)	loss 6.3773 (6.6115)	grad_norm 1.8510 (1.8204)	mem 14838MB
[2022-11-05 19:52:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][900/1251]	eta 0:02:44 lr 0.000087	time 0.4631 (0.4693)	loss 6.6767 (6.6072)	grad_norm 1.4234 (1.8307)	mem 14838MB
[2022-11-05 19:53:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][950/1251]	eta 0:02:21 lr 0.000089	time 0.4695 (0.4692)	loss 6.5038 (6.6018)	grad_norm 2.0083 (1.8435)	mem 14838MB
[2022-11-05 19:53:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][1000/1251]	eta 0:01:57 lr 0.000091	time 0.4607 (0.4692)	loss 6.6036 (6.5946)	grad_norm 1.7505 (inf)	mem 14838MB
[2022-11-05 19:54:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][1050/1251]	eta 0:01:34 lr 0.000093	time 0.4682 (0.4691)	loss 6.2877 (6.5890)	grad_norm 1.6475 (inf)	mem 14838MB
[2022-11-05 19:54:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][1100/1251]	eta 0:01:10 lr 0.000095	time 0.4660 (0.4689)	loss 6.2347 (6.5857)	grad_norm 1.7294 (inf)	mem 14838MB
[2022-11-05 19:54:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][1150/1251]	eta 0:00:47 lr 0.000097	time 0.4609 (0.4688)	loss 6.7334 (6.5800)	grad_norm 1.6797 (inf)	mem 14838MB
[2022-11-05 19:55:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][1200/1251]	eta 0:00:23 lr 0.000099	time 0.4745 (0.4687)	loss 6.6616 (6.5761)	grad_norm 1.7796 (inf)	mem 14838MB
[2022-11-05 19:55:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [1/300][1250/1251]	eta 0:00:00 lr 0.000101	time 0.4574 (0.4685)	loss 6.1775 (6.5705)	grad_norm 1.8752 (inf)	mem 14838MB
[2022-11-05 19:55:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 1 training takes 0:09:46
[2022-11-05 19:55:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_1.pth saving......
[2022-11-05 19:55:34 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_1.pth saved !!!
[2022-11-05 19:55:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.620 (1.620)	Loss 5.6209 (5.6209)	Acc@1 4.590 (4.590)	Acc@5 16.504 (16.504)	Mem 14838MB
[2022-11-05 19:55:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 5.802 Acc@5 17.278
[2022-11-05 19:55:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 5.8%
[2022-11-05 19:55:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.687 (1.687)	Loss 6.9545 (6.9545)	Acc@1 0.000 (0.000)	Acc@5 0.000 (0.000)	Mem 14838MB
[2022-11-05 19:55:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.112 Acc@5 0.528
[2022-11-05 19:55:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.1%
[2022-11-05 19:55:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 5.80% at 1 epoch
[2022-11-05 19:55:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][0/1251]	eta 0:42:29 lr 0.000101	time 2.0379 (2.0379)	loss 6.3915 (6.3915)	grad_norm 1.6844 (1.6844)	mem 14849MB
[2022-11-05 19:56:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][50/1251]	eta 0:10:00 lr 0.000103	time 0.4536 (0.5000)	loss 6.5953 (6.4662)	grad_norm 2.1358 (1.9542)	mem 14849MB
[2022-11-05 19:56:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][100/1251]	eta 0:09:16 lr 0.000105	time 0.4679 (0.4832)	loss 6.3526 (6.4779)	grad_norm 2.1683 (2.0180)	mem 14849MB
[2022-11-05 19:57:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][150/1251]	eta 0:08:45 lr 0.000107	time 0.4740 (0.4777)	loss 6.1351 (6.4513)	grad_norm 1.6960 (1.9879)	mem 14849MB
[2022-11-05 19:57:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][200/1251]	eta 0:08:19 lr 0.000109	time 0.4670 (0.4748)	loss 6.4671 (6.4566)	grad_norm 1.5255 (1.9850)	mem 14849MB
[2022-11-05 19:57:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][250/1251]	eta 0:07:53 lr 0.000111	time 0.4712 (0.4730)	loss 6.0306 (6.4406)	grad_norm 1.5082 (1.9853)	mem 14849MB
[2022-11-05 19:58:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][300/1251]	eta 0:07:28 lr 0.000113	time 0.4720 (0.4715)	loss 6.5315 (6.4280)	grad_norm 1.9057 (1.9683)	mem 14849MB
[2022-11-05 19:58:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][350/1251]	eta 0:07:03 lr 0.000115	time 0.4639 (0.4704)	loss 6.4047 (6.4191)	grad_norm 1.7190 (1.9738)	mem 14849MB
[2022-11-05 19:59:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][400/1251]	eta 0:06:39 lr 0.000117	time 0.4642 (0.4700)	loss 6.0482 (6.4038)	grad_norm 2.7176 (1.9760)	mem 14849MB
[2022-11-05 19:59:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][450/1251]	eta 0:06:16 lr 0.000119	time 0.4590 (0.4694)	loss 6.5265 (6.3962)	grad_norm 2.2242 (1.9785)	mem 14849MB
[2022-11-05 19:59:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][500/1251]	eta 0:05:52 lr 0.000121	time 0.4610 (0.4692)	loss 6.4716 (6.3922)	grad_norm 2.3631 (1.9873)	mem 14849MB
[2022-11-05 20:00:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][550/1251]	eta 0:05:28 lr 0.000123	time 0.4631 (0.4689)	loss 6.2319 (6.3865)	grad_norm 2.8091 (1.9921)	mem 14849MB
[2022-11-05 20:00:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][600/1251]	eta 0:05:05 lr 0.000125	time 0.4626 (0.4686)	loss 6.1796 (6.3800)	grad_norm 1.8659 (1.9910)	mem 14849MB
[2022-11-05 20:00:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][650/1251]	eta 0:04:41 lr 0.000127	time 0.4608 (0.4683)	loss 6.4126 (6.3747)	grad_norm 1.8116 (1.9895)	mem 14849MB
[2022-11-05 20:01:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][700/1251]	eta 0:04:18 lr 0.000129	time 0.4615 (0.4683)	loss 6.3708 (6.3718)	grad_norm 1.4736 (2.0083)	mem 14849MB
[2022-11-05 20:01:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][750/1251]	eta 0:03:54 lr 0.000131	time 0.4613 (0.4683)	loss 5.9829 (6.3702)	grad_norm 2.0339 (2.0042)	mem 14849MB
[2022-11-05 20:02:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][800/1251]	eta 0:03:31 lr 0.000133	time 0.4649 (0.4682)	loss 6.5047 (6.3652)	grad_norm 2.7052 (2.0014)	mem 14849MB
[2022-11-05 20:02:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][850/1251]	eta 0:03:07 lr 0.000135	time 0.4563 (0.4680)	loss 6.5246 (6.3599)	grad_norm 1.5778 (2.0093)	mem 14849MB
[2022-11-05 20:02:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][900/1251]	eta 0:02:44 lr 0.000137	time 0.4759 (0.4679)	loss 6.3751 (6.3539)	grad_norm 2.2737 (2.0081)	mem 14849MB
[2022-11-05 20:03:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][950/1251]	eta 0:02:20 lr 0.000139	time 0.4619 (0.4678)	loss 6.4491 (6.3491)	grad_norm 1.9825 (2.0158)	mem 14849MB
[2022-11-05 20:03:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][1000/1251]	eta 0:01:57 lr 0.000141	time 0.4633 (0.4679)	loss 6.5194 (6.3422)	grad_norm 2.0195 (2.0147)	mem 14849MB
[2022-11-05 20:04:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][1050/1251]	eta 0:01:34 lr 0.000143	time 0.4604 (0.4679)	loss 6.5150 (6.3357)	grad_norm 2.0474 (2.0185)	mem 14849MB
[2022-11-05 20:04:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][1100/1251]	eta 0:01:10 lr 0.000145	time 0.4627 (0.4677)	loss 5.8008 (6.3323)	grad_norm 2.3537 (2.0256)	mem 14849MB
[2022-11-05 20:04:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][1150/1251]	eta 0:00:47 lr 0.000147	time 0.4591 (0.4677)	loss 6.3727 (6.3315)	grad_norm 2.3069 (2.0330)	mem 14849MB
[2022-11-05 20:05:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][1200/1251]	eta 0:00:23 lr 0.000149	time 0.4729 (0.4676)	loss 6.1757 (6.3270)	grad_norm 2.0675 (2.0376)	mem 14849MB
[2022-11-05 20:05:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [2/300][1250/1251]	eta 0:00:00 lr 0.000151	time 0.4585 (0.4675)	loss 6.3918 (6.3222)	grad_norm 1.9564 (2.0389)	mem 14849MB
[2022-11-05 20:05:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 2 training takes 0:09:44
[2022-11-05 20:05:36 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_2.pth saving......
[2022-11-05 20:05:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_2.pth saved !!!
[2022-11-05 20:05:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.488 (1.488)	Loss 4.9247 (4.9247)	Acc@1 12.891 (12.891)	Acc@5 29.883 (29.883)	Mem 14849MB
[2022-11-05 20:05:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 12.712 Acc@5 29.452
[2022-11-05 20:05:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 12.7%
[2022-11-05 20:05:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.562 (1.562)	Loss 6.9229 (6.9229)	Acc@1 0.195 (0.195)	Acc@5 0.586 (0.586)	Mem 14849MB
[2022-11-05 20:05:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.142 Acc@5 0.678
[2022-11-05 20:05:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.1%
[2022-11-05 20:05:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 12.71% at 2 epoch
[2022-11-05 20:05:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][0/1251]	eta 0:41:10 lr 0.000151	time 1.9750 (1.9750)	loss 6.1522 (6.1522)	grad_norm 2.4091 (2.4091)	mem 14849MB
[2022-11-05 20:06:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][50/1251]	eta 0:10:03 lr 0.000153	time 0.4628 (0.5022)	loss 6.2270 (6.2281)	grad_norm 2.3425 (2.0595)	mem 14849MB
[2022-11-05 20:06:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][100/1251]	eta 0:09:18 lr 0.000155	time 0.4599 (0.4852)	loss 6.0556 (6.2012)	grad_norm 2.0359 (2.0496)	mem 14849MB
[2022-11-05 20:07:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][150/1251]	eta 0:08:47 lr 0.000157	time 0.4623 (0.4788)	loss 6.2595 (6.1663)	grad_norm 2.1530 (2.0962)	mem 14849MB
[2022-11-05 20:07:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][200/1251]	eta 0:08:19 lr 0.000159	time 0.4623 (0.4753)	loss 6.2322 (6.1502)	grad_norm 2.0865 (2.0860)	mem 14849MB
[2022-11-05 20:07:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][250/1251]	eta 0:07:53 lr 0.000161	time 0.4625 (0.4734)	loss 6.2902 (6.1570)	grad_norm 2.0711 (inf)	mem 14849MB
[2022-11-05 20:08:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][300/1251]	eta 0:07:29 lr 0.000163	time 0.4644 (0.4723)	loss 5.9154 (6.1619)	grad_norm 1.8512 (inf)	mem 14849MB
[2022-11-05 20:08:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][350/1251]	eta 0:07:04 lr 0.000165	time 0.4591 (0.4714)	loss 6.2263 (6.1435)	grad_norm 2.5013 (inf)	mem 14849MB
[2022-11-05 20:09:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][400/1251]	eta 0:06:40 lr 0.000167	time 0.4728 (0.4707)	loss 5.7299 (6.1389)	grad_norm 2.1719 (inf)	mem 14849MB
[2022-11-05 20:09:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][450/1251]	eta 0:06:16 lr 0.000169	time 0.4591 (0.4699)	loss 5.9625 (6.1340)	grad_norm 1.9284 (inf)	mem 14849MB
[2022-11-05 20:09:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][500/1251]	eta 0:05:52 lr 0.000171	time 0.4659 (0.4697)	loss 6.3184 (6.1322)	grad_norm 2.3653 (inf)	mem 14849MB
[2022-11-05 20:10:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][550/1251]	eta 0:05:29 lr 0.000173	time 0.4606 (0.4697)	loss 6.1148 (6.1242)	grad_norm 2.0535 (inf)	mem 14849MB
[2022-11-05 20:10:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][600/1251]	eta 0:05:05 lr 0.000175	time 0.4661 (0.4694)	loss 6.1417 (6.1185)	grad_norm 2.1630 (inf)	mem 14849MB
[2022-11-05 20:11:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][650/1251]	eta 0:04:41 lr 0.000177	time 0.4622 (0.4690)	loss 6.2872 (6.1164)	grad_norm 2.2535 (inf)	mem 14849MB
[2022-11-05 20:11:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][700/1251]	eta 0:04:18 lr 0.000179	time 0.4631 (0.4686)	loss 5.8770 (6.1146)	grad_norm 2.1859 (inf)	mem 14849MB
[2022-11-05 20:11:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][750/1251]	eta 0:03:54 lr 0.000181	time 0.4641 (0.4686)	loss 5.5829 (6.1102)	grad_norm 2.5179 (inf)	mem 14849MB
[2022-11-05 20:12:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][800/1251]	eta 0:03:31 lr 0.000183	time 0.4567 (0.4684)	loss 6.2369 (6.1076)	grad_norm 2.0169 (inf)	mem 14849MB
[2022-11-05 20:12:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][850/1251]	eta 0:03:07 lr 0.000185	time 0.4581 (0.4686)	loss 6.1761 (6.1068)	grad_norm 2.2123 (inf)	mem 14849MB
[2022-11-05 20:12:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][900/1251]	eta 0:02:44 lr 0.000187	time 0.4642 (0.4683)	loss 6.1542 (6.1017)	grad_norm 2.0584 (inf)	mem 14849MB
[2022-11-05 20:13:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][950/1251]	eta 0:02:20 lr 0.000189	time 0.4730 (0.4681)	loss 6.1636 (6.1020)	grad_norm 1.9991 (inf)	mem 14849MB
[2022-11-05 20:13:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][1000/1251]	eta 0:01:57 lr 0.000191	time 0.4650 (0.4682)	loss 6.3363 (6.0974)	grad_norm 2.2820 (inf)	mem 14849MB
[2022-11-05 20:14:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][1050/1251]	eta 0:01:34 lr 0.000193	time 0.4520 (0.4681)	loss 6.4983 (6.0918)	grad_norm 2.3017 (inf)	mem 14849MB
[2022-11-05 20:14:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][1100/1251]	eta 0:01:10 lr 0.000195	time 0.4568 (0.4680)	loss 6.3107 (6.0857)	grad_norm 1.9423 (inf)	mem 14849MB
[2022-11-05 20:14:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][1150/1251]	eta 0:00:47 lr 0.000197	time 0.4697 (0.4679)	loss 5.6373 (6.0763)	grad_norm 2.2551 (inf)	mem 14849MB
[2022-11-05 20:15:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][1200/1251]	eta 0:00:23 lr 0.000199	time 0.4593 (0.4678)	loss 6.2652 (6.0748)	grad_norm 1.9853 (inf)	mem 14849MB
[2022-11-05 20:15:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [3/300][1250/1251]	eta 0:00:00 lr 0.000201	time 0.4563 (0.4676)	loss 6.2837 (6.0714)	grad_norm 1.8706 (inf)	mem 14849MB
[2022-11-05 20:15:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 3 training takes 0:09:45
[2022-11-05 20:15:40 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_3.pth saving......
[2022-11-05 20:15:40 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_3.pth saved !!!
[2022-11-05 20:15:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.564 (1.564)	Loss 4.3090 (4.3090)	Acc@1 21.582 (21.582)	Acc@5 42.285 (42.285)	Mem 14849MB
[2022-11-05 20:15:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 20.172 Acc@5 41.468
[2022-11-05 20:15:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 20.2%
[2022-11-05 20:15:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.665 (1.665)	Loss 6.9015 (6.9015)	Acc@1 0.000 (0.000)	Acc@5 1.270 (1.270)	Mem 14849MB
[2022-11-05 20:15:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.178 Acc@5 0.898
[2022-11-05 20:15:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.2%
[2022-11-05 20:15:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 20.17% at 3 epoch
[2022-11-05 20:15:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][0/1251]	eta 0:42:07 lr 0.000201	time 2.0203 (2.0203)	loss 6.2646 (6.2646)	grad_norm 2.5160 (2.5160)	mem 14849MB
[2022-11-05 20:16:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][50/1251]	eta 0:10:03 lr 0.000203	time 0.4540 (0.5023)	loss 6.1086 (5.8961)	grad_norm 2.5496 (2.2811)	mem 14849MB
[2022-11-05 20:16:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][100/1251]	eta 0:09:18 lr 0.000205	time 0.4642 (0.4856)	loss 6.3425 (5.9200)	grad_norm 1.9568 (2.2974)	mem 14849MB
[2022-11-05 20:17:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][150/1251]	eta 0:08:47 lr 0.000207	time 0.4556 (0.4787)	loss 5.8512 (5.9219)	grad_norm 2.5034 (2.3052)	mem 14849MB
[2022-11-05 20:17:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][200/1251]	eta 0:08:19 lr 0.000209	time 0.4593 (0.4757)	loss 6.1504 (5.9205)	grad_norm 2.6941 (2.3612)	mem 14849MB
[2022-11-05 20:17:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][250/1251]	eta 0:07:54 lr 0.000211	time 0.4646 (0.4739)	loss 6.4939 (5.9317)	grad_norm 2.0445 (2.3419)	mem 14849MB
[2022-11-05 20:18:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][300/1251]	eta 0:07:29 lr 0.000213	time 0.4585 (0.4726)	loss 5.8740 (5.9389)	grad_norm 2.2776 (2.3640)	mem 14849MB
[2022-11-05 20:18:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][350/1251]	eta 0:07:04 lr 0.000215	time 0.4662 (0.4714)	loss 6.3379 (5.9303)	grad_norm 2.2775 (2.3661)	mem 14849MB
[2022-11-05 20:19:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][400/1251]	eta 0:06:40 lr 0.000217	time 0.4578 (0.4710)	loss 6.3386 (5.9251)	grad_norm 2.1379 (2.3612)	mem 14849MB
[2022-11-05 20:19:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][450/1251]	eta 0:06:16 lr 0.000219	time 0.4685 (0.4705)	loss 5.7521 (5.9222)	grad_norm 3.1290 (2.3660)	mem 14849MB
[2022-11-05 20:19:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][500/1251]	eta 0:05:52 lr 0.000221	time 0.4711 (0.4699)	loss 6.0630 (5.9191)	grad_norm 2.0257 (2.3589)	mem 14849MB
[2022-11-05 20:20:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][550/1251]	eta 0:05:29 lr 0.000223	time 0.4629 (0.4698)	loss 5.9810 (5.9076)	grad_norm 2.2765 (2.3493)	mem 14849MB
[2022-11-05 20:20:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][600/1251]	eta 0:05:05 lr 0.000225	time 0.4625 (0.4694)	loss 5.3670 (5.9004)	grad_norm 3.7934 (2.3509)	mem 14849MB
[2022-11-05 20:21:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][650/1251]	eta 0:04:42 lr 0.000227	time 0.4633 (0.4693)	loss 5.7638 (5.8969)	grad_norm 2.7341 (2.3532)	mem 14849MB
[2022-11-05 20:21:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][700/1251]	eta 0:04:18 lr 0.000229	time 0.4653 (0.4691)	loss 5.5418 (5.8944)	grad_norm 2.2927 (2.3510)	mem 14849MB
[2022-11-05 20:21:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][750/1251]	eta 0:03:55 lr 0.000231	time 0.4700 (0.4691)	loss 5.7759 (5.8847)	grad_norm 1.7573 (2.3495)	mem 14849MB
[2022-11-05 20:22:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][800/1251]	eta 0:03:31 lr 0.000233	time 0.4583 (0.4690)	loss 6.3180 (5.8800)	grad_norm 2.6164 (2.3572)	mem 14849MB
[2022-11-05 20:22:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][850/1251]	eta 0:03:07 lr 0.000235	time 0.4658 (0.4688)	loss 6.0396 (5.8779)	grad_norm 2.1008 (2.3521)	mem 14849MB
[2022-11-05 20:23:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][900/1251]	eta 0:02:44 lr 0.000237	time 0.4758 (0.4687)	loss 6.2659 (5.8792)	grad_norm 1.8467 (2.3455)	mem 14849MB
[2022-11-05 20:23:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][950/1251]	eta 0:02:21 lr 0.000239	time 0.4620 (0.4686)	loss 5.3288 (5.8782)	grad_norm 2.8077 (2.3466)	mem 14849MB
[2022-11-05 20:23:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][1000/1251]	eta 0:01:57 lr 0.000241	time 0.4625 (0.4686)	loss 6.0127 (5.8755)	grad_norm 2.4688 (2.3470)	mem 14849MB
[2022-11-05 20:24:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][1050/1251]	eta 0:01:34 lr 0.000243	time 0.4617 (0.4686)	loss 5.7555 (5.8686)	grad_norm 2.6139 (inf)	mem 14849MB
[2022-11-05 20:24:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][1100/1251]	eta 0:01:10 lr 0.000245	time 0.4705 (0.4686)	loss 5.3040 (5.8666)	grad_norm 2.2790 (inf)	mem 14849MB
[2022-11-05 20:24:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][1150/1251]	eta 0:00:47 lr 0.000247	time 0.4634 (0.4684)	loss 6.0062 (5.8588)	grad_norm 2.2720 (inf)	mem 14849MB
[2022-11-05 20:25:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][1200/1251]	eta 0:00:23 lr 0.000249	time 0.4663 (0.4683)	loss 5.4217 (5.8585)	grad_norm 2.1275 (inf)	mem 14849MB
[2022-11-05 20:25:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [4/300][1250/1251]	eta 0:00:00 lr 0.000251	time 0.4572 (0.4682)	loss 5.3133 (5.8541)	grad_norm 2.5192 (inf)	mem 14849MB
[2022-11-05 20:25:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 4 training takes 0:09:45
[2022-11-05 20:25:43 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_4.pth saving......
[2022-11-05 20:25:44 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_4.pth saved !!!
[2022-11-05 20:25:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.551 (1.551)	Loss 3.7733 (3.7733)	Acc@1 25.293 (25.293)	Acc@5 49.414 (49.414)	Mem 14849MB
[2022-11-05 20:25:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 27.262 Acc@5 51.146
[2022-11-05 20:25:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 27.3%
[2022-11-05 20:25:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.705 (1.705)	Loss 6.9212 (6.9212)	Acc@1 0.098 (0.098)	Acc@5 0.684 (0.684)	Mem 14849MB
[2022-11-05 20:26:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.248 Acc@5 1.070
[2022-11-05 20:26:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.2%
[2022-11-05 20:26:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 27.26% at 4 epoch
[2022-11-05 20:26:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][0/1251]	eta 0:39:10 lr 0.000251	time 1.8792 (1.8792)	loss 5.8928 (5.8928)	grad_norm 2.8919 (2.8919)	mem 14849MB
[2022-11-05 20:26:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][50/1251]	eta 0:09:56 lr 0.000253	time 0.4780 (0.4968)	loss 6.0162 (5.8928)	grad_norm 1.9217 (2.4800)	mem 14849MB
[2022-11-05 20:26:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][100/1251]	eta 0:09:17 lr 0.000255	time 0.4634 (0.4844)	loss 5.4515 (5.8209)	grad_norm 2.0936 (2.3876)	mem 14849MB
[2022-11-05 20:27:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][150/1251]	eta 0:08:46 lr 0.000257	time 0.4645 (0.4780)	loss 6.1764 (5.8040)	grad_norm 2.1098 (2.3834)	mem 14849MB
[2022-11-05 20:27:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][200/1251]	eta 0:08:19 lr 0.000259	time 0.4700 (0.4753)	loss 4.8375 (5.7633)	grad_norm 2.3250 (2.4017)	mem 14849MB
[2022-11-05 20:28:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][250/1251]	eta 0:07:53 lr 0.000261	time 0.4728 (0.4733)	loss 5.9315 (5.7316)	grad_norm 2.4163 (2.3942)	mem 14849MB
[2022-11-05 20:28:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][300/1251]	eta 0:07:29 lr 0.000263	time 0.4625 (0.4725)	loss 5.3797 (5.7333)	grad_norm 2.3801 (2.4024)	mem 14849MB
[2022-11-05 20:28:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][350/1251]	eta 0:07:05 lr 0.000265	time 0.4591 (0.4718)	loss 5.1268 (5.7233)	grad_norm 2.1101 (2.3659)	mem 14849MB
[2022-11-05 20:29:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][400/1251]	eta 0:06:40 lr 0.000267	time 0.4638 (0.4711)	loss 5.7905 (5.7182)	grad_norm 2.2582 (2.3562)	mem 14849MB
[2022-11-05 20:29:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][450/1251]	eta 0:06:16 lr 0.000269	time 0.4614 (0.4705)	loss 6.1282 (5.7127)	grad_norm 2.7141 (2.3487)	mem 14849MB
[2022-11-05 20:29:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][500/1251]	eta 0:05:53 lr 0.000271	time 0.4507 (0.4700)	loss 6.2553 (5.7119)	grad_norm 1.9995 (2.3537)	mem 14849MB
[2022-11-05 20:30:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][550/1251]	eta 0:05:29 lr 0.000273	time 0.4664 (0.4696)	loss 4.8441 (5.7072)	grad_norm 2.0929 (2.3393)	mem 14849MB
[2022-11-05 20:30:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][600/1251]	eta 0:05:05 lr 0.000275	time 0.4664 (0.4695)	loss 5.3930 (5.6957)	grad_norm 1.7573 (2.3349)	mem 14849MB
[2022-11-05 20:31:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][650/1251]	eta 0:04:41 lr 0.000277	time 0.4703 (0.4692)	loss 4.6491 (5.6876)	grad_norm 1.8234 (2.3252)	mem 14849MB
[2022-11-05 20:31:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][700/1251]	eta 0:04:18 lr 0.000279	time 0.4686 (0.4688)	loss 5.5940 (5.6784)	grad_norm 2.1601 (2.3337)	mem 14849MB
[2022-11-05 20:31:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][750/1251]	eta 0:03:54 lr 0.000281	time 0.4654 (0.4685)	loss 5.2879 (5.6679)	grad_norm 2.1268 (2.3356)	mem 14849MB
[2022-11-05 20:32:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][800/1251]	eta 0:03:31 lr 0.000283	time 0.4612 (0.4685)	loss 5.8232 (5.6627)	grad_norm 2.0654 (2.3262)	mem 14849MB
[2022-11-05 20:32:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][850/1251]	eta 0:03:07 lr 0.000285	time 0.4634 (0.4685)	loss 5.1713 (5.6596)	grad_norm 2.1385 (2.3266)	mem 14849MB
[2022-11-05 20:33:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][900/1251]	eta 0:02:44 lr 0.000287	time 0.4639 (0.4685)	loss 6.0977 (5.6589)	grad_norm 1.9646 (2.3248)	mem 14849MB
[2022-11-05 20:33:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][950/1251]	eta 0:02:20 lr 0.000289	time 0.4643 (0.4684)	loss 4.9968 (5.6581)	grad_norm 2.6794 (2.3312)	mem 14849MB
[2022-11-05 20:33:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][1000/1251]	eta 0:01:57 lr 0.000291	time 0.4615 (0.4682)	loss 5.7248 (5.6544)	grad_norm 1.7228 (2.3283)	mem 14849MB
[2022-11-05 20:34:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][1050/1251]	eta 0:01:34 lr 0.000293	time 0.4671 (0.4682)	loss 5.4092 (5.6508)	grad_norm 2.1998 (2.3200)	mem 14849MB
[2022-11-05 20:34:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][1100/1251]	eta 0:01:10 lr 0.000295	time 0.4696 (0.4682)	loss 5.9332 (5.6474)	grad_norm 1.9645 (2.3185)	mem 14849MB
[2022-11-05 20:35:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][1150/1251]	eta 0:00:47 lr 0.000297	time 0.4563 (0.4681)	loss 5.2637 (5.6420)	grad_norm 3.0555 (2.3134)	mem 14849MB
[2022-11-05 20:35:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][1200/1251]	eta 0:00:23 lr 0.000299	time 0.4723 (0.4681)	loss 6.0510 (5.6366)	grad_norm 2.1034 (2.3109)	mem 14849MB
[2022-11-05 20:35:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [5/300][1250/1251]	eta 0:00:00 lr 0.000301	time 0.4558 (0.4679)	loss 5.5084 (5.6330)	grad_norm 1.9413 (2.3095)	mem 14849MB
[2022-11-05 20:35:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 5 training takes 0:09:45
[2022-11-05 20:35:47 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_5.pth saving......
[2022-11-05 20:35:48 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_5.pth saved !!!
[2022-11-05 20:35:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.582 (1.582)	Loss 3.3646 (3.3646)	Acc@1 32.812 (32.812)	Acc@5 58.398 (58.398)	Mem 14849MB
[2022-11-05 20:35:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 32.850 Acc@5 58.228
[2022-11-05 20:35:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 32.8%
[2022-11-05 20:35:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.723 (1.723)	Loss 6.8873 (6.8873)	Acc@1 0.195 (0.195)	Acc@5 0.977 (0.977)	Mem 14849MB
[2022-11-05 20:36:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.308 Acc@5 1.268
[2022-11-05 20:36:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.3%
[2022-11-05 20:36:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 32.85% at 5 epoch
[2022-11-05 20:36:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][0/1251]	eta 0:42:16 lr 0.000301	time 2.0274 (2.0274)	loss 4.8238 (4.8238)	grad_norm 2.4649 (2.4649)	mem 14849MB
[2022-11-05 20:36:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][50/1251]	eta 0:10:02 lr 0.000303	time 0.4701 (0.5018)	loss 4.4042 (5.4456)	grad_norm 2.1847 (2.2870)	mem 14849MB
[2022-11-05 20:36:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][100/1251]	eta 0:09:18 lr 0.000305	time 0.4603 (0.4855)	loss 5.9512 (5.4755)	grad_norm 2.8132 (2.2603)	mem 14849MB
[2022-11-05 20:37:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][150/1251]	eta 0:08:47 lr 0.000307	time 0.4623 (0.4787)	loss 5.9138 (5.4913)	grad_norm 2.3498 (2.2892)	mem 14849MB
[2022-11-05 20:37:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][200/1251]	eta 0:08:20 lr 0.000309	time 0.4674 (0.4758)	loss 5.3217 (5.5119)	grad_norm 2.6026 (2.2694)	mem 14849MB
[2022-11-05 20:38:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][250/1251]	eta 0:07:54 lr 0.000311	time 0.4700 (0.4738)	loss 5.6569 (5.4888)	grad_norm 2.4147 (2.2789)	mem 14849MB
[2022-11-05 20:38:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][300/1251]	eta 0:07:29 lr 0.000313	time 0.4652 (0.4722)	loss 5.7256 (5.4853)	grad_norm 2.0760 (2.2476)	mem 14849MB
[2022-11-05 20:38:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][350/1251]	eta 0:07:04 lr 0.000315	time 0.4512 (0.4712)	loss 5.7739 (5.4794)	grad_norm 2.1674 (2.2400)	mem 14849MB
[2022-11-05 20:39:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][400/1251]	eta 0:06:40 lr 0.000317	time 0.4725 (0.4706)	loss 6.0220 (5.4947)	grad_norm 2.0074 (2.2663)	mem 14849MB
[2022-11-05 20:39:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][450/1251]	eta 0:06:16 lr 0.000319	time 0.4553 (0.4699)	loss 5.7607 (5.4832)	grad_norm 1.9062 (2.2571)	mem 14849MB
[2022-11-05 20:40:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][500/1251]	eta 0:05:52 lr 0.000321	time 0.4601 (0.4698)	loss 5.7386 (5.4776)	grad_norm 2.0315 (2.2534)	mem 14849MB
[2022-11-05 20:40:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][550/1251]	eta 0:05:29 lr 0.000323	time 0.4597 (0.4697)	loss 4.4024 (5.4633)	grad_norm 2.2002 (2.2624)	mem 14849MB
[2022-11-05 20:40:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][600/1251]	eta 0:05:05 lr 0.000325	time 0.4548 (0.4693)	loss 5.8168 (5.4698)	grad_norm 1.7530 (2.2490)	mem 14849MB
[2022-11-05 20:41:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][650/1251]	eta 0:04:41 lr 0.000327	time 0.4636 (0.4688)	loss 5.2934 (5.4654)	grad_norm 1.6959 (inf)	mem 14849MB
[2022-11-05 20:41:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][700/1251]	eta 0:04:18 lr 0.000329	time 0.4610 (0.4688)	loss 5.8426 (5.4628)	grad_norm 1.8994 (inf)	mem 14849MB
[2022-11-05 20:41:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][750/1251]	eta 0:03:54 lr 0.000331	time 0.4728 (0.4686)	loss 5.8954 (5.4552)	grad_norm 1.7924 (inf)	mem 14849MB
[2022-11-05 20:42:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][800/1251]	eta 0:03:31 lr 0.000333	time 0.4604 (0.4688)	loss 4.6770 (5.4543)	grad_norm 1.9117 (inf)	mem 14849MB
[2022-11-05 20:42:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][850/1251]	eta 0:03:07 lr 0.000335	time 0.4572 (0.4686)	loss 5.9118 (5.4502)	grad_norm 1.7619 (inf)	mem 14849MB
[2022-11-05 20:43:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][900/1251]	eta 0:02:44 lr 0.000337	time 0.4623 (0.4684)	loss 5.1848 (5.4519)	grad_norm 2.1397 (inf)	mem 14849MB
[2022-11-05 20:43:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][950/1251]	eta 0:02:20 lr 0.000339	time 0.4684 (0.4682)	loss 5.5989 (5.4508)	grad_norm 2.0575 (inf)	mem 14849MB
[2022-11-05 20:43:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][1000/1251]	eta 0:01:57 lr 0.000341	time 0.4691 (0.4682)	loss 6.1144 (5.4489)	grad_norm 1.6930 (inf)	mem 14849MB
[2022-11-05 20:44:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][1050/1251]	eta 0:01:34 lr 0.000343	time 0.4537 (0.4683)	loss 5.4125 (5.4491)	grad_norm 2.7763 (inf)	mem 14849MB
[2022-11-05 20:44:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][1100/1251]	eta 0:01:10 lr 0.000345	time 0.4640 (0.4682)	loss 5.5457 (5.4472)	grad_norm 1.8935 (inf)	mem 14849MB
[2022-11-05 20:45:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][1150/1251]	eta 0:00:47 lr 0.000347	time 0.4636 (0.4680)	loss 5.1330 (5.4438)	grad_norm 1.5055 (inf)	mem 14849MB
[2022-11-05 20:45:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][1200/1251]	eta 0:00:23 lr 0.000349	time 0.5394 (0.4680)	loss 4.9296 (5.4371)	grad_norm 2.2142 (inf)	mem 14849MB
[2022-11-05 20:45:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [6/300][1250/1251]	eta 0:00:00 lr 0.000351	time 0.4577 (0.4679)	loss 5.4093 (5.4374)	grad_norm 1.7253 (inf)	mem 14849MB
[2022-11-05 20:45:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 6 training takes 0:09:45
[2022-11-05 20:45:51 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_6.pth saving......
[2022-11-05 20:45:51 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_6.pth saved !!!
[2022-11-05 20:45:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.696 (1.696)	Loss 3.0422 (3.0422)	Acc@1 38.477 (38.477)	Acc@5 62.598 (62.598)	Mem 14849MB
[2022-11-05 20:46:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 38.646 Acc@5 64.306
[2022-11-05 20:46:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 38.6%
[2022-11-05 20:46:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.555 (1.555)	Loss 6.8214 (6.8214)	Acc@1 0.195 (0.195)	Acc@5 1.953 (1.953)	Mem 14849MB
[2022-11-05 20:46:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.448 Acc@5 1.636
[2022-11-05 20:46:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.4%
[2022-11-05 20:46:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 38.65% at 6 epoch
[2022-11-05 20:46:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][0/1251]	eta 0:41:57 lr 0.000351	time 2.0127 (2.0127)	loss 5.7356 (5.7356)	grad_norm 1.6676 (1.6676)	mem 14849MB
[2022-11-05 20:46:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][50/1251]	eta 0:10:04 lr 0.000353	time 0.4605 (0.5036)	loss 5.2004 (5.3294)	grad_norm 2.5670 (2.2978)	mem 14849MB
[2022-11-05 20:46:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][100/1251]	eta 0:09:17 lr 0.000355	time 0.4782 (0.4845)	loss 4.9429 (5.3468)	grad_norm 1.8414 (2.1986)	mem 14849MB
[2022-11-05 20:47:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][150/1251]	eta 0:08:46 lr 0.000357	time 0.4642 (0.4778)	loss 5.7337 (5.3231)	grad_norm 1.8911 (2.1374)	mem 14849MB
[2022-11-05 20:47:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][200/1251]	eta 0:08:20 lr 0.000359	time 0.4557 (0.4758)	loss 5.6982 (5.3314)	grad_norm 1.6933 (2.1210)	mem 14849MB
[2022-11-05 20:48:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][250/1251]	eta 0:07:54 lr 0.000361	time 0.4757 (0.4742)	loss 4.4402 (5.3171)	grad_norm 1.7614 (2.1258)	mem 14849MB
[2022-11-05 20:48:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][300/1251]	eta 0:07:29 lr 0.000363	time 0.4684 (0.4728)	loss 5.0066 (5.3046)	grad_norm 1.8797 (2.1179)	mem 14849MB
[2022-11-05 20:48:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][350/1251]	eta 0:07:04 lr 0.000365	time 0.4615 (0.4716)	loss 5.6471 (5.3041)	grad_norm 1.6779 (2.1383)	mem 14849MB
[2022-11-05 20:49:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][400/1251]	eta 0:06:40 lr 0.000367	time 0.4616 (0.4708)	loss 5.1975 (5.2939)	grad_norm 2.8546 (2.1492)	mem 14849MB
[2022-11-05 20:49:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][450/1251]	eta 0:06:16 lr 0.000369	time 0.4697 (0.4703)	loss 5.6989 (5.2857)	grad_norm 2.9703 (2.1380)	mem 14849MB
[2022-11-05 20:50:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][500/1251]	eta 0:05:52 lr 0.000371	time 0.4709 (0.4698)	loss 4.9406 (5.2834)	grad_norm 1.8969 (2.1310)	mem 14849MB
[2022-11-05 20:50:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][550/1251]	eta 0:05:29 lr 0.000373	time 0.4607 (0.4700)	loss 5.2119 (5.2833)	grad_norm 2.2016 (2.1286)	mem 14849MB
[2022-11-05 20:50:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][600/1251]	eta 0:05:05 lr 0.000375	time 0.4562 (0.4696)	loss 4.9458 (5.2734)	grad_norm 2.1367 (2.1227)	mem 14849MB
[2022-11-05 20:51:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][650/1251]	eta 0:04:42 lr 0.000377	time 0.4622 (0.4693)	loss 5.8464 (5.2784)	grad_norm 4.2362 (2.1260)	mem 14849MB
[2022-11-05 20:51:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][700/1251]	eta 0:04:18 lr 0.000379	time 0.4522 (0.4691)	loss 5.3695 (5.2847)	grad_norm 1.7767 (2.1319)	mem 14849MB
[2022-11-05 20:52:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][750/1251]	eta 0:03:54 lr 0.000381	time 0.4711 (0.4688)	loss 4.8230 (5.2814)	grad_norm 1.9078 (2.1228)	mem 14849MB
[2022-11-05 20:52:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][800/1251]	eta 0:03:31 lr 0.000383	time 0.4644 (0.4689)	loss 5.9428 (5.2804)	grad_norm 2.6061 (2.1154)	mem 14849MB
[2022-11-05 20:52:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][850/1251]	eta 0:03:07 lr 0.000385	time 0.4639 (0.4687)	loss 5.3753 (5.2728)	grad_norm 1.9273 (2.1133)	mem 14849MB
[2022-11-05 20:53:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][900/1251]	eta 0:02:44 lr 0.000387	time 0.4526 (0.4686)	loss 4.5974 (5.2697)	grad_norm 1.8338 (2.1182)	mem 14849MB
[2022-11-05 20:53:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][950/1251]	eta 0:02:21 lr 0.000389	time 0.4739 (0.4684)	loss 5.3268 (5.2768)	grad_norm 1.9512 (2.1110)	mem 14849MB
[2022-11-05 20:53:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][1000/1251]	eta 0:01:57 lr 0.000391	time 0.4741 (0.4683)	loss 5.8241 (5.2734)	grad_norm 1.6316 (2.1090)	mem 14849MB
[2022-11-05 20:54:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][1050/1251]	eta 0:01:34 lr 0.000393	time 0.4756 (0.4683)	loss 4.9733 (5.2705)	grad_norm 2.0937 (2.1036)	mem 14849MB
[2022-11-05 20:54:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][1100/1251]	eta 0:01:10 lr 0.000395	time 0.4644 (0.4681)	loss 5.2270 (5.2640)	grad_norm 1.7582 (2.1026)	mem 14849MB
[2022-11-05 20:55:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][1150/1251]	eta 0:00:47 lr 0.000397	time 0.4797 (0.4681)	loss 4.8885 (5.2597)	grad_norm 2.2648 (2.1032)	mem 14849MB
[2022-11-05 20:55:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][1200/1251]	eta 0:00:23 lr 0.000399	time 0.4738 (0.4681)	loss 4.1644 (5.2566)	grad_norm 2.2539 (2.1032)	mem 14849MB
[2022-11-05 20:55:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [7/300][1250/1251]	eta 0:00:00 lr 0.000401	time 0.4575 (0.4679)	loss 5.1909 (5.2512)	grad_norm 1.7995 (2.0986)	mem 14849MB
[2022-11-05 20:55:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 7 training takes 0:09:45
[2022-11-05 20:55:54 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_7.pth saving......
[2022-11-05 20:55:55 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_7.pth saved !!!
[2022-11-05 20:55:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.608 (1.608)	Loss 2.7206 (2.7206)	Acc@1 42.383 (42.383)	Acc@5 68.457 (68.457)	Mem 14849MB
[2022-11-05 20:56:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 43.972 Acc@5 69.376
[2022-11-05 20:56:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 44.0%
[2022-11-05 20:56:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.600 (1.600)	Loss 6.7600 (6.7600)	Acc@1 0.488 (0.488)	Acc@5 1.855 (1.855)	Mem 14849MB
[2022-11-05 20:56:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.604 Acc@5 2.310
[2022-11-05 20:56:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.6%
[2022-11-05 20:56:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 43.97% at 7 epoch
[2022-11-05 20:56:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][0/1251]	eta 0:39:40 lr 0.000401	time 1.9031 (1.9031)	loss 5.5931 (5.5931)	grad_norm 2.0588 (2.0588)	mem 14849MB
[2022-11-05 20:56:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][50/1251]	eta 0:09:58 lr 0.000403	time 0.4642 (0.4984)	loss 5.8742 (5.3296)	grad_norm 1.9711 (1.9818)	mem 14849MB
[2022-11-05 20:57:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][100/1251]	eta 0:09:18 lr 0.000405	time 0.4625 (0.4854)	loss 4.5776 (5.2706)	grad_norm 1.8853 (2.0520)	mem 14849MB
[2022-11-05 20:57:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][150/1251]	eta 0:08:47 lr 0.000407	time 0.4623 (0.4796)	loss 4.7664 (5.2311)	grad_norm 1.7502 (2.0245)	mem 14849MB
[2022-11-05 20:57:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][200/1251]	eta 0:08:20 lr 0.000409	time 0.4624 (0.4767)	loss 4.4362 (5.2370)	grad_norm 2.4241 (2.0448)	mem 14849MB
[2022-11-05 20:58:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][250/1251]	eta 0:07:55 lr 0.000411	time 0.4573 (0.4746)	loss 5.5169 (5.2305)	grad_norm 1.8842 (2.0507)	mem 14849MB
[2022-11-05 20:58:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][300/1251]	eta 0:07:29 lr 0.000413	time 0.4629 (0.4729)	loss 4.4997 (5.2268)	grad_norm 2.1165 (2.0326)	mem 14849MB
[2022-11-05 20:58:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][350/1251]	eta 0:07:05 lr 0.000415	time 0.4765 (0.4719)	loss 5.2351 (5.2100)	grad_norm 1.6081 (2.0328)	mem 14849MB
[2022-11-05 20:59:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][400/1251]	eta 0:06:41 lr 0.000417	time 0.4665 (0.4712)	loss 5.5780 (5.1857)	grad_norm 1.8758 (inf)	mem 14849MB
[2022-11-05 20:59:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][450/1251]	eta 0:06:17 lr 0.000419	time 0.4712 (0.4708)	loss 4.9759 (5.1761)	grad_norm 1.8748 (inf)	mem 14849MB
[2022-11-05 21:00:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][500/1251]	eta 0:05:53 lr 0.000421	time 0.4610 (0.4702)	loss 5.3276 (5.1759)	grad_norm 2.1136 (inf)	mem 14849MB
[2022-11-05 21:00:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][550/1251]	eta 0:05:29 lr 0.000423	time 0.4730 (0.4701)	loss 4.0060 (5.1677)	grad_norm 1.8817 (inf)	mem 14849MB
[2022-11-05 21:00:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][600/1251]	eta 0:05:05 lr 0.000425	time 0.4674 (0.4699)	loss 5.5431 (5.1523)	grad_norm 1.6827 (inf)	mem 14849MB
[2022-11-05 21:01:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][650/1251]	eta 0:04:42 lr 0.000427	time 0.4635 (0.4694)	loss 5.0804 (5.1472)	grad_norm 2.0093 (inf)	mem 14849MB
[2022-11-05 21:01:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][700/1251]	eta 0:04:18 lr 0.000429	time 0.4570 (0.4694)	loss 4.3730 (5.1494)	grad_norm 2.5101 (inf)	mem 14849MB
[2022-11-05 21:02:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][750/1251]	eta 0:03:55 lr 0.000431	time 0.4733 (0.4692)	loss 4.4557 (5.1451)	grad_norm 1.7325 (inf)	mem 14849MB
[2022-11-05 21:02:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][800/1251]	eta 0:03:31 lr 0.000433	time 0.5343 (0.4693)	loss 5.4831 (5.1375)	grad_norm 1.7718 (inf)	mem 14849MB
[2022-11-05 21:02:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][850/1251]	eta 0:03:08 lr 0.000435	time 0.4535 (0.4691)	loss 5.2179 (5.1366)	grad_norm 1.7721 (inf)	mem 14849MB
[2022-11-05 21:03:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][900/1251]	eta 0:02:44 lr 0.000437	time 0.4725 (0.4689)	loss 4.9720 (5.1336)	grad_norm 1.7838 (inf)	mem 14849MB
[2022-11-05 21:03:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][950/1251]	eta 0:02:21 lr 0.000439	time 0.4604 (0.4687)	loss 4.0706 (5.1271)	grad_norm 1.6215 (inf)	mem 14849MB
[2022-11-05 21:04:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][1000/1251]	eta 0:01:57 lr 0.000441	time 0.4742 (0.4686)	loss 5.4409 (5.1292)	grad_norm 2.5045 (inf)	mem 14849MB
[2022-11-05 21:04:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][1050/1251]	eta 0:01:34 lr 0.000443	time 0.4704 (0.4687)	loss 5.3264 (5.1284)	grad_norm 2.7095 (inf)	mem 14849MB
[2022-11-05 21:04:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][1100/1251]	eta 0:01:10 lr 0.000445	time 0.4678 (0.4687)	loss 5.4728 (5.1266)	grad_norm 1.7973 (inf)	mem 14849MB
[2022-11-05 21:05:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][1150/1251]	eta 0:00:47 lr 0.000447	time 0.4541 (0.4685)	loss 5.1013 (5.1266)	grad_norm 1.9112 (inf)	mem 14849MB
[2022-11-05 21:05:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][1200/1251]	eta 0:00:23 lr 0.000449	time 0.4662 (0.4685)	loss 4.5575 (5.1207)	grad_norm 1.9850 (inf)	mem 14849MB
[2022-11-05 21:05:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [8/300][1250/1251]	eta 0:00:00 lr 0.000451	time 0.4561 (0.4683)	loss 5.5124 (5.1171)	grad_norm 2.7872 (inf)	mem 14849MB
[2022-11-05 21:05:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 8 training takes 0:09:46
[2022-11-05 21:05:58 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_8.pth saving......
[2022-11-05 21:05:59 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_8.pth saved !!!
[2022-11-05 21:06:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.660 (1.660)	Loss 2.4684 (2.4684)	Acc@1 47.070 (47.070)	Acc@5 72.168 (72.168)	Mem 14849MB
[2022-11-05 21:06:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 47.742 Acc@5 73.318
[2022-11-05 21:06:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 47.7%
[2022-11-05 21:06:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.600 (1.600)	Loss 6.6493 (6.6493)	Acc@1 0.879 (0.879)	Acc@5 2.930 (2.930)	Mem 14849MB
[2022-11-05 21:06:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.836 Acc@5 2.990
[2022-11-05 21:06:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.8%
[2022-11-05 21:06:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 47.74% at 8 epoch
[2022-11-05 21:06:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][0/1251]	eta 0:41:36 lr 0.000451	time 1.9956 (1.9956)	loss 4.3238 (4.3238)	grad_norm 1.6171 (1.6171)	mem 14849MB
[2022-11-05 21:06:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][50/1251]	eta 0:10:04 lr 0.000453	time 0.4656 (0.5034)	loss 4.0581 (5.0281)	grad_norm 1.8666 (2.0075)	mem 14849MB
[2022-11-05 21:07:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][100/1251]	eta 0:09:18 lr 0.000455	time 0.4733 (0.4853)	loss 5.3588 (5.0742)	grad_norm 1.6374 (1.9214)	mem 14849MB
[2022-11-05 21:07:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][150/1251]	eta 0:08:48 lr 0.000457	time 0.4637 (0.4799)	loss 3.9488 (5.0405)	grad_norm 1.7671 (1.9139)	mem 14849MB
[2022-11-05 21:07:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][200/1251]	eta 0:08:20 lr 0.000459	time 0.4633 (0.4761)	loss 5.1123 (5.0546)	grad_norm 1.8033 (1.9510)	mem 14849MB
[2022-11-05 21:08:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][250/1251]	eta 0:07:54 lr 0.000461	time 0.4561 (0.4738)	loss 5.0182 (5.0866)	grad_norm 1.8653 (1.9510)	mem 14849MB
[2022-11-05 21:08:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][300/1251]	eta 0:07:29 lr 0.000463	time 0.4663 (0.4727)	loss 4.3128 (5.0718)	grad_norm 1.8186 (1.9373)	mem 14849MB
[2022-11-05 21:09:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][350/1251]	eta 0:07:04 lr 0.000465	time 0.4789 (0.4716)	loss 5.3510 (5.0560)	grad_norm 2.0182 (1.9322)	mem 14849MB
[2022-11-05 21:09:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][400/1251]	eta 0:06:40 lr 0.000467	time 0.4684 (0.4707)	loss 4.3084 (5.0379)	grad_norm 1.7916 (1.9401)	mem 14849MB
[2022-11-05 21:09:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][450/1251]	eta 0:06:16 lr 0.000469	time 0.4685 (0.4702)	loss 5.0135 (5.0369)	grad_norm 1.8403 (1.9365)	mem 14849MB
[2022-11-05 21:10:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][500/1251]	eta 0:05:52 lr 0.000471	time 0.4706 (0.4697)	loss 5.4843 (5.0167)	grad_norm 1.7730 (1.9293)	mem 14849MB
[2022-11-05 21:10:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][550/1251]	eta 0:05:29 lr 0.000473	time 0.4596 (0.4699)	loss 5.5780 (5.0160)	grad_norm 2.1937 (1.9287)	mem 14849MB
[2022-11-05 21:10:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][600/1251]	eta 0:05:05 lr 0.000475	time 0.4593 (0.4698)	loss 4.3710 (5.0106)	grad_norm 1.6094 (1.9183)	mem 14849MB
[2022-11-05 21:11:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][650/1251]	eta 0:04:42 lr 0.000477	time 0.4578 (0.4696)	loss 4.7663 (5.0113)	grad_norm 1.8184 (1.9182)	mem 14849MB
[2022-11-05 21:11:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][700/1251]	eta 0:04:18 lr 0.000478	time 0.4675 (0.4693)	loss 5.5175 (5.0071)	grad_norm 1.5728 (1.9133)	mem 14849MB
[2022-11-05 21:12:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][750/1251]	eta 0:03:55 lr 0.000480	time 0.4610 (0.4691)	loss 4.9837 (5.0055)	grad_norm 1.8365 (1.9114)	mem 14849MB
[2022-11-05 21:12:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][800/1251]	eta 0:03:31 lr 0.000482	time 0.4719 (0.4691)	loss 4.0877 (5.0045)	grad_norm 1.9761 (1.9089)	mem 14849MB
[2022-11-05 21:12:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][850/1251]	eta 0:03:08 lr 0.000484	time 0.4659 (0.4689)	loss 5.2128 (5.0003)	grad_norm 1.8259 (1.9079)	mem 14849MB
[2022-11-05 21:13:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][900/1251]	eta 0:02:44 lr 0.000486	time 0.4628 (0.4689)	loss 5.6985 (4.9919)	grad_norm 1.9413 (1.9135)	mem 14849MB
[2022-11-05 21:13:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][950/1251]	eta 0:02:21 lr 0.000488	time 0.4619 (0.4687)	loss 5.6031 (4.9824)	grad_norm 1.8617 (1.9104)	mem 14849MB
[2022-11-05 21:14:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][1000/1251]	eta 0:01:57 lr 0.000490	time 0.4519 (0.4684)	loss 5.4948 (4.9793)	grad_norm 1.8077 (1.9040)	mem 14849MB
[2022-11-05 21:14:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][1050/1251]	eta 0:01:34 lr 0.000492	time 0.4599 (0.4684)	loss 4.8487 (4.9769)	grad_norm 1.8355 (1.8983)	mem 14849MB
[2022-11-05 21:14:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][1100/1251]	eta 0:01:10 lr 0.000494	time 0.4540 (0.4684)	loss 4.6769 (4.9750)	grad_norm 1.5990 (1.8936)	mem 14849MB
[2022-11-05 21:15:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][1150/1251]	eta 0:00:47 lr 0.000496	time 0.4624 (0.4684)	loss 5.6104 (4.9685)	grad_norm 2.0995 (1.8930)	mem 14849MB
[2022-11-05 21:15:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][1200/1251]	eta 0:00:23 lr 0.000498	time 0.4755 (0.4683)	loss 4.0554 (4.9599)	grad_norm 1.5592 (1.8893)	mem 14849MB
[2022-11-05 21:16:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [9/300][1250/1251]	eta 0:00:00 lr 0.000500	time 0.4572 (0.4681)	loss 5.0769 (4.9608)	grad_norm 1.7504 (1.8842)	mem 14849MB
[2022-11-05 21:16:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 9 training takes 0:09:45
[2022-11-05 21:16:02 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_9.pth saving......
[2022-11-05 21:16:02 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_9.pth saved !!!
[2022-11-05 21:16:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.517 (1.517)	Loss 2.3516 (2.3516)	Acc@1 50.098 (50.098)	Acc@5 73.438 (73.438)	Mem 14849MB
[2022-11-05 21:16:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 50.856 Acc@5 75.822
[2022-11-05 21:16:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 50.9%
[2022-11-05 21:16:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.638 (1.638)	Loss 6.5205 (6.5205)	Acc@1 1.367 (1.367)	Acc@5 4.102 (4.102)	Mem 14849MB
[2022-11-05 21:16:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 1.084 Acc@5 3.960
[2022-11-05 21:16:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 1.1%
[2022-11-05 21:16:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 50.86% at 9 epoch
[2022-11-05 21:16:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][0/1251]	eta 0:43:13 lr 0.000501	time 2.0732 (2.0732)	loss 5.2428 (5.2428)	grad_norm 2.3639 (2.3639)	mem 14849MB
[2022-11-05 21:16:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][50/1251]	eta 0:10:03 lr 0.000502	time 0.4679 (0.5028)	loss 4.1785 (4.9614)	grad_norm 2.1937 (1.8104)	mem 14849MB
[2022-11-05 21:17:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][100/1251]	eta 0:09:20 lr 0.000504	time 0.5439 (0.4870)	loss 4.8184 (4.9401)	grad_norm 1.6748 (1.8200)	mem 14849MB
[2022-11-05 21:17:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][150/1251]	eta 0:08:48 lr 0.000506	time 0.4657 (0.4803)	loss 5.4085 (4.9450)	grad_norm 1.6660 (inf)	mem 14849MB
[2022-11-05 21:17:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][200/1251]	eta 0:08:20 lr 0.000508	time 0.4606 (0.4764)	loss 4.8841 (4.9543)	grad_norm 1.8096 (inf)	mem 14849MB
[2022-11-05 21:18:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][250/1251]	eta 0:07:54 lr 0.000510	time 0.4713 (0.4740)	loss 3.7795 (4.9396)	grad_norm 1.8377 (inf)	mem 14849MB
[2022-11-05 21:18:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][300/1251]	eta 0:07:29 lr 0.000512	time 0.4630 (0.4726)	loss 5.3586 (4.9180)	grad_norm 2.1050 (inf)	mem 14849MB
[2022-11-05 21:19:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][350/1251]	eta 0:07:05 lr 0.000514	time 0.4564 (0.4718)	loss 4.1825 (4.9203)	grad_norm 1.7097 (inf)	mem 14849MB
[2022-11-05 21:19:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][400/1251]	eta 0:06:40 lr 0.000516	time 0.4606 (0.4712)	loss 4.0299 (4.9325)	grad_norm 1.7179 (inf)	mem 14849MB
[2022-11-05 21:19:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][450/1251]	eta 0:06:16 lr 0.000518	time 0.4680 (0.4704)	loss 5.0831 (4.9317)	grad_norm 1.6472 (inf)	mem 14849MB
[2022-11-05 21:20:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][500/1251]	eta 0:05:53 lr 0.000520	time 0.4704 (0.4701)	loss 5.2557 (4.9198)	grad_norm 1.5166 (inf)	mem 14849MB
[2022-11-05 21:20:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][550/1251]	eta 0:05:29 lr 0.000522	time 0.4629 (0.4700)	loss 3.9038 (4.9226)	grad_norm 1.4241 (inf)	mem 14849MB
[2022-11-05 21:21:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][600/1251]	eta 0:05:05 lr 0.000524	time 0.4640 (0.4698)	loss 4.9862 (4.9167)	grad_norm 2.9632 (inf)	mem 14849MB
[2022-11-05 21:21:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][650/1251]	eta 0:04:42 lr 0.000526	time 0.4681 (0.4695)	loss 4.7161 (4.9161)	grad_norm 1.7712 (inf)	mem 14849MB
[2022-11-05 21:21:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][700/1251]	eta 0:04:18 lr 0.000528	time 0.4600 (0.4693)	loss 5.3019 (4.9143)	grad_norm 1.8177 (inf)	mem 14849MB
[2022-11-05 21:22:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][750/1251]	eta 0:03:54 lr 0.000530	time 0.4705 (0.4689)	loss 4.5601 (4.9060)	grad_norm 1.6631 (inf)	mem 14849MB
[2022-11-05 21:22:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][800/1251]	eta 0:03:31 lr 0.000532	time 0.5383 (0.4690)	loss 3.3238 (4.8933)	grad_norm 1.7948 (inf)	mem 14849MB
[2022-11-05 21:22:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][850/1251]	eta 0:03:08 lr 0.000534	time 0.4655 (0.4690)	loss 3.5743 (4.8907)	grad_norm 1.6014 (inf)	mem 14849MB
[2022-11-05 21:23:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][900/1251]	eta 0:02:44 lr 0.000536	time 0.4685 (0.4688)	loss 3.6218 (4.8836)	grad_norm 1.5804 (inf)	mem 14849MB
[2022-11-05 21:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][950/1251]	eta 0:02:21 lr 0.000538	time 0.4608 (0.4686)	loss 4.2776 (4.8898)	grad_norm 1.9430 (inf)	mem 14849MB
[2022-11-05 21:24:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][1000/1251]	eta 0:01:57 lr 0.000540	time 0.4669 (0.4685)	loss 5.5151 (4.8887)	grad_norm 1.8437 (inf)	mem 14849MB
[2022-11-05 21:24:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][1050/1251]	eta 0:01:34 lr 0.000542	time 0.4704 (0.4686)	loss 5.2424 (4.8778)	grad_norm 1.7099 (inf)	mem 14849MB
[2022-11-05 21:24:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][1100/1251]	eta 0:01:10 lr 0.000544	time 0.4638 (0.4685)	loss 5.1608 (4.8690)	grad_norm 1.6016 (inf)	mem 14849MB
[2022-11-05 21:25:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][1150/1251]	eta 0:00:47 lr 0.000546	time 0.4611 (0.4684)	loss 4.2328 (4.8590)	grad_norm 1.3981 (inf)	mem 14849MB
[2022-11-05 21:25:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][1200/1251]	eta 0:00:23 lr 0.000548	time 0.4680 (0.4683)	loss 5.3274 (4.8583)	grad_norm 1.5396 (inf)	mem 14849MB
[2022-11-05 21:26:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [10/300][1250/1251]	eta 0:00:00 lr 0.000550	time 0.4571 (0.4681)	loss 5.0957 (4.8583)	grad_norm 1.7775 (inf)	mem 14849MB
[2022-11-05 21:26:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 10 training takes 0:09:45
[2022-11-05 21:26:06 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_10.pth saving......
[2022-11-05 21:26:06 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_10.pth saved !!!
[2022-11-05 21:26:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.635 (1.635)	Loss 2.1701 (2.1701)	Acc@1 51.758 (51.758)	Acc@5 77.441 (77.441)	Mem 14849MB
[2022-11-05 21:26:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 53.278 Acc@5 77.922
[2022-11-05 21:26:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 53.3%
[2022-11-05 21:26:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.620 (1.620)	Loss 6.3532 (6.3532)	Acc@1 1.270 (1.270)	Acc@5 5.469 (5.469)	Mem 14849MB
[2022-11-05 21:26:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 1.442 Acc@5 5.486
[2022-11-05 21:26:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 1.4%
[2022-11-05 21:26:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 53.28% at 10 epoch
[2022-11-05 21:26:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][0/1251]	eta 0:40:21 lr 0.000550	time 1.9355 (1.9355)	loss 5.1505 (5.1505)	grad_norm 1.7219 (1.7219)	mem 14849MB
[2022-11-05 21:26:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][50/1251]	eta 0:09:59 lr 0.000552	time 0.4700 (0.4992)	loss 4.5753 (4.7843)	grad_norm 1.6429 (1.7430)	mem 14849MB
[2022-11-05 21:27:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][100/1251]	eta 0:09:18 lr 0.000554	time 0.4652 (0.4853)	loss 4.7936 (4.8091)	grad_norm 2.0333 (1.7760)	mem 14849MB
[2022-11-05 21:27:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][150/1251]	eta 0:08:47 lr 0.000556	time 0.4652 (0.4789)	loss 5.0032 (4.8083)	grad_norm 2.0137 (1.8027)	mem 14849MB
[2022-11-05 21:27:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][200/1251]	eta 0:08:19 lr 0.000558	time 0.4617 (0.4755)	loss 4.1051 (4.7618)	grad_norm 1.7399 (1.7946)	mem 14849MB
[2022-11-05 21:28:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][250/1251]	eta 0:07:54 lr 0.000560	time 0.4627 (0.4740)	loss 4.7388 (4.7879)	grad_norm 1.6742 (1.7863)	mem 14849MB
[2022-11-05 21:28:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][300/1251]	eta 0:07:29 lr 0.000562	time 0.4692 (0.4725)	loss 5.4206 (4.7821)	grad_norm 1.5945 (1.7812)	mem 14849MB
[2022-11-05 21:29:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][350/1251]	eta 0:07:04 lr 0.000564	time 0.4671 (0.4714)	loss 3.3529 (4.7994)	grad_norm 1.8785 (1.7693)	mem 14849MB
[2022-11-05 21:29:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][400/1251]	eta 0:06:40 lr 0.000566	time 0.4621 (0.4708)	loss 5.1702 (4.8074)	grad_norm 1.4106 (1.7701)	mem 14849MB
[2022-11-05 21:29:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][450/1251]	eta 0:06:16 lr 0.000568	time 0.4632 (0.4702)	loss 4.3316 (4.7974)	grad_norm 1.7652 (1.7750)	mem 14849MB
[2022-11-05 21:30:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][500/1251]	eta 0:05:52 lr 0.000570	time 0.4712 (0.4697)	loss 5.0240 (4.7933)	grad_norm 2.3564 (1.7810)	mem 14849MB
[2022-11-05 21:30:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][550/1251]	eta 0:05:29 lr 0.000572	time 0.4576 (0.4697)	loss 4.9339 (4.7846)	grad_norm 1.9922 (1.7815)	mem 14849MB
[2022-11-05 21:31:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][600/1251]	eta 0:05:05 lr 0.000574	time 0.4650 (0.4697)	loss 5.2853 (4.7847)	grad_norm 1.6595 (1.7713)	mem 14849MB
[2022-11-05 21:31:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][650/1251]	eta 0:04:42 lr 0.000576	time 0.4676 (0.4692)	loss 5.1892 (4.7944)	grad_norm 1.6667 (1.7681)	mem 14849MB
[2022-11-05 21:31:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][700/1251]	eta 0:04:18 lr 0.000578	time 0.4541 (0.4690)	loss 5.2217 (4.7989)	grad_norm 2.0835 (1.7698)	mem 14849MB
[2022-11-05 21:32:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][750/1251]	eta 0:03:54 lr 0.000580	time 0.4713 (0.4687)	loss 4.9924 (4.8039)	grad_norm 1.5532 (1.7659)	mem 14849MB
[2022-11-05 21:32:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][800/1251]	eta 0:03:31 lr 0.000582	time 0.4706 (0.4687)	loss 4.8142 (4.8050)	grad_norm 1.9106 (1.7629)	mem 14849MB
[2022-11-05 21:33:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][850/1251]	eta 0:03:08 lr 0.000584	time 0.4759 (0.4688)	loss 5.1667 (4.8088)	grad_norm 1.5089 (1.7631)	mem 14849MB
[2022-11-05 21:33:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][900/1251]	eta 0:02:44 lr 0.000586	time 0.4786 (0.4688)	loss 3.9074 (4.8046)	grad_norm 1.8185 (1.7594)	mem 14849MB
[2022-11-05 21:33:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][950/1251]	eta 0:02:21 lr 0.000588	time 0.4703 (0.4686)	loss 4.5623 (4.7900)	grad_norm 2.1231 (1.7556)	mem 14849MB
[2022-11-05 21:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][1000/1251]	eta 0:01:57 lr 0.000590	time 0.4623 (0.4684)	loss 5.0079 (4.7851)	grad_norm 1.4952 (1.7534)	mem 14849MB
[2022-11-05 21:34:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][1050/1251]	eta 0:01:34 lr 0.000592	time 0.4711 (0.4685)	loss 4.2901 (4.7825)	grad_norm 1.6338 (1.7511)	mem 14849MB
[2022-11-05 21:34:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][1100/1251]	eta 0:01:10 lr 0.000594	time 0.4625 (0.4685)	loss 5.5811 (4.7811)	grad_norm 1.6439 (1.7491)	mem 14849MB
[2022-11-05 21:35:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][1150/1251]	eta 0:00:47 lr 0.000596	time 0.4573 (0.4685)	loss 5.3516 (4.7802)	grad_norm 1.7350 (1.7480)	mem 14849MB
[2022-11-05 21:35:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][1200/1251]	eta 0:00:23 lr 0.000598	time 0.4714 (0.4684)	loss 4.6672 (4.7786)	grad_norm 1.7200 (1.7453)	mem 14849MB
[2022-11-05 21:36:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [11/300][1250/1251]	eta 0:00:00 lr 0.000600	time 0.4572 (0.4682)	loss 5.0720 (4.7765)	grad_norm 1.4873 (1.7396)	mem 14849MB
[2022-11-05 21:36:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 11 training takes 0:09:45
[2022-11-05 21:36:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_11.pth saving......
[2022-11-05 21:36:10 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_11.pth saved !!!
[2022-11-05 21:36:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.450 (1.450)	Loss 1.9229 (1.9229)	Acc@1 58.008 (58.008)	Acc@5 82.617 (82.617)	Mem 14849MB
[2022-11-05 21:36:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 55.688 Acc@5 79.842
[2022-11-05 21:36:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 55.7%
[2022-11-05 21:36:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.540 (1.540)	Loss 6.1689 (6.1689)	Acc@1 1.660 (1.660)	Acc@5 7.520 (7.520)	Mem 14849MB
[2022-11-05 21:36:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 2.052 Acc@5 7.920
[2022-11-05 21:36:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 2.1%
[2022-11-05 21:36:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 55.69% at 11 epoch
[2022-11-05 21:36:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][0/1251]	eta 0:46:05 lr 0.000600	time 2.2103 (2.2103)	loss 5.5700 (5.5700)	grad_norm 1.6227 (1.6227)	mem 14849MB
[2022-11-05 21:36:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][50/1251]	eta 0:10:07 lr 0.000602	time 0.4639 (0.5058)	loss 4.8916 (4.5859)	grad_norm 1.5534 (1.6599)	mem 14849MB
[2022-11-05 21:37:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][100/1251]	eta 0:09:24 lr 0.000604	time 0.4692 (0.4900)	loss 4.5251 (4.6374)	grad_norm 2.5191 (1.7168)	mem 14849MB
[2022-11-05 21:37:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][150/1251]	eta 0:08:50 lr 0.000606	time 0.4634 (0.4817)	loss 4.4801 (4.6797)	grad_norm 1.5796 (1.7290)	mem 14849MB
[2022-11-05 21:38:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][200/1251]	eta 0:08:22 lr 0.000608	time 0.4752 (0.4778)	loss 5.0866 (4.7003)	grad_norm 2.3057 (1.7214)	mem 14849MB
[2022-11-05 21:38:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][250/1251]	eta 0:07:55 lr 0.000610	time 0.4624 (0.4753)	loss 5.2999 (4.6972)	grad_norm 1.4834 (1.7112)	mem 14849MB
[2022-11-05 21:38:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][300/1251]	eta 0:07:30 lr 0.000612	time 0.4607 (0.4737)	loss 5.4317 (4.7185)	grad_norm 1.4617 (1.7183)	mem 14849MB
[2022-11-05 21:39:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][350/1251]	eta 0:07:05 lr 0.000614	time 0.4738 (0.4724)	loss 4.4390 (4.7058)	grad_norm 1.6211 (1.7064)	mem 14849MB
[2022-11-05 21:39:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][400/1251]	eta 0:06:41 lr 0.000616	time 0.4712 (0.4717)	loss 4.8894 (4.6990)	grad_norm 2.0422 (1.7037)	mem 14849MB
[2022-11-05 21:40:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][450/1251]	eta 0:06:17 lr 0.000618	time 0.4703 (0.4709)	loss 4.1928 (4.6876)	grad_norm 1.7908 (1.7034)	mem 14849MB
[2022-11-05 21:40:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][500/1251]	eta 0:05:53 lr 0.000620	time 0.4523 (0.4704)	loss 5.3456 (4.6904)	grad_norm 1.6544 (1.7047)	mem 14849MB
[2022-11-05 21:40:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][550/1251]	eta 0:05:29 lr 0.000622	time 0.4728 (0.4704)	loss 4.8382 (4.6981)	grad_norm 1.7208 (1.7023)	mem 14849MB
[2022-11-05 21:41:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][600/1251]	eta 0:05:06 lr 0.000624	time 0.4633 (0.4702)	loss 5.1588 (4.6966)	grad_norm 1.5309 (1.7049)	mem 14849MB
[2022-11-05 21:41:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][650/1251]	eta 0:04:42 lr 0.000626	time 0.4595 (0.4697)	loss 3.7767 (4.6990)	grad_norm 1.6123 (1.7032)	mem 14849MB
[2022-11-05 21:41:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][700/1251]	eta 0:04:18 lr 0.000628	time 0.4657 (0.4695)	loss 4.8915 (4.6972)	grad_norm 1.3663 (1.6946)	mem 14849MB
[2022-11-05 21:42:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][750/1251]	eta 0:03:55 lr 0.000630	time 0.4642 (0.4693)	loss 4.7265 (4.6963)	grad_norm 2.0053 (1.6914)	mem 14849MB
[2022-11-05 21:42:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][800/1251]	eta 0:03:31 lr 0.000632	time 0.4607 (0.4694)	loss 4.0918 (4.6892)	grad_norm 2.0210 (1.6901)	mem 14849MB
[2022-11-05 21:43:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][850/1251]	eta 0:03:08 lr 0.000634	time 0.4726 (0.4693)	loss 4.3720 (4.6962)	grad_norm 1.4034 (1.6889)	mem 14849MB
[2022-11-05 21:43:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][900/1251]	eta 0:02:44 lr 0.000636	time 0.4646 (0.4691)	loss 5.0190 (4.7010)	grad_norm 1.7321 (1.6798)	mem 14849MB
[2022-11-05 21:43:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][950/1251]	eta 0:02:21 lr 0.000638	time 0.4619 (0.4689)	loss 5.4100 (4.6906)	grad_norm 1.4177 (1.6798)	mem 14849MB
[2022-11-05 21:44:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][1000/1251]	eta 0:01:57 lr 0.000640	time 0.4636 (0.4689)	loss 4.6815 (4.6858)	grad_norm 1.7035 (1.6765)	mem 14849MB
[2022-11-05 21:44:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][1050/1251]	eta 0:01:34 lr 0.000642	time 0.4685 (0.4690)	loss 5.0189 (4.6822)	grad_norm 1.3746 (1.6710)	mem 14849MB
[2022-11-05 21:45:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][1100/1251]	eta 0:01:10 lr 0.000644	time 0.4678 (0.4690)	loss 4.4482 (4.6805)	grad_norm 1.4556 (1.6724)	mem 14849MB
[2022-11-05 21:45:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][1150/1251]	eta 0:00:47 lr 0.000646	time 0.4663 (0.4688)	loss 3.7715 (4.6847)	grad_norm 1.4838 (1.6710)	mem 14849MB
[2022-11-05 21:45:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][1200/1251]	eta 0:00:23 lr 0.000648	time 0.4661 (0.4686)	loss 4.0947 (4.6834)	grad_norm 1.5525 (1.6713)	mem 14849MB
[2022-11-05 21:46:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [12/300][1250/1251]	eta 0:00:00 lr 0.000650	time 0.4566 (0.4684)	loss 5.2951 (4.6809)	grad_norm 1.4211 (1.6699)	mem 14849MB
[2022-11-05 21:46:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 12 training takes 0:09:46
[2022-11-05 21:46:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_12.pth saving......
[2022-11-05 21:46:14 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_12.pth saved !!!
[2022-11-05 21:46:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.623 (1.623)	Loss 1.9221 (1.9221)	Acc@1 57.422 (57.422)	Acc@5 81.445 (81.445)	Mem 14849MB
[2022-11-05 21:46:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 57.502 Acc@5 81.236
[2022-11-05 21:46:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 57.5%
[2022-11-05 21:46:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.579 (1.579)	Loss 5.9181 (5.9181)	Acc@1 3.223 (3.223)	Acc@5 11.035 (11.035)	Mem 14849MB
[2022-11-05 21:46:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 3.226 Acc@5 11.152
[2022-11-05 21:46:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 3.2%
[2022-11-05 21:46:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 57.50% at 12 epoch
[2022-11-05 21:46:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][0/1251]	eta 0:41:36 lr 0.000650	time 1.9956 (1.9956)	loss 3.6180 (3.6180)	grad_norm 1.6093 (1.6093)	mem 14849MB
[2022-11-05 21:46:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][50/1251]	eta 0:10:07 lr 0.000652	time 0.4705 (0.5056)	loss 5.1737 (4.5488)	grad_norm 1.3958 (1.6072)	mem 14849MB
[2022-11-05 21:47:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][100/1251]	eta 0:09:20 lr 0.000654	time 0.4651 (0.4871)	loss 3.8285 (4.5613)	grad_norm 2.1787 (1.6619)	mem 14849MB
[2022-11-05 21:47:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][150/1251]	eta 0:08:48 lr 0.000656	time 0.4610 (0.4799)	loss 4.8097 (4.5721)	grad_norm 1.8015 (1.6339)	mem 14849MB
[2022-11-05 21:48:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][200/1251]	eta 0:08:20 lr 0.000658	time 0.4560 (0.4761)	loss 4.3287 (4.5795)	grad_norm 1.6749 (1.6330)	mem 14849MB
[2022-11-05 21:48:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][250/1251]	eta 0:07:54 lr 0.000660	time 0.4630 (0.4741)	loss 4.9833 (4.6081)	grad_norm 1.5688 (1.6319)	mem 14849MB
[2022-11-05 21:48:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][300/1251]	eta 0:07:29 lr 0.000662	time 0.4595 (0.4726)	loss 4.8534 (4.5846)	grad_norm 1.5778 (1.6168)	mem 14849MB
[2022-11-05 21:49:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][350/1251]	eta 0:07:05 lr 0.000664	time 0.4658 (0.4718)	loss 4.3432 (4.5884)	grad_norm 2.4416 (1.6156)	mem 14849MB
[2022-11-05 21:49:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][400/1251]	eta 0:06:40 lr 0.000666	time 0.4695 (0.4709)	loss 5.2007 (4.5913)	grad_norm 1.5618 (1.6069)	mem 14849MB
[2022-11-05 21:50:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][450/1251]	eta 0:06:16 lr 0.000668	time 0.4551 (0.4701)	loss 5.0538 (4.6004)	grad_norm 1.3322 (inf)	mem 14849MB
[2022-11-05 21:50:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][500/1251]	eta 0:05:52 lr 0.000670	time 0.4641 (0.4696)	loss 5.5225 (4.6041)	grad_norm 1.4189 (inf)	mem 14849MB
[2022-11-05 21:50:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][550/1251]	eta 0:05:29 lr 0.000672	time 0.5432 (0.4697)	loss 4.8724 (4.5967)	grad_norm 1.3013 (inf)	mem 14849MB
[2022-11-05 21:51:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][600/1251]	eta 0:05:05 lr 0.000674	time 0.4528 (0.4693)	loss 5.2997 (4.6012)	grad_norm 1.4595 (inf)	mem 14849MB
[2022-11-05 21:51:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][650/1251]	eta 0:04:41 lr 0.000676	time 0.4607 (0.4690)	loss 4.9101 (4.5987)	grad_norm 1.4673 (inf)	mem 14849MB
[2022-11-05 21:52:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][700/1251]	eta 0:04:18 lr 0.000678	time 0.4615 (0.4687)	loss 5.4327 (4.6035)	grad_norm 1.7843 (inf)	mem 14849MB
[2022-11-05 21:52:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][750/1251]	eta 0:03:54 lr 0.000680	time 0.4698 (0.4685)	loss 4.3455 (4.6006)	grad_norm 1.5225 (inf)	mem 14849MB
[2022-11-05 21:52:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][800/1251]	eta 0:03:31 lr 0.000682	time 0.4509 (0.4686)	loss 4.3818 (4.5918)	grad_norm 1.5516 (inf)	mem 14849MB
[2022-11-05 21:53:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][850/1251]	eta 0:03:07 lr 0.000684	time 0.4593 (0.4686)	loss 4.7193 (4.5934)	grad_norm 1.7501 (inf)	mem 14849MB
[2022-11-05 21:53:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][900/1251]	eta 0:02:44 lr 0.000686	time 0.4678 (0.4684)	loss 3.8248 (4.5976)	grad_norm 1.4191 (inf)	mem 14849MB
[2022-11-05 21:53:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][950/1251]	eta 0:02:20 lr 0.000688	time 0.4711 (0.4682)	loss 4.4530 (4.5933)	grad_norm 1.2838 (inf)	mem 14849MB
[2022-11-05 21:54:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][1000/1251]	eta 0:01:57 lr 0.000690	time 0.4688 (0.4681)	loss 4.5255 (4.5931)	grad_norm 1.3186 (inf)	mem 14849MB
[2022-11-05 21:54:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][1050/1251]	eta 0:01:34 lr 0.000692	time 0.5445 (0.4682)	loss 4.3642 (4.5976)	grad_norm 1.3625 (inf)	mem 14849MB
[2022-11-05 21:55:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][1100/1251]	eta 0:01:10 lr 0.000694	time 0.4609 (0.4682)	loss 5.2077 (4.5943)	grad_norm 1.5842 (inf)	mem 14849MB
[2022-11-05 21:55:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][1150/1251]	eta 0:00:47 lr 0.000696	time 0.4766 (0.4681)	loss 4.8880 (4.5926)	grad_norm 1.4225 (inf)	mem 14849MB
[2022-11-05 21:55:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][1200/1251]	eta 0:00:23 lr 0.000698	time 0.4605 (0.4680)	loss 4.4377 (4.5910)	grad_norm 2.0313 (inf)	mem 14849MB
[2022-11-05 21:56:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [13/300][1250/1251]	eta 0:00:00 lr 0.000700	time 0.4576 (0.4678)	loss 4.7415 (4.5883)	grad_norm 1.9323 (inf)	mem 14849MB
[2022-11-05 21:56:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 13 training takes 0:09:45
[2022-11-05 21:56:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_13.pth saving......
[2022-11-05 21:56:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_13.pth saved !!!
[2022-11-05 21:56:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.654 (1.654)	Loss 1.7664 (1.7664)	Acc@1 61.523 (61.523)	Acc@5 84.375 (84.375)	Mem 14849MB
[2022-11-05 21:56:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 58.734 Acc@5 82.138
[2022-11-05 21:56:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 58.7%
[2022-11-05 21:56:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.609 (1.609)	Loss 5.6814 (5.6814)	Acc@1 5.273 (5.273)	Acc@5 15.234 (15.234)	Mem 14849MB
[2022-11-05 21:56:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 4.862 Acc@5 15.564
[2022-11-05 21:56:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 4.9%
[2022-11-05 21:56:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 58.73% at 13 epoch
[2022-11-05 21:56:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][0/1251]	eta 0:42:25 lr 0.000700	time 2.0346 (2.0346)	loss 4.9892 (4.9892)	grad_norm 1.8236 (1.8236)	mem 14851MB
[2022-11-05 21:57:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][50/1251]	eta 0:10:00 lr 0.000702	time 0.4689 (0.4998)	loss 4.9321 (4.5554)	grad_norm 1.1884 (1.6351)	mem 14851MB
[2022-11-05 21:57:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][100/1251]	eta 0:09:20 lr 0.000704	time 0.4613 (0.4867)	loss 4.1998 (4.6020)	grad_norm 1.9983 (1.6365)	mem 14851MB
[2022-11-05 21:57:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][150/1251]	eta 0:08:47 lr 0.000706	time 0.4619 (0.4794)	loss 4.3193 (4.5800)	grad_norm 1.4616 (1.6053)	mem 14851MB
[2022-11-05 21:58:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][200/1251]	eta 0:08:20 lr 0.000708	time 0.4601 (0.4758)	loss 5.0690 (4.5561)	grad_norm 1.2616 (1.5883)	mem 14851MB
[2022-11-05 21:58:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][250/1251]	eta 0:07:54 lr 0.000710	time 0.4558 (0.4740)	loss 5.5140 (4.5582)	grad_norm 1.6877 (1.5893)	mem 14851MB
[2022-11-05 21:58:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][300/1251]	eta 0:07:29 lr 0.000712	time 0.4597 (0.4726)	loss 4.3746 (4.5555)	grad_norm 1.3682 (1.5796)	mem 14851MB
[2022-11-05 21:59:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][350/1251]	eta 0:07:05 lr 0.000714	time 0.4530 (0.4717)	loss 3.6156 (4.5393)	grad_norm 1.4745 (1.5791)	mem 14851MB
[2022-11-05 21:59:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][400/1251]	eta 0:06:40 lr 0.000716	time 0.4769 (0.4709)	loss 4.6086 (4.5456)	grad_norm 1.5971 (1.5857)	mem 14851MB
[2022-11-05 22:00:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][450/1251]	eta 0:06:16 lr 0.000718	time 0.4743 (0.4703)	loss 4.3261 (4.5374)	grad_norm 1.6695 (1.5780)	mem 14851MB
[2022-11-05 22:00:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][500/1251]	eta 0:05:52 lr 0.000720	time 0.4518 (0.4698)	loss 5.2840 (4.5366)	grad_norm 1.4292 (1.5778)	mem 14851MB
[2022-11-05 22:00:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][550/1251]	eta 0:05:29 lr 0.000722	time 0.4653 (0.4699)	loss 3.2219 (4.5292)	grad_norm 1.4483 (1.5736)	mem 14851MB
[2022-11-05 22:01:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][600/1251]	eta 0:05:05 lr 0.000724	time 0.4653 (0.4694)	loss 4.7824 (4.5455)	grad_norm 1.5237 (1.5676)	mem 14851MB
[2022-11-05 22:01:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][650/1251]	eta 0:04:42 lr 0.000726	time 0.4524 (0.4692)	loss 3.4273 (4.5405)	grad_norm 1.3860 (1.5652)	mem 14851MB
[2022-11-05 22:02:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][700/1251]	eta 0:04:18 lr 0.000728	time 0.5479 (0.4692)	loss 4.4744 (4.5467)	grad_norm 1.4744 (1.5637)	mem 14851MB
[2022-11-05 22:02:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][750/1251]	eta 0:03:54 lr 0.000730	time 0.4609 (0.4690)	loss 4.5610 (4.5594)	grad_norm 1.7020 (1.5656)	mem 14851MB
[2022-11-05 22:02:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][800/1251]	eta 0:03:31 lr 0.000732	time 0.4645 (0.4691)	loss 3.0643 (4.5624)	grad_norm 1.7103 (1.5672)	mem 14851MB
[2022-11-05 22:03:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][850/1251]	eta 0:03:08 lr 0.000734	time 0.4576 (0.4689)	loss 4.7045 (4.5579)	grad_norm 1.3425 (1.5618)	mem 14851MB
[2022-11-05 22:03:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][900/1251]	eta 0:02:44 lr 0.000736	time 0.4678 (0.4687)	loss 3.8626 (4.5618)	grad_norm 1.6643 (1.5581)	mem 14851MB
[2022-11-05 22:04:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][950/1251]	eta 0:02:21 lr 0.000738	time 0.4660 (0.4686)	loss 3.9543 (4.5511)	grad_norm 1.8507 (1.5568)	mem 14851MB
[2022-11-05 22:04:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][1000/1251]	eta 0:01:57 lr 0.000740	time 0.4597 (0.4686)	loss 4.1965 (4.5554)	grad_norm 1.3744 (1.5562)	mem 14851MB
[2022-11-05 22:04:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][1050/1251]	eta 0:01:34 lr 0.000742	time 0.4650 (0.4687)	loss 3.5512 (4.5522)	grad_norm 1.4340 (1.5547)	mem 14851MB
[2022-11-05 22:05:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][1100/1251]	eta 0:01:10 lr 0.000744	time 0.4547 (0.4685)	loss 5.0001 (4.5523)	grad_norm 1.4048 (1.5535)	mem 14851MB
[2022-11-05 22:05:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][1150/1251]	eta 0:00:47 lr 0.000746	time 0.4603 (0.4684)	loss 4.7235 (4.5482)	grad_norm 1.5533 (1.5500)	mem 14851MB
[2022-11-05 22:05:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][1200/1251]	eta 0:00:23 lr 0.000748	time 0.5315 (0.4683)	loss 4.4058 (4.5452)	grad_norm 1.6101 (nan)	mem 14851MB
[2022-11-05 22:06:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [14/300][1250/1251]	eta 0:00:00 lr 0.000750	time 0.4566 (0.4682)	loss 4.6177 (4.5399)	grad_norm 1.3565 (nan)	mem 14851MB
[2022-11-05 22:06:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 14 training takes 0:09:45
[2022-11-05 22:06:20 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_14.pth saving......
[2022-11-05 22:06:21 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_14.pth saved !!!
[2022-11-05 22:06:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.534 (1.534)	Loss 1.8560 (1.8560)	Acc@1 60.645 (60.645)	Acc@5 81.934 (81.934)	Mem 14851MB
[2022-11-05 22:06:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 60.060 Acc@5 83.262
[2022-11-05 22:06:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 60.1%
[2022-11-05 22:06:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.702 (1.702)	Loss 5.3474 (5.3474)	Acc@1 7.324 (7.324)	Acc@5 21.191 (21.191)	Mem 14851MB
[2022-11-05 22:06:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 7.540 Acc@5 21.828
[2022-11-05 22:06:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 7.5%
[2022-11-05 22:06:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 60.06% at 14 epoch
[2022-11-05 22:06:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][0/1251]	eta 0:41:21 lr 0.000750	time 1.9837 (1.9837)	loss 4.7812 (4.7812)	grad_norm 1.3821 (1.3821)	mem 14851MB
[2022-11-05 22:07:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][50/1251]	eta 0:10:04 lr 0.000752	time 0.4782 (0.5036)	loss 5.0162 (4.6280)	grad_norm 1.5095 (1.4959)	mem 14851MB
[2022-11-05 22:07:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][100/1251]	eta 0:09:19 lr 0.000754	time 0.4750 (0.4865)	loss 5.0083 (4.4962)	grad_norm 1.3676 (1.5419)	mem 14851MB
[2022-11-05 22:07:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][150/1251]	eta 0:08:48 lr 0.000756	time 0.4728 (0.4796)	loss 4.4892 (4.5119)	grad_norm 1.5198 (1.5591)	mem 14851MB
[2022-11-05 22:08:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][200/1251]	eta 0:08:20 lr 0.000758	time 0.4634 (0.4760)	loss 5.3268 (4.4845)	grad_norm 1.3903 (1.5513)	mem 14851MB
[2022-11-05 22:08:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][250/1251]	eta 0:07:54 lr 0.000760	time 0.4706 (0.4741)	loss 4.2206 (4.4973)	grad_norm 1.5096 (1.5486)	mem 14851MB
[2022-11-05 22:09:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][300/1251]	eta 0:07:29 lr 0.000762	time 0.4662 (0.4726)	loss 4.9851 (4.4939)	grad_norm 1.3834 (1.5424)	mem 14851MB
[2022-11-05 22:09:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][350/1251]	eta 0:07:04 lr 0.000764	time 0.4600 (0.4717)	loss 3.2691 (4.5115)	grad_norm 1.3906 (1.5461)	mem 14851MB
[2022-11-05 22:09:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][400/1251]	eta 0:06:40 lr 0.000766	time 0.4607 (0.4708)	loss 3.7941 (4.4980)	grad_norm 1.5085 (1.5458)	mem 14851MB
[2022-11-05 22:10:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][450/1251]	eta 0:06:16 lr 0.000768	time 0.4675 (0.4702)	loss 5.1252 (4.5153)	grad_norm 1.3809 (1.5335)	mem 14851MB
[2022-11-05 22:10:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][500/1251]	eta 0:05:52 lr 0.000770	time 0.4760 (0.4697)	loss 3.8111 (4.5153)	grad_norm 1.4348 (1.5316)	mem 14851MB
[2022-11-05 22:10:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][550/1251]	eta 0:05:29 lr 0.000772	time 0.4689 (0.4699)	loss 3.4808 (4.5033)	grad_norm 1.2915 (1.5295)	mem 14851MB
[2022-11-05 22:11:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][600/1251]	eta 0:05:05 lr 0.000774	time 0.4632 (0.4696)	loss 4.0918 (4.5046)	grad_norm 1.7469 (1.5291)	mem 14851MB
[2022-11-05 22:11:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][650/1251]	eta 0:04:42 lr 0.000776	time 0.4692 (0.4693)	loss 5.2842 (4.4985)	grad_norm 1.7878 (1.5272)	mem 14851MB
[2022-11-05 22:12:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][700/1251]	eta 0:04:18 lr 0.000778	time 0.4596 (0.4691)	loss 4.9253 (4.5104)	grad_norm 1.5189 (1.5287)	mem 14851MB
[2022-11-05 22:12:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][750/1251]	eta 0:03:54 lr 0.000780	time 0.4617 (0.4688)	loss 4.8163 (4.5094)	grad_norm 1.5601 (1.5277)	mem 14851MB
[2022-11-05 22:12:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][800/1251]	eta 0:03:31 lr 0.000782	time 0.4690 (0.4688)	loss 4.6942 (4.5126)	grad_norm 1.3062 (1.5279)	mem 14851MB
[2022-11-05 22:13:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][850/1251]	eta 0:03:07 lr 0.000784	time 0.4714 (0.4687)	loss 3.8057 (4.5136)	grad_norm 1.5826 (1.5229)	mem 14851MB
[2022-11-05 22:13:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][900/1251]	eta 0:02:44 lr 0.000786	time 0.4589 (0.4685)	loss 4.2469 (4.5075)	grad_norm 1.3274 (1.5221)	mem 14851MB
[2022-11-05 22:14:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][950/1251]	eta 0:02:20 lr 0.000788	time 0.4638 (0.4684)	loss 3.8742 (4.5052)	grad_norm 1.4031 (1.5198)	mem 14851MB
[2022-11-05 22:14:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][1000/1251]	eta 0:01:57 lr 0.000790	time 0.4757 (0.4682)	loss 5.1841 (4.5115)	grad_norm 1.4164 (1.5150)	mem 14851MB
[2022-11-05 22:14:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][1050/1251]	eta 0:01:34 lr 0.000792	time 0.4608 (0.4683)	loss 3.6671 (4.5040)	grad_norm 1.3312 (1.5146)	mem 14851MB
[2022-11-05 22:15:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][1100/1251]	eta 0:01:10 lr 0.000794	time 0.4638 (0.4682)	loss 5.0523 (4.5027)	grad_norm 1.8163 (1.5127)	mem 14851MB
[2022-11-05 22:15:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][1150/1251]	eta 0:00:47 lr 0.000796	time 0.4616 (0.4681)	loss 5.3126 (4.5052)	grad_norm 1.4558 (1.5164)	mem 14851MB
[2022-11-05 22:16:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][1200/1251]	eta 0:00:23 lr 0.000798	time 0.4818 (0.4681)	loss 5.1606 (4.5046)	grad_norm 1.5318 (1.5150)	mem 14851MB
[2022-11-05 22:16:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [15/300][1250/1251]	eta 0:00:00 lr 0.000800	time 0.4565 (0.4679)	loss 5.0295 (4.5005)	grad_norm 1.6357 (1.5110)	mem 14851MB
[2022-11-05 22:16:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 15 training takes 0:09:45
[2022-11-05 22:16:24 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_15.pth saving......
[2022-11-05 22:16:25 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_15.pth saved !!!
[2022-11-05 22:16:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.595 (1.595)	Loss 1.7379 (1.7379)	Acc@1 61.621 (61.621)	Acc@5 84.863 (84.863)	Mem 14851MB
[2022-11-05 22:16:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 60.866 Acc@5 84.052
[2022-11-05 22:16:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 60.9%
[2022-11-05 22:16:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.678 (1.678)	Loss 4.9207 (4.9207)	Acc@1 11.523 (11.523)	Acc@5 30.078 (30.078)	Mem 14851MB
[2022-11-05 22:16:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 11.632 Acc@5 29.476
[2022-11-05 22:16:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 11.6%
[2022-11-05 22:16:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 60.87% at 15 epoch
[2022-11-05 22:16:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][0/1251]	eta 0:46:02 lr 0.000800	time 2.2083 (2.2083)	loss 4.5891 (4.5891)	grad_norm 1.4411 (1.4411)	mem 14851MB
[2022-11-05 22:17:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][50/1251]	eta 0:10:08 lr 0.000802	time 0.4637 (0.5068)	loss 3.4697 (4.5811)	grad_norm 1.5365 (1.5292)	mem 14851MB
[2022-11-05 22:17:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][100/1251]	eta 0:09:22 lr 0.000804	time 0.4677 (0.4887)	loss 4.5679 (4.5541)	grad_norm 1.4662 (1.5009)	mem 14851MB
[2022-11-05 22:17:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][150/1251]	eta 0:08:50 lr 0.000806	time 0.4667 (0.4816)	loss 4.9504 (4.5074)	grad_norm 1.4975 (1.5059)	mem 14851MB
[2022-11-05 22:18:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][200/1251]	eta 0:08:21 lr 0.000808	time 0.4684 (0.4775)	loss 4.9546 (4.4685)	grad_norm 1.4206 (1.5097)	mem 14851MB
[2022-11-05 22:18:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][250/1251]	eta 0:07:55 lr 0.000810	time 0.4543 (0.4755)	loss 4.6393 (4.4667)	grad_norm 1.4806 (1.5049)	mem 14851MB
[2022-11-05 22:19:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][300/1251]	eta 0:07:30 lr 0.000812	time 0.4566 (0.4740)	loss 4.3436 (4.4782)	grad_norm 1.2762 (1.5002)	mem 14851MB
[2022-11-05 22:19:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][350/1251]	eta 0:07:05 lr 0.000814	time 0.4731 (0.4728)	loss 4.6379 (4.4657)	grad_norm 1.5508 (1.4935)	mem 14851MB
[2022-11-05 22:19:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][400/1251]	eta 0:06:41 lr 0.000816	time 0.4603 (0.4722)	loss 5.0631 (4.4654)	grad_norm 1.9420 (1.4949)	mem 14851MB
[2022-11-05 22:20:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][450/1251]	eta 0:06:17 lr 0.000818	time 0.4626 (0.4715)	loss 4.0801 (4.4663)	grad_norm 1.3369 (1.4956)	mem 14851MB
[2022-11-05 22:20:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][500/1251]	eta 0:05:53 lr 0.000820	time 0.4586 (0.4708)	loss 4.8219 (4.4697)	grad_norm 1.4585 (1.4871)	mem 14851MB
[2022-11-05 22:21:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][550/1251]	eta 0:05:30 lr 0.000822	time 0.4736 (0.4710)	loss 5.0036 (4.4729)	grad_norm 1.3011 (1.4845)	mem 14851MB
[2022-11-05 22:21:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][600/1251]	eta 0:05:06 lr 0.000824	time 0.4736 (0.4707)	loss 4.8547 (4.4854)	grad_norm 1.3589 (1.4839)	mem 14851MB
[2022-11-05 22:21:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][650/1251]	eta 0:04:42 lr 0.000826	time 0.4636 (0.4706)	loss 3.7752 (4.4791)	grad_norm 1.6441 (1.4821)	mem 14851MB
[2022-11-05 22:22:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][700/1251]	eta 0:04:19 lr 0.000828	time 0.4628 (0.4702)	loss 4.7306 (4.4791)	grad_norm 1.4294 (1.4799)	mem 14851MB
[2022-11-05 22:22:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][750/1251]	eta 0:03:55 lr 0.000830	time 0.4629 (0.4701)	loss 4.2079 (4.4773)	grad_norm 1.3244 (inf)	mem 14851MB
[2022-11-05 22:22:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][800/1251]	eta 0:03:32 lr 0.000832	time 0.4735 (0.4701)	loss 4.7995 (4.4754)	grad_norm 1.5775 (inf)	mem 14851MB
[2022-11-05 22:23:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][850/1251]	eta 0:03:08 lr 0.000834	time 0.4661 (0.4699)	loss 3.3609 (4.4716)	grad_norm 1.9332 (inf)	mem 14851MB
[2022-11-05 22:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][900/1251]	eta 0:02:44 lr 0.000836	time 0.4624 (0.4697)	loss 4.8601 (4.4692)	grad_norm 1.2787 (inf)	mem 14851MB
[2022-11-05 22:24:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][950/1251]	eta 0:02:21 lr 0.000838	time 0.4529 (0.4695)	loss 4.9348 (4.4664)	grad_norm 1.5361 (inf)	mem 14851MB
[2022-11-05 22:24:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][1000/1251]	eta 0:01:57 lr 0.000840	time 0.4692 (0.4696)	loss 4.6567 (4.4615)	grad_norm 1.5611 (inf)	mem 14851MB
[2022-11-05 22:24:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][1050/1251]	eta 0:01:34 lr 0.000842	time 0.4595 (0.4695)	loss 4.7662 (4.4584)	grad_norm 1.4293 (inf)	mem 14851MB
[2022-11-05 22:25:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][1100/1251]	eta 0:01:10 lr 0.000844	time 0.4532 (0.4694)	loss 4.6299 (4.4514)	grad_norm 1.3084 (inf)	mem 14851MB
[2022-11-05 22:25:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][1150/1251]	eta 0:00:47 lr 0.000846	time 0.4727 (0.4693)	loss 3.9700 (4.4468)	grad_norm 1.2229 (inf)	mem 14851MB
[2022-11-05 22:26:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][1200/1251]	eta 0:00:23 lr 0.000848	time 0.4522 (0.4692)	loss 4.3947 (4.4504)	grad_norm 1.7458 (inf)	mem 14851MB
[2022-11-05 22:26:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [16/300][1250/1251]	eta 0:00:00 lr 0.000850	time 0.4585 (0.4690)	loss 4.7325 (4.4494)	grad_norm 1.3390 (inf)	mem 14851MB
[2022-11-05 22:26:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 16 training takes 0:09:46
[2022-11-05 22:26:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_16.pth saving......
[2022-11-05 22:26:30 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_16.pth saved !!!
[2022-11-05 22:26:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.597 (1.597)	Loss 1.7011 (1.7011)	Acc@1 61.230 (61.230)	Acc@5 86.621 (86.621)	Mem 14851MB
[2022-11-05 22:26:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 61.600 Acc@5 84.652
[2022-11-05 22:26:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 61.6%
[2022-11-05 22:26:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.575 (1.575)	Loss 4.4611 (4.4611)	Acc@1 18.066 (18.066)	Acc@5 40.039 (40.039)	Mem 14851MB
[2022-11-05 22:26:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 17.336 Acc@5 38.462
[2022-11-05 22:26:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 17.3%
[2022-11-05 22:26:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 61.60% at 16 epoch
[2022-11-05 22:26:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][0/1251]	eta 0:39:23 lr 0.000850	time 1.8892 (1.8892)	loss 4.2796 (4.2796)	grad_norm 1.2361 (1.2361)	mem 14851MB
[2022-11-05 22:27:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][50/1251]	eta 0:10:01 lr 0.000852	time 0.4752 (0.5006)	loss 4.6770 (4.2391)	grad_norm 1.2449 (1.4374)	mem 14851MB
[2022-11-05 22:27:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][100/1251]	eta 0:09:16 lr 0.000854	time 0.4526 (0.4839)	loss 4.4582 (4.3874)	grad_norm 1.2814 (1.4328)	mem 14851MB
[2022-11-05 22:27:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][150/1251]	eta 0:08:46 lr 0.000856	time 0.4564 (0.4785)	loss 3.8365 (4.3665)	grad_norm 1.4334 (1.4161)	mem 14851MB
[2022-11-05 22:28:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][200/1251]	eta 0:08:19 lr 0.000858	time 0.4684 (0.4754)	loss 4.9538 (4.3540)	grad_norm 1.3820 (1.4334)	mem 14851MB
[2022-11-05 22:28:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][250/1251]	eta 0:07:53 lr 0.000860	time 0.4639 (0.4734)	loss 3.2804 (4.3401)	grad_norm 1.4437 (1.4326)	mem 14851MB
[2022-11-05 22:29:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][300/1251]	eta 0:07:29 lr 0.000862	time 0.4690 (0.4722)	loss 4.6317 (4.3420)	grad_norm 1.8922 (1.4326)	mem 14851MB
[2022-11-05 22:29:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][350/1251]	eta 0:07:04 lr 0.000864	time 0.4512 (0.4710)	loss 4.7695 (4.3261)	grad_norm 1.6052 (1.4355)	mem 14851MB
[2022-11-05 22:29:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][400/1251]	eta 0:06:40 lr 0.000866	time 0.4662 (0.4703)	loss 4.9156 (4.3412)	grad_norm 1.3940 (1.4382)	mem 14851MB
[2022-11-05 22:30:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][450/1251]	eta 0:06:16 lr 0.000868	time 0.4752 (0.4697)	loss 4.3369 (4.3563)	grad_norm 1.2316 (1.4347)	mem 14851MB
[2022-11-05 22:30:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][500/1251]	eta 0:05:52 lr 0.000870	time 0.4643 (0.4693)	loss 3.6323 (4.3674)	grad_norm 1.6141 (1.4335)	mem 14851MB
[2022-11-05 22:31:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][550/1251]	eta 0:05:29 lr 0.000872	time 0.4822 (0.4694)	loss 4.4767 (4.3722)	grad_norm 1.7506 (1.4326)	mem 14851MB
[2022-11-05 22:31:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][600/1251]	eta 0:05:05 lr 0.000874	time 0.4569 (0.4692)	loss 5.2772 (4.3767)	grad_norm 1.4053 (1.4367)	mem 14851MB
[2022-11-05 22:31:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][650/1251]	eta 0:04:41 lr 0.000876	time 0.4596 (0.4688)	loss 4.3338 (4.3804)	grad_norm 1.5099 (1.4317)	mem 14851MB
[2022-11-05 22:32:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][700/1251]	eta 0:04:18 lr 0.000878	time 0.4624 (0.4686)	loss 4.7956 (4.3794)	grad_norm 1.3078 (1.4291)	mem 14851MB
[2022-11-05 22:32:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][750/1251]	eta 0:03:54 lr 0.000880	time 0.4605 (0.4682)	loss 3.3456 (4.3868)	grad_norm 1.5028 (1.4340)	mem 14851MB
[2022-11-05 22:33:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][800/1251]	eta 0:03:31 lr 0.000882	time 0.4594 (0.4685)	loss 4.7288 (4.3832)	grad_norm 1.5531 (1.4328)	mem 14851MB
[2022-11-05 22:33:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][850/1251]	eta 0:03:07 lr 0.000884	time 0.4687 (0.4684)	loss 4.7231 (4.3778)	grad_norm 1.1411 (1.4362)	mem 14851MB
[2022-11-05 22:33:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][900/1251]	eta 0:02:44 lr 0.000886	time 0.4858 (0.4682)	loss 4.5641 (4.3775)	grad_norm 1.2801 (1.4406)	mem 14851MB
[2022-11-05 22:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][950/1251]	eta 0:02:20 lr 0.000888	time 0.4794 (0.4681)	loss 4.4948 (4.3792)	grad_norm 1.3111 (1.4373)	mem 14851MB
[2022-11-05 22:34:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][1000/1251]	eta 0:01:57 lr 0.000890	time 0.4657 (0.4679)	loss 4.7881 (4.3847)	grad_norm 1.6325 (1.4354)	mem 14851MB
[2022-11-05 22:34:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][1050/1251]	eta 0:01:34 lr 0.000892	time 0.4687 (0.4681)	loss 4.5740 (4.3864)	grad_norm 1.7292 (1.4336)	mem 14851MB
[2022-11-05 22:35:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][1100/1251]	eta 0:01:10 lr 0.000894	time 0.4579 (0.4680)	loss 4.8145 (4.3931)	grad_norm 1.4160 (1.4305)	mem 14851MB
[2022-11-05 22:35:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][1150/1251]	eta 0:00:47 lr 0.000896	time 0.4598 (0.4679)	loss 4.6426 (4.3956)	grad_norm 1.4220 (1.4286)	mem 14851MB
[2022-11-05 22:36:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][1200/1251]	eta 0:00:23 lr 0.000898	time 0.4659 (0.4678)	loss 5.0207 (4.4009)	grad_norm 1.5304 (1.4291)	mem 14851MB
[2022-11-05 22:36:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [17/300][1250/1251]	eta 0:00:00 lr 0.000900	time 0.4574 (0.4676)	loss 4.8412 (4.4033)	grad_norm 1.8158 (1.4277)	mem 14851MB
[2022-11-05 22:36:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 17 training takes 0:09:45
[2022-11-05 22:36:32 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_17.pth saving......
[2022-11-05 22:36:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_17.pth saved !!!
[2022-11-05 22:36:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.539 (1.539)	Loss 1.6466 (1.6466)	Acc@1 65.625 (65.625)	Acc@5 86.523 (86.523)	Mem 14851MB
[2022-11-05 22:36:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 62.866 Acc@5 85.092
[2022-11-05 22:36:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 62.9%
[2022-11-05 22:36:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.530 (1.530)	Loss 3.9783 (3.9783)	Acc@1 23.438 (23.438)	Acc@5 48.145 (48.145)	Mem 14851MB
[2022-11-05 22:36:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 24.128 Acc@5 47.642
[2022-11-05 22:36:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 24.1%
[2022-11-05 22:36:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 62.87% at 17 epoch
[2022-11-05 22:36:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][0/1251]	eta 0:42:57 lr 0.000900	time 2.0605 (2.0605)	loss 4.3715 (4.3715)	grad_norm 1.4066 (1.4066)	mem 14851MB
[2022-11-05 22:37:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][50/1251]	eta 0:10:03 lr 0.000902	time 0.4583 (0.5026)	loss 3.8353 (4.2431)	grad_norm 1.5317 (1.4409)	mem 14851MB
[2022-11-05 22:37:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][100/1251]	eta 0:09:21 lr 0.000904	time 0.4626 (0.4874)	loss 4.0887 (4.3088)	grad_norm 1.5438 (1.4032)	mem 14851MB
[2022-11-05 22:38:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][150/1251]	eta 0:08:48 lr 0.000906	time 0.4573 (0.4800)	loss 4.6668 (4.3721)	grad_norm 1.2299 (1.4261)	mem 14851MB
[2022-11-05 22:38:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][200/1251]	eta 0:08:21 lr 0.000908	time 0.4634 (0.4769)	loss 4.9597 (4.3169)	grad_norm 1.2606 (1.4283)	mem 14851MB
[2022-11-05 22:38:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][250/1251]	eta 0:07:54 lr 0.000910	time 0.4655 (0.4743)	loss 3.7624 (4.3367)	grad_norm 1.3109 (1.4321)	mem 14851MB
[2022-11-05 22:39:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][300/1251]	eta 0:07:29 lr 0.000912	time 0.4611 (0.4730)	loss 4.5742 (4.3462)	grad_norm 1.2918 (1.4252)	mem 14851MB
[2022-11-05 22:39:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][350/1251]	eta 0:07:05 lr 0.000914	time 0.4605 (0.4723)	loss 5.2179 (4.3416)	grad_norm 1.3729 (1.4177)	mem 14851MB
[2022-11-05 22:39:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][400/1251]	eta 0:06:41 lr 0.000916	time 0.4760 (0.4713)	loss 4.7663 (4.3312)	grad_norm 1.4485 (1.4175)	mem 14851MB
[2022-11-05 22:40:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][450/1251]	eta 0:06:16 lr 0.000918	time 0.4644 (0.4705)	loss 4.6016 (4.3378)	grad_norm 1.4967 (1.4163)	mem 14851MB
[2022-11-05 22:40:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][500/1251]	eta 0:05:53 lr 0.000920	time 0.4844 (0.4701)	loss 4.3925 (4.3440)	grad_norm 1.3997 (nan)	mem 14851MB
[2022-11-05 22:41:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][550/1251]	eta 0:05:29 lr 0.000922	time 0.4712 (0.4700)	loss 4.6496 (4.3326)	grad_norm 1.4358 (nan)	mem 14851MB
[2022-11-05 22:41:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][600/1251]	eta 0:05:05 lr 0.000924	time 0.4607 (0.4696)	loss 5.4891 (4.3298)	grad_norm 1.3004 (nan)	mem 14851MB
[2022-11-05 22:41:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][650/1251]	eta 0:04:42 lr 0.000926	time 0.4560 (0.4695)	loss 3.1597 (4.3291)	grad_norm 1.5175 (nan)	mem 14851MB
[2022-11-05 22:42:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][700/1251]	eta 0:04:18 lr 0.000928	time 0.4751 (0.4692)	loss 4.8861 (4.3281)	grad_norm 1.4188 (nan)	mem 14851MB
[2022-11-05 22:42:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][750/1251]	eta 0:03:54 lr 0.000930	time 0.4634 (0.4690)	loss 3.9235 (4.3383)	grad_norm 1.4118 (nan)	mem 14851MB
[2022-11-05 22:43:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][800/1251]	eta 0:03:31 lr 0.000932	time 0.4649 (0.4689)	loss 4.2072 (4.3391)	grad_norm 1.2964 (nan)	mem 14851MB
[2022-11-05 22:43:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][850/1251]	eta 0:03:08 lr 0.000934	time 0.4643 (0.4689)	loss 4.9002 (4.3394)	grad_norm 1.8360 (nan)	mem 14851MB
[2022-11-05 22:43:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][900/1251]	eta 0:02:44 lr 0.000936	time 0.5610 (0.4689)	loss 4.9370 (4.3299)	grad_norm 1.1469 (nan)	mem 14851MB
[2022-11-05 22:44:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][950/1251]	eta 0:02:21 lr 0.000938	time 0.4697 (0.4687)	loss 4.8652 (4.3328)	grad_norm 1.2960 (nan)	mem 14851MB
[2022-11-05 22:44:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][1000/1251]	eta 0:01:57 lr 0.000940	time 0.4630 (0.4685)	loss 4.9598 (4.3313)	grad_norm 1.2580 (nan)	mem 14851MB
[2022-11-05 22:45:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][1050/1251]	eta 0:01:34 lr 0.000942	time 0.4636 (0.4686)	loss 4.3350 (4.3349)	grad_norm 1.5100 (nan)	mem 14851MB
[2022-11-05 22:45:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][1100/1251]	eta 0:01:10 lr 0.000944	time 0.4687 (0.4686)	loss 4.6602 (4.3426)	grad_norm 1.1668 (nan)	mem 14851MB
[2022-11-05 22:45:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][1150/1251]	eta 0:00:47 lr 0.000946	time 0.4651 (0.4685)	loss 4.2689 (4.3418)	grad_norm 1.3302 (nan)	mem 14851MB
[2022-11-05 22:46:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][1200/1251]	eta 0:00:23 lr 0.000948	time 0.4797 (0.4684)	loss 5.2474 (4.3450)	grad_norm 1.3474 (nan)	mem 14851MB
[2022-11-05 22:46:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [18/300][1250/1251]	eta 0:00:00 lr 0.000950	time 0.4561 (0.4682)	loss 4.0591 (4.3433)	grad_norm 1.3342 (nan)	mem 14851MB
[2022-11-05 22:46:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 18 training takes 0:09:45
[2022-11-05 22:46:36 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_18.pth saving......
[2022-11-05 22:46:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_18.pth saved !!!
[2022-11-05 22:46:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.692 (1.692)	Loss 1.6983 (1.6983)	Acc@1 62.988 (62.988)	Acc@5 85.449 (85.449)	Mem 14851MB
[2022-11-05 22:46:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 63.266 Acc@5 85.596
[2022-11-05 22:46:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 63.3%
[2022-11-05 22:46:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.544 (1.544)	Loss 3.5505 (3.5505)	Acc@1 31.152 (31.152)	Acc@5 54.297 (54.297)	Mem 14851MB
[2022-11-05 22:46:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 31.088 Acc@5 55.948
[2022-11-05 22:46:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 31.1%
[2022-11-05 22:46:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 63.27% at 18 epoch
[2022-11-05 22:46:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][0/1251]	eta 0:39:10 lr 0.000950	time 1.8786 (1.8786)	loss 4.0795 (4.0795)	grad_norm 1.9716 (1.9716)	mem 14851MB
[2022-11-05 22:47:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][50/1251]	eta 0:09:58 lr 0.000952	time 0.4590 (0.4987)	loss 4.2114 (4.1779)	grad_norm 1.3111 (1.4157)	mem 14851MB
[2022-11-05 22:47:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][100/1251]	eta 0:09:17 lr 0.000954	time 0.4666 (0.4843)	loss 4.2832 (4.2659)	grad_norm 1.4163 (1.4065)	mem 14851MB
[2022-11-05 22:48:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][150/1251]	eta 0:08:45 lr 0.000956	time 0.4603 (0.4774)	loss 4.2089 (4.2970)	grad_norm 1.5893 (1.4020)	mem 14851MB
[2022-11-05 22:48:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][200/1251]	eta 0:08:18 lr 0.000958	time 0.4713 (0.4747)	loss 3.4526 (4.3204)	grad_norm 1.2777 (1.3997)	mem 14851MB
[2022-11-05 22:48:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][250/1251]	eta 0:07:53 lr 0.000960	time 0.4548 (0.4730)	loss 3.7580 (4.3369)	grad_norm 1.6774 (1.3889)	mem 14851MB
[2022-11-05 22:49:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][300/1251]	eta 0:07:28 lr 0.000962	time 0.4588 (0.4716)	loss 3.5034 (4.3194)	grad_norm 1.7764 (1.3902)	mem 14851MB
[2022-11-05 22:49:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][350/1251]	eta 0:07:04 lr 0.000964	time 0.4772 (0.4706)	loss 4.4510 (4.3305)	grad_norm 1.2326 (1.3903)	mem 14851MB
[2022-11-05 22:50:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][400/1251]	eta 0:06:40 lr 0.000966	time 0.4718 (0.4703)	loss 3.6415 (4.3433)	grad_norm 1.4033 (1.3911)	mem 14851MB
[2022-11-05 22:50:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][450/1251]	eta 0:06:16 lr 0.000968	time 0.4691 (0.4698)	loss 4.5783 (4.3386)	grad_norm 1.3910 (1.3897)	mem 14851MB
[2022-11-05 22:50:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][500/1251]	eta 0:05:52 lr 0.000970	time 0.4547 (0.4693)	loss 4.6058 (4.3445)	grad_norm 1.4801 (1.3911)	mem 14851MB
[2022-11-05 22:51:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][550/1251]	eta 0:05:28 lr 0.000972	time 0.4608 (0.4691)	loss 4.2034 (4.3488)	grad_norm 1.2641 (1.3880)	mem 14851MB
[2022-11-05 22:51:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][600/1251]	eta 0:05:05 lr 0.000974	time 0.4617 (0.4689)	loss 4.1071 (4.3469)	grad_norm 1.5173 (1.3880)	mem 14851MB
[2022-11-05 22:51:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][650/1251]	eta 0:04:41 lr 0.000976	time 0.4633 (0.4688)	loss 4.7508 (4.3469)	grad_norm 1.1442 (1.3848)	mem 14851MB
[2022-11-05 22:52:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][700/1251]	eta 0:04:18 lr 0.000978	time 0.5446 (0.4686)	loss 4.8763 (4.3327)	grad_norm 1.3622 (1.3849)	mem 14851MB
[2022-11-05 22:52:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][750/1251]	eta 0:03:54 lr 0.000980	time 0.4642 (0.4685)	loss 4.4404 (4.3341)	grad_norm 1.1339 (1.3804)	mem 14851MB
[2022-11-05 22:53:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][800/1251]	eta 0:03:31 lr 0.000982	time 0.4573 (0.4684)	loss 4.5413 (4.3384)	grad_norm 1.2415 (1.3804)	mem 14851MB
[2022-11-05 22:53:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][850/1251]	eta 0:03:07 lr 0.000984	time 0.4519 (0.4682)	loss 5.1181 (4.3367)	grad_norm 1.2799 (1.3794)	mem 14851MB
[2022-11-05 22:53:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][900/1251]	eta 0:02:44 lr 0.000986	time 0.4682 (0.4681)	loss 4.2500 (4.3435)	grad_norm 1.1838 (1.3754)	mem 14851MB
[2022-11-05 22:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][950/1251]	eta 0:02:20 lr 0.000988	time 0.4608 (0.4682)	loss 5.0940 (4.3451)	grad_norm 1.2526 (1.3722)	mem 14851MB
[2022-11-05 22:54:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][1000/1251]	eta 0:01:57 lr 0.000990	time 0.4547 (0.4680)	loss 4.6782 (4.3540)	grad_norm 1.3427 (1.3720)	mem 14851MB
[2022-11-05 22:55:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][1050/1251]	eta 0:01:34 lr 0.000992	time 0.4690 (0.4680)	loss 5.0406 (4.3473)	grad_norm 1.3206 (1.3713)	mem 14851MB
[2022-11-05 22:55:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][1100/1251]	eta 0:01:10 lr 0.000994	time 0.4645 (0.4679)	loss 4.7999 (4.3476)	grad_norm 1.4046 (1.3696)	mem 14851MB
[2022-11-05 22:55:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][1150/1251]	eta 0:00:47 lr 0.000996	time 0.4693 (0.4677)	loss 4.0636 (4.3417)	grad_norm 1.2889 (1.3674)	mem 14851MB
[2022-11-05 22:56:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][1200/1251]	eta 0:00:23 lr 0.000998	time 0.5360 (0.4677)	loss 4.7755 (4.3438)	grad_norm 1.2339 (1.3640)	mem 14851MB
[2022-11-05 22:56:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [19/300][1250/1251]	eta 0:00:00 lr 0.001000	time 0.4563 (0.4675)	loss 4.7082 (4.3459)	grad_norm 1.3359 (1.3634)	mem 14851MB
[2022-11-05 22:56:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 19 training takes 0:09:44
[2022-11-05 22:56:39 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_19.pth saving......
[2022-11-05 22:56:40 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_19.pth saved !!!
[2022-11-05 22:56:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.585 (1.585)	Loss 1.6042 (1.6042)	Acc@1 64.551 (64.551)	Acc@5 85.742 (85.742)	Mem 14851MB
[2022-11-05 22:56:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 63.968 Acc@5 86.070
[2022-11-05 22:56:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 64.0%
[2022-11-05 22:56:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.770 (1.770)	Loss 3.0439 (3.0439)	Acc@1 39.551 (39.551)	Acc@5 64.551 (64.551)	Mem 14851MB
[2022-11-05 22:56:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 37.628 Acc@5 62.954
[2022-11-05 22:56:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 37.6%
[2022-11-05 22:56:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 63.97% at 19 epoch
[2022-11-05 22:56:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][0/1251]	eta 0:39:56 lr 0.000989	time 1.9156 (1.9156)	loss 3.9473 (3.9473)	grad_norm 1.3218 (1.3218)	mem 14851MB
[2022-11-05 22:57:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][50/1251]	eta 0:09:58 lr 0.000989	time 0.4632 (0.4980)	loss 3.1231 (4.2078)	grad_norm 1.3214 (1.3919)	mem 14851MB
[2022-11-05 22:57:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][100/1251]	eta 0:09:15 lr 0.000989	time 0.4645 (0.4825)	loss 4.3177 (4.1929)	grad_norm 1.4178 (1.3507)	mem 14851MB
[2022-11-05 22:58:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][150/1251]	eta 0:08:45 lr 0.000989	time 0.4633 (0.4777)	loss 4.0430 (4.2133)	grad_norm 1.2967 (1.3512)	mem 14851MB
[2022-11-05 22:58:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][200/1251]	eta 0:08:18 lr 0.000989	time 0.4652 (0.4741)	loss 3.9176 (4.2584)	grad_norm 1.1671 (1.3400)	mem 14851MB
[2022-11-05 22:58:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][250/1251]	eta 0:07:52 lr 0.000989	time 0.4615 (0.4722)	loss 4.5203 (4.2691)	grad_norm 1.0837 (1.3260)	mem 14851MB
[2022-11-05 22:59:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][300/1251]	eta 0:07:28 lr 0.000989	time 0.4608 (0.4719)	loss 3.1811 (4.2775)	grad_norm 1.3769 (nan)	mem 14851MB
[2022-11-05 22:59:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][350/1251]	eta 0:07:04 lr 0.000989	time 0.4698 (0.4711)	loss 4.7414 (4.2839)	grad_norm 1.2394 (nan)	mem 14851MB
[2022-11-05 23:00:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][400/1251]	eta 0:06:40 lr 0.000989	time 0.4609 (0.4703)	loss 4.7646 (4.2749)	grad_norm 1.5011 (nan)	mem 14851MB
[2022-11-05 23:00:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][450/1251]	eta 0:06:16 lr 0.000989	time 0.4532 (0.4697)	loss 5.0557 (4.2757)	grad_norm 1.2092 (nan)	mem 14851MB
[2022-11-05 23:00:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][500/1251]	eta 0:05:52 lr 0.000989	time 0.4698 (0.4692)	loss 4.5500 (4.2631)	grad_norm 1.3737 (nan)	mem 14851MB
[2022-11-05 23:01:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][550/1251]	eta 0:05:28 lr 0.000989	time 0.4642 (0.4691)	loss 4.0154 (4.2533)	grad_norm 1.2181 (nan)	mem 14851MB
[2022-11-05 23:01:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][600/1251]	eta 0:05:05 lr 0.000989	time 0.4626 (0.4688)	loss 4.2190 (4.2624)	grad_norm 1.2285 (nan)	mem 14851MB
[2022-11-05 23:02:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][650/1251]	eta 0:04:41 lr 0.000989	time 0.4638 (0.4689)	loss 5.1426 (4.2578)	grad_norm 1.1550 (nan)	mem 14851MB
[2022-11-05 23:02:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][700/1251]	eta 0:04:18 lr 0.000989	time 0.4679 (0.4685)	loss 4.8287 (4.2590)	grad_norm 1.2310 (nan)	mem 14851MB
[2022-11-05 23:02:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][750/1251]	eta 0:03:54 lr 0.000989	time 0.4509 (0.4684)	loss 4.9228 (4.2589)	grad_norm 1.1863 (nan)	mem 14851MB
[2022-11-05 23:03:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][800/1251]	eta 0:03:31 lr 0.000988	time 0.4637 (0.4684)	loss 3.7863 (4.2587)	grad_norm 1.2655 (nan)	mem 14851MB
[2022-11-05 23:03:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][850/1251]	eta 0:03:07 lr 0.000988	time 0.4720 (0.4683)	loss 4.6233 (4.2670)	grad_norm 1.3282 (nan)	mem 14851MB
[2022-11-05 23:03:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][900/1251]	eta 0:02:44 lr 0.000988	time 0.4716 (0.4682)	loss 4.7995 (4.2553)	grad_norm 1.2059 (nan)	mem 14851MB
[2022-11-05 23:04:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][950/1251]	eta 0:02:20 lr 0.000988	time 0.4678 (0.4680)	loss 4.0106 (4.2524)	grad_norm 1.1636 (nan)	mem 14851MB
[2022-11-05 23:04:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][1000/1251]	eta 0:01:57 lr 0.000988	time 0.4610 (0.4680)	loss 4.9202 (4.2452)	grad_norm 1.2920 (nan)	mem 14851MB
[2022-11-05 23:05:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][1050/1251]	eta 0:01:34 lr 0.000988	time 0.4640 (0.4680)	loss 4.5597 (4.2434)	grad_norm 1.3076 (nan)	mem 14851MB
[2022-11-05 23:05:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][1100/1251]	eta 0:01:10 lr 0.000988	time 0.4671 (0.4679)	loss 4.3217 (4.2451)	grad_norm 1.3787 (nan)	mem 14851MB
[2022-11-05 23:05:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][1150/1251]	eta 0:00:47 lr 0.000988	time 0.4631 (0.4679)	loss 5.0804 (4.2452)	grad_norm 1.1273 (nan)	mem 14851MB
[2022-11-05 23:06:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][1200/1251]	eta 0:00:23 lr 0.000988	time 0.4675 (0.4680)	loss 4.1171 (4.2461)	grad_norm 1.2102 (nan)	mem 14851MB
[2022-11-05 23:06:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [20/300][1250/1251]	eta 0:00:00 lr 0.000988	time 0.4580 (0.4678)	loss 4.5224 (4.2501)	grad_norm 1.2023 (nan)	mem 14851MB
[2022-11-05 23:06:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 20 training takes 0:09:45
[2022-11-05 23:06:43 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_20.pth saving......
[2022-11-05 23:06:43 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_20.pth saved !!!
[2022-11-05 23:06:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.594 (1.594)	Loss 1.5754 (1.5754)	Acc@1 65.527 (65.527)	Acc@5 86.133 (86.133)	Mem 14851MB
[2022-11-05 23:06:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 64.818 Acc@5 86.942
[2022-11-05 23:06:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 64.8%
[2022-11-05 23:06:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.582 (1.582)	Loss 2.8226 (2.8226)	Acc@1 41.309 (41.309)	Acc@5 67.676 (67.676)	Mem 14851MB
[2022-11-05 23:07:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 43.184 Acc@5 68.494
[2022-11-05 23:07:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 43.2%
[2022-11-05 23:07:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 64.82% at 20 epoch
[2022-11-05 23:07:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][0/1251]	eta 0:44:33 lr 0.000988	time 2.1372 (2.1372)	loss 3.6838 (3.6838)	grad_norm 1.5943 (1.5943)	mem 14851MB
[2022-11-05 23:07:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][50/1251]	eta 0:10:01 lr 0.000988	time 0.4755 (0.5009)	loss 4.4822 (4.3295)	grad_norm 1.3064 (1.3604)	mem 14851MB
[2022-11-05 23:07:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][100/1251]	eta 0:09:19 lr 0.000988	time 0.4627 (0.4857)	loss 4.4334 (4.3208)	grad_norm 1.2708 (1.3360)	mem 14851MB
[2022-11-05 23:08:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][150/1251]	eta 0:08:48 lr 0.000988	time 0.4643 (0.4800)	loss 4.7352 (4.3219)	grad_norm 1.2907 (1.3339)	mem 14851MB
[2022-11-05 23:08:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][200/1251]	eta 0:08:21 lr 0.000988	time 0.4543 (0.4770)	loss 4.4596 (4.3125)	grad_norm 1.1611 (1.3178)	mem 14851MB
[2022-11-05 23:09:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][250/1251]	eta 0:07:55 lr 0.000988	time 0.4696 (0.4747)	loss 4.2512 (4.2825)	grad_norm 1.5695 (1.3184)	mem 14851MB
[2022-11-05 23:09:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][300/1251]	eta 0:07:30 lr 0.000988	time 0.5449 (0.4734)	loss 4.0845 (4.2943)	grad_norm 1.2004 (1.3108)	mem 14851MB
[2022-11-05 23:09:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][350/1251]	eta 0:07:05 lr 0.000988	time 0.4556 (0.4721)	loss 3.9095 (4.2792)	grad_norm 1.5118 (1.3147)	mem 14851MB
[2022-11-05 23:10:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][400/1251]	eta 0:06:41 lr 0.000988	time 0.4635 (0.4716)	loss 3.8289 (4.2771)	grad_norm 1.0354 (1.3107)	mem 14851MB
[2022-11-05 23:10:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][450/1251]	eta 0:06:17 lr 0.000988	time 0.4631 (0.4709)	loss 4.7184 (4.2856)	grad_norm 1.5107 (1.3106)	mem 14851MB
[2022-11-05 23:10:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][500/1251]	eta 0:05:53 lr 0.000988	time 0.4605 (0.4704)	loss 4.5860 (4.2733)	grad_norm 1.2168 (1.3105)	mem 14851MB
[2022-11-05 23:11:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][550/1251]	eta 0:05:29 lr 0.000988	time 0.4637 (0.4699)	loss 4.1395 (4.2760)	grad_norm 1.2783 (1.3090)	mem 14851MB
[2022-11-05 23:11:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][600/1251]	eta 0:05:05 lr 0.000988	time 0.4598 (0.4698)	loss 4.9194 (4.2789)	grad_norm 1.1284 (1.3101)	mem 14851MB
[2022-11-05 23:12:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][650/1251]	eta 0:04:42 lr 0.000987	time 0.4535 (0.4696)	loss 4.7002 (4.2802)	grad_norm 1.3730 (1.3099)	mem 14851MB
[2022-11-05 23:12:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][700/1251]	eta 0:04:18 lr 0.000987	time 0.4743 (0.4695)	loss 3.6928 (4.2787)	grad_norm 1.2808 (1.3093)	mem 14851MB
[2022-11-05 23:12:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][750/1251]	eta 0:03:55 lr 0.000987	time 0.4587 (0.4693)	loss 4.7864 (4.2693)	grad_norm 1.3304 (1.3088)	mem 14851MB
[2022-11-05 23:13:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][800/1251]	eta 0:03:31 lr 0.000987	time 0.4664 (0.4690)	loss 3.2628 (4.2681)	grad_norm 1.3056 (1.3071)	mem 14851MB
[2022-11-05 23:13:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][850/1251]	eta 0:03:08 lr 0.000987	time 0.4604 (0.4691)	loss 4.2065 (4.2762)	grad_norm 1.2398 (1.3087)	mem 14851MB
[2022-11-05 23:14:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][900/1251]	eta 0:02:44 lr 0.000987	time 0.5363 (0.4690)	loss 4.2534 (4.2784)	grad_norm 1.0543 (1.3087)	mem 14851MB
[2022-11-05 23:14:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][950/1251]	eta 0:02:21 lr 0.000987	time 0.4669 (0.4690)	loss 3.0265 (4.2712)	grad_norm 1.3828 (1.3070)	mem 14851MB
[2022-11-05 23:14:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][1000/1251]	eta 0:01:57 lr 0.000987	time 0.4535 (0.4688)	loss 4.6001 (4.2681)	grad_norm 1.2365 (1.3087)	mem 14851MB
[2022-11-05 23:15:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][1050/1251]	eta 0:01:34 lr 0.000987	time 0.4689 (0.4686)	loss 5.1025 (4.2641)	grad_norm 1.2246 (1.3058)	mem 14851MB
[2022-11-05 23:15:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][1100/1251]	eta 0:01:10 lr 0.000987	time 0.4626 (0.4686)	loss 4.9923 (4.2599)	grad_norm 1.4987 (1.3053)	mem 14851MB
[2022-11-05 23:16:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][1150/1251]	eta 0:00:47 lr 0.000987	time 0.4620 (0.4685)	loss 3.7137 (4.2608)	grad_norm 1.2066 (1.3047)	mem 14851MB
[2022-11-05 23:16:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][1200/1251]	eta 0:00:23 lr 0.000987	time 0.4685 (0.4685)	loss 4.1592 (4.2565)	grad_norm 1.6061 (1.3046)	mem 14851MB
[2022-11-05 23:16:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [21/300][1250/1251]	eta 0:00:00 lr 0.000987	time 0.5414 (0.4683)	loss 3.8275 (4.2558)	grad_norm 1.4123 (1.3035)	mem 14851MB
[2022-11-05 23:16:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 21 training takes 0:09:46
[2022-11-05 23:16:47 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_21.pth saving......
[2022-11-05 23:16:48 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_21.pth saved !!!
[2022-11-05 23:16:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.601 (1.601)	Loss 1.5898 (1.5898)	Acc@1 65.625 (65.625)	Acc@5 87.402 (87.402)	Mem 14851MB
[2022-11-05 23:16:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 65.672 Acc@5 87.030
[2022-11-05 23:16:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 65.7%
[2022-11-05 23:16:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.662 (1.662)	Loss 2.5220 (2.5220)	Acc@1 48.047 (48.047)	Acc@5 72.168 (72.168)	Mem 14851MB
[2022-11-05 23:17:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 47.900 Acc@5 73.036
[2022-11-05 23:17:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 47.9%
[2022-11-05 23:17:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 65.67% at 21 epoch
[2022-11-05 23:17:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][0/1251]	eta 0:40:54 lr 0.000987	time 1.9622 (1.9622)	loss 5.0182 (5.0182)	grad_norm 1.2829 (1.2829)	mem 14851MB
[2022-11-05 23:17:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][50/1251]	eta 0:10:01 lr 0.000987	time 0.4437 (0.5007)	loss 3.9054 (4.3036)	grad_norm nan (nan)	mem 14851MB
[2022-11-05 23:17:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][100/1251]	eta 0:09:17 lr 0.000987	time 0.4693 (0.4840)	loss 4.5378 (4.2060)	grad_norm 1.1402 (nan)	mem 14851MB
[2022-11-05 23:18:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][150/1251]	eta 0:08:47 lr 0.000987	time 0.4672 (0.4791)	loss 3.3654 (4.2169)	grad_norm 1.2590 (nan)	mem 14851MB
[2022-11-05 23:18:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][200/1251]	eta 0:08:19 lr 0.000987	time 0.4622 (0.4755)	loss 4.3168 (4.2010)	grad_norm 1.2190 (nan)	mem 14851MB
[2022-11-05 23:19:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][250/1251]	eta 0:07:54 lr 0.000987	time 0.4654 (0.4740)	loss 4.3606 (4.1863)	grad_norm 1.2117 (nan)	mem 14851MB
[2022-11-05 23:19:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][300/1251]	eta 0:07:29 lr 0.000987	time 0.4643 (0.4726)	loss 4.1287 (4.2015)	grad_norm 1.6256 (nan)	mem 14851MB
[2022-11-05 23:19:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][350/1251]	eta 0:07:05 lr 0.000987	time 0.4645 (0.4718)	loss 3.8106 (4.2207)	grad_norm 1.4192 (nan)	mem 14851MB
[2022-11-05 23:20:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][400/1251]	eta 0:06:40 lr 0.000987	time 0.4655 (0.4708)	loss 4.7160 (4.2114)	grad_norm 1.2693 (nan)	mem 14851MB
[2022-11-05 23:20:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][450/1251]	eta 0:06:16 lr 0.000986	time 0.4721 (0.4706)	loss 4.2859 (4.2105)	grad_norm 1.4031 (nan)	mem 14851MB
[2022-11-05 23:21:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][500/1251]	eta 0:05:53 lr 0.000986	time 0.4732 (0.4700)	loss 4.5848 (4.2084)	grad_norm 1.1737 (nan)	mem 14851MB
[2022-11-05 23:21:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][550/1251]	eta 0:05:29 lr 0.000986	time 0.4652 (0.4697)	loss 3.2257 (4.2014)	grad_norm 1.2250 (nan)	mem 14851MB
[2022-11-05 23:21:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][600/1251]	eta 0:05:05 lr 0.000986	time 0.4634 (0.4696)	loss 4.1053 (4.2052)	grad_norm 1.1736 (nan)	mem 14851MB
[2022-11-05 23:22:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][650/1251]	eta 0:04:42 lr 0.000986	time 0.4661 (0.4694)	loss 5.0436 (4.2023)	grad_norm 1.4263 (nan)	mem 14851MB
[2022-11-05 23:22:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][700/1251]	eta 0:04:18 lr 0.000986	time 0.4582 (0.4692)	loss 4.3964 (4.1990)	grad_norm 1.3172 (nan)	mem 14851MB
[2022-11-05 23:22:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][750/1251]	eta 0:03:55 lr 0.000986	time 0.4659 (0.4692)	loss 2.6521 (4.2007)	grad_norm 1.3053 (nan)	mem 14851MB
[2022-11-05 23:23:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][800/1251]	eta 0:03:31 lr 0.000986	time 0.4736 (0.4690)	loss 4.5838 (4.2013)	grad_norm 1.2147 (nan)	mem 14851MB
[2022-11-05 23:23:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][850/1251]	eta 0:03:08 lr 0.000986	time 0.4723 (0.4691)	loss 4.3653 (4.1966)	grad_norm 1.7428 (nan)	mem 14851MB
[2022-11-05 23:24:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][900/1251]	eta 0:02:44 lr 0.000986	time 0.4550 (0.4688)	loss 4.4925 (4.1920)	grad_norm 1.1918 (nan)	mem 14851MB
[2022-11-05 23:24:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][950/1251]	eta 0:02:21 lr 0.000986	time 0.4619 (0.4687)	loss 4.4835 (4.1897)	grad_norm 1.4162 (nan)	mem 14851MB
[2022-11-05 23:24:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][1000/1251]	eta 0:01:57 lr 0.000986	time 0.4674 (0.4686)	loss 4.4632 (4.1905)	grad_norm 1.1144 (nan)	mem 14851MB
[2022-11-05 23:25:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][1050/1251]	eta 0:01:34 lr 0.000986	time 0.4596 (0.4685)	loss 4.6839 (4.1918)	grad_norm 1.3734 (nan)	mem 14851MB
[2022-11-05 23:25:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][1100/1251]	eta 0:01:10 lr 0.000986	time 0.4702 (0.4685)	loss 5.0167 (4.1989)	grad_norm 1.2601 (nan)	mem 14851MB
[2022-11-05 23:26:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][1150/1251]	eta 0:00:47 lr 0.000986	time 0.4763 (0.4685)	loss 3.8461 (4.1988)	grad_norm 1.3311 (nan)	mem 14851MB
[2022-11-05 23:26:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][1200/1251]	eta 0:00:23 lr 0.000986	time 0.4707 (0.4684)	loss 4.4028 (4.2043)	grad_norm 1.4446 (nan)	mem 14851MB
[2022-11-05 23:26:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [22/300][1250/1251]	eta 0:00:00 lr 0.000986	time 0.4590 (0.4682)	loss 4.4830 (4.2023)	grad_norm 1.1982 (nan)	mem 14851MB
[2022-11-05 23:26:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 22 training takes 0:09:45
[2022-11-05 23:26:51 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_22.pth saving......
[2022-11-05 23:26:52 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_22.pth saved !!!
[2022-11-05 23:26:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.536 (1.536)	Loss 1.5600 (1.5600)	Acc@1 65.234 (65.234)	Acc@5 87.207 (87.207)	Mem 14851MB
[2022-11-05 23:27:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 65.970 Acc@5 87.748
[2022-11-05 23:27:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 66.0%
[2022-11-05 23:27:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.631 (1.631)	Loss 2.3082 (2.3082)	Acc@1 50.391 (50.391)	Acc@5 75.586 (75.586)	Mem 14851MB
[2022-11-05 23:27:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 51.890 Acc@5 76.646
[2022-11-05 23:27:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 51.9%
[2022-11-05 23:27:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 65.97% at 22 epoch
[2022-11-05 23:27:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][0/1251]	eta 0:41:41 lr 0.000986	time 1.9995 (1.9995)	loss 3.5459 (3.5459)	grad_norm 1.1224 (1.1224)	mem 14851MB
[2022-11-05 23:27:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][50/1251]	eta 0:09:58 lr 0.000986	time 0.4767 (0.4981)	loss 4.6852 (4.1298)	grad_norm 1.4223 (1.2508)	mem 14851MB
[2022-11-05 23:27:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][100/1251]	eta 0:09:15 lr 0.000986	time 0.4527 (0.4828)	loss 4.0578 (4.1815)	grad_norm 1.1871 (1.2607)	mem 14851MB
[2022-11-05 23:28:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][150/1251]	eta 0:08:46 lr 0.000986	time 0.4543 (0.4781)	loss 3.3579 (4.1131)	grad_norm 1.2277 (1.2604)	mem 14851MB
[2022-11-05 23:28:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][200/1251]	eta 0:08:19 lr 0.000986	time 0.4656 (0.4751)	loss 4.3601 (4.1172)	grad_norm 1.4115 (1.2650)	mem 14851MB
[2022-11-05 23:29:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][250/1251]	eta 0:07:54 lr 0.000985	time 0.4612 (0.4736)	loss 4.2686 (4.1254)	grad_norm 1.2935 (1.2721)	mem 14851MB
[2022-11-05 23:29:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][300/1251]	eta 0:07:29 lr 0.000985	time 0.4711 (0.4725)	loss 4.0910 (4.1371)	grad_norm 1.2133 (1.2733)	mem 14851MB
[2022-11-05 23:29:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][350/1251]	eta 0:07:04 lr 0.000985	time 0.4651 (0.4716)	loss 3.0110 (4.1449)	grad_norm 1.2293 (1.2743)	mem 14851MB
[2022-11-05 23:30:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][400/1251]	eta 0:06:40 lr 0.000985	time 0.4622 (0.4711)	loss 4.8182 (4.1526)	grad_norm 1.3043 (1.2782)	mem 14851MB
[2022-11-05 23:30:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][450/1251]	eta 0:06:16 lr 0.000985	time 0.4667 (0.4704)	loss 3.2564 (4.1437)	grad_norm 1.2405 (1.2742)	mem 14851MB
[2022-11-05 23:31:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][500/1251]	eta 0:05:52 lr 0.000985	time 0.4605 (0.4698)	loss 4.3398 (4.1459)	grad_norm 1.5193 (1.2742)	mem 14851MB
[2022-11-05 23:31:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][550/1251]	eta 0:05:29 lr 0.000985	time 0.4629 (0.4695)	loss 3.5305 (4.1568)	grad_norm 1.4429 (1.2772)	mem 14851MB
[2022-11-05 23:31:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][600/1251]	eta 0:05:05 lr 0.000985	time 0.4517 (0.4694)	loss 5.1061 (4.1699)	grad_norm 1.2528 (1.2815)	mem 14851MB
[2022-11-05 23:32:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][650/1251]	eta 0:04:41 lr 0.000985	time 0.4674 (0.4689)	loss 4.4714 (4.1651)	grad_norm 1.2575 (1.2821)	mem 14851MB
[2022-11-05 23:32:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][700/1251]	eta 0:04:18 lr 0.000985	time 0.4643 (0.4688)	loss 4.5877 (4.1602)	grad_norm 1.1937 (1.2809)	mem 14851MB
[2022-11-05 23:33:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][750/1251]	eta 0:03:54 lr 0.000985	time 0.4707 (0.4686)	loss 4.1836 (4.1667)	grad_norm 1.1966 (1.2805)	mem 14851MB
[2022-11-05 23:33:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][800/1251]	eta 0:03:31 lr 0.000985	time 0.4628 (0.4685)	loss 4.3996 (4.1653)	grad_norm 1.1385 (1.2812)	mem 14851MB
[2022-11-05 23:33:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][850/1251]	eta 0:03:07 lr 0.000985	time 0.4770 (0.4683)	loss 3.1285 (4.1568)	grad_norm 1.0462 (inf)	mem 14851MB
[2022-11-05 23:34:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][900/1251]	eta 0:02:44 lr 0.000985	time 0.4643 (0.4681)	loss 4.2720 (4.1609)	grad_norm 1.1786 (inf)	mem 14851MB
[2022-11-05 23:34:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][950/1251]	eta 0:02:20 lr 0.000985	time 0.4623 (0.4682)	loss 4.8780 (4.1678)	grad_norm 1.3814 (inf)	mem 14851MB
[2022-11-05 23:34:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][1000/1251]	eta 0:01:57 lr 0.000985	time 0.5242 (0.4681)	loss 3.1219 (4.1617)	grad_norm 1.1811 (inf)	mem 14851MB
[2022-11-05 23:35:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][1050/1251]	eta 0:01:34 lr 0.000985	time 0.4603 (0.4682)	loss 5.0133 (4.1640)	grad_norm 1.0660 (inf)	mem 14851MB
[2022-11-05 23:35:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][1100/1251]	eta 0:01:10 lr 0.000985	time 0.4596 (0.4680)	loss 4.9726 (4.1650)	grad_norm 1.2787 (inf)	mem 14851MB
[2022-11-05 23:36:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][1150/1251]	eta 0:00:47 lr 0.000985	time 0.4659 (0.4679)	loss 3.9924 (4.1586)	grad_norm 1.1585 (inf)	mem 14851MB
[2022-11-05 23:36:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][1200/1251]	eta 0:00:23 lr 0.000985	time 0.4631 (0.4679)	loss 3.1883 (4.1618)	grad_norm 1.0507 (inf)	mem 14851MB
[2022-11-05 23:36:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [23/300][1250/1251]	eta 0:00:00 lr 0.000984	time 0.4570 (0.4678)	loss 3.7203 (4.1628)	grad_norm 1.1673 (inf)	mem 14851MB
[2022-11-05 23:36:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 23 training takes 0:09:45
[2022-11-05 23:36:55 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_23.pth saving......
[2022-11-05 23:36:55 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_23.pth saved !!!
[2022-11-05 23:36:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.721 (1.721)	Loss 1.5385 (1.5385)	Acc@1 66.895 (66.895)	Acc@5 87.500 (87.500)	Mem 14851MB
[2022-11-05 23:37:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 67.228 Acc@5 88.188
[2022-11-05 23:37:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 67.2%
[2022-11-05 23:37:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.601 (1.601)	Loss 2.0956 (2.0956)	Acc@1 54.492 (54.492)	Acc@5 79.297 (79.297)	Mem 14851MB
[2022-11-05 23:37:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 55.020 Acc@5 79.106
[2022-11-05 23:37:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 55.0%
[2022-11-05 23:37:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 67.23% at 23 epoch
[2022-11-05 23:37:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][0/1251]	eta 0:45:33 lr 0.000984	time 2.1850 (2.1850)	loss 4.4537 (4.4537)	grad_norm 1.2492 (1.2492)	mem 14851MB
[2022-11-05 23:37:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][50/1251]	eta 0:10:03 lr 0.000984	time 0.4555 (0.5026)	loss 4.8905 (4.1658)	grad_norm 1.1741 (1.2180)	mem 14851MB
[2022-11-05 23:38:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][100/1251]	eta 0:09:20 lr 0.000984	time 0.4688 (0.4872)	loss 4.4011 (4.1667)	grad_norm 1.3525 (1.2396)	mem 14851MB
[2022-11-05 23:38:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][150/1251]	eta 0:08:49 lr 0.000984	time 0.5524 (0.4811)	loss 4.3684 (4.1916)	grad_norm 1.2115 (1.2555)	mem 14851MB
[2022-11-05 23:38:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][200/1251]	eta 0:08:21 lr 0.000984	time 0.4754 (0.4775)	loss 2.6801 (4.1614)	grad_norm 1.1936 (1.2487)	mem 14851MB
[2022-11-05 23:39:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][250/1251]	eta 0:07:55 lr 0.000984	time 0.4542 (0.4752)	loss 3.6602 (4.1633)	grad_norm 1.1425 (1.2509)	mem 14851MB
[2022-11-05 23:39:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][300/1251]	eta 0:07:30 lr 0.000984	time 0.4745 (0.4736)	loss 4.0197 (4.1557)	grad_norm 1.2817 (1.2491)	mem 14851MB
[2022-11-05 23:39:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][350/1251]	eta 0:07:05 lr 0.000984	time 0.4736 (0.4723)	loss 3.1327 (4.1571)	grad_norm 1.4671 (1.2554)	mem 14851MB
[2022-11-05 23:40:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][400/1251]	eta 0:06:41 lr 0.000984	time 0.4658 (0.4715)	loss 4.1675 (4.1516)	grad_norm 1.1858 (1.2519)	mem 14851MB
[2022-11-05 23:40:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][450/1251]	eta 0:06:17 lr 0.000984	time 0.4712 (0.4711)	loss 4.1234 (4.1538)	grad_norm 1.2880 (1.2485)	mem 14851MB
[2022-11-05 23:41:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][500/1251]	eta 0:05:53 lr 0.000984	time 0.4629 (0.4707)	loss 2.9346 (4.1421)	grad_norm 1.2681 (1.2516)	mem 14851MB
[2022-11-05 23:41:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][550/1251]	eta 0:05:29 lr 0.000984	time 0.4623 (0.4705)	loss 4.3752 (4.1280)	grad_norm 1.5210 (1.2534)	mem 14851MB
[2022-11-05 23:41:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][600/1251]	eta 0:05:06 lr 0.000984	time 0.4663 (0.4702)	loss 4.2134 (4.1306)	grad_norm 1.2674 (1.2532)	mem 14851MB
[2022-11-05 23:42:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][650/1251]	eta 0:04:42 lr 0.000984	time 0.4597 (0.4700)	loss 4.8984 (4.1336)	grad_norm 1.2113 (1.2543)	mem 14851MB
[2022-11-05 23:42:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][700/1251]	eta 0:04:18 lr 0.000984	time 0.4628 (0.4696)	loss 4.8295 (4.1283)	grad_norm 1.5817 (1.2555)	mem 14851MB
[2022-11-05 23:43:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][750/1251]	eta 0:03:55 lr 0.000984	time 0.4590 (0.4694)	loss 4.3488 (4.1358)	grad_norm 1.0470 (1.2549)	mem 14851MB
[2022-11-05 23:43:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][800/1251]	eta 0:03:31 lr 0.000984	time 0.4690 (0.4694)	loss 4.3415 (4.1499)	grad_norm 1.1033 (1.2542)	mem 14851MB
[2022-11-05 23:43:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][850/1251]	eta 0:03:08 lr 0.000984	time 0.4621 (0.4692)	loss 4.0066 (4.1563)	grad_norm 1.1538 (1.2527)	mem 14851MB
[2022-11-05 23:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][900/1251]	eta 0:02:44 lr 0.000984	time 0.4712 (0.4691)	loss 3.0534 (4.1541)	grad_norm 1.1712 (1.2522)	mem 14851MB
[2022-11-05 23:44:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][950/1251]	eta 0:02:21 lr 0.000983	time 0.4690 (0.4690)	loss 4.2840 (4.1480)	grad_norm 1.4555 (1.2523)	mem 14851MB
[2022-11-05 23:45:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][1000/1251]	eta 0:01:57 lr 0.000983	time 0.4690 (0.4688)	loss 5.0728 (4.1436)	grad_norm 1.3033 (1.2528)	mem 14851MB
[2022-11-05 23:45:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][1050/1251]	eta 0:01:34 lr 0.000983	time 0.4618 (0.4689)	loss 3.5449 (4.1505)	grad_norm 1.4209 (1.2541)	mem 14851MB
[2022-11-05 23:45:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][1100/1251]	eta 0:01:10 lr 0.000983	time 0.4713 (0.4689)	loss 4.4012 (4.1572)	grad_norm 1.1657 (nan)	mem 14851MB
[2022-11-05 23:46:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][1150/1251]	eta 0:00:47 lr 0.000983	time 0.4628 (0.4688)	loss 5.0822 (4.1603)	grad_norm 1.2358 (nan)	mem 14851MB
[2022-11-05 23:46:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][1200/1251]	eta 0:00:23 lr 0.000983	time 0.4627 (0.4688)	loss 4.3742 (4.1520)	grad_norm 1.0853 (nan)	mem 14851MB
[2022-11-05 23:46:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [24/300][1250/1251]	eta 0:00:00 lr 0.000983	time 0.4567 (0.4686)	loss 3.8062 (4.1533)	grad_norm 1.3310 (nan)	mem 14851MB
[2022-11-05 23:46:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 24 training takes 0:09:46
[2022-11-05 23:46:59 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_24.pth saving......
[2022-11-05 23:47:00 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_24.pth saved !!!
[2022-11-05 23:47:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.660 (1.660)	Loss 1.4875 (1.4875)	Acc@1 67.090 (67.090)	Acc@5 87.793 (87.793)	Mem 14851MB
[2022-11-05 23:47:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 67.702 Acc@5 88.466
[2022-11-05 23:47:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 67.7%
[2022-11-05 23:47:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.644 (1.644)	Loss 1.9599 (1.9599)	Acc@1 59.570 (59.570)	Acc@5 79.883 (79.883)	Mem 14851MB
[2022-11-05 23:47:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 57.682 Acc@5 81.252
[2022-11-05 23:47:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 57.7%
[2022-11-05 23:47:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 67.70% at 24 epoch
[2022-11-05 23:47:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][0/1251]	eta 0:40:28 lr 0.000983	time 1.9413 (1.9413)	loss 5.1068 (5.1068)	grad_norm 1.1232 (1.1232)	mem 14851MB
[2022-11-05 23:47:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][50/1251]	eta 0:09:59 lr 0.000983	time 0.4655 (0.4993)	loss 3.4026 (4.0174)	grad_norm 1.3666 (1.2730)	mem 14851MB
[2022-11-05 23:48:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][100/1251]	eta 0:09:19 lr 0.000983	time 0.4599 (0.4858)	loss 5.1385 (4.1242)	grad_norm 1.2617 (1.2502)	mem 14851MB
[2022-11-05 23:48:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][150/1251]	eta 0:08:47 lr 0.000983	time 0.4583 (0.4791)	loss 4.4686 (4.1247)	grad_norm 1.4687 (1.2674)	mem 14851MB
[2022-11-05 23:48:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][200/1251]	eta 0:08:20 lr 0.000983	time 0.4887 (0.4762)	loss 4.3044 (4.1401)	grad_norm 1.3870 (1.2643)	mem 14851MB
[2022-11-05 23:49:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][250/1251]	eta 0:07:54 lr 0.000983	time 0.4595 (0.4739)	loss 4.3517 (4.1450)	grad_norm 1.2709 (1.2632)	mem 14851MB
[2022-11-05 23:49:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][300/1251]	eta 0:07:29 lr 0.000983	time 0.4605 (0.4726)	loss 4.7982 (4.1302)	grad_norm 1.2791 (1.2602)	mem 14851MB
[2022-11-05 23:50:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][350/1251]	eta 0:07:04 lr 0.000983	time 0.4609 (0.4717)	loss 5.0489 (4.1312)	grad_norm 1.4026 (1.2595)	mem 14851MB
[2022-11-05 23:50:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][400/1251]	eta 0:06:40 lr 0.000983	time 0.4625 (0.4710)	loss 5.1057 (4.1321)	grad_norm 1.0973 (1.2543)	mem 14851MB
[2022-11-05 23:50:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][450/1251]	eta 0:06:16 lr 0.000983	time 0.4590 (0.4703)	loss 4.4383 (4.1391)	grad_norm 1.1993 (1.2543)	mem 14851MB
[2022-11-05 23:51:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][500/1251]	eta 0:05:52 lr 0.000983	time 0.4737 (0.4698)	loss 4.5207 (4.1387)	grad_norm 1.1122 (1.2542)	mem 14851MB
[2022-11-05 23:51:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][550/1251]	eta 0:05:29 lr 0.000983	time 0.4639 (0.4698)	loss 3.8602 (4.1410)	grad_norm 1.2467 (1.2547)	mem 14851MB
[2022-11-05 23:51:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][600/1251]	eta 0:05:05 lr 0.000982	time 0.4539 (0.4694)	loss 4.5764 (4.1366)	grad_norm 1.0374 (1.2556)	mem 14851MB
[2022-11-05 23:52:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][650/1251]	eta 0:04:41 lr 0.000982	time 0.4576 (0.4692)	loss 4.0194 (4.1333)	grad_norm 1.2796 (1.2597)	mem 14851MB
[2022-11-05 23:52:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][700/1251]	eta 0:04:18 lr 0.000982	time 0.4678 (0.4688)	loss 2.9461 (4.1288)	grad_norm 1.2470 (1.2582)	mem 14851MB
[2022-11-05 23:53:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][750/1251]	eta 0:03:54 lr 0.000982	time 0.4740 (0.4687)	loss 3.6863 (4.1197)	grad_norm 1.2901 (1.2571)	mem 14851MB
[2022-11-05 23:53:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][800/1251]	eta 0:03:31 lr 0.000982	time 0.4681 (0.4688)	loss 4.0985 (4.1176)	grad_norm 1.1416 (1.2561)	mem 14851MB
[2022-11-05 23:53:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][850/1251]	eta 0:03:07 lr 0.000982	time 0.4643 (0.4687)	loss 3.2124 (4.1228)	grad_norm 1.2158 (1.2575)	mem 14851MB
[2022-11-05 23:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][900/1251]	eta 0:02:44 lr 0.000982	time 0.4573 (0.4685)	loss 4.1462 (4.1283)	grad_norm 1.3221 (1.2555)	mem 14851MB
[2022-11-05 23:54:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][950/1251]	eta 0:02:20 lr 0.000982	time 0.4620 (0.4684)	loss 3.8385 (4.1322)	grad_norm 1.3365 (1.2564)	mem 14851MB
[2022-11-05 23:55:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][1000/1251]	eta 0:01:57 lr 0.000982	time 0.4596 (0.4682)	loss 3.5212 (4.1304)	grad_norm 1.2264 (1.2568)	mem 14851MB
[2022-11-05 23:55:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][1050/1251]	eta 0:01:34 lr 0.000982	time 0.4600 (0.4682)	loss 2.9907 (4.1322)	grad_norm 1.2607 (1.2559)	mem 14851MB
[2022-11-05 23:55:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][1100/1251]	eta 0:01:10 lr 0.000982	time 0.4612 (0.4682)	loss 3.5373 (4.1326)	grad_norm 1.2705 (1.2537)	mem 14851MB
[2022-11-05 23:56:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][1150/1251]	eta 0:00:47 lr 0.000982	time 0.4653 (0.4680)	loss 3.4264 (4.1336)	grad_norm 1.3168 (1.2539)	mem 14851MB
[2022-11-05 23:56:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][1200/1251]	eta 0:00:23 lr 0.000982	time 0.4616 (0.4679)	loss 4.7621 (4.1331)	grad_norm 1.1979 (1.2529)	mem 14851MB
[2022-11-05 23:57:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [25/300][1250/1251]	eta 0:00:00 lr 0.000982	time 0.4594 (0.4678)	loss 4.5707 (4.1281)	grad_norm 1.0946 (1.2524)	mem 14851MB
[2022-11-05 23:57:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 25 training takes 0:09:45
[2022-11-05 23:57:03 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_25.pth saving......
[2022-11-05 23:57:03 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_25.pth saved !!!
[2022-11-05 23:57:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.633 (1.633)	Loss 1.4079 (1.4079)	Acc@1 66.992 (66.992)	Acc@5 89.648 (89.648)	Mem 14851MB
[2022-11-05 23:57:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 67.972 Acc@5 88.668
[2022-11-05 23:57:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 68.0%
[2022-11-05 23:57:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.707 (1.707)	Loss 1.8653 (1.8653)	Acc@1 59.570 (59.570)	Acc@5 82.715 (82.715)	Mem 14851MB
[2022-11-05 23:57:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 59.932 Acc@5 82.864
[2022-11-05 23:57:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 59.9%
[2022-11-05 23:57:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 67.97% at 25 epoch
[2022-11-05 23:57:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][0/1251]	eta 0:41:21 lr 0.000982	time 1.9839 (1.9839)	loss 3.9868 (3.9868)	grad_norm 1.1587 (1.1587)	mem 14851MB
[2022-11-05 23:57:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][50/1251]	eta 0:09:59 lr 0.000982	time 0.4607 (0.4990)	loss 4.6049 (4.0627)	grad_norm 1.5874 (1.3120)	mem 14851MB
[2022-11-05 23:58:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][100/1251]	eta 0:09:16 lr 0.000982	time 0.4625 (0.4832)	loss 4.8145 (4.0446)	grad_norm 1.5271 (1.2736)	mem 14851MB
[2022-11-05 23:58:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][150/1251]	eta 0:08:45 lr 0.000982	time 0.5317 (0.4774)	loss 2.9005 (4.0599)	grad_norm 1.1212 (1.2637)	mem 14851MB
[2022-11-05 23:58:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][200/1251]	eta 0:08:19 lr 0.000982	time 0.4662 (0.4753)	loss 4.5793 (4.0573)	grad_norm 1.3174 (1.2622)	mem 14851MB
[2022-11-05 23:59:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][250/1251]	eta 0:07:54 lr 0.000981	time 0.4653 (0.4735)	loss 4.5018 (4.0659)	grad_norm 1.1177 (1.2532)	mem 14851MB
[2022-11-05 23:59:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][300/1251]	eta 0:07:28 lr 0.000981	time 0.4617 (0.4721)	loss 5.0933 (4.0811)	grad_norm 1.1005 (1.2497)	mem 14851MB
[2022-11-06 00:00:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][350/1251]	eta 0:07:04 lr 0.000981	time 0.4665 (0.4712)	loss 3.1157 (4.0907)	grad_norm 1.2817 (1.2464)	mem 14851MB
[2022-11-06 00:00:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][400/1251]	eta 0:06:40 lr 0.000981	time 0.4720 (0.4703)	loss 4.5569 (4.0796)	grad_norm 1.1998 (1.2488)	mem 14851MB
[2022-11-06 00:00:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][450/1251]	eta 0:06:16 lr 0.000981	time 0.4609 (0.4698)	loss 3.7908 (4.0768)	grad_norm 1.2751 (1.2460)	mem 14851MB
[2022-11-06 00:01:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][500/1251]	eta 0:05:52 lr 0.000981	time 0.4593 (0.4695)	loss 4.4707 (4.0767)	grad_norm 1.2262 (1.2444)	mem 14851MB
[2022-11-06 00:01:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][550/1251]	eta 0:05:29 lr 0.000981	time 0.4688 (0.4696)	loss 4.3282 (4.0834)	grad_norm 1.1440 (1.2452)	mem 14851MB
[2022-11-06 00:02:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][600/1251]	eta 0:05:05 lr 0.000981	time 0.4631 (0.4691)	loss 4.3572 (4.0921)	grad_norm 1.2420 (1.2451)	mem 14851MB
[2022-11-06 00:02:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][650/1251]	eta 0:04:41 lr 0.000981	time 0.4746 (0.4688)	loss 4.1758 (4.0834)	grad_norm 1.1773 (1.2461)	mem 14851MB
[2022-11-06 00:02:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][700/1251]	eta 0:04:18 lr 0.000981	time 0.4630 (0.4688)	loss 4.8992 (4.0816)	grad_norm 1.4959 (1.2447)	mem 14851MB
[2022-11-06 00:03:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][750/1251]	eta 0:03:54 lr 0.000981	time 0.4566 (0.4686)	loss 3.3394 (4.0815)	grad_norm 1.2392 (1.2465)	mem 14851MB
[2022-11-06 00:03:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][800/1251]	eta 0:03:31 lr 0.000981	time 0.4654 (0.4686)	loss 4.7145 (4.0864)	grad_norm 1.2710 (1.2474)	mem 14851MB
[2022-11-06 00:03:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][850/1251]	eta 0:03:07 lr 0.000981	time 0.4586 (0.4684)	loss 4.4069 (4.0857)	grad_norm 1.2306 (1.2465)	mem 14851MB
[2022-11-06 00:04:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][900/1251]	eta 0:02:44 lr 0.000981	time 0.4655 (0.4683)	loss 4.6433 (4.0791)	grad_norm 1.0533 (1.2471)	mem 14851MB
[2022-11-06 00:04:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][950/1251]	eta 0:02:20 lr 0.000981	time 0.4694 (0.4682)	loss 4.5477 (4.0800)	grad_norm 1.2380 (1.2480)	mem 14851MB
[2022-11-06 00:05:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][1000/1251]	eta 0:01:57 lr 0.000981	time 0.4621 (0.4681)	loss 4.3652 (4.0869)	grad_norm 1.1513 (1.2470)	mem 14851MB
[2022-11-06 00:05:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][1050/1251]	eta 0:01:34 lr 0.000981	time 0.4563 (0.4681)	loss 3.3787 (4.0913)	grad_norm 1.2316 (1.2467)	mem 14851MB
[2022-11-06 00:05:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][1100/1251]	eta 0:01:10 lr 0.000981	time 0.4695 (0.4680)	loss 4.3211 (4.0955)	grad_norm 1.1646 (1.2461)	mem 14851MB
[2022-11-06 00:06:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][1150/1251]	eta 0:00:47 lr 0.000980	time 0.4583 (0.4679)	loss 2.9762 (4.0887)	grad_norm 1.4322 (1.2448)	mem 14851MB
[2022-11-06 00:06:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][1200/1251]	eta 0:00:23 lr 0.000980	time 0.5454 (0.4679)	loss 4.1372 (4.0841)	grad_norm 1.1925 (1.2450)	mem 14851MB
[2022-11-06 00:07:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [26/300][1250/1251]	eta 0:00:00 lr 0.000980	time 0.4575 (0.4677)	loss 4.3290 (4.0827)	grad_norm 1.1830 (1.2459)	mem 14851MB
[2022-11-06 00:07:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 26 training takes 0:09:45
[2022-11-06 00:07:06 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_26.pth saving......
[2022-11-06 00:07:07 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_26.pth saved !!!
[2022-11-06 00:07:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.540 (1.540)	Loss 1.4039 (1.4039)	Acc@1 69.434 (69.434)	Acc@5 88.672 (88.672)	Mem 14851MB
[2022-11-06 00:07:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 68.522 Acc@5 88.992
[2022-11-06 00:07:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 68.5%
[2022-11-06 00:07:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.699 (1.699)	Loss 1.7051 (1.7051)	Acc@1 61.719 (61.719)	Acc@5 84.277 (84.277)	Mem 14851MB
[2022-11-06 00:07:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 61.794 Acc@5 84.192
[2022-11-06 00:07:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 61.8%
[2022-11-06 00:07:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 68.52% at 26 epoch
[2022-11-06 00:07:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][0/1251]	eta 0:43:05 lr 0.000980	time 2.0665 (2.0665)	loss 4.3552 (4.3552)	grad_norm 1.1322 (1.1322)	mem 14851MB
[2022-11-06 00:07:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][50/1251]	eta 0:10:02 lr 0.000980	time 0.4545 (0.5018)	loss 3.8316 (4.0546)	grad_norm 1.6027 (1.2167)	mem 14851MB
[2022-11-06 00:08:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][100/1251]	eta 0:09:18 lr 0.000980	time 0.4639 (0.4852)	loss 4.4713 (4.1449)	grad_norm 1.1903 (1.2274)	mem 14851MB
[2022-11-06 00:08:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][150/1251]	eta 0:08:46 lr 0.000980	time 0.4635 (0.4785)	loss 4.1571 (4.0720)	grad_norm 1.3632 (1.2312)	mem 14851MB
[2022-11-06 00:09:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][200/1251]	eta 0:08:20 lr 0.000980	time 0.4759 (0.4758)	loss 4.3942 (4.0375)	grad_norm 1.3872 (1.2390)	mem 14851MB
[2022-11-06 00:09:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][250/1251]	eta 0:07:54 lr 0.000980	time 0.4672 (0.4736)	loss 3.1706 (4.0199)	grad_norm 1.3194 (1.2451)	mem 14851MB
[2022-11-06 00:09:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][300/1251]	eta 0:07:29 lr 0.000980	time 0.4721 (0.4725)	loss 3.8822 (4.0306)	grad_norm 1.1870 (1.2413)	mem 14851MB
[2022-11-06 00:10:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][350/1251]	eta 0:07:04 lr 0.000980	time 0.4574 (0.4717)	loss 2.9990 (4.0211)	grad_norm 1.1269 (1.2432)	mem 14851MB
[2022-11-06 00:10:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][400/1251]	eta 0:06:40 lr 0.000980	time 0.4645 (0.4709)	loss 3.7200 (4.0205)	grad_norm 1.2824 (1.2415)	mem 14851MB
[2022-11-06 00:10:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][450/1251]	eta 0:06:16 lr 0.000980	time 0.4708 (0.4703)	loss 3.8297 (4.0036)	grad_norm 1.5823 (1.2397)	mem 14851MB
[2022-11-06 00:11:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][500/1251]	eta 0:05:52 lr 0.000980	time 0.4515 (0.4697)	loss 3.9951 (4.0122)	grad_norm 1.3447 (1.2395)	mem 14851MB
[2022-11-06 00:11:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][550/1251]	eta 0:05:29 lr 0.000980	time 0.4547 (0.4701)	loss 4.2575 (4.0229)	grad_norm 1.2058 (1.2359)	mem 14851MB
[2022-11-06 00:12:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][600/1251]	eta 0:05:05 lr 0.000980	time 0.4596 (0.4698)	loss 4.4633 (4.0292)	grad_norm 1.1783 (1.2374)	mem 14851MB
[2022-11-06 00:12:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][650/1251]	eta 0:04:42 lr 0.000980	time 0.4710 (0.4693)	loss 3.6016 (4.0284)	grad_norm 1.1432 (1.2377)	mem 14851MB
[2022-11-06 00:12:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][700/1251]	eta 0:04:18 lr 0.000980	time 0.4737 (0.4692)	loss 3.6217 (4.0294)	grad_norm 1.2421 (1.2387)	mem 14851MB
[2022-11-06 00:13:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][750/1251]	eta 0:03:54 lr 0.000979	time 0.4532 (0.4689)	loss 3.8465 (4.0349)	grad_norm 1.1142 (1.2379)	mem 14851MB
[2022-11-06 00:13:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][800/1251]	eta 0:03:31 lr 0.000979	time 0.4776 (0.4690)	loss 3.9961 (4.0351)	grad_norm 1.1516 (1.2361)	mem 14851MB
[2022-11-06 00:14:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][850/1251]	eta 0:03:08 lr 0.000979	time 0.4627 (0.4689)	loss 5.0684 (4.0264)	grad_norm 1.2292 (1.2366)	mem 14851MB
[2022-11-06 00:14:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][900/1251]	eta 0:02:44 lr 0.000979	time 0.4662 (0.4687)	loss 3.4576 (4.0222)	grad_norm 1.2069 (1.2376)	mem 14851MB
[2022-11-06 00:14:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][950/1251]	eta 0:02:20 lr 0.000979	time 0.4638 (0.4684)	loss 3.8646 (4.0354)	grad_norm 1.4386 (1.2376)	mem 14851MB
[2022-11-06 00:15:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][1000/1251]	eta 0:01:57 lr 0.000979	time 0.4536 (0.4682)	loss 4.7275 (4.0384)	grad_norm 1.2190 (1.2361)	mem 14851MB
[2022-11-06 00:15:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][1050/1251]	eta 0:01:34 lr 0.000979	time 0.4624 (0.4684)	loss 4.5902 (4.0352)	grad_norm 1.2358 (1.2351)	mem 14851MB
[2022-11-06 00:16:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][1100/1251]	eta 0:01:10 lr 0.000979	time 0.4642 (0.4683)	loss 3.5535 (4.0417)	grad_norm 1.2435 (1.2355)	mem 14851MB
[2022-11-06 00:16:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][1150/1251]	eta 0:00:47 lr 0.000979	time 0.4496 (0.4681)	loss 3.8580 (4.0411)	grad_norm 1.1847 (1.2336)	mem 14851MB
[2022-11-06 00:16:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][1200/1251]	eta 0:00:23 lr 0.000979	time 0.4632 (0.4682)	loss 3.4119 (4.0461)	grad_norm 1.2777 (1.2348)	mem 14851MB
[2022-11-06 00:17:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [27/300][1250/1251]	eta 0:00:00 lr 0.000979	time 0.4560 (0.4679)	loss 2.7875 (4.0396)	grad_norm 1.2433 (1.2337)	mem 14851MB
[2022-11-06 00:17:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 27 training takes 0:09:45
[2022-11-06 00:17:10 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_27.pth saving......
[2022-11-06 00:17:10 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_27.pth saved !!!
[2022-11-06 00:17:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.663 (1.663)	Loss 1.4123 (1.4123)	Acc@1 69.336 (69.336)	Acc@5 88.477 (88.477)	Mem 14851MB
[2022-11-06 00:17:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 68.578 Acc@5 88.896
[2022-11-06 00:17:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 68.6%
[2022-11-06 00:17:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.617 (1.617)	Loss 1.7134 (1.7134)	Acc@1 62.012 (62.012)	Acc@5 85.059 (85.059)	Mem 14851MB
[2022-11-06 00:17:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 63.342 Acc@5 85.348
[2022-11-06 00:17:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 63.3%
[2022-11-06 00:17:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 68.58% at 27 epoch
[2022-11-06 00:17:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][0/1251]	eta 0:41:12 lr 0.000979	time 1.9761 (1.9761)	loss 4.4409 (4.4409)	grad_norm 1.2470 (1.2470)	mem 14851MB
[2022-11-06 00:17:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][50/1251]	eta 0:10:01 lr 0.000979	time 0.4724 (0.5008)	loss 2.8758 (3.9501)	grad_norm 1.3852 (1.2386)	mem 14851MB
[2022-11-06 00:18:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][100/1251]	eta 0:09:18 lr 0.000979	time 0.4549 (0.4852)	loss 4.7499 (4.0398)	grad_norm 1.2050 (1.2601)	mem 14851MB
[2022-11-06 00:18:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][150/1251]	eta 0:08:47 lr 0.000979	time 0.4577 (0.4787)	loss 4.1178 (4.0739)	grad_norm 1.1659 (1.2459)	mem 14851MB
[2022-11-06 00:19:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][200/1251]	eta 0:08:20 lr 0.000979	time 0.4590 (0.4758)	loss 3.5451 (4.0988)	grad_norm 1.1560 (1.2408)	mem 14851MB
[2022-11-06 00:19:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][250/1251]	eta 0:07:54 lr 0.000979	time 0.4697 (0.4740)	loss 3.9464 (4.1037)	grad_norm 1.1424 (1.2415)	mem 14851MB
[2022-11-06 00:19:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][300/1251]	eta 0:07:29 lr 0.000979	time 0.4578 (0.4725)	loss 3.4212 (4.0784)	grad_norm 1.0923 (1.2436)	mem 14851MB
[2022-11-06 00:20:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][350/1251]	eta 0:07:04 lr 0.000978	time 0.4647 (0.4713)	loss 2.9693 (4.0520)	grad_norm 1.1437 (nan)	mem 14851MB
[2022-11-06 00:20:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][400/1251]	eta 0:06:40 lr 0.000978	time 0.4709 (0.4709)	loss 3.2936 (4.0372)	grad_norm 1.1315 (nan)	mem 14851MB
[2022-11-06 00:21:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][450/1251]	eta 0:06:16 lr 0.000978	time 0.4724 (0.4702)	loss 4.0790 (4.0395)	grad_norm 1.0812 (nan)	mem 14851MB
[2022-11-06 00:21:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][500/1251]	eta 0:05:52 lr 0.000978	time 0.4688 (0.4698)	loss 3.6019 (4.0426)	grad_norm 1.0950 (nan)	mem 14851MB
[2022-11-06 00:21:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][550/1251]	eta 0:05:29 lr 0.000978	time 0.4604 (0.4699)	loss 4.1431 (4.0397)	grad_norm 1.1895 (nan)	mem 14851MB
[2022-11-06 00:22:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][600/1251]	eta 0:05:05 lr 0.000978	time 0.4593 (0.4696)	loss 4.2344 (4.0530)	grad_norm 1.4216 (nan)	mem 14851MB
[2022-11-06 00:22:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][650/1251]	eta 0:04:42 lr 0.000978	time 0.4697 (0.4694)	loss 3.3270 (4.0384)	grad_norm 1.1566 (nan)	mem 14851MB
[2022-11-06 00:22:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][700/1251]	eta 0:04:18 lr 0.000978	time 0.4835 (0.4692)	loss 3.8638 (4.0319)	grad_norm 1.3797 (nan)	mem 14851MB
[2022-11-06 00:23:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][750/1251]	eta 0:03:54 lr 0.000978	time 0.4570 (0.4689)	loss 3.8055 (4.0238)	grad_norm 1.3711 (nan)	mem 14851MB
[2022-11-06 00:23:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][800/1251]	eta 0:03:31 lr 0.000978	time 0.4642 (0.4690)	loss 4.1555 (4.0274)	grad_norm 1.0887 (nan)	mem 14851MB
[2022-11-06 00:24:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][850/1251]	eta 0:03:07 lr 0.000978	time 0.4609 (0.4687)	loss 3.0636 (4.0238)	grad_norm 1.1832 (nan)	mem 14851MB
[2022-11-06 00:24:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][900/1251]	eta 0:02:44 lr 0.000978	time 0.4542 (0.4685)	loss 4.4267 (4.0158)	grad_norm 1.2219 (nan)	mem 14851MB
[2022-11-06 00:24:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][950/1251]	eta 0:02:21 lr 0.000978	time 0.4583 (0.4684)	loss 3.0235 (4.0128)	grad_norm 1.2726 (nan)	mem 14851MB
[2022-11-06 00:25:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][1000/1251]	eta 0:01:57 lr 0.000978	time 0.4640 (0.4684)	loss 3.7778 (4.0068)	grad_norm 1.0111 (nan)	mem 14851MB
[2022-11-06 00:25:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][1050/1251]	eta 0:01:34 lr 0.000978	time 0.4547 (0.4684)	loss 4.4734 (4.0090)	grad_norm 1.4436 (nan)	mem 14851MB
[2022-11-06 00:26:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][1100/1251]	eta 0:01:10 lr 0.000978	time 0.4636 (0.4682)	loss 4.0345 (4.0033)	grad_norm 1.0573 (nan)	mem 14851MB
[2022-11-06 00:26:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][1150/1251]	eta 0:00:47 lr 0.000977	time 0.4584 (0.4681)	loss 3.7855 (4.0050)	grad_norm 1.4803 (nan)	mem 14851MB
[2022-11-06 00:26:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][1200/1251]	eta 0:00:23 lr 0.000977	time 0.4588 (0.4680)	loss 4.3802 (4.0027)	grad_norm 0.9617 (nan)	mem 14851MB
[2022-11-06 00:27:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [28/300][1250/1251]	eta 0:00:00 lr 0.000977	time 0.4573 (0.4679)	loss 3.9538 (4.0079)	grad_norm 1.2175 (nan)	mem 14851MB
[2022-11-06 00:27:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 28 training takes 0:09:45
[2022-11-06 00:27:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_28.pth saving......
[2022-11-06 00:27:14 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_28.pth saved !!!
[2022-11-06 00:27:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.668 (1.668)	Loss 1.3283 (1.3283)	Acc@1 70.898 (70.898)	Acc@5 90.137 (90.137)	Mem 14851MB
[2022-11-06 00:27:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 69.064 Acc@5 89.402
[2022-11-06 00:27:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 69.1%
[2022-11-06 00:27:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.550 (1.550)	Loss 1.5959 (1.5959)	Acc@1 63.867 (63.867)	Acc@5 85.645 (85.645)	Mem 14851MB
[2022-11-06 00:27:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 64.774 Acc@5 86.180
[2022-11-06 00:27:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 64.8%
[2022-11-06 00:27:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 69.06% at 28 epoch
[2022-11-06 00:27:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][0/1251]	eta 0:43:09 lr 0.000977	time 2.0698 (2.0698)	loss 4.1928 (4.1928)	grad_norm 1.2330 (1.2330)	mem 14851MB
[2022-11-06 00:27:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][50/1251]	eta 0:10:02 lr 0.000977	time 0.4677 (0.5018)	loss 2.9310 (3.9209)	grad_norm 1.1095 (1.2026)	mem 14851MB
[2022-11-06 00:28:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][100/1251]	eta 0:09:20 lr 0.000977	time 0.4677 (0.4866)	loss 4.5941 (3.9172)	grad_norm 1.1496 (1.2291)	mem 14851MB
[2022-11-06 00:28:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][150/1251]	eta 0:08:48 lr 0.000977	time 0.4625 (0.4804)	loss 3.5420 (3.9333)	grad_norm 1.3046 (1.2296)	mem 14851MB
[2022-11-06 00:29:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][200/1251]	eta 0:08:21 lr 0.000977	time 0.4623 (0.4775)	loss 3.1618 (3.9451)	grad_norm 1.1409 (1.2358)	mem 14851MB
[2022-11-06 00:29:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][250/1251]	eta 0:07:55 lr 0.000977	time 0.4665 (0.4752)	loss 3.8376 (3.9704)	grad_norm 1.3542 (1.2311)	mem 14851MB
[2022-11-06 00:29:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][300/1251]	eta 0:07:30 lr 0.000977	time 0.4694 (0.4737)	loss 3.9382 (3.9588)	grad_norm 1.0436 (1.2284)	mem 14851MB
[2022-11-06 00:30:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][350/1251]	eta 0:07:05 lr 0.000977	time 0.4691 (0.4726)	loss 4.3085 (3.9652)	grad_norm 1.3231 (1.2273)	mem 14851MB
[2022-11-06 00:30:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][400/1251]	eta 0:06:41 lr 0.000977	time 0.4718 (0.4720)	loss 3.5197 (3.9751)	grad_norm 1.2154 (1.2287)	mem 14851MB
[2022-11-06 00:31:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][450/1251]	eta 0:06:17 lr 0.000977	time 0.4630 (0.4717)	loss 2.7655 (3.9712)	grad_norm 1.2108 (1.2295)	mem 14851MB
[2022-11-06 00:31:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][500/1251]	eta 0:05:53 lr 0.000977	time 0.4679 (0.4709)	loss 3.4006 (3.9809)	grad_norm 1.2352 (1.2329)	mem 14851MB
[2022-11-06 00:31:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][550/1251]	eta 0:05:30 lr 0.000977	time 0.4703 (0.4711)	loss 2.8705 (3.9741)	grad_norm 1.0878 (1.2331)	mem 14851MB
[2022-11-06 00:32:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][600/1251]	eta 0:05:06 lr 0.000977	time 0.4611 (0.4704)	loss 3.8596 (3.9760)	grad_norm 1.0525 (1.2320)	mem 14851MB
[2022-11-06 00:32:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][650/1251]	eta 0:04:42 lr 0.000977	time 0.4678 (0.4700)	loss 5.0686 (3.9813)	grad_norm 1.1962 (1.2286)	mem 14851MB
[2022-11-06 00:33:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][700/1251]	eta 0:04:18 lr 0.000976	time 0.4637 (0.4699)	loss 4.7708 (3.9874)	grad_norm 1.2353 (1.2271)	mem 14851MB
[2022-11-06 00:33:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][750/1251]	eta 0:03:55 lr 0.000976	time 0.4700 (0.4697)	loss 4.2496 (3.9882)	grad_norm 1.4387 (1.2263)	mem 14851MB
[2022-11-06 00:33:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][800/1251]	eta 0:03:31 lr 0.000976	time 0.4695 (0.4698)	loss 4.4047 (3.9894)	grad_norm 1.3038 (1.2273)	mem 14851MB
[2022-11-06 00:34:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][850/1251]	eta 0:03:08 lr 0.000976	time 0.4610 (0.4696)	loss 4.3360 (3.9870)	grad_norm 1.0969 (1.2268)	mem 14851MB
[2022-11-06 00:34:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][900/1251]	eta 0:02:44 lr 0.000976	time 0.5565 (0.4696)	loss 4.4969 (3.9928)	grad_norm 1.0335 (1.2257)	mem 14851MB
[2022-11-06 00:34:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][950/1251]	eta 0:02:21 lr 0.000976	time 0.4651 (0.4693)	loss 3.0385 (3.9848)	grad_norm 1.2414 (1.2252)	mem 14851MB
[2022-11-06 00:35:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][1000/1251]	eta 0:01:57 lr 0.000976	time 0.4663 (0.4691)	loss 4.9342 (3.9868)	grad_norm 1.1192 (1.2263)	mem 14851MB
[2022-11-06 00:35:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][1050/1251]	eta 0:01:34 lr 0.000976	time 0.4627 (0.4692)	loss 4.8539 (3.9934)	grad_norm 1.2026 (1.2261)	mem 14851MB
[2022-11-06 00:36:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][1100/1251]	eta 0:01:10 lr 0.000976	time 0.4535 (0.4691)	loss 3.7541 (3.9911)	grad_norm 1.1275 (1.2254)	mem 14851MB
[2022-11-06 00:36:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][1150/1251]	eta 0:00:47 lr 0.000976	time 0.4689 (0.4691)	loss 3.4587 (3.9940)	grad_norm 1.0837 (1.2257)	mem 14851MB
[2022-11-06 00:36:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][1200/1251]	eta 0:00:23 lr 0.000976	time 0.4642 (0.4690)	loss 4.8416 (3.9930)	grad_norm 1.2347 (1.2245)	mem 14851MB
[2022-11-06 00:37:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [29/300][1250/1251]	eta 0:00:00 lr 0.000976	time 0.4567 (0.4688)	loss 4.5471 (3.9893)	grad_norm 1.3521 (1.2260)	mem 14851MB
[2022-11-06 00:37:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 29 training takes 0:09:46
[2022-11-06 00:37:18 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_29.pth saving......
[2022-11-06 00:37:19 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_29.pth saved !!!
[2022-11-06 00:37:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.692 (1.692)	Loss 1.4337 (1.4337)	Acc@1 69.238 (69.238)	Acc@5 88.477 (88.477)	Mem 14851MB
[2022-11-06 00:37:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 69.300 Acc@5 89.442
[2022-11-06 00:37:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 69.3%
[2022-11-06 00:37:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.592 (1.592)	Loss 1.6277 (1.6277)	Acc@1 62.891 (62.891)	Acc@5 86.133 (86.133)	Mem 14851MB
[2022-11-06 00:37:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 65.918 Acc@5 86.944
[2022-11-06 00:37:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 65.9%
[2022-11-06 00:37:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 69.30% at 29 epoch
[2022-11-06 00:37:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][0/1251]	eta 0:40:42 lr 0.000976	time 1.9527 (1.9527)	loss 4.6132 (4.6132)	grad_norm 1.1591 (1.1591)	mem 14851MB
[2022-11-06 00:38:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][50/1251]	eta 0:10:04 lr 0.000976	time 0.4751 (0.5036)	loss 4.0943 (4.0230)	grad_norm 1.3459 (1.2137)	mem 14851MB
[2022-11-06 00:38:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][100/1251]	eta 0:09:19 lr 0.000976	time 0.4664 (0.4864)	loss 2.9896 (3.9554)	grad_norm 1.3108 (1.2212)	mem 14851MB
[2022-11-06 00:38:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][150/1251]	eta 0:08:48 lr 0.000976	time 0.4603 (0.4798)	loss 3.3508 (3.9887)	grad_norm 1.3278 (1.2147)	mem 14851MB
[2022-11-06 00:39:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][200/1251]	eta 0:08:20 lr 0.000976	time 0.4584 (0.4764)	loss 4.4840 (4.0172)	grad_norm 1.2208 (1.2178)	mem 14851MB
[2022-11-06 00:39:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][250/1251]	eta 0:07:54 lr 0.000975	time 0.4653 (0.4744)	loss 3.5508 (3.9839)	grad_norm 1.1861 (1.2172)	mem 14851MB
[2022-11-06 00:39:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][300/1251]	eta 0:07:30 lr 0.000975	time 0.4662 (0.4735)	loss 3.7436 (3.9717)	grad_norm 1.2720 (1.2172)	mem 14851MB
[2022-11-06 00:40:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][350/1251]	eta 0:07:05 lr 0.000975	time 0.4631 (0.4724)	loss 3.8840 (3.9843)	grad_norm 1.2654 (1.2145)	mem 14851MB
[2022-11-06 00:40:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][400/1251]	eta 0:06:41 lr 0.000975	time 0.4726 (0.4718)	loss 4.2240 (3.9909)	grad_norm 1.2869 (1.2178)	mem 14851MB
[2022-11-06 00:41:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][450/1251]	eta 0:06:17 lr 0.000975	time 0.4624 (0.4711)	loss 2.6156 (3.9767)	grad_norm 1.1546 (1.2169)	mem 14851MB
[2022-11-06 00:41:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][500/1251]	eta 0:05:53 lr 0.000975	time 0.4769 (0.4705)	loss 2.6856 (3.9693)	grad_norm 1.1746 (1.2166)	mem 14851MB
[2022-11-06 00:41:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][550/1251]	eta 0:05:29 lr 0.000975	time 0.4603 (0.4705)	loss 2.9797 (3.9616)	grad_norm 1.0994 (1.2178)	mem 14851MB
[2022-11-06 00:42:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][600/1251]	eta 0:05:06 lr 0.000975	time 0.4603 (0.4703)	loss 2.9674 (3.9744)	grad_norm 1.1379 (nan)	mem 14851MB
[2022-11-06 00:42:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][650/1251]	eta 0:04:42 lr 0.000975	time 0.4763 (0.4701)	loss 2.6889 (3.9762)	grad_norm 1.3145 (nan)	mem 14851MB
[2022-11-06 00:43:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][700/1251]	eta 0:04:18 lr 0.000975	time 0.4762 (0.4698)	loss 3.6122 (3.9783)	grad_norm 1.1855 (nan)	mem 14851MB
[2022-11-06 00:43:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][750/1251]	eta 0:03:55 lr 0.000975	time 0.4611 (0.4696)	loss 3.5733 (3.9887)	grad_norm 1.4433 (nan)	mem 14851MB
[2022-11-06 00:43:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][800/1251]	eta 0:03:31 lr 0.000975	time 0.4551 (0.4696)	loss 4.3747 (3.9805)	grad_norm 1.1431 (nan)	mem 14851MB
[2022-11-06 00:44:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][850/1251]	eta 0:03:08 lr 0.000975	time 0.4629 (0.4695)	loss 2.9261 (3.9790)	grad_norm 1.1232 (nan)	mem 14851MB
[2022-11-06 00:44:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][900/1251]	eta 0:02:44 lr 0.000975	time 0.4653 (0.4694)	loss 3.4498 (3.9799)	grad_norm 1.3519 (nan)	mem 14851MB
[2022-11-06 00:45:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][950/1251]	eta 0:02:21 lr 0.000975	time 0.4669 (0.4691)	loss 3.5316 (3.9781)	grad_norm 1.2089 (nan)	mem 14851MB
[2022-11-06 00:45:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][1000/1251]	eta 0:01:57 lr 0.000974	time 0.4769 (0.4690)	loss 4.1630 (3.9814)	grad_norm 1.1684 (nan)	mem 14851MB
[2022-11-06 00:45:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][1050/1251]	eta 0:01:34 lr 0.000974	time 0.4624 (0.4691)	loss 4.7891 (3.9767)	grad_norm 1.6154 (nan)	mem 14851MB
[2022-11-06 00:46:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][1100/1251]	eta 0:01:10 lr 0.000974	time 0.4661 (0.4690)	loss 4.1211 (3.9741)	grad_norm 1.0597 (nan)	mem 14851MB
[2022-11-06 00:46:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][1150/1251]	eta 0:00:47 lr 0.000974	time 0.4587 (0.4689)	loss 4.1591 (3.9717)	grad_norm 1.3544 (nan)	mem 14851MB
[2022-11-06 00:46:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][1200/1251]	eta 0:00:23 lr 0.000974	time 0.4615 (0.4688)	loss 4.4668 (3.9734)	grad_norm 1.2996 (nan)	mem 14851MB
[2022-11-06 00:47:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [30/300][1250/1251]	eta 0:00:00 lr 0.000974	time 0.4569 (0.4686)	loss 3.9092 (3.9715)	grad_norm 1.2397 (nan)	mem 14851MB
[2022-11-06 00:47:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 30 training takes 0:09:46
[2022-11-06 00:47:23 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_30.pth saving......
[2022-11-06 00:47:23 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_30.pth saved !!!
[2022-11-06 00:47:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.602 (1.602)	Loss 1.4072 (1.4072)	Acc@1 67.969 (67.969)	Acc@5 89.062 (89.062)	Mem 14851MB
[2022-11-06 00:47:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 70.082 Acc@5 89.794
[2022-11-06 00:47:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 70.1%
[2022-11-06 00:47:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.590 (1.590)	Loss 1.4930 (1.4930)	Acc@1 65.625 (65.625)	Acc@5 88.281 (88.281)	Mem 14851MB
[2022-11-06 00:47:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 66.966 Acc@5 87.600
[2022-11-06 00:47:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 67.0%
[2022-11-06 00:47:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 70.08% at 30 epoch
[2022-11-06 00:47:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][0/1251]	eta 0:39:35 lr 0.000974	time 1.8990 (1.8990)	loss 2.9743 (2.9743)	grad_norm 1.1787 (1.1787)	mem 14851MB
[2022-11-06 00:48:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][50/1251]	eta 0:09:56 lr 0.000974	time 0.4653 (0.4966)	loss 4.2993 (3.8253)	grad_norm 1.3551 (1.2449)	mem 14851MB
[2022-11-06 00:48:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][100/1251]	eta 0:09:18 lr 0.000974	time 0.4693 (0.4848)	loss 4.3061 (3.8727)	grad_norm 1.2262 (1.2212)	mem 14851MB
[2022-11-06 00:48:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][150/1251]	eta 0:08:46 lr 0.000974	time 0.4616 (0.4782)	loss 3.5957 (3.9281)	grad_norm 1.1875 (1.2176)	mem 14851MB
[2022-11-06 00:49:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][200/1251]	eta 0:08:20 lr 0.000974	time 0.4680 (0.4759)	loss 4.2710 (3.9419)	grad_norm 1.0992 (1.2211)	mem 14851MB
[2022-11-06 00:49:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][250/1251]	eta 0:07:54 lr 0.000974	time 0.4543 (0.4740)	loss 4.5457 (3.9323)	grad_norm 1.0931 (1.2150)	mem 14851MB
[2022-11-06 00:50:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][300/1251]	eta 0:07:29 lr 0.000974	time 0.5449 (0.4729)	loss 3.6561 (3.9623)	grad_norm 1.1068 (1.2199)	mem 14851MB
[2022-11-06 00:50:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][350/1251]	eta 0:07:05 lr 0.000974	time 0.4585 (0.4718)	loss 3.6626 (3.9727)	grad_norm 1.0628 (1.2209)	mem 14851MB
[2022-11-06 00:50:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][400/1251]	eta 0:06:40 lr 0.000974	time 0.4556 (0.4711)	loss 3.6387 (3.9693)	grad_norm 1.0644 (1.2226)	mem 14851MB
[2022-11-06 00:51:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][450/1251]	eta 0:06:17 lr 0.000974	time 0.4726 (0.4712)	loss 4.3422 (3.9702)	grad_norm 1.1240 (1.2250)	mem 14851MB
[2022-11-06 00:51:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][500/1251]	eta 0:05:53 lr 0.000973	time 0.4675 (0.4706)	loss 3.5037 (3.9659)	grad_norm 1.1611 (1.2263)	mem 14851MB
[2022-11-06 00:52:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][550/1251]	eta 0:05:29 lr 0.000973	time 0.4587 (0.4705)	loss 3.1939 (3.9566)	grad_norm 1.3117 (1.2277)	mem 14851MB
[2022-11-06 00:52:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][600/1251]	eta 0:05:06 lr 0.000973	time 0.4650 (0.4701)	loss 4.1081 (3.9589)	grad_norm 1.2069 (1.2263)	mem 14851MB
[2022-11-06 00:52:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][650/1251]	eta 0:04:42 lr 0.000973	time 0.5438 (0.4699)	loss 2.7603 (3.9661)	grad_norm 1.1575 (1.2233)	mem 14851MB
[2022-11-06 00:53:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][700/1251]	eta 0:04:18 lr 0.000973	time 0.4603 (0.4696)	loss 4.4453 (3.9697)	grad_norm 1.0857 (1.2240)	mem 14851MB
[2022-11-06 00:53:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][750/1251]	eta 0:03:55 lr 0.000973	time 0.4529 (0.4695)	loss 4.8716 (3.9768)	grad_norm 1.1868 (1.2242)	mem 14851MB
[2022-11-06 00:53:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][800/1251]	eta 0:03:31 lr 0.000973	time 0.4637 (0.4694)	loss 3.8696 (3.9725)	grad_norm 1.2990 (1.2236)	mem 14851MB
[2022-11-06 00:54:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][850/1251]	eta 0:03:08 lr 0.000973	time 0.4604 (0.4692)	loss 3.2684 (3.9784)	grad_norm 1.2513 (1.2222)	mem 14851MB
[2022-11-06 00:54:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][900/1251]	eta 0:02:44 lr 0.000973	time 0.5582 (0.4692)	loss 4.1694 (3.9778)	grad_norm 1.3052 (1.2233)	mem 14851MB
[2022-11-06 00:55:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][950/1251]	eta 0:02:21 lr 0.000973	time 0.4683 (0.4691)	loss 3.8373 (3.9810)	grad_norm 1.2106 (1.2240)	mem 14851MB
[2022-11-06 00:55:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][1000/1251]	eta 0:01:57 lr 0.000973	time 0.4662 (0.4690)	loss 4.2010 (3.9810)	grad_norm 1.3749 (1.2224)	mem 14851MB
[2022-11-06 00:55:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][1050/1251]	eta 0:01:34 lr 0.000973	time 0.4630 (0.4690)	loss 4.0189 (3.9810)	grad_norm 1.2013 (1.2217)	mem 14851MB
[2022-11-06 00:56:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][1100/1251]	eta 0:01:10 lr 0.000973	time 0.4631 (0.4688)	loss 2.6406 (3.9769)	grad_norm 1.2110 (1.2229)	mem 14851MB
[2022-11-06 00:56:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][1150/1251]	eta 0:00:47 lr 0.000973	time 0.4700 (0.4688)	loss 4.1769 (3.9719)	grad_norm 1.0943 (1.2223)	mem 14851MB
[2022-11-06 00:57:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][1200/1251]	eta 0:00:23 lr 0.000973	time 0.4632 (0.4687)	loss 4.4762 (3.9719)	grad_norm 1.0407 (1.2226)	mem 14851MB
[2022-11-06 00:57:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [31/300][1250/1251]	eta 0:00:00 lr 0.000972	time 0.4571 (0.4686)	loss 3.9221 (3.9710)	grad_norm 1.0551 (1.2226)	mem 14851MB
[2022-11-06 00:57:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 31 training takes 0:09:46
[2022-11-06 00:57:27 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_31.pth saving......
[2022-11-06 00:57:28 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_31.pth saved !!!
[2022-11-06 00:57:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.530 (1.530)	Loss 1.2496 (1.2496)	Acc@1 71.777 (71.777)	Acc@5 91.211 (91.211)	Mem 14851MB
[2022-11-06 00:57:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 70.228 Acc@5 89.926
[2022-11-06 00:57:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 70.2%
[2022-11-06 00:57:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.654 (1.654)	Loss 1.4099 (1.4099)	Acc@1 70.117 (70.117)	Acc@5 87.793 (87.793)	Mem 14851MB
[2022-11-06 00:57:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 67.848 Acc@5 88.160
[2022-11-06 00:57:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 67.8%
[2022-11-06 00:57:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 70.23% at 31 epoch
[2022-11-06 00:57:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][0/1251]	eta 0:41:37 lr 0.000972	time 1.9960 (1.9960)	loss 3.4968 (3.4968)	grad_norm 1.2074 (1.2074)	mem 14851MB
[2022-11-06 00:58:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][50/1251]	eta 0:10:00 lr 0.000972	time 0.4622 (0.5001)	loss 4.1658 (3.9037)	grad_norm 1.2441 (1.2478)	mem 14851MB
[2022-11-06 00:58:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][100/1251]	eta 0:09:18 lr 0.000972	time 0.4712 (0.4855)	loss 3.8039 (3.9054)	grad_norm 1.1325 (1.2402)	mem 14851MB
[2022-11-06 00:58:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][150/1251]	eta 0:08:47 lr 0.000972	time 0.4630 (0.4795)	loss 4.5674 (3.9425)	grad_norm 1.0586 (nan)	mem 14851MB
[2022-11-06 00:59:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][200/1251]	eta 0:08:20 lr 0.000972	time 0.4715 (0.4761)	loss 3.8955 (3.9331)	grad_norm 1.1136 (nan)	mem 14851MB
[2022-11-06 00:59:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][250/1251]	eta 0:07:54 lr 0.000972	time 0.4655 (0.4741)	loss 3.8868 (3.9454)	grad_norm 1.1488 (nan)	mem 14851MB
[2022-11-06 01:00:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][300/1251]	eta 0:07:30 lr 0.000972	time 0.4711 (0.4733)	loss 3.8899 (3.9578)	grad_norm 1.1470 (nan)	mem 14851MB
[2022-11-06 01:00:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][350/1251]	eta 0:07:05 lr 0.000972	time 0.4683 (0.4722)	loss 4.8517 (3.9538)	grad_norm 1.3616 (nan)	mem 14851MB
[2022-11-06 01:00:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][400/1251]	eta 0:06:41 lr 0.000972	time 0.4646 (0.4717)	loss 3.4779 (3.9494)	grad_norm 1.3344 (nan)	mem 14851MB
[2022-11-06 01:01:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][450/1251]	eta 0:06:17 lr 0.000972	time 0.4642 (0.4710)	loss 4.8470 (3.9624)	grad_norm 1.2053 (nan)	mem 14851MB
[2022-11-06 01:01:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][500/1251]	eta 0:05:53 lr 0.000972	time 0.4563 (0.4706)	loss 3.8990 (3.9726)	grad_norm 1.4010 (nan)	mem 14851MB
[2022-11-06 01:02:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][550/1251]	eta 0:05:29 lr 0.000972	time 0.4597 (0.4702)	loss 2.8712 (3.9618)	grad_norm 1.3016 (nan)	mem 14851MB
[2022-11-06 01:02:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][600/1251]	eta 0:05:06 lr 0.000972	time 0.4712 (0.4702)	loss 3.0467 (3.9539)	grad_norm 1.3193 (nan)	mem 14851MB
[2022-11-06 01:02:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][650/1251]	eta 0:04:42 lr 0.000972	time 0.4615 (0.4699)	loss 2.9213 (3.9565)	grad_norm 1.2495 (nan)	mem 14851MB
[2022-11-06 01:03:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][700/1251]	eta 0:04:18 lr 0.000972	time 0.4843 (0.4696)	loss 3.9740 (3.9539)	grad_norm 1.0378 (nan)	mem 14851MB
[2022-11-06 01:03:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][750/1251]	eta 0:03:55 lr 0.000971	time 0.4803 (0.4695)	loss 4.5692 (3.9474)	grad_norm 1.2417 (nan)	mem 14851MB
[2022-11-06 01:04:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][800/1251]	eta 0:03:31 lr 0.000971	time 0.4737 (0.4694)	loss 3.9492 (3.9400)	grad_norm 1.3356 (nan)	mem 14851MB
[2022-11-06 01:04:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][850/1251]	eta 0:03:08 lr 0.000971	time 0.4537 (0.4694)	loss 4.2888 (3.9392)	grad_norm 1.3009 (nan)	mem 14851MB
[2022-11-06 01:04:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][900/1251]	eta 0:02:44 lr 0.000971	time 0.4636 (0.4693)	loss 4.3380 (3.9363)	grad_norm 1.2579 (nan)	mem 14851MB
[2022-11-06 01:05:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][950/1251]	eta 0:02:21 lr 0.000971	time 0.4681 (0.4691)	loss 4.7472 (3.9339)	grad_norm 1.3178 (nan)	mem 14851MB
[2022-11-06 01:05:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][1000/1251]	eta 0:01:57 lr 0.000971	time 0.4624 (0.4690)	loss 4.0322 (3.9386)	grad_norm 1.2362 (nan)	mem 14851MB
[2022-11-06 01:05:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][1050/1251]	eta 0:01:34 lr 0.000971	time 0.4658 (0.4689)	loss 2.9702 (3.9412)	grad_norm 1.1113 (nan)	mem 14851MB
[2022-11-06 01:06:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][1100/1251]	eta 0:01:10 lr 0.000971	time 0.4604 (0.4688)	loss 4.0518 (3.9478)	grad_norm 1.1902 (nan)	mem 14851MB
[2022-11-06 01:06:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][1150/1251]	eta 0:00:47 lr 0.000971	time 0.4657 (0.4689)	loss 4.1443 (3.9532)	grad_norm 1.3262 (nan)	mem 14851MB
[2022-11-06 01:07:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][1200/1251]	eta 0:00:23 lr 0.000971	time 0.4758 (0.4688)	loss 4.8690 (3.9518)	grad_norm 1.3094 (nan)	mem 14851MB
[2022-11-06 01:07:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [32/300][1250/1251]	eta 0:00:00 lr 0.000971	time 0.4576 (0.4685)	loss 4.7156 (3.9510)	grad_norm 1.2288 (nan)	mem 14851MB
[2022-11-06 01:07:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 32 training takes 0:09:46
[2022-11-06 01:07:31 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_32.pth saving......
[2022-11-06 01:07:32 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_32.pth saved !!!
[2022-11-06 01:07:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.513 (1.513)	Loss 1.2807 (1.2807)	Acc@1 70.117 (70.117)	Acc@5 90.723 (90.723)	Mem 14851MB
[2022-11-06 01:07:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 70.090 Acc@5 90.146
[2022-11-06 01:07:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 70.1%
[2022-11-06 01:07:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.634 (1.634)	Loss 1.4223 (1.4223)	Acc@1 69.434 (69.434)	Acc@5 89.258 (89.258)	Mem 14851MB
[2022-11-06 01:07:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 68.646 Acc@5 88.746
[2022-11-06 01:07:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 68.6%
[2022-11-06 01:07:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 70.23% at 31 epoch
[2022-11-06 01:07:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][0/1251]	eta 0:43:27 lr 0.000971	time 2.0844 (2.0844)	loss 4.3840 (4.3840)	grad_norm 1.1728 (1.1728)	mem 14851MB
[2022-11-06 01:08:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][50/1251]	eta 0:10:04 lr 0.000971	time 0.4710 (0.5030)	loss 3.3986 (3.9317)	grad_norm 1.1043 (1.2147)	mem 14851MB
[2022-11-06 01:08:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][100/1251]	eta 0:09:21 lr 0.000971	time 0.4680 (0.4880)	loss 4.1219 (3.9452)	grad_norm 1.2462 (1.2275)	mem 14851MB
[2022-11-06 01:09:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][150/1251]	eta 0:08:49 lr 0.000971	time 0.4612 (0.4805)	loss 3.8044 (3.9383)	grad_norm 1.2163 (1.2281)	mem 14851MB
[2022-11-06 01:09:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][200/1251]	eta 0:08:21 lr 0.000970	time 0.4756 (0.4773)	loss 2.8823 (3.9097)	grad_norm 1.1406 (1.2366)	mem 14851MB
[2022-11-06 01:09:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][250/1251]	eta 0:07:55 lr 0.000970	time 0.4656 (0.4747)	loss 4.3406 (3.8975)	grad_norm 1.2364 (1.2385)	mem 14851MB
[2022-11-06 01:10:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][300/1251]	eta 0:07:30 lr 0.000970	time 0.5531 (0.4736)	loss 4.6124 (3.9193)	grad_norm 1.1362 (1.2405)	mem 14851MB
[2022-11-06 01:10:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][350/1251]	eta 0:07:05 lr 0.000970	time 0.4655 (0.4726)	loss 3.1732 (3.9068)	grad_norm 1.2536 (1.2395)	mem 14851MB
[2022-11-06 01:10:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][400/1251]	eta 0:06:41 lr 0.000970	time 0.4576 (0.4717)	loss 3.4083 (3.8953)	grad_norm 1.2329 (1.2377)	mem 14851MB
[2022-11-06 01:11:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][450/1251]	eta 0:06:17 lr 0.000970	time 0.4615 (0.4709)	loss 4.4666 (3.8900)	grad_norm 1.1805 (1.2370)	mem 14851MB
[2022-11-06 01:11:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][500/1251]	eta 0:05:53 lr 0.000970	time 0.4642 (0.4707)	loss 4.2954 (3.8884)	grad_norm 1.1421 (1.2355)	mem 14851MB
[2022-11-06 01:12:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][550/1251]	eta 0:05:29 lr 0.000970	time 0.4695 (0.4706)	loss 4.8105 (3.8866)	grad_norm 1.3063 (1.2365)	mem 14851MB
[2022-11-06 01:12:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][600/1251]	eta 0:05:06 lr 0.000970	time 0.4624 (0.4703)	loss 3.2418 (3.8829)	grad_norm 1.2192 (1.2388)	mem 14851MB
[2022-11-06 01:12:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][650/1251]	eta 0:04:42 lr 0.000970	time 0.4514 (0.4701)	loss 4.1494 (3.8888)	grad_norm 1.1540 (1.2352)	mem 14851MB
[2022-11-06 01:13:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][700/1251]	eta 0:04:18 lr 0.000970	time 0.4557 (0.4698)	loss 4.4667 (3.9030)	grad_norm 1.0880 (1.2350)	mem 14851MB
[2022-11-06 01:13:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][750/1251]	eta 0:03:55 lr 0.000970	time 0.4822 (0.4698)	loss 4.3266 (3.9051)	grad_norm 1.3681 (1.2344)	mem 14851MB
[2022-11-06 01:14:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][800/1251]	eta 0:03:31 lr 0.000970	time 0.4682 (0.4698)	loss 3.1273 (3.9084)	grad_norm 1.2748 (1.2345)	mem 14851MB
[2022-11-06 01:14:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][850/1251]	eta 0:03:08 lr 0.000970	time 0.4675 (0.4696)	loss 4.1804 (3.9087)	grad_norm 1.3747 (1.2354)	mem 14851MB
[2022-11-06 01:14:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][900/1251]	eta 0:02:44 lr 0.000969	time 0.4552 (0.4693)	loss 3.1334 (3.9096)	grad_norm 1.2271 (nan)	mem 14851MB
[2022-11-06 01:15:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][950/1251]	eta 0:02:21 lr 0.000969	time 0.4668 (0.4693)	loss 4.6019 (3.9157)	grad_norm 1.1084 (nan)	mem 14851MB
[2022-11-06 01:15:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][1000/1251]	eta 0:01:57 lr 0.000969	time 0.4644 (0.4692)	loss 4.0855 (3.9140)	grad_norm 1.2316 (nan)	mem 14851MB
[2022-11-06 01:16:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][1050/1251]	eta 0:01:34 lr 0.000969	time 0.4712 (0.4692)	loss 4.0541 (3.9175)	grad_norm 1.1723 (nan)	mem 14851MB
[2022-11-06 01:16:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][1100/1251]	eta 0:01:10 lr 0.000969	time 0.4540 (0.4691)	loss 3.9643 (3.9143)	grad_norm 1.2729 (nan)	mem 14851MB
[2022-11-06 01:16:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][1150/1251]	eta 0:00:47 lr 0.000969	time 0.4704 (0.4690)	loss 2.6633 (3.9168)	grad_norm 1.3692 (nan)	mem 14851MB
[2022-11-06 01:17:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][1200/1251]	eta 0:00:23 lr 0.000969	time 0.4723 (0.4689)	loss 4.4780 (3.9187)	grad_norm 1.2593 (nan)	mem 14851MB
[2022-11-06 01:17:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [33/300][1250/1251]	eta 0:00:00 lr 0.000969	time 0.4568 (0.4687)	loss 4.2014 (3.9216)	grad_norm 1.3653 (nan)	mem 14851MB
[2022-11-06 01:17:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 33 training takes 0:09:46
[2022-11-06 01:17:36 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_33.pth saving......
[2022-11-06 01:17:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_33.pth saved !!!
[2022-11-06 01:17:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.509 (1.509)	Loss 1.3605 (1.3605)	Acc@1 69.141 (69.141)	Acc@5 88.965 (88.965)	Mem 14851MB
[2022-11-06 01:17:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 70.504 Acc@5 90.232
[2022-11-06 01:17:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 70.5%
[2022-11-06 01:17:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.591 (1.591)	Loss 1.3840 (1.3840)	Acc@1 68.555 (68.555)	Acc@5 89.355 (89.355)	Mem 14851MB
[2022-11-06 01:17:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 69.268 Acc@5 89.182
[2022-11-06 01:17:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 69.3%
[2022-11-06 01:17:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 70.50% at 33 epoch
[2022-11-06 01:17:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][0/1251]	eta 0:40:41 lr 0.000969	time 1.9520 (1.9520)	loss 4.0299 (4.0299)	grad_norm 1.0246 (1.0246)	mem 14851MB
[2022-11-06 01:18:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][50/1251]	eta 0:10:01 lr 0.000969	time 0.4597 (0.5012)	loss 4.2895 (3.7999)	grad_norm 1.2393 (1.2301)	mem 14851MB
[2022-11-06 01:18:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][100/1251]	eta 0:09:19 lr 0.000969	time 0.4602 (0.4865)	loss 4.4142 (3.8124)	grad_norm 1.3156 (1.2258)	mem 14851MB
[2022-11-06 01:19:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][150/1251]	eta 0:08:48 lr 0.000969	time 0.4702 (0.4803)	loss 4.9162 (3.8405)	grad_norm 1.1320 (1.2423)	mem 14851MB
[2022-11-06 01:19:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][200/1251]	eta 0:08:21 lr 0.000969	time 0.4639 (0.4768)	loss 4.1917 (3.8524)	grad_norm 1.2002 (1.2333)	mem 14851MB
[2022-11-06 01:19:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][250/1251]	eta 0:07:55 lr 0.000969	time 0.4776 (0.4750)	loss 4.5243 (3.8600)	grad_norm 1.2061 (1.2328)	mem 14851MB
[2022-11-06 01:20:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][300/1251]	eta 0:07:30 lr 0.000969	time 0.4657 (0.4736)	loss 4.0246 (3.8763)	grad_norm 1.2233 (1.2309)	mem 14851MB
[2022-11-06 01:20:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][350/1251]	eta 0:07:05 lr 0.000968	time 0.4539 (0.4725)	loss 4.3471 (3.8886)	grad_norm 1.1382 (1.2277)	mem 14851MB
[2022-11-06 01:21:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][400/1251]	eta 0:06:41 lr 0.000968	time 0.4682 (0.4718)	loss 3.0896 (3.8811)	grad_norm 1.2086 (1.2346)	mem 14851MB
[2022-11-06 01:21:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][450/1251]	eta 0:06:17 lr 0.000968	time 0.4744 (0.4710)	loss 3.3944 (3.8745)	grad_norm 1.3340 (1.2357)	mem 14851MB
[2022-11-06 01:21:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][500/1251]	eta 0:05:53 lr 0.000968	time 0.4597 (0.4705)	loss 4.0866 (3.8789)	grad_norm 1.2811 (1.2355)	mem 14851MB
[2022-11-06 01:22:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][550/1251]	eta 0:05:29 lr 0.000968	time 0.4608 (0.4704)	loss 4.1981 (3.8897)	grad_norm 1.1555 (1.2383)	mem 14851MB
[2022-11-06 01:22:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][600/1251]	eta 0:05:06 lr 0.000968	time 0.4753 (0.4701)	loss 3.2445 (3.8906)	grad_norm 1.3502 (1.2362)	mem 14851MB
[2022-11-06 01:23:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][650/1251]	eta 0:04:42 lr 0.000968	time 0.4615 (0.4699)	loss 4.1502 (3.8980)	grad_norm 1.3688 (1.2356)	mem 14851MB
[2022-11-06 01:23:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][700/1251]	eta 0:04:18 lr 0.000968	time 0.4690 (0.4695)	loss 4.0610 (3.9010)	grad_norm 1.1583 (1.2360)	mem 14851MB
[2022-11-06 01:23:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][750/1251]	eta 0:03:55 lr 0.000968	time 0.4653 (0.4693)	loss 3.4475 (3.9127)	grad_norm 1.0869 (1.2354)	mem 14851MB
[2022-11-06 01:24:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][800/1251]	eta 0:03:31 lr 0.000968	time 0.4562 (0.4694)	loss 3.7699 (3.9124)	grad_norm 1.2260 (1.2342)	mem 14851MB
[2022-11-06 01:24:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][850/1251]	eta 0:03:08 lr 0.000968	time 0.4584 (0.4693)	loss 4.7040 (3.9144)	grad_norm 1.4314 (1.2337)	mem 14851MB
[2022-11-06 01:24:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][900/1251]	eta 0:02:44 lr 0.000968	time 0.4548 (0.4692)	loss 4.1692 (3.9172)	grad_norm 1.1135 (1.2329)	mem 14851MB
[2022-11-06 01:25:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][950/1251]	eta 0:02:21 lr 0.000968	time 0.4514 (0.4690)	loss 4.1496 (3.9157)	grad_norm 1.4941 (1.2324)	mem 14851MB
[2022-11-06 01:25:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][1000/1251]	eta 0:01:57 lr 0.000967	time 0.4632 (0.4688)	loss 4.2032 (3.9083)	grad_norm 1.1572 (1.2317)	mem 14851MB
[2022-11-06 01:26:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][1050/1251]	eta 0:01:34 lr 0.000967	time 0.4699 (0.4689)	loss 2.7861 (3.9049)	grad_norm 1.1247 (1.2304)	mem 14851MB
[2022-11-06 01:26:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][1100/1251]	eta 0:01:10 lr 0.000967	time 0.4630 (0.4688)	loss 3.8894 (3.9040)	grad_norm 1.3193 (1.2302)	mem 14851MB
[2022-11-06 01:26:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][1150/1251]	eta 0:00:47 lr 0.000967	time 0.4650 (0.4688)	loss 3.8089 (3.9056)	grad_norm 1.0639 (1.2298)	mem 14851MB
[2022-11-06 01:27:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][1200/1251]	eta 0:00:23 lr 0.000967	time 0.4542 (0.4686)	loss 4.8410 (3.9044)	grad_norm 1.1631 (1.2291)	mem 14851MB
[2022-11-06 01:27:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [34/300][1250/1251]	eta 0:00:00 lr 0.000967	time 0.4565 (0.4685)	loss 4.3314 (3.9036)	grad_norm 1.3812 (1.2290)	mem 14851MB
[2022-11-06 01:27:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 34 training takes 0:09:46
[2022-11-06 01:27:40 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_34.pth saving......
[2022-11-06 01:27:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_34.pth saved !!!
[2022-11-06 01:27:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.531 (1.531)	Loss 1.3867 (1.3867)	Acc@1 68.164 (68.164)	Acc@5 89.551 (89.551)	Mem 14851MB
[2022-11-06 01:27:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 70.260 Acc@5 90.316
[2022-11-06 01:27:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 70.3%
[2022-11-06 01:27:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.600 (1.600)	Loss 1.3825 (1.3825)	Acc@1 68.359 (68.359)	Acc@5 88.574 (88.574)	Mem 14851MB
[2022-11-06 01:27:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 69.854 Acc@5 89.576
[2022-11-06 01:27:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 69.9%
[2022-11-06 01:27:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 70.50% at 33 epoch
[2022-11-06 01:28:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][0/1251]	eta 0:43:24 lr 0.000967	time 2.0820 (2.0820)	loss 4.4126 (4.4126)	grad_norm 1.4485 (1.4485)	mem 14851MB
[2022-11-06 01:28:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][50/1251]	eta 0:10:04 lr 0.000967	time 0.4527 (0.5035)	loss 3.2031 (3.8943)	grad_norm 1.1972 (1.2181)	mem 14851MB
[2022-11-06 01:28:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][100/1251]	eta 0:09:20 lr 0.000967	time 0.4729 (0.4870)	loss 2.7887 (3.8778)	grad_norm 1.2258 (1.2419)	mem 14851MB
[2022-11-06 01:29:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][150/1251]	eta 0:08:48 lr 0.000967	time 0.4741 (0.4802)	loss 3.9455 (3.9003)	grad_norm 1.5483 (1.2490)	mem 14851MB
[2022-11-06 01:29:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][200/1251]	eta 0:08:21 lr 0.000967	time 0.4641 (0.4770)	loss 3.1566 (3.8937)	grad_norm 1.2388 (1.2473)	mem 14851MB
[2022-11-06 01:29:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][250/1251]	eta 0:07:55 lr 0.000967	time 0.4718 (0.4746)	loss 3.8363 (3.9011)	grad_norm 1.1648 (1.2492)	mem 14851MB
[2022-11-06 01:30:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][300/1251]	eta 0:07:29 lr 0.000967	time 0.4672 (0.4731)	loss 4.3720 (3.9093)	grad_norm 1.2986 (1.2473)	mem 14851MB
[2022-11-06 01:30:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][350/1251]	eta 0:07:05 lr 0.000967	time 0.4676 (0.4718)	loss 2.9442 (3.9147)	grad_norm 1.1606 (1.2461)	mem 14851MB
[2022-11-06 01:31:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][400/1251]	eta 0:06:40 lr 0.000967	time 0.4707 (0.4712)	loss 4.3589 (3.9267)	grad_norm 1.0847 (nan)	mem 14851MB
[2022-11-06 01:31:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][450/1251]	eta 0:06:16 lr 0.000966	time 0.4713 (0.4705)	loss 3.6336 (3.9297)	grad_norm 1.2074 (nan)	mem 14851MB
[2022-11-06 01:31:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][500/1251]	eta 0:05:53 lr 0.000966	time 0.4642 (0.4702)	loss 4.2235 (3.9294)	grad_norm 1.1404 (nan)	mem 14851MB
[2022-11-06 01:32:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][550/1251]	eta 0:05:29 lr 0.000966	time 0.4706 (0.4701)	loss 3.4083 (3.9249)	grad_norm 1.4962 (nan)	mem 14851MB
[2022-11-06 01:32:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][600/1251]	eta 0:05:05 lr 0.000966	time 0.4648 (0.4699)	loss 3.6287 (3.9334)	grad_norm 1.2088 (nan)	mem 14851MB
[2022-11-06 01:33:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][650/1251]	eta 0:04:42 lr 0.000966	time 0.4714 (0.4695)	loss 4.2085 (3.9233)	grad_norm 1.3460 (nan)	mem 14851MB
[2022-11-06 01:33:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][700/1251]	eta 0:04:18 lr 0.000966	time 0.4580 (0.4695)	loss 4.2819 (3.9150)	grad_norm 1.1960 (nan)	mem 14851MB
[2022-11-06 01:33:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][750/1251]	eta 0:03:55 lr 0.000966	time 0.4727 (0.4693)	loss 3.1032 (3.9047)	grad_norm 1.2526 (nan)	mem 14851MB
[2022-11-06 01:34:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][800/1251]	eta 0:03:31 lr 0.000966	time 0.4621 (0.4693)	loss 4.4384 (3.9105)	grad_norm 1.3120 (nan)	mem 14851MB
[2022-11-06 01:34:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][850/1251]	eta 0:03:08 lr 0.000966	time 0.4744 (0.4690)	loss 4.0896 (3.9110)	grad_norm 1.1624 (nan)	mem 14851MB
[2022-11-06 01:35:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][900/1251]	eta 0:02:44 lr 0.000966	time 0.5519 (0.4689)	loss 3.5363 (3.9131)	grad_norm 1.1165 (nan)	mem 14851MB
[2022-11-06 01:35:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][950/1251]	eta 0:02:21 lr 0.000966	time 0.4593 (0.4687)	loss 4.4021 (3.9186)	grad_norm 1.1892 (nan)	mem 14851MB
[2022-11-06 01:35:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][1000/1251]	eta 0:01:57 lr 0.000966	time 0.4745 (0.4687)	loss 3.7151 (3.9175)	grad_norm 1.2892 (nan)	mem 14851MB
[2022-11-06 01:36:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][1050/1251]	eta 0:01:34 lr 0.000966	time 0.4685 (0.4687)	loss 3.7215 (3.9160)	grad_norm 1.2156 (nan)	mem 14851MB
[2022-11-06 01:36:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][1100/1251]	eta 0:01:10 lr 0.000965	time 0.4738 (0.4687)	loss 4.8731 (3.9103)	grad_norm 1.3449 (nan)	mem 14851MB
[2022-11-06 01:36:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][1150/1251]	eta 0:00:47 lr 0.000965	time 0.4585 (0.4686)	loss 4.2725 (3.9081)	grad_norm 1.1832 (nan)	mem 14851MB
[2022-11-06 01:37:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][1200/1251]	eta 0:00:23 lr 0.000965	time 0.4618 (0.4684)	loss 3.1072 (3.9096)	grad_norm 1.3235 (nan)	mem 14851MB
[2022-11-06 01:37:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [35/300][1250/1251]	eta 0:00:00 lr 0.000965	time 0.4578 (0.4682)	loss 4.0403 (3.9120)	grad_norm 1.2663 (nan)	mem 14851MB
[2022-11-06 01:37:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 35 training takes 0:09:45
[2022-11-06 01:37:44 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_35.pth saving......
[2022-11-06 01:37:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_35.pth saved !!!
[2022-11-06 01:37:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.655 (1.655)	Loss 1.2437 (1.2437)	Acc@1 72.070 (72.070)	Acc@5 90.918 (90.918)	Mem 14851MB
[2022-11-06 01:37:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.198 Acc@5 90.610
[2022-11-06 01:37:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 71.2%
[2022-11-06 01:37:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.700 (1.700)	Loss 1.3181 (1.3181)	Acc@1 70.312 (70.312)	Acc@5 89.160 (89.160)	Mem 14851MB
[2022-11-06 01:38:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 70.430 Acc@5 89.904
[2022-11-06 01:38:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 70.4%
[2022-11-06 01:38:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 71.20% at 35 epoch
[2022-11-06 01:38:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][0/1251]	eta 0:41:25 lr 0.000965	time 1.9866 (1.9866)	loss 4.1294 (4.1294)	grad_norm 1.1009 (1.1009)	mem 14851MB
[2022-11-06 01:38:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][50/1251]	eta 0:10:05 lr 0.000965	time 0.4685 (0.5038)	loss 4.0588 (3.9889)	grad_norm 1.2163 (1.2421)	mem 14851MB
[2022-11-06 01:38:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][100/1251]	eta 0:09:18 lr 0.000965	time 0.4750 (0.4856)	loss 3.9999 (3.9764)	grad_norm 1.0776 (1.2304)	mem 14851MB
[2022-11-06 01:39:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][150/1251]	eta 0:08:47 lr 0.000965	time 0.4794 (0.4793)	loss 3.6487 (3.9493)	grad_norm 1.1302 (1.2277)	mem 14851MB
[2022-11-06 01:39:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][200/1251]	eta 0:08:20 lr 0.000965	time 0.4653 (0.4760)	loss 4.8350 (3.9216)	grad_norm 1.4569 (1.2307)	mem 14851MB
[2022-11-06 01:40:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][250/1251]	eta 0:07:54 lr 0.000965	time 0.4695 (0.4742)	loss 3.9608 (3.9123)	grad_norm 1.1641 (1.2330)	mem 14851MB
[2022-11-06 01:40:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][300/1251]	eta 0:07:29 lr 0.000965	time 0.4700 (0.4726)	loss 4.0691 (3.9145)	grad_norm 1.1684 (1.2360)	mem 14851MB
[2022-11-06 01:40:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][350/1251]	eta 0:07:04 lr 0.000965	time 0.4664 (0.4716)	loss 4.1409 (3.8944)	grad_norm 1.2577 (1.2366)	mem 14851MB
[2022-11-06 01:41:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][400/1251]	eta 0:06:40 lr 0.000965	time 0.4635 (0.4707)	loss 4.2470 (3.8980)	grad_norm 1.1774 (1.2373)	mem 14851MB
[2022-11-06 01:41:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][450/1251]	eta 0:06:16 lr 0.000965	time 0.4637 (0.4701)	loss 4.7697 (3.8993)	grad_norm 1.1628 (1.2368)	mem 14851MB
[2022-11-06 01:41:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][500/1251]	eta 0:05:52 lr 0.000964	time 0.4625 (0.4698)	loss 3.7578 (3.8791)	grad_norm 1.1049 (1.2354)	mem 14851MB
[2022-11-06 01:42:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][550/1251]	eta 0:05:29 lr 0.000964	time 0.4594 (0.4702)	loss 4.2480 (3.8764)	grad_norm 1.3257 (1.2348)	mem 14851MB
[2022-11-06 01:42:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][600/1251]	eta 0:05:05 lr 0.000964	time 0.4647 (0.4699)	loss 4.0891 (3.8780)	grad_norm 1.1882 (1.2372)	mem 14851MB
[2022-11-06 01:43:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][650/1251]	eta 0:04:42 lr 0.000964	time 0.4577 (0.4695)	loss 3.8910 (3.8809)	grad_norm 1.1634 (1.2384)	mem 14851MB
[2022-11-06 01:43:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][700/1251]	eta 0:04:18 lr 0.000964	time 0.4573 (0.4692)	loss 4.2478 (3.8815)	grad_norm 1.1802 (1.2384)	mem 14851MB
[2022-11-06 01:43:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][750/1251]	eta 0:03:55 lr 0.000964	time 0.4665 (0.4692)	loss 4.3226 (3.8819)	grad_norm 1.2971 (1.2360)	mem 14851MB
[2022-11-06 01:44:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][800/1251]	eta 0:03:31 lr 0.000964	time 0.4685 (0.4692)	loss 3.4917 (3.8772)	grad_norm 1.2452 (1.2331)	mem 14851MB
[2022-11-06 01:44:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][850/1251]	eta 0:03:08 lr 0.000964	time 0.4665 (0.4691)	loss 4.2809 (3.8656)	grad_norm 1.1844 (1.2339)	mem 14851MB
[2022-11-06 01:45:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][900/1251]	eta 0:02:44 lr 0.000964	time 0.4714 (0.4690)	loss 3.2072 (3.8680)	grad_norm 1.2080 (1.2325)	mem 14851MB
[2022-11-06 01:45:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][950/1251]	eta 0:02:21 lr 0.000964	time 0.4699 (0.4688)	loss 4.0785 (3.8756)	grad_norm 1.3672 (1.2331)	mem 14851MB
[2022-11-06 01:45:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][1000/1251]	eta 0:01:57 lr 0.000964	time 0.4578 (0.4687)	loss 2.8769 (3.8779)	grad_norm 1.2349 (1.2328)	mem 14851MB
[2022-11-06 01:46:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][1050/1251]	eta 0:01:34 lr 0.000964	time 0.4591 (0.4688)	loss 3.5646 (3.8776)	grad_norm 1.2900 (1.2332)	mem 14851MB
[2022-11-06 01:46:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][1100/1251]	eta 0:01:10 lr 0.000964	time 0.4637 (0.4687)	loss 3.7617 (3.8784)	grad_norm 1.3738 (1.2342)	mem 14851MB
[2022-11-06 01:47:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][1150/1251]	eta 0:00:47 lr 0.000963	time 0.4631 (0.4686)	loss 4.3530 (3.8831)	grad_norm 1.3406 (1.2328)	mem 14851MB
[2022-11-06 01:47:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][1200/1251]	eta 0:00:23 lr 0.000963	time 0.4686 (0.4684)	loss 4.3474 (3.8862)	grad_norm 1.0797 (1.2321)	mem 14851MB
[2022-11-06 01:47:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [36/300][1250/1251]	eta 0:00:00 lr 0.000963	time 0.4582 (0.4682)	loss 3.8145 (3.8860)	grad_norm 1.3448 (nan)	mem 14851MB
[2022-11-06 01:47:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 36 training takes 0:09:45
[2022-11-06 01:47:48 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_36.pth saving......
[2022-11-06 01:47:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_36.pth saved !!!
[2022-11-06 01:47:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.568 (1.568)	Loss 1.2777 (1.2777)	Acc@1 69.824 (69.824)	Acc@5 91.016 (91.016)	Mem 14851MB
[2022-11-06 01:47:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.268 Acc@5 90.854
[2022-11-06 01:47:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 71.3%
[2022-11-06 01:47:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.705 (1.705)	Loss 1.3093 (1.3093)	Acc@1 71.582 (71.582)	Acc@5 89.453 (89.453)	Mem 14851MB
[2022-11-06 01:48:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 70.868 Acc@5 90.226
[2022-11-06 01:48:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 70.9%
[2022-11-06 01:48:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 71.27% at 36 epoch
[2022-11-06 01:48:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][0/1251]	eta 0:41:14 lr 0.000963	time 1.9784 (1.9784)	loss 3.9982 (3.9982)	grad_norm 1.1340 (1.1340)	mem 14851MB
[2022-11-06 01:48:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][50/1251]	eta 0:10:06 lr 0.000963	time 0.4712 (0.5052)	loss 4.1340 (3.9471)	grad_norm 1.1900 (1.2523)	mem 14851MB
[2022-11-06 01:48:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][100/1251]	eta 0:09:21 lr 0.000963	time 0.4608 (0.4882)	loss 4.3538 (3.9160)	grad_norm 1.1986 (1.2546)	mem 14851MB
[2022-11-06 01:49:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][150/1251]	eta 0:08:49 lr 0.000963	time 0.4652 (0.4807)	loss 3.8819 (3.9082)	grad_norm 1.3179 (1.2475)	mem 14851MB
[2022-11-06 01:49:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][200/1251]	eta 0:08:21 lr 0.000963	time 0.4628 (0.4768)	loss 3.8110 (3.8930)	grad_norm 1.3445 (1.2413)	mem 14851MB
[2022-11-06 01:50:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][250/1251]	eta 0:07:55 lr 0.000963	time 0.4661 (0.4748)	loss 2.7505 (3.8674)	grad_norm 1.1685 (nan)	mem 14851MB
[2022-11-06 01:50:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][300/1251]	eta 0:07:30 lr 0.000963	time 0.4622 (0.4733)	loss 3.1355 (3.8693)	grad_norm 1.2133 (nan)	mem 14851MB
[2022-11-06 01:50:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][350/1251]	eta 0:07:05 lr 0.000963	time 0.4536 (0.4725)	loss 4.5191 (3.8723)	grad_norm 1.1927 (nan)	mem 14851MB
[2022-11-06 01:51:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][400/1251]	eta 0:06:41 lr 0.000963	time 0.4556 (0.4719)	loss 4.0349 (3.8642)	grad_norm 1.1274 (nan)	mem 14851MB
[2022-11-06 01:51:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][450/1251]	eta 0:06:17 lr 0.000963	time 0.4657 (0.4711)	loss 3.6246 (3.8519)	grad_norm 1.1889 (nan)	mem 14851MB
[2022-11-06 01:52:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][500/1251]	eta 0:05:53 lr 0.000963	time 0.4705 (0.4705)	loss 3.3520 (3.8589)	grad_norm 1.1015 (nan)	mem 14851MB
[2022-11-06 01:52:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][550/1251]	eta 0:05:29 lr 0.000962	time 0.4559 (0.4704)	loss 4.2472 (3.8711)	grad_norm 1.1820 (nan)	mem 14851MB
[2022-11-06 01:52:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][600/1251]	eta 0:05:05 lr 0.000962	time 0.4547 (0.4700)	loss 4.5388 (3.8695)	grad_norm 1.1064 (nan)	mem 14851MB
[2022-11-06 01:53:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][650/1251]	eta 0:04:42 lr 0.000962	time 0.4588 (0.4696)	loss 4.0979 (3.8687)	grad_norm 1.3523 (nan)	mem 14851MB
[2022-11-06 01:53:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][700/1251]	eta 0:04:18 lr 0.000962	time 0.4635 (0.4696)	loss 2.9598 (3.8741)	grad_norm 1.0647 (nan)	mem 14851MB
[2022-11-06 01:53:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][750/1251]	eta 0:03:55 lr 0.000962	time 0.4766 (0.4693)	loss 3.4965 (3.8851)	grad_norm 1.2005 (nan)	mem 14851MB
[2022-11-06 01:54:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][800/1251]	eta 0:03:31 lr 0.000962	time 0.4662 (0.4692)	loss 3.7617 (3.8840)	grad_norm 1.1430 (nan)	mem 14851MB
[2022-11-06 01:54:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][850/1251]	eta 0:03:08 lr 0.000962	time 0.4650 (0.4692)	loss 4.2336 (3.8795)	grad_norm 1.0344 (nan)	mem 14851MB
[2022-11-06 01:55:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][900/1251]	eta 0:02:44 lr 0.000962	time 0.4681 (0.4689)	loss 3.9329 (3.8835)	grad_norm 1.2340 (nan)	mem 14851MB
[2022-11-06 01:55:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][950/1251]	eta 0:02:21 lr 0.000962	time 0.4731 (0.4688)	loss 4.5127 (3.8822)	grad_norm 1.0587 (nan)	mem 14851MB
[2022-11-06 01:55:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][1000/1251]	eta 0:01:57 lr 0.000962	time 0.4657 (0.4687)	loss 4.0729 (3.8846)	grad_norm 1.2630 (nan)	mem 14851MB
[2022-11-06 01:56:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][1050/1251]	eta 0:01:34 lr 0.000962	time 0.5386 (0.4688)	loss 4.3657 (3.8824)	grad_norm 1.3320 (nan)	mem 14851MB
[2022-11-06 01:56:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][1100/1251]	eta 0:01:10 lr 0.000962	time 0.4695 (0.4688)	loss 4.6304 (3.8851)	grad_norm 1.1739 (nan)	mem 14851MB
[2022-11-06 01:57:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][1150/1251]	eta 0:00:47 lr 0.000961	time 0.4805 (0.4686)	loss 4.3490 (3.8913)	grad_norm 1.2375 (nan)	mem 14851MB
[2022-11-06 01:57:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][1200/1251]	eta 0:00:23 lr 0.000961	time 0.4655 (0.4686)	loss 4.4218 (3.8930)	grad_norm 1.1462 (nan)	mem 14851MB
[2022-11-06 01:57:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [37/300][1250/1251]	eta 0:00:00 lr 0.000961	time 0.4577 (0.4684)	loss 3.8738 (3.8913)	grad_norm 1.3956 (nan)	mem 14851MB
[2022-11-06 01:57:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 37 training takes 0:09:46
[2022-11-06 01:57:52 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_37.pth saving......
[2022-11-06 01:57:53 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_37.pth saved !!!
[2022-11-06 01:57:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.573 (1.573)	Loss 1.1195 (1.1195)	Acc@1 74.316 (74.316)	Acc@5 92.090 (92.090)	Mem 14851MB
[2022-11-06 01:58:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.478 Acc@5 90.908
[2022-11-06 01:58:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 71.5%
[2022-11-06 01:58:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.650 (1.650)	Loss 1.3307 (1.3307)	Acc@1 70.312 (70.312)	Acc@5 88.574 (88.574)	Mem 14851MB
[2022-11-06 01:58:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.386 Acc@5 90.498
[2022-11-06 01:58:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 71.4%
[2022-11-06 01:58:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 71.48% at 37 epoch
[2022-11-06 01:58:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][0/1251]	eta 0:40:19 lr 0.000961	time 1.9340 (1.9340)	loss 4.1447 (4.1447)	grad_norm 1.3144 (1.3144)	mem 14851MB
[2022-11-06 01:58:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][50/1251]	eta 0:09:57 lr 0.000961	time 0.4725 (0.4975)	loss 4.0122 (3.8076)	grad_norm 1.1980 (1.2553)	mem 14851MB
[2022-11-06 01:58:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][100/1251]	eta 0:09:16 lr 0.000961	time 0.4552 (0.4832)	loss 2.8261 (3.8425)	grad_norm 1.3098 (1.2365)	mem 14851MB
[2022-11-06 01:59:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][150/1251]	eta 0:08:45 lr 0.000961	time 0.4638 (0.4777)	loss 4.1314 (3.8670)	grad_norm 1.2041 (1.2320)	mem 14851MB
[2022-11-06 01:59:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][200/1251]	eta 0:08:18 lr 0.000961	time 0.4742 (0.4746)	loss 4.4196 (3.8858)	grad_norm 1.2778 (1.2356)	mem 14851MB
[2022-11-06 02:00:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][250/1251]	eta 0:07:53 lr 0.000961	time 0.4630 (0.4735)	loss 4.6367 (3.9200)	grad_norm 1.2946 (1.2348)	mem 14851MB
[2022-11-06 02:00:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][300/1251]	eta 0:07:29 lr 0.000961	time 0.4627 (0.4722)	loss 3.6873 (3.9125)	grad_norm 1.1555 (1.2356)	mem 14851MB
[2022-11-06 02:00:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][350/1251]	eta 0:07:04 lr 0.000961	time 0.4673 (0.4714)	loss 2.7971 (3.9122)	grad_norm 1.2729 (1.2355)	mem 14851MB
[2022-11-06 02:01:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][400/1251]	eta 0:06:40 lr 0.000961	time 0.4699 (0.4708)	loss 4.6718 (3.9148)	grad_norm 1.2525 (1.2344)	mem 14851MB
[2022-11-06 02:01:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][450/1251]	eta 0:06:16 lr 0.000961	time 0.4676 (0.4705)	loss 4.2582 (3.9071)	grad_norm 1.2983 (1.2338)	mem 14851MB
[2022-11-06 02:02:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][500/1251]	eta 0:05:52 lr 0.000961	time 0.4590 (0.4698)	loss 4.0375 (3.9152)	grad_norm 1.1470 (1.2337)	mem 14851MB
[2022-11-06 02:02:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][550/1251]	eta 0:05:29 lr 0.000960	time 0.4674 (0.4696)	loss 2.9604 (3.9081)	grad_norm 1.3184 (1.2315)	mem 14851MB
[2022-11-06 02:02:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][600/1251]	eta 0:05:05 lr 0.000960	time 0.4651 (0.4692)	loss 4.3862 (3.8998)	grad_norm 1.2889 (1.2350)	mem 14851MB
[2022-11-06 02:03:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][650/1251]	eta 0:04:41 lr 0.000960	time 0.4562 (0.4692)	loss 4.5140 (3.9003)	grad_norm 1.2913 (1.2338)	mem 14851MB
[2022-11-06 02:03:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][700/1251]	eta 0:04:18 lr 0.000960	time 0.4543 (0.4691)	loss 4.8572 (3.9059)	grad_norm 1.2149 (1.2350)	mem 14851MB
[2022-11-06 02:04:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][750/1251]	eta 0:03:54 lr 0.000960	time 0.4695 (0.4689)	loss 4.5612 (3.9092)	grad_norm 1.3472 (1.2387)	mem 14851MB
[2022-11-06 02:04:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][800/1251]	eta 0:03:31 lr 0.000960	time 0.4625 (0.4688)	loss 3.7555 (3.8999)	grad_norm 1.2003 (1.2400)	mem 14851MB
[2022-11-06 02:04:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][850/1251]	eta 0:03:07 lr 0.000960	time 0.4633 (0.4687)	loss 4.2024 (3.8983)	grad_norm 1.1354 (1.2392)	mem 14851MB
[2022-11-06 02:05:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][900/1251]	eta 0:02:44 lr 0.000960	time 0.4616 (0.4686)	loss 3.6853 (3.9058)	grad_norm 1.1858 (1.2410)	mem 14851MB
[2022-11-06 02:05:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][950/1251]	eta 0:02:21 lr 0.000960	time 0.4731 (0.4686)	loss 4.5374 (3.9059)	grad_norm 1.2854 (1.2410)	mem 14851MB
[2022-11-06 02:05:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][1000/1251]	eta 0:01:57 lr 0.000960	time 0.4655 (0.4684)	loss 4.4752 (3.9008)	grad_norm 1.1377 (1.2394)	mem 14851MB
[2022-11-06 02:06:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][1050/1251]	eta 0:01:34 lr 0.000960	time 0.4581 (0.4684)	loss 4.2737 (3.8969)	grad_norm 1.1834 (1.2392)	mem 14851MB
[2022-11-06 02:06:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][1100/1251]	eta 0:01:10 lr 0.000960	time 0.4609 (0.4683)	loss 2.6319 (3.8947)	grad_norm 1.1402 (1.2383)	mem 14851MB
[2022-11-06 02:07:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][1150/1251]	eta 0:00:47 lr 0.000959	time 0.4654 (0.4682)	loss 4.7556 (3.8938)	grad_norm 1.2043 (1.2366)	mem 14851MB
[2022-11-06 02:07:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][1200/1251]	eta 0:00:23 lr 0.000959	time 0.4573 (0.4682)	loss 3.5637 (3.8891)	grad_norm 1.2166 (1.2368)	mem 14851MB
[2022-11-06 02:07:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [38/300][1250/1251]	eta 0:00:00 lr 0.000959	time 0.4607 (0.4680)	loss 4.6269 (3.8887)	grad_norm 1.2493 (1.2354)	mem 14851MB
[2022-11-06 02:07:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 38 training takes 0:09:45
[2022-11-06 02:07:56 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_38.pth saving......
[2022-11-06 02:07:57 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_38.pth saved !!!
[2022-11-06 02:07:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.474 (1.474)	Loss 1.1712 (1.1712)	Acc@1 73.535 (73.535)	Acc@5 91.211 (91.211)	Mem 14851MB
[2022-11-06 02:08:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.968 Acc@5 90.974
[2022-11-06 02:08:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.0%
[2022-11-06 02:08:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.622 (1.622)	Loss 1.2017 (1.2017)	Acc@1 72.754 (72.754)	Acc@5 91.504 (91.504)	Mem 14851MB
[2022-11-06 02:08:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.866 Acc@5 90.746
[2022-11-06 02:08:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 71.9%
[2022-11-06 02:08:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 71.97% at 38 epoch
[2022-11-06 02:08:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][0/1251]	eta 0:42:45 lr 0.000959	time 2.0505 (2.0505)	loss 3.5299 (3.5299)	grad_norm 1.1260 (1.1260)	mem 14851MB
[2022-11-06 02:08:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][50/1251]	eta 0:10:02 lr 0.000959	time 0.4586 (0.5013)	loss 4.1050 (3.8345)	grad_norm 1.2364 (1.2371)	mem 14851MB
[2022-11-06 02:09:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][100/1251]	eta 0:09:18 lr 0.000959	time 0.4632 (0.4849)	loss 3.6290 (3.8612)	grad_norm 1.2902 (1.2522)	mem 14851MB
[2022-11-06 02:09:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][150/1251]	eta 0:08:48 lr 0.000959	time 0.4660 (0.4798)	loss 4.1659 (3.9104)	grad_norm 1.3042 (1.2526)	mem 14851MB
[2022-11-06 02:09:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][200/1251]	eta 0:08:20 lr 0.000959	time 0.4605 (0.4762)	loss 2.8853 (3.9041)	grad_norm 1.2226 (1.2442)	mem 14851MB
[2022-11-06 02:10:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][250/1251]	eta 0:07:54 lr 0.000959	time 0.4633 (0.4744)	loss 3.5745 (3.8869)	grad_norm 1.1984 (1.2320)	mem 14851MB
[2022-11-06 02:10:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][300/1251]	eta 0:07:30 lr 0.000959	time 0.4586 (0.4732)	loss 4.5265 (3.8821)	grad_norm 1.1360 (1.2268)	mem 14851MB
[2022-11-06 02:11:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][350/1251]	eta 0:07:05 lr 0.000959	time 0.4591 (0.4725)	loss 4.0478 (3.8831)	grad_norm 1.2735 (1.2217)	mem 14851MB
[2022-11-06 02:11:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][400/1251]	eta 0:06:41 lr 0.000959	time 0.4706 (0.4716)	loss 4.2337 (3.8856)	grad_norm 1.1957 (1.2250)	mem 14851MB
[2022-11-06 02:11:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][450/1251]	eta 0:06:17 lr 0.000959	time 0.4673 (0.4710)	loss 3.7635 (3.8880)	grad_norm 1.2166 (1.2269)	mem 14851MB
[2022-11-06 02:12:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][500/1251]	eta 0:05:53 lr 0.000958	time 0.4679 (0.4704)	loss 4.1709 (3.8841)	grad_norm 1.4648 (1.2295)	mem 14851MB
[2022-11-06 02:12:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][550/1251]	eta 0:05:29 lr 0.000958	time 0.4691 (0.4702)	loss 3.9412 (3.8799)	grad_norm 1.1930 (1.2298)	mem 14851MB
[2022-11-06 02:12:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][600/1251]	eta 0:05:06 lr 0.000958	time 0.4548 (0.4702)	loss 4.0447 (3.8922)	grad_norm 1.2238 (1.2299)	mem 14851MB
[2022-11-06 02:13:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][650/1251]	eta 0:04:42 lr 0.000958	time 0.4576 (0.4700)	loss 3.8753 (3.8921)	grad_norm 1.2054 (1.2289)	mem 14851MB
[2022-11-06 02:13:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][700/1251]	eta 0:04:18 lr 0.000958	time 0.4730 (0.4699)	loss 2.8304 (3.8816)	grad_norm 1.1926 (1.2275)	mem 14851MB
[2022-11-06 02:14:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][750/1251]	eta 0:03:55 lr 0.000958	time 0.4666 (0.4696)	loss 4.4239 (3.8784)	grad_norm 1.4642 (1.2269)	mem 14851MB
[2022-11-06 02:14:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][800/1251]	eta 0:03:31 lr 0.000958	time 0.4700 (0.4696)	loss 4.2819 (3.8732)	grad_norm 1.4580 (1.2275)	mem 14851MB
[2022-11-06 02:14:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][850/1251]	eta 0:03:08 lr 0.000958	time 0.4612 (0.4695)	loss 3.0525 (3.8671)	grad_norm 1.2330 (1.2271)	mem 14851MB
[2022-11-06 02:15:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][900/1251]	eta 0:02:44 lr 0.000958	time 0.4617 (0.4694)	loss 3.9053 (3.8702)	grad_norm 1.1868 (1.2281)	mem 14851MB
[2022-11-06 02:15:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][950/1251]	eta 0:02:21 lr 0.000958	time 0.4625 (0.4693)	loss 4.1569 (3.8670)	grad_norm 1.2399 (1.2283)	mem 14851MB
[2022-11-06 02:16:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][1000/1251]	eta 0:01:57 lr 0.000958	time 0.4656 (0.4691)	loss 3.6567 (3.8644)	grad_norm 1.2451 (1.2302)	mem 14851MB
[2022-11-06 02:16:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][1050/1251]	eta 0:01:34 lr 0.000958	time 0.4871 (0.4691)	loss 3.1361 (3.8672)	grad_norm 1.3435 (1.2297)	mem 14851MB
[2022-11-06 02:16:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][1100/1251]	eta 0:01:10 lr 0.000957	time 0.4628 (0.4690)	loss 3.8997 (3.8697)	grad_norm 1.2076 (1.2300)	mem 14851MB
[2022-11-06 02:17:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][1150/1251]	eta 0:00:47 lr 0.000957	time 0.4523 (0.4690)	loss 4.2503 (3.8755)	grad_norm 1.2541 (1.2312)	mem 14851MB
[2022-11-06 02:17:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][1200/1251]	eta 0:00:23 lr 0.000957	time 0.4641 (0.4689)	loss 4.2040 (3.8782)	grad_norm 1.3458 (1.2312)	mem 14851MB
[2022-11-06 02:18:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [39/300][1250/1251]	eta 0:00:00 lr 0.000957	time 0.4574 (0.4687)	loss 3.1644 (3.8705)	grad_norm 1.1764 (1.2316)	mem 14851MB
[2022-11-06 02:18:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 39 training takes 0:09:46
[2022-11-06 02:18:01 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_39.pth saving......
[2022-11-06 02:18:01 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_39.pth saved !!!
[2022-11-06 02:18:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.489 (1.489)	Loss 1.1810 (1.1810)	Acc@1 73.535 (73.535)	Acc@5 92.285 (92.285)	Mem 14851MB
[2022-11-06 02:18:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.836 Acc@5 90.952
[2022-11-06 02:18:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 71.8%
[2022-11-06 02:18:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.553 (1.553)	Loss 1.2152 (1.2152)	Acc@1 72.559 (72.559)	Acc@5 91.406 (91.406)	Mem 14851MB
[2022-11-06 02:18:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.216 Acc@5 91.010
[2022-11-06 02:18:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 72.2%
[2022-11-06 02:18:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 72.22% at 39 epoch
[2022-11-06 02:18:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][0/1251]	eta 0:41:51 lr 0.000957	time 2.0073 (2.0073)	loss 2.7311 (2.7311)	grad_norm 1.2933 (1.2933)	mem 14851MB
[2022-11-06 02:18:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][50/1251]	eta 0:10:01 lr 0.000957	time 0.4700 (0.5009)	loss 2.8287 (3.8574)	grad_norm 1.3152 (1.3027)	mem 14851MB
[2022-11-06 02:19:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][100/1251]	eta 0:09:18 lr 0.000957	time 0.4696 (0.4850)	loss 4.3878 (3.7957)	grad_norm 1.1772 (1.2819)	mem 14851MB
[2022-11-06 02:19:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][150/1251]	eta 0:08:49 lr 0.000957	time 0.4535 (0.4806)	loss 4.0082 (3.8090)	grad_norm 1.1771 (1.2769)	mem 14851MB
[2022-11-06 02:19:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][200/1251]	eta 0:08:21 lr 0.000957	time 0.4599 (0.4773)	loss 3.3697 (3.7959)	grad_norm 1.1901 (1.2577)	mem 14851MB
[2022-11-06 02:20:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][250/1251]	eta 0:07:55 lr 0.000957	time 0.4633 (0.4750)	loss 4.2122 (3.8099)	grad_norm 1.0179 (1.2505)	mem 14851MB
[2022-11-06 02:20:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][300/1251]	eta 0:07:30 lr 0.000957	time 0.4627 (0.4733)	loss 4.4635 (3.8030)	grad_norm 1.1089 (1.2492)	mem 14851MB
[2022-11-06 02:21:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][350/1251]	eta 0:07:05 lr 0.000957	time 0.4735 (0.4723)	loss 3.4044 (3.8091)	grad_norm 1.2749 (1.2519)	mem 14851MB
[2022-11-06 02:21:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][400/1251]	eta 0:06:41 lr 0.000957	time 0.4703 (0.4717)	loss 4.6604 (3.8466)	grad_norm 1.1517 (1.2476)	mem 14851MB
[2022-11-06 02:21:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][450/1251]	eta 0:06:17 lr 0.000956	time 0.4648 (0.4712)	loss 3.2704 (3.8480)	grad_norm 1.2683 (1.2468)	mem 14851MB
[2022-11-06 02:22:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][500/1251]	eta 0:05:53 lr 0.000956	time 0.4597 (0.4706)	loss 4.6498 (3.8561)	grad_norm 1.3583 (nan)	mem 14851MB
[2022-11-06 02:22:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][550/1251]	eta 0:05:29 lr 0.000956	time 0.4695 (0.4704)	loss 4.3489 (3.8648)	grad_norm 1.5077 (nan)	mem 14851MB
[2022-11-06 02:23:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][600/1251]	eta 0:05:06 lr 0.000956	time 0.4644 (0.4704)	loss 3.7413 (3.8497)	grad_norm 1.2853 (nan)	mem 14851MB
[2022-11-06 02:23:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][650/1251]	eta 0:04:42 lr 0.000956	time 0.4651 (0.4701)	loss 4.1484 (3.8525)	grad_norm 1.1363 (nan)	mem 14851MB
[2022-11-06 02:23:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][700/1251]	eta 0:04:18 lr 0.000956	time 0.4526 (0.4700)	loss 4.0856 (3.8623)	grad_norm 1.2481 (nan)	mem 14851MB
[2022-11-06 02:24:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][750/1251]	eta 0:03:55 lr 0.000956	time 0.4665 (0.4698)	loss 3.7194 (3.8479)	grad_norm 1.2516 (nan)	mem 14851MB
[2022-11-06 02:24:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][800/1251]	eta 0:03:31 lr 0.000956	time 0.5376 (0.4697)	loss 3.4705 (3.8456)	grad_norm 1.3452 (nan)	mem 14851MB
[2022-11-06 02:24:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][850/1251]	eta 0:03:08 lr 0.000956	time 0.4724 (0.4697)	loss 2.9120 (3.8541)	grad_norm 1.2448 (nan)	mem 14851MB
[2022-11-06 02:25:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][900/1251]	eta 0:02:44 lr 0.000956	time 0.4730 (0.4695)	loss 4.2644 (3.8591)	grad_norm 1.1732 (nan)	mem 14851MB
[2022-11-06 02:25:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][950/1251]	eta 0:02:21 lr 0.000956	time 0.4767 (0.4694)	loss 4.0604 (3.8578)	grad_norm 1.2840 (nan)	mem 14851MB
[2022-11-06 02:26:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][1000/1251]	eta 0:01:57 lr 0.000956	time 0.4670 (0.4692)	loss 3.7211 (3.8592)	grad_norm 1.1644 (nan)	mem 14851MB
[2022-11-06 02:26:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][1050/1251]	eta 0:01:34 lr 0.000955	time 0.4682 (0.4692)	loss 4.0449 (3.8597)	grad_norm 1.1608 (nan)	mem 14851MB
[2022-11-06 02:26:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][1100/1251]	eta 0:01:10 lr 0.000955	time 0.4684 (0.4691)	loss 4.0528 (3.8551)	grad_norm 1.2224 (nan)	mem 14851MB
[2022-11-06 02:27:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][1150/1251]	eta 0:00:47 lr 0.000955	time 0.4688 (0.4690)	loss 3.4430 (3.8571)	grad_norm 1.3155 (nan)	mem 14851MB
[2022-11-06 02:27:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][1200/1251]	eta 0:00:23 lr 0.000955	time 0.4822 (0.4690)	loss 4.3557 (3.8487)	grad_norm 1.1997 (nan)	mem 14851MB
[2022-11-06 02:28:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [40/300][1250/1251]	eta 0:00:00 lr 0.000955	time 0.4568 (0.4687)	loss 4.0762 (3.8472)	grad_norm 1.2569 (nan)	mem 14851MB
[2022-11-06 02:28:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 40 training takes 0:09:46
[2022-11-06 02:28:05 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_40.pth saving......
[2022-11-06 02:28:06 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_40.pth saved !!!
[2022-11-06 02:28:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.585 (1.585)	Loss 1.3415 (1.3415)	Acc@1 71.875 (71.875)	Acc@5 89.844 (89.844)	Mem 14851MB
[2022-11-06 02:28:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.120 Acc@5 91.104
[2022-11-06 02:28:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.1%
[2022-11-06 02:28:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.717 (1.717)	Loss 1.2342 (1.2342)	Acc@1 70.898 (70.898)	Acc@5 91.895 (91.895)	Mem 14851MB
[2022-11-06 02:28:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.534 Acc@5 91.200
[2022-11-06 02:28:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 72.5%
[2022-11-06 02:28:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 72.53% at 40 epoch
[2022-11-06 02:28:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][0/1251]	eta 0:39:01 lr 0.000955	time 1.8713 (1.8713)	loss 4.0660 (4.0660)	grad_norm 1.2282 (1.2282)	mem 14851MB
[2022-11-06 02:28:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][50/1251]	eta 0:09:57 lr 0.000955	time 0.4689 (0.4977)	loss 3.9858 (3.7864)	grad_norm 1.1746 (1.2590)	mem 14851MB
[2022-11-06 02:29:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][100/1251]	eta 0:09:17 lr 0.000955	time 0.4598 (0.4842)	loss 3.7493 (3.8395)	grad_norm 1.6698 (1.2614)	mem 14851MB
[2022-11-06 02:29:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][150/1251]	eta 0:08:46 lr 0.000955	time 0.4730 (0.4785)	loss 3.6722 (3.8717)	grad_norm 1.3147 (1.2475)	mem 14851MB
[2022-11-06 02:29:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][200/1251]	eta 0:08:19 lr 0.000955	time 0.4605 (0.4756)	loss 3.8376 (3.8307)	grad_norm 1.2850 (1.2457)	mem 14851MB
[2022-11-06 02:30:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][250/1251]	eta 0:07:54 lr 0.000955	time 0.4664 (0.4737)	loss 4.5219 (3.8651)	grad_norm 1.2585 (1.2428)	mem 14851MB
[2022-11-06 02:30:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][300/1251]	eta 0:07:29 lr 0.000955	time 0.4653 (0.4725)	loss 4.4250 (3.8520)	grad_norm 1.2393 (1.2429)	mem 14851MB
[2022-11-06 02:31:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][350/1251]	eta 0:07:04 lr 0.000954	time 0.4637 (0.4715)	loss 4.4791 (3.8693)	grad_norm 1.1886 (1.2381)	mem 14851MB
[2022-11-06 02:31:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][400/1251]	eta 0:06:40 lr 0.000954	time 0.4644 (0.4707)	loss 4.5666 (3.8595)	grad_norm 1.1767 (1.2378)	mem 14851MB
[2022-11-06 02:31:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][450/1251]	eta 0:06:16 lr 0.000954	time 0.4639 (0.4700)	loss 3.4104 (3.8465)	grad_norm 1.2617 (1.2378)	mem 14851MB
[2022-11-06 02:32:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][500/1251]	eta 0:05:52 lr 0.000954	time 0.4680 (0.4696)	loss 2.4348 (3.8335)	grad_norm 1.1076 (1.2382)	mem 14851MB
[2022-11-06 02:32:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][550/1251]	eta 0:05:29 lr 0.000954	time 0.5326 (0.4699)	loss 3.0890 (3.8300)	grad_norm 1.2305 (1.2387)	mem 14851MB
[2022-11-06 02:33:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][600/1251]	eta 0:05:05 lr 0.000954	time 0.4717 (0.4697)	loss 4.5687 (3.8331)	grad_norm 1.2215 (1.2375)	mem 14851MB
[2022-11-06 02:33:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][650/1251]	eta 0:04:42 lr 0.000954	time 0.4694 (0.4693)	loss 3.7432 (3.8431)	grad_norm 1.2913 (1.2371)	mem 14851MB
[2022-11-06 02:33:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][700/1251]	eta 0:04:18 lr 0.000954	time 0.4527 (0.4692)	loss 3.3197 (3.8436)	grad_norm 1.5639 (1.2406)	mem 14851MB
[2022-11-06 02:34:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][750/1251]	eta 0:03:54 lr 0.000954	time 0.4635 (0.4689)	loss 4.3272 (3.8476)	grad_norm 1.0906 (1.2399)	mem 14851MB
[2022-11-06 02:34:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][800/1251]	eta 0:03:31 lr 0.000954	time 0.4679 (0.4690)	loss 2.8075 (3.8413)	grad_norm 1.2318 (1.2387)	mem 14851MB
[2022-11-06 02:35:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][850/1251]	eta 0:03:07 lr 0.000954	time 0.4599 (0.4688)	loss 4.1156 (3.8523)	grad_norm 1.1223 (1.2378)	mem 14851MB
[2022-11-06 02:35:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][900/1251]	eta 0:02:44 lr 0.000954	time 0.4706 (0.4687)	loss 3.8567 (3.8550)	grad_norm 1.1126 (1.2399)	mem 14851MB
[2022-11-06 02:35:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][950/1251]	eta 0:02:21 lr 0.000953	time 0.4701 (0.4686)	loss 4.3213 (3.8570)	grad_norm 1.3388 (1.2386)	mem 14851MB
[2022-11-06 02:36:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][1000/1251]	eta 0:01:57 lr 0.000953	time 0.4654 (0.4684)	loss 3.5537 (3.8552)	grad_norm 1.1225 (1.2389)	mem 14851MB
[2022-11-06 02:36:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][1050/1251]	eta 0:01:34 lr 0.000953	time 0.5439 (0.4686)	loss 2.8306 (3.8566)	grad_norm 1.0440 (1.2399)	mem 14851MB
[2022-11-06 02:36:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][1100/1251]	eta 0:01:10 lr 0.000953	time 0.4701 (0.4685)	loss 3.2355 (3.8578)	grad_norm 1.1026 (1.2386)	mem 14851MB
[2022-11-06 02:37:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][1150/1251]	eta 0:00:47 lr 0.000953	time 0.4579 (0.4683)	loss 4.0211 (3.8609)	grad_norm 1.2396 (1.2398)	mem 14851MB
[2022-11-06 02:37:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][1200/1251]	eta 0:00:23 lr 0.000953	time 0.4596 (0.4683)	loss 3.1563 (3.8598)	grad_norm 1.0850 (1.2395)	mem 14851MB
[2022-11-06 02:38:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [41/300][1250/1251]	eta 0:00:00 lr 0.000953	time 0.4572 (0.4681)	loss 4.1912 (3.8595)	grad_norm 1.2206 (1.2390)	mem 14851MB
[2022-11-06 02:38:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 41 training takes 0:09:45
[2022-11-06 02:38:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_41.pth saving......
[2022-11-06 02:38:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_41.pth saved !!!
[2022-11-06 02:38:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.667 (1.667)	Loss 1.2570 (1.2570)	Acc@1 71.191 (71.191)	Acc@5 91.016 (91.016)	Mem 14851MB
[2022-11-06 02:38:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.196 Acc@5 91.234
[2022-11-06 02:38:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.2%
[2022-11-06 02:38:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.646 (1.646)	Loss 1.1685 (1.1685)	Acc@1 73.828 (73.828)	Acc@5 91.016 (91.016)	Mem 14851MB
[2022-11-06 02:38:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.878 Acc@5 91.392
[2022-11-06 02:38:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 72.9%
[2022-11-06 02:38:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 72.88% at 41 epoch
[2022-11-06 02:38:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][0/1251]	eta 0:41:55 lr 0.000953	time 2.0111 (2.0111)	loss 3.7341 (3.7341)	grad_norm 1.2672 (1.2672)	mem 14851MB
[2022-11-06 02:38:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][50/1251]	eta 0:10:03 lr 0.000953	time 0.4525 (0.5022)	loss 3.3551 (4.0045)	grad_norm 1.3150 (nan)	mem 14851MB
[2022-11-06 02:39:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][100/1251]	eta 0:09:19 lr 0.000953	time 0.4576 (0.4864)	loss 4.5405 (3.9147)	grad_norm 1.2839 (nan)	mem 14851MB
[2022-11-06 02:39:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][150/1251]	eta 0:08:48 lr 0.000953	time 0.4580 (0.4798)	loss 3.9368 (3.8205)	grad_norm 1.3206 (nan)	mem 14851MB
[2022-11-06 02:40:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][200/1251]	eta 0:08:20 lr 0.000953	time 0.4624 (0.4759)	loss 2.9467 (3.8275)	grad_norm 1.4420 (nan)	mem 14851MB
[2022-11-06 02:40:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][250/1251]	eta 0:07:54 lr 0.000952	time 0.4647 (0.4740)	loss 3.9959 (3.8121)	grad_norm 1.3598 (nan)	mem 14851MB
[2022-11-06 02:40:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][300/1251]	eta 0:07:29 lr 0.000952	time 0.4570 (0.4724)	loss 3.6937 (3.7992)	grad_norm 1.1592 (nan)	mem 14851MB
[2022-11-06 02:41:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][350/1251]	eta 0:07:04 lr 0.000952	time 0.4692 (0.4714)	loss 3.5890 (3.8051)	grad_norm 1.1819 (nan)	mem 14851MB
[2022-11-06 02:41:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][400/1251]	eta 0:06:40 lr 0.000952	time 0.4755 (0.4710)	loss 3.8092 (3.7959)	grad_norm 1.2640 (nan)	mem 14851MB
[2022-11-06 02:41:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][450/1251]	eta 0:06:16 lr 0.000952	time 0.4634 (0.4704)	loss 4.4030 (3.7921)	grad_norm 1.2328 (nan)	mem 14851MB
[2022-11-06 02:42:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][500/1251]	eta 0:05:52 lr 0.000952	time 0.4596 (0.4699)	loss 3.8032 (3.8021)	grad_norm 1.2055 (nan)	mem 14851MB
[2022-11-06 02:42:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][550/1251]	eta 0:05:29 lr 0.000952	time 0.4650 (0.4698)	loss 2.8098 (3.8015)	grad_norm 1.1565 (nan)	mem 14851MB
[2022-11-06 02:43:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][600/1251]	eta 0:05:05 lr 0.000952	time 0.4653 (0.4695)	loss 3.7407 (3.8076)	grad_norm 1.1717 (nan)	mem 14851MB
[2022-11-06 02:43:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][650/1251]	eta 0:04:42 lr 0.000952	time 0.5364 (0.4694)	loss 4.0143 (3.8095)	grad_norm 1.1787 (nan)	mem 14851MB
[2022-11-06 02:43:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][700/1251]	eta 0:04:18 lr 0.000952	time 0.4545 (0.4692)	loss 3.2381 (3.7992)	grad_norm 1.1093 (nan)	mem 14851MB
[2022-11-06 02:44:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][750/1251]	eta 0:03:54 lr 0.000952	time 0.4639 (0.4689)	loss 3.2300 (3.7978)	grad_norm 1.1604 (nan)	mem 14851MB
[2022-11-06 02:44:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][800/1251]	eta 0:03:31 lr 0.000951	time 0.4599 (0.4689)	loss 3.3433 (3.7937)	grad_norm 1.1751 (nan)	mem 14851MB
[2022-11-06 02:45:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][850/1251]	eta 0:03:08 lr 0.000951	time 0.4731 (0.4688)	loss 3.6374 (3.7914)	grad_norm 1.2697 (nan)	mem 14851MB
[2022-11-06 02:45:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][900/1251]	eta 0:02:44 lr 0.000951	time 0.4573 (0.4687)	loss 4.4621 (3.7942)	grad_norm 1.2577 (nan)	mem 14851MB
[2022-11-06 02:45:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][950/1251]	eta 0:02:21 lr 0.000951	time 0.4642 (0.4685)	loss 4.6377 (3.7941)	grad_norm 1.4178 (nan)	mem 14851MB
[2022-11-06 02:46:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][1000/1251]	eta 0:01:57 lr 0.000951	time 0.4842 (0.4684)	loss 3.3051 (3.7982)	grad_norm 1.2755 (nan)	mem 14851MB
[2022-11-06 02:46:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][1050/1251]	eta 0:01:34 lr 0.000951	time 0.4689 (0.4684)	loss 4.1817 (3.8036)	grad_norm 1.3012 (nan)	mem 14851MB
[2022-11-06 02:47:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][1100/1251]	eta 0:01:10 lr 0.000951	time 0.4690 (0.4683)	loss 3.9630 (3.7988)	grad_norm 1.0839 (nan)	mem 14851MB
[2022-11-06 02:47:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][1150/1251]	eta 0:00:47 lr 0.000951	time 0.4706 (0.4683)	loss 3.9283 (3.7983)	grad_norm 1.2433 (nan)	mem 14851MB
[2022-11-06 02:47:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][1200/1251]	eta 0:00:23 lr 0.000951	time 0.4576 (0.4681)	loss 4.5173 (3.8021)	grad_norm 1.2732 (nan)	mem 14851MB
[2022-11-06 02:48:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [42/300][1250/1251]	eta 0:00:00 lr 0.000951	time 0.4569 (0.4680)	loss 2.7812 (3.8024)	grad_norm 1.3463 (nan)	mem 14851MB
[2022-11-06 02:48:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 42 training takes 0:09:45
[2022-11-06 02:48:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_42.pth saving......
[2022-11-06 02:48:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_42.pth saved !!!
[2022-11-06 02:48:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.529 (1.529)	Loss 1.1933 (1.1933)	Acc@1 74.707 (74.707)	Acc@5 91.211 (91.211)	Mem 14851MB
[2022-11-06 02:48:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.190 Acc@5 91.322
[2022-11-06 02:48:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.2%
[2022-11-06 02:48:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.634 (1.634)	Loss 1.1919 (1.1919)	Acc@1 74.219 (74.219)	Acc@5 91.309 (91.309)	Mem 14851MB
[2022-11-06 02:48:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.204 Acc@5 91.574
[2022-11-06 02:48:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 73.2%
[2022-11-06 02:48:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 73.20% at 42 epoch
[2022-11-06 02:48:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][0/1251]	eta 0:40:35 lr 0.000951	time 1.9465 (1.9465)	loss 4.1119 (4.1119)	grad_norm 1.0856 (1.0856)	mem 14851MB
[2022-11-06 02:48:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][50/1251]	eta 0:10:01 lr 0.000951	time 0.4793 (0.5005)	loss 3.0069 (3.9036)	grad_norm 1.2918 (1.2048)	mem 14851MB
[2022-11-06 02:49:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][100/1251]	eta 0:09:17 lr 0.000950	time 0.4634 (0.4841)	loss 3.7869 (3.8706)	grad_norm 1.3756 (1.2236)	mem 14851MB
[2022-11-06 02:49:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][150/1251]	eta 0:08:47 lr 0.000950	time 0.4603 (0.4789)	loss 3.8806 (3.8613)	grad_norm 1.1651 (1.2298)	mem 14851MB
[2022-11-06 02:50:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][200/1251]	eta 0:08:20 lr 0.000950	time 0.4746 (0.4763)	loss 4.3963 (3.8341)	grad_norm 1.3803 (1.2339)	mem 14851MB
[2022-11-06 02:50:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][250/1251]	eta 0:07:54 lr 0.000950	time 0.4602 (0.4743)	loss 3.0980 (3.8295)	grad_norm 1.2970 (1.2369)	mem 14851MB
[2022-11-06 02:50:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][300/1251]	eta 0:07:29 lr 0.000950	time 0.4552 (0.4728)	loss 3.9629 (3.8472)	grad_norm 1.2703 (1.2358)	mem 14851MB
[2022-11-06 02:51:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][350/1251]	eta 0:07:05 lr 0.000950	time 0.4640 (0.4718)	loss 3.3648 (3.8225)	grad_norm 1.1450 (1.2419)	mem 14851MB
[2022-11-06 02:51:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][400/1251]	eta 0:06:40 lr 0.000950	time 0.4666 (0.4709)	loss 3.0147 (3.8195)	grad_norm 1.1377 (1.2392)	mem 14851MB
[2022-11-06 02:52:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][450/1251]	eta 0:06:16 lr 0.000950	time 0.4730 (0.4703)	loss 4.7919 (3.8263)	grad_norm 1.2643 (1.2411)	mem 14851MB
[2022-11-06 02:52:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][500/1251]	eta 0:05:52 lr 0.000950	time 0.4635 (0.4700)	loss 3.9045 (3.8323)	grad_norm 1.0610 (1.2411)	mem 14851MB
[2022-11-06 02:52:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][550/1251]	eta 0:05:29 lr 0.000950	time 0.4700 (0.4701)	loss 4.1650 (3.8277)	grad_norm 1.2084 (1.2427)	mem 14851MB
[2022-11-06 02:53:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][600/1251]	eta 0:05:05 lr 0.000950	time 0.4542 (0.4698)	loss 3.7513 (3.8188)	grad_norm 1.1613 (1.2429)	mem 14851MB
[2022-11-06 02:53:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][650/1251]	eta 0:04:42 lr 0.000949	time 0.4618 (0.4694)	loss 3.8903 (3.8081)	grad_norm 1.1636 (1.2428)	mem 14851MB
[2022-11-06 02:53:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][700/1251]	eta 0:04:18 lr 0.000949	time 0.4692 (0.4691)	loss 4.5738 (3.8074)	grad_norm 1.1767 (1.2405)	mem 14851MB
[2022-11-06 02:54:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][750/1251]	eta 0:03:54 lr 0.000949	time 0.4697 (0.4688)	loss 4.1245 (3.8050)	grad_norm 1.1796 (1.2407)	mem 14851MB
[2022-11-06 02:54:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][800/1251]	eta 0:03:31 lr 0.000949	time 0.4517 (0.4690)	loss 4.5996 (3.8140)	grad_norm 1.2414 (1.2428)	mem 14851MB
[2022-11-06 02:55:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][850/1251]	eta 0:03:08 lr 0.000949	time 0.4602 (0.4689)	loss 4.1583 (3.8110)	grad_norm 1.5216 (1.2427)	mem 14851MB
[2022-11-06 02:55:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][900/1251]	eta 0:02:44 lr 0.000949	time 0.4771 (0.4687)	loss 3.8784 (3.8062)	grad_norm 1.2894 (1.2430)	mem 14851MB
[2022-11-06 02:55:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][950/1251]	eta 0:02:21 lr 0.000949	time 0.4628 (0.4685)	loss 4.2766 (3.7952)	grad_norm 1.2134 (1.2412)	mem 14851MB
[2022-11-06 02:56:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][1000/1251]	eta 0:01:57 lr 0.000949	time 0.4645 (0.4683)	loss 3.3582 (3.7934)	grad_norm 1.1043 (nan)	mem 14851MB
[2022-11-06 02:56:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][1050/1251]	eta 0:01:34 lr 0.000949	time 0.4712 (0.4685)	loss 4.1851 (3.7934)	grad_norm 1.1628 (nan)	mem 14851MB
[2022-11-06 02:57:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][1100/1251]	eta 0:01:10 lr 0.000949	time 0.4646 (0.4685)	loss 4.0297 (3.7979)	grad_norm 1.0950 (nan)	mem 14851MB
[2022-11-06 02:57:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][1150/1251]	eta 0:00:47 lr 0.000949	time 0.4512 (0.4683)	loss 4.7778 (3.8018)	grad_norm 1.2309 (nan)	mem 14851MB
[2022-11-06 02:57:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][1200/1251]	eta 0:00:23 lr 0.000948	time 0.4688 (0.4682)	loss 2.6428 (3.7975)	grad_norm 1.2495 (nan)	mem 14851MB
[2022-11-06 02:58:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [43/300][1250/1251]	eta 0:00:00 lr 0.000948	time 0.4597 (0.4681)	loss 4.2224 (3.7976)	grad_norm 1.2080 (nan)	mem 14851MB
[2022-11-06 02:58:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 43 training takes 0:09:45
[2022-11-06 02:58:16 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_43.pth saving......
[2022-11-06 02:58:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_43.pth saved !!!
[2022-11-06 02:58:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.581 (1.581)	Loss 1.2590 (1.2590)	Acc@1 72.754 (72.754)	Acc@5 91.602 (91.602)	Mem 14851MB
[2022-11-06 02:58:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.956 Acc@5 91.200
[2022-11-06 02:58:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.0%
[2022-11-06 02:58:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.715 (1.715)	Loss 1.1595 (1.1595)	Acc@1 72.363 (72.363)	Acc@5 91.992 (91.992)	Mem 14851MB
[2022-11-06 02:58:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.508 Acc@5 91.726
[2022-11-06 02:58:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 73.5%
[2022-11-06 02:58:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 73.51% at 43 epoch
[2022-11-06 02:58:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][0/1251]	eta 0:43:57 lr 0.000948	time 2.1083 (2.1083)	loss 3.8078 (3.8078)	grad_norm 1.1667 (1.1667)	mem 14851MB
[2022-11-06 02:59:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][50/1251]	eta 0:10:01 lr 0.000948	time 0.4593 (0.5005)	loss 2.6504 (3.8390)	grad_norm 1.0846 (nan)	mem 14851MB
[2022-11-06 02:59:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][100/1251]	eta 0:09:19 lr 0.000948	time 0.4583 (0.4860)	loss 3.8478 (3.8307)	grad_norm 1.0932 (nan)	mem 14851MB
[2022-11-06 02:59:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][150/1251]	eta 0:08:47 lr 0.000948	time 0.4702 (0.4794)	loss 4.3541 (3.8170)	grad_norm 1.2621 (nan)	mem 14851MB
[2022-11-06 03:00:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][200/1251]	eta 0:08:20 lr 0.000948	time 0.4573 (0.4761)	loss 2.7206 (3.8120)	grad_norm 1.2449 (nan)	mem 14851MB
[2022-11-06 03:00:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][250/1251]	eta 0:07:54 lr 0.000948	time 0.4613 (0.4744)	loss 3.2080 (3.7982)	grad_norm 1.3036 (nan)	mem 14851MB
[2022-11-06 03:00:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][300/1251]	eta 0:07:29 lr 0.000948	time 0.4672 (0.4728)	loss 3.9464 (3.8000)	grad_norm 1.2254 (nan)	mem 14851MB
[2022-11-06 03:01:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][350/1251]	eta 0:07:05 lr 0.000948	time 0.4660 (0.4723)	loss 3.5263 (3.8115)	grad_norm 1.0578 (nan)	mem 14851MB
[2022-11-06 03:01:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][400/1251]	eta 0:06:41 lr 0.000948	time 0.4679 (0.4713)	loss 3.7600 (3.8189)	grad_norm 1.2879 (nan)	mem 14851MB
[2022-11-06 03:02:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][450/1251]	eta 0:06:17 lr 0.000948	time 0.4680 (0.4707)	loss 3.0672 (3.8240)	grad_norm 1.1944 (nan)	mem 14851MB
[2022-11-06 03:02:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][500/1251]	eta 0:05:53 lr 0.000947	time 0.4772 (0.4701)	loss 2.9186 (3.8188)	grad_norm 1.4958 (nan)	mem 14851MB
[2022-11-06 03:02:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][550/1251]	eta 0:05:29 lr 0.000947	time 0.4600 (0.4700)	loss 3.4234 (3.8212)	grad_norm 1.1857 (nan)	mem 14851MB
[2022-11-06 03:03:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][600/1251]	eta 0:05:05 lr 0.000947	time 0.4598 (0.4696)	loss 3.8204 (3.8243)	grad_norm 1.2353 (nan)	mem 14851MB
[2022-11-06 03:03:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][650/1251]	eta 0:04:42 lr 0.000947	time 0.4663 (0.4695)	loss 4.3608 (3.8202)	grad_norm 1.1792 (nan)	mem 14851MB
[2022-11-06 03:04:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][700/1251]	eta 0:04:18 lr 0.000947	time 0.4678 (0.4692)	loss 2.6711 (3.8142)	grad_norm 1.6655 (nan)	mem 14851MB
[2022-11-06 03:04:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][750/1251]	eta 0:03:54 lr 0.000947	time 0.4627 (0.4690)	loss 2.8441 (3.8134)	grad_norm 1.3136 (nan)	mem 14851MB
[2022-11-06 03:04:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][800/1251]	eta 0:03:31 lr 0.000947	time 0.4548 (0.4690)	loss 3.6346 (3.8086)	grad_norm 1.4580 (nan)	mem 14851MB
[2022-11-06 03:05:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][850/1251]	eta 0:03:08 lr 0.000947	time 0.4733 (0.4689)	loss 4.0060 (3.8076)	grad_norm 1.0588 (nan)	mem 14851MB
[2022-11-06 03:05:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][900/1251]	eta 0:02:44 lr 0.000947	time 0.4706 (0.4688)	loss 3.7116 (3.8086)	grad_norm 1.2129 (nan)	mem 14851MB
[2022-11-06 03:06:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][950/1251]	eta 0:02:21 lr 0.000947	time 0.4704 (0.4687)	loss 4.2213 (3.8147)	grad_norm 1.3192 (nan)	mem 14851MB
[2022-11-06 03:06:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][1000/1251]	eta 0:01:57 lr 0.000947	time 0.4680 (0.4685)	loss 3.9308 (3.8134)	grad_norm 1.1524 (nan)	mem 14851MB
[2022-11-06 03:06:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][1050/1251]	eta 0:01:34 lr 0.000946	time 0.4654 (0.4685)	loss 3.8601 (3.8110)	grad_norm 1.1249 (nan)	mem 14851MB
[2022-11-06 03:07:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][1100/1251]	eta 0:01:10 lr 0.000946	time 0.4742 (0.4686)	loss 4.0874 (3.8090)	grad_norm 1.2595 (nan)	mem 14851MB
[2022-11-06 03:07:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][1150/1251]	eta 0:00:47 lr 0.000946	time 0.4654 (0.4685)	loss 4.0093 (3.8068)	grad_norm 1.3615 (nan)	mem 14851MB
[2022-11-06 03:07:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][1200/1251]	eta 0:00:23 lr 0.000946	time 0.4687 (0.4683)	loss 3.8250 (3.8068)	grad_norm 1.3820 (nan)	mem 14851MB
[2022-11-06 03:08:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [44/300][1250/1251]	eta 0:00:00 lr 0.000946	time 0.4580 (0.4682)	loss 3.2422 (3.8056)	grad_norm 1.1096 (nan)	mem 14851MB
[2022-11-06 03:08:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 44 training takes 0:09:45
[2022-11-06 03:08:20 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_44.pth saving......
[2022-11-06 03:08:21 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_44.pth saved !!!
[2022-11-06 03:08:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.477 (1.477)	Loss 1.1387 (1.1387)	Acc@1 73.340 (73.340)	Acc@5 92.188 (92.188)	Mem 14851MB
[2022-11-06 03:08:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.470 Acc@5 91.468
[2022-11-06 03:08:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.5%
[2022-11-06 03:08:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.867 (1.867)	Loss 1.1595 (1.1595)	Acc@1 73.340 (73.340)	Acc@5 91.504 (91.504)	Mem 14851MB
[2022-11-06 03:08:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.742 Acc@5 91.846
[2022-11-06 03:08:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 73.7%
[2022-11-06 03:08:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 73.74% at 44 epoch
[2022-11-06 03:08:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][0/1251]	eta 0:40:27 lr 0.000946	time 1.9406 (1.9406)	loss 4.5369 (4.5369)	grad_norm 1.1706 (1.1706)	mem 14851MB
[2022-11-06 03:09:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][50/1251]	eta 0:10:00 lr 0.000946	time 0.4682 (0.4999)	loss 3.8643 (3.9153)	grad_norm 1.2267 (1.2420)	mem 14851MB
[2022-11-06 03:09:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][100/1251]	eta 0:09:18 lr 0.000946	time 0.4605 (0.4856)	loss 3.7134 (3.9192)	grad_norm 1.2355 (1.2457)	mem 14851MB
[2022-11-06 03:09:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][150/1251]	eta 0:08:47 lr 0.000946	time 0.4612 (0.4792)	loss 3.7500 (3.8910)	grad_norm 1.1523 (1.2359)	mem 14851MB
[2022-11-06 03:10:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][200/1251]	eta 0:08:20 lr 0.000946	time 0.4594 (0.4763)	loss 4.3893 (3.8562)	grad_norm 1.1804 (1.2481)	mem 14851MB
[2022-11-06 03:10:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][250/1251]	eta 0:07:55 lr 0.000946	time 0.4711 (0.4747)	loss 4.3823 (3.8409)	grad_norm 1.4172 (1.2471)	mem 14851MB
[2022-11-06 03:11:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][300/1251]	eta 0:07:30 lr 0.000945	time 0.4672 (0.4733)	loss 3.0763 (3.8208)	grad_norm 1.2859 (1.2443)	mem 14851MB
[2022-11-06 03:11:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][350/1251]	eta 0:07:05 lr 0.000945	time 0.4666 (0.4720)	loss 2.7775 (3.8229)	grad_norm 1.2058 (1.2431)	mem 14851MB
[2022-11-06 03:11:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][400/1251]	eta 0:06:40 lr 0.000945	time 0.4764 (0.4711)	loss 3.9098 (3.8267)	grad_norm 1.1857 (1.2433)	mem 14851MB
[2022-11-06 03:12:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][450/1251]	eta 0:06:16 lr 0.000945	time 0.4692 (0.4704)	loss 3.7212 (3.8214)	grad_norm 1.1767 (1.2406)	mem 14851MB
[2022-11-06 03:12:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][500/1251]	eta 0:05:53 lr 0.000945	time 0.4758 (0.4700)	loss 2.9741 (3.8203)	grad_norm 1.2376 (1.2407)	mem 14851MB
[2022-11-06 03:12:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][550/1251]	eta 0:05:29 lr 0.000945	time 0.4621 (0.4701)	loss 3.9011 (3.8268)	grad_norm 1.1110 (1.2426)	mem 14851MB
[2022-11-06 03:13:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][600/1251]	eta 0:05:05 lr 0.000945	time 0.4671 (0.4697)	loss 4.0671 (3.8174)	grad_norm 1.4534 (1.2418)	mem 14851MB
[2022-11-06 03:13:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][650/1251]	eta 0:04:42 lr 0.000945	time 0.4620 (0.4695)	loss 4.1977 (3.8165)	grad_norm 1.1705 (1.2419)	mem 14851MB
[2022-11-06 03:14:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][700/1251]	eta 0:04:18 lr 0.000945	time 0.5391 (0.4693)	loss 4.1565 (3.8156)	grad_norm 1.4858 (1.2456)	mem 14851MB
[2022-11-06 03:14:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][750/1251]	eta 0:03:55 lr 0.000945	time 0.4688 (0.4691)	loss 4.7385 (3.8088)	grad_norm 1.3146 (1.2453)	mem 14851MB
[2022-11-06 03:14:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][800/1251]	eta 0:03:31 lr 0.000945	time 0.4675 (0.4690)	loss 4.1467 (3.8096)	grad_norm 1.2350 (1.2448)	mem 14851MB
[2022-11-06 03:15:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][850/1251]	eta 0:03:08 lr 0.000944	time 0.4697 (0.4688)	loss 4.1010 (3.8101)	grad_norm 1.3277 (1.2447)	mem 14851MB
[2022-11-06 03:15:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][900/1251]	eta 0:02:44 lr 0.000944	time 0.4592 (0.4688)	loss 3.9334 (3.8044)	grad_norm 1.2846 (1.2431)	mem 14851MB
[2022-11-06 03:16:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][950/1251]	eta 0:02:21 lr 0.000944	time 0.4691 (0.4687)	loss 4.1434 (3.7989)	grad_norm 1.0800 (1.2435)	mem 14851MB
[2022-11-06 03:16:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][1000/1251]	eta 0:01:57 lr 0.000944	time 0.4597 (0.4686)	loss 4.7296 (3.7982)	grad_norm 1.3470 (1.2436)	mem 14851MB
[2022-11-06 03:16:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][1050/1251]	eta 0:01:34 lr 0.000944	time 0.4645 (0.4687)	loss 3.7637 (3.8026)	grad_norm 1.1992 (1.2437)	mem 14851MB
[2022-11-06 03:17:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][1100/1251]	eta 0:01:10 lr 0.000944	time 0.4603 (0.4686)	loss 2.8590 (3.7979)	grad_norm 1.5563 (1.2431)	mem 14851MB
[2022-11-06 03:17:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][1150/1251]	eta 0:00:47 lr 0.000944	time 0.4661 (0.4685)	loss 3.7429 (3.7979)	grad_norm 1.2434 (1.2430)	mem 14851MB
[2022-11-06 03:18:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][1200/1251]	eta 0:00:23 lr 0.000944	time 0.5404 (0.4685)	loss 4.4946 (3.7943)	grad_norm 1.0811 (1.2428)	mem 14851MB
[2022-11-06 03:18:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [45/300][1250/1251]	eta 0:00:00 lr 0.000944	time 0.4569 (0.4683)	loss 3.8563 (3.7959)	grad_norm 1.1812 (1.2422)	mem 14851MB
[2022-11-06 03:18:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 45 training takes 0:09:46
[2022-11-06 03:18:24 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_45.pth saving......
[2022-11-06 03:18:25 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_45.pth saved !!!
[2022-11-06 03:18:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.616 (1.616)	Loss 1.2225 (1.2225)	Acc@1 72.168 (72.168)	Acc@5 91.797 (91.797)	Mem 14851MB
[2022-11-06 03:18:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.350 Acc@5 91.306
[2022-11-06 03:18:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.3%
[2022-11-06 03:18:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.538 (1.538)	Loss 1.0956 (1.0956)	Acc@1 74.707 (74.707)	Acc@5 93.359 (93.359)	Mem 14851MB
[2022-11-06 03:18:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.014 Acc@5 91.940
[2022-11-06 03:18:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 74.0%
[2022-11-06 03:18:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 74.01% at 45 epoch
[2022-11-06 03:18:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][0/1251]	eta 0:41:02 lr 0.000944	time 1.9683 (1.9683)	loss 2.8166 (2.8166)	grad_norm 1.4396 (1.4396)	mem 14851MB
[2022-11-06 03:19:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][50/1251]	eta 0:10:03 lr 0.000944	time 0.4662 (0.5029)	loss 3.4647 (3.8078)	grad_norm 1.2214 (1.2661)	mem 14851MB
[2022-11-06 03:19:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][100/1251]	eta 0:09:20 lr 0.000943	time 0.4617 (0.4871)	loss 4.1408 (3.9002)	grad_norm 1.3860 (1.2549)	mem 14851MB
[2022-11-06 03:19:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][150/1251]	eta 0:08:49 lr 0.000943	time 0.4671 (0.4808)	loss 4.0802 (3.8664)	grad_norm 1.1505 (1.2506)	mem 14851MB
[2022-11-06 03:20:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][200/1251]	eta 0:08:21 lr 0.000943	time 0.4584 (0.4770)	loss 3.8420 (3.8286)	grad_norm 1.2592 (1.2395)	mem 14851MB
[2022-11-06 03:20:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][250/1251]	eta 0:07:55 lr 0.000943	time 0.4581 (0.4746)	loss 3.3424 (3.8325)	grad_norm 1.1491 (1.2435)	mem 14851MB
[2022-11-06 03:21:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][300/1251]	eta 0:07:30 lr 0.000943	time 0.4513 (0.4732)	loss 4.2528 (3.8190)	grad_norm 1.3960 (1.2478)	mem 14851MB
[2022-11-06 03:21:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][350/1251]	eta 0:07:05 lr 0.000943	time 0.4703 (0.4722)	loss 3.1305 (3.7885)	grad_norm 1.2946 (1.2462)	mem 14851MB
[2022-11-06 03:21:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][400/1251]	eta 0:06:41 lr 0.000943	time 0.4630 (0.4715)	loss 4.4284 (3.7802)	grad_norm 1.3089 (1.2437)	mem 14851MB
[2022-11-06 03:22:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][450/1251]	eta 0:06:17 lr 0.000943	time 0.4645 (0.4710)	loss 4.0867 (3.7752)	grad_norm 1.1101 (1.2415)	mem 14851MB
[2022-11-06 03:22:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][500/1251]	eta 0:05:53 lr 0.000943	time 0.4658 (0.4704)	loss 4.1503 (3.7850)	grad_norm 1.0961 (1.2396)	mem 14851MB
[2022-11-06 03:23:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][550/1251]	eta 0:05:29 lr 0.000943	time 0.4559 (0.4702)	loss 3.9887 (3.7895)	grad_norm 1.2318 (1.2417)	mem 14851MB
[2022-11-06 03:23:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][600/1251]	eta 0:05:06 lr 0.000943	time 0.4602 (0.4701)	loss 3.1548 (3.7896)	grad_norm 1.2822 (1.2435)	mem 14851MB
[2022-11-06 03:23:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][650/1251]	eta 0:04:42 lr 0.000942	time 0.4628 (0.4701)	loss 3.7602 (3.7843)	grad_norm 1.3204 (1.2434)	mem 14851MB
[2022-11-06 03:24:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][700/1251]	eta 0:04:18 lr 0.000942	time 0.4627 (0.4699)	loss 3.6563 (3.7838)	grad_norm 1.1277 (1.2405)	mem 14851MB
[2022-11-06 03:24:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][750/1251]	eta 0:03:55 lr 0.000942	time 0.4718 (0.4695)	loss 4.4941 (3.7816)	grad_norm 1.0996 (1.2406)	mem 14851MB
[2022-11-06 03:24:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][800/1251]	eta 0:03:31 lr 0.000942	time 0.4751 (0.4694)	loss 4.3899 (3.7879)	grad_norm 1.2436 (1.2439)	mem 14851MB
[2022-11-06 03:25:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][850/1251]	eta 0:03:08 lr 0.000942	time 0.4619 (0.4693)	loss 4.4457 (3.7800)	grad_norm 1.2197 (1.2431)	mem 14851MB
[2022-11-06 03:25:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][900/1251]	eta 0:02:44 lr 0.000942	time 0.4662 (0.4692)	loss 4.2002 (3.7809)	grad_norm 1.4274 (1.2431)	mem 14851MB
[2022-11-06 03:26:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][950/1251]	eta 0:02:21 lr 0.000942	time 0.4682 (0.4690)	loss 3.8309 (3.7851)	grad_norm 1.1846 (1.2441)	mem 14851MB
[2022-11-06 03:26:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][1000/1251]	eta 0:01:57 lr 0.000942	time 0.4648 (0.4690)	loss 3.0989 (3.7818)	grad_norm 1.2772 (1.2416)	mem 14851MB
[2022-11-06 03:26:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][1050/1251]	eta 0:01:34 lr 0.000942	time 0.4626 (0.4690)	loss 4.1542 (3.7774)	grad_norm 1.1221 (1.2421)	mem 14851MB
[2022-11-06 03:27:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][1100/1251]	eta 0:01:10 lr 0.000942	time 0.4594 (0.4689)	loss 2.8817 (3.7720)	grad_norm 1.2573 (inf)	mem 14851MB
[2022-11-06 03:27:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][1150/1251]	eta 0:00:47 lr 0.000941	time 0.4653 (0.4689)	loss 4.0400 (3.7771)	grad_norm 1.1567 (inf)	mem 14851MB
[2022-11-06 03:28:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][1200/1251]	eta 0:00:23 lr 0.000941	time 0.4624 (0.4688)	loss 4.0688 (3.7775)	grad_norm 1.2819 (inf)	mem 14851MB
[2022-11-06 03:28:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [46/300][1250/1251]	eta 0:00:00 lr 0.000941	time 0.4578 (0.4685)	loss 3.9051 (3.7813)	grad_norm 1.4588 (inf)	mem 14851MB
[2022-11-06 03:28:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 46 training takes 0:09:46
[2022-11-06 03:28:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_46.pth saving......
[2022-11-06 03:28:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_46.pth saved !!!
[2022-11-06 03:28:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.585 (1.585)	Loss 1.1815 (1.1815)	Acc@1 73.340 (73.340)	Acc@5 90.527 (90.527)	Mem 14851MB
[2022-11-06 03:28:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.596 Acc@5 91.336
[2022-11-06 03:28:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.6%
[2022-11-06 03:28:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.753 (1.753)	Loss 1.1267 (1.1267)	Acc@1 73.828 (73.828)	Acc@5 91.992 (91.992)	Mem 14851MB
[2022-11-06 03:28:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.298 Acc@5 92.098
[2022-11-06 03:28:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 74.3%
[2022-11-06 03:28:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 74.30% at 46 epoch
[2022-11-06 03:28:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][0/1251]	eta 0:40:45 lr 0.000941	time 1.9550 (1.9550)	loss 4.1031 (4.1031)	grad_norm 1.4859 (1.4859)	mem 14851MB
[2022-11-06 03:29:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][50/1251]	eta 0:09:57 lr 0.000941	time 0.4543 (0.4978)	loss 3.6042 (3.6901)	grad_norm 1.2352 (1.2475)	mem 14851MB
[2022-11-06 03:29:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][100/1251]	eta 0:09:16 lr 0.000941	time 0.4654 (0.4839)	loss 3.7896 (3.7560)	grad_norm 1.1727 (1.2367)	mem 14851MB
[2022-11-06 03:29:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][150/1251]	eta 0:08:46 lr 0.000941	time 0.4670 (0.4784)	loss 4.2481 (3.7260)	grad_norm 1.4301 (1.2374)	mem 14851MB
[2022-11-06 03:30:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][200/1251]	eta 0:08:19 lr 0.000941	time 0.4815 (0.4753)	loss 3.8714 (3.7666)	grad_norm 1.1866 (1.2434)	mem 14851MB
[2022-11-06 03:30:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][250/1251]	eta 0:07:54 lr 0.000941	time 0.4766 (0.4735)	loss 3.1581 (3.7630)	grad_norm 1.2922 (1.2438)	mem 14851MB
[2022-11-06 03:31:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][300/1251]	eta 0:07:29 lr 0.000941	time 0.4662 (0.4722)	loss 3.9823 (3.7610)	grad_norm 1.1269 (1.2460)	mem 14851MB
[2022-11-06 03:31:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][350/1251]	eta 0:07:04 lr 0.000941	time 0.4543 (0.4713)	loss 2.8737 (3.7667)	grad_norm 1.0731 (1.2399)	mem 14851MB
[2022-11-06 03:31:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][400/1251]	eta 0:06:40 lr 0.000940	time 0.4837 (0.4708)	loss 3.9813 (3.7642)	grad_norm 1.5561 (1.2382)	mem 14851MB
[2022-11-06 03:32:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][450/1251]	eta 0:06:16 lr 0.000940	time 0.4616 (0.4702)	loss 3.9649 (3.7733)	grad_norm 1.2520 (1.2412)	mem 14851MB
[2022-11-06 03:32:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][500/1251]	eta 0:05:52 lr 0.000940	time 0.4597 (0.4697)	loss 3.6516 (3.7575)	grad_norm 1.1679 (1.2416)	mem 14851MB
[2022-11-06 03:33:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][550/1251]	eta 0:05:29 lr 0.000940	time 0.4718 (0.4698)	loss 4.3193 (3.7688)	grad_norm 1.1850 (1.2423)	mem 14851MB
[2022-11-06 03:33:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][600/1251]	eta 0:05:05 lr 0.000940	time 0.4736 (0.4695)	loss 3.1943 (3.7799)	grad_norm 1.2102 (1.2375)	mem 14851MB
[2022-11-06 03:33:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][650/1251]	eta 0:04:42 lr 0.000940	time 0.5402 (0.4693)	loss 3.2842 (3.7861)	grad_norm 1.1844 (1.2353)	mem 14851MB
[2022-11-06 03:34:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][700/1251]	eta 0:04:18 lr 0.000940	time 0.4619 (0.4691)	loss 4.1994 (3.7773)	grad_norm 1.1986 (1.2370)	mem 14851MB
[2022-11-06 03:34:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][750/1251]	eta 0:03:54 lr 0.000940	time 0.4787 (0.4688)	loss 4.2656 (3.7831)	grad_norm 1.2221 (1.2392)	mem 14851MB
[2022-11-06 03:35:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][800/1251]	eta 0:03:31 lr 0.000940	time 0.4563 (0.4689)	loss 4.4946 (3.7864)	grad_norm 1.2477 (1.2403)	mem 14851MB
[2022-11-06 03:35:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][850/1251]	eta 0:03:08 lr 0.000940	time 0.4595 (0.4688)	loss 2.9520 (3.7875)	grad_norm 1.2153 (1.2396)	mem 14851MB
[2022-11-06 03:35:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][900/1251]	eta 0:02:44 lr 0.000939	time 0.4672 (0.4689)	loss 2.8466 (3.7741)	grad_norm 1.6952 (1.2406)	mem 14851MB
[2022-11-06 03:36:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][950/1251]	eta 0:02:21 lr 0.000939	time 0.4786 (0.4687)	loss 4.0099 (3.7726)	grad_norm 1.0972 (1.2410)	mem 14851MB
[2022-11-06 03:36:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][1000/1251]	eta 0:01:57 lr 0.000939	time 0.4677 (0.4685)	loss 4.6214 (3.7685)	grad_norm 1.1641 (1.2395)	mem 14851MB
[2022-11-06 03:36:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][1050/1251]	eta 0:01:34 lr 0.000939	time 0.4611 (0.4686)	loss 4.2249 (3.7703)	grad_norm 1.1751 (1.2383)	mem 14851MB
[2022-11-06 03:37:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][1100/1251]	eta 0:01:10 lr 0.000939	time 0.4593 (0.4685)	loss 4.1973 (3.7725)	grad_norm 1.0595 (1.2391)	mem 14851MB
[2022-11-06 03:37:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][1150/1251]	eta 0:00:47 lr 0.000939	time 0.4768 (0.4685)	loss 3.8459 (3.7716)	grad_norm 1.2308 (1.2394)	mem 14851MB
[2022-11-06 03:38:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][1200/1251]	eta 0:00:23 lr 0.000939	time 0.4612 (0.4684)	loss 3.7621 (3.7761)	grad_norm 1.3040 (1.2405)	mem 14851MB
[2022-11-06 03:38:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [47/300][1250/1251]	eta 0:00:00 lr 0.000939	time 0.4574 (0.4682)	loss 3.5225 (3.7770)	grad_norm 1.2306 (1.2402)	mem 14851MB
[2022-11-06 03:38:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 47 training takes 0:09:45
[2022-11-06 03:38:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_47.pth saving......
[2022-11-06 03:38:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_47.pth saved !!!
[2022-11-06 03:38:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.528 (1.528)	Loss 1.2525 (1.2525)	Acc@1 71.680 (71.680)	Acc@5 89.941 (89.941)	Mem 14851MB
[2022-11-06 03:38:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.458 Acc@5 91.416
[2022-11-06 03:38:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.5%
[2022-11-06 03:38:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.634 (1.634)	Loss 1.0879 (1.0879)	Acc@1 74.805 (74.805)	Acc@5 92.676 (92.676)	Mem 14851MB
[2022-11-06 03:38:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.480 Acc@5 92.218
[2022-11-06 03:38:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 74.5%
[2022-11-06 03:38:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 74.48% at 47 epoch
[2022-11-06 03:38:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][0/1251]	eta 0:39:16 lr 0.000939	time 1.8837 (1.8837)	loss 4.0586 (4.0586)	grad_norm 1.3400 (1.3400)	mem 14851MB
[2022-11-06 03:39:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][50/1251]	eta 0:10:02 lr 0.000939	time 0.4636 (0.5015)	loss 3.9547 (3.8099)	grad_norm 1.4148 (1.2258)	mem 14851MB
[2022-11-06 03:39:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][100/1251]	eta 0:09:18 lr 0.000939	time 0.4610 (0.4853)	loss 2.8823 (3.7981)	grad_norm 1.1992 (1.2450)	mem 14851MB
[2022-11-06 03:40:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][150/1251]	eta 0:08:46 lr 0.000938	time 0.4651 (0.4786)	loss 3.3517 (3.7761)	grad_norm 1.2018 (1.2415)	mem 14851MB
[2022-11-06 03:40:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][200/1251]	eta 0:08:20 lr 0.000938	time 0.4675 (0.4758)	loss 4.3164 (3.7886)	grad_norm 1.1517 (1.2451)	mem 14851MB
[2022-11-06 03:40:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][250/1251]	eta 0:07:54 lr 0.000938	time 0.4568 (0.4740)	loss 3.1378 (3.7665)	grad_norm 1.2717 (1.2377)	mem 14851MB
[2022-11-06 03:41:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][300/1251]	eta 0:07:29 lr 0.000938	time 0.4602 (0.4727)	loss 4.0973 (3.7598)	grad_norm 1.0690 (1.2383)	mem 14851MB
[2022-11-06 03:41:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][350/1251]	eta 0:07:05 lr 0.000938	time 0.4771 (0.4719)	loss 3.3698 (3.7614)	grad_norm 1.2942 (1.2424)	mem 14851MB
[2022-11-06 03:42:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][400/1251]	eta 0:06:40 lr 0.000938	time 0.4695 (0.4709)	loss 4.1801 (3.7607)	grad_norm 1.2452 (1.2425)	mem 14851MB
[2022-11-06 03:42:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][450/1251]	eta 0:06:16 lr 0.000938	time 0.4743 (0.4703)	loss 3.4809 (3.7536)	grad_norm 1.2505 (1.2432)	mem 14851MB
[2022-11-06 03:42:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][500/1251]	eta 0:05:52 lr 0.000938	time 0.4645 (0.4697)	loss 4.2657 (3.7707)	grad_norm 1.2228 (1.2414)	mem 14851MB
[2022-11-06 03:43:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][550/1251]	eta 0:05:29 lr 0.000938	time 0.4624 (0.4695)	loss 2.6975 (3.7703)	grad_norm 1.3342 (1.2410)	mem 14851MB
[2022-11-06 03:43:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][600/1251]	eta 0:05:05 lr 0.000938	time 0.4556 (0.4693)	loss 4.0452 (3.7750)	grad_norm 1.2386 (1.2385)	mem 14851MB
[2022-11-06 03:43:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][650/1251]	eta 0:04:41 lr 0.000937	time 0.4577 (0.4692)	loss 3.2697 (3.7795)	grad_norm 1.2457 (1.2400)	mem 14851MB
[2022-11-06 03:44:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][700/1251]	eta 0:04:18 lr 0.000937	time 0.4730 (0.4690)	loss 3.9748 (3.7839)	grad_norm 1.3260 (1.2417)	mem 14851MB
[2022-11-06 03:44:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][750/1251]	eta 0:03:54 lr 0.000937	time 0.4676 (0.4688)	loss 4.3804 (3.7848)	grad_norm 1.3134 (1.2414)	mem 14851MB
[2022-11-06 03:45:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][800/1251]	eta 0:03:31 lr 0.000937	time 0.4673 (0.4688)	loss 3.7740 (3.7915)	grad_norm 1.1556 (1.2397)	mem 14851MB
[2022-11-06 03:45:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][850/1251]	eta 0:03:07 lr 0.000937	time 0.4675 (0.4687)	loss 3.4110 (3.7820)	grad_norm 1.5722 (1.2407)	mem 14851MB
[2022-11-06 03:45:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][900/1251]	eta 0:02:44 lr 0.000937	time 0.4641 (0.4684)	loss 3.9727 (3.7823)	grad_norm 1.3857 (1.2424)	mem 14851MB
[2022-11-06 03:46:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][950/1251]	eta 0:02:21 lr 0.000937	time 0.4684 (0.4685)	loss 4.1145 (3.7859)	grad_norm 1.2543 (1.2405)	mem 14851MB
[2022-11-06 03:46:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][1000/1251]	eta 0:01:57 lr 0.000937	time 0.4681 (0.4684)	loss 3.4563 (3.7836)	grad_norm 1.1673 (1.2404)	mem 14851MB
[2022-11-06 03:47:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][1050/1251]	eta 0:01:34 lr 0.000937	time 0.4562 (0.4684)	loss 3.7399 (3.7793)	grad_norm 1.1478 (1.2412)	mem 14851MB
[2022-11-06 03:47:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][1100/1251]	eta 0:01:10 lr 0.000937	time 0.4732 (0.4683)	loss 4.0626 (3.7808)	grad_norm 1.1266 (1.2408)	mem 14851MB
[2022-11-06 03:47:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][1150/1251]	eta 0:00:47 lr 0.000936	time 0.4715 (0.4682)	loss 3.8694 (3.7748)	grad_norm 1.1395 (1.2411)	mem 14851MB
[2022-11-06 03:48:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][1200/1251]	eta 0:00:23 lr 0.000936	time 0.4667 (0.4681)	loss 3.1788 (3.7777)	grad_norm 1.3235 (1.2408)	mem 14851MB
[2022-11-06 03:48:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [48/300][1250/1251]	eta 0:00:00 lr 0.000936	time 0.4581 (0.4680)	loss 3.9584 (3.7778)	grad_norm 1.1909 (1.2403)	mem 14851MB
[2022-11-06 03:48:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 48 training takes 0:09:45
[2022-11-06 03:48:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_48.pth saving......
[2022-11-06 03:48:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_48.pth saved !!!
[2022-11-06 03:48:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.658 (1.658)	Loss 1.1177 (1.1177)	Acc@1 74.512 (74.512)	Acc@5 91.895 (91.895)	Mem 14851MB
[2022-11-06 03:48:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.658 Acc@5 91.628
[2022-11-06 03:48:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.7%
[2022-11-06 03:48:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.634 (1.634)	Loss 0.9912 (0.9912)	Acc@1 76.953 (76.953)	Acc@5 94.141 (94.141)	Mem 14851MB
[2022-11-06 03:48:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.676 Acc@5 92.306
[2022-11-06 03:48:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 74.7%
[2022-11-06 03:48:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 74.68% at 48 epoch
[2022-11-06 03:48:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][0/1251]	eta 0:40:21 lr 0.000936	time 1.9355 (1.9355)	loss 3.9243 (3.9243)	grad_norm 1.2303 (1.2303)	mem 14851MB
[2022-11-06 03:49:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][50/1251]	eta 0:10:01 lr 0.000936	time 0.4727 (0.5006)	loss 2.5194 (3.7858)	grad_norm 1.2599 (1.2567)	mem 14851MB
[2022-11-06 03:49:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][100/1251]	eta 0:09:17 lr 0.000936	time 0.4572 (0.4844)	loss 3.3394 (3.6989)	grad_norm 1.0731 (1.2459)	mem 14851MB
[2022-11-06 03:50:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][150/1251]	eta 0:08:48 lr 0.000936	time 0.4717 (0.4800)	loss 3.8955 (3.6940)	grad_norm 1.1164 (1.2347)	mem 14851MB
[2022-11-06 03:50:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][200/1251]	eta 0:08:21 lr 0.000936	time 0.4605 (0.4768)	loss 3.6818 (3.6896)	grad_norm 1.0539 (1.2376)	mem 14851MB
[2022-11-06 03:50:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][250/1251]	eta 0:07:55 lr 0.000936	time 0.4625 (0.4746)	loss 3.9580 (3.6907)	grad_norm 1.2653 (1.2348)	mem 14851MB
[2022-11-06 03:51:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][300/1251]	eta 0:07:30 lr 0.000936	time 0.4620 (0.4733)	loss 3.6552 (3.6947)	grad_norm 1.4111 (1.2348)	mem 14851MB
[2022-11-06 03:51:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][350/1251]	eta 0:07:05 lr 0.000936	time 0.4761 (0.4721)	loss 3.8713 (3.6934)	grad_norm 1.2604 (1.2354)	mem 14851MB
[2022-11-06 03:52:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][400/1251]	eta 0:06:41 lr 0.000935	time 0.4745 (0.4713)	loss 3.7870 (3.7114)	grad_norm 1.1855 (1.2384)	mem 14851MB
[2022-11-06 03:52:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][450/1251]	eta 0:06:17 lr 0.000935	time 0.4596 (0.4708)	loss 3.9479 (3.7209)	grad_norm 1.2044 (nan)	mem 14851MB
[2022-11-06 03:52:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][500/1251]	eta 0:05:53 lr 0.000935	time 0.4674 (0.4706)	loss 3.0737 (3.7139)	grad_norm 1.0274 (nan)	mem 14851MB
[2022-11-06 03:53:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][550/1251]	eta 0:05:29 lr 0.000935	time 0.4657 (0.4704)	loss 3.8499 (3.7150)	grad_norm 1.2178 (nan)	mem 14851MB
[2022-11-06 03:53:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][600/1251]	eta 0:05:05 lr 0.000935	time 0.4617 (0.4700)	loss 4.3010 (3.7260)	grad_norm 1.0734 (nan)	mem 14851MB
[2022-11-06 03:54:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][650/1251]	eta 0:04:42 lr 0.000935	time 0.4644 (0.4698)	loss 4.0979 (3.7426)	grad_norm 1.1186 (nan)	mem 14851MB
[2022-11-06 03:54:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][700/1251]	eta 0:04:18 lr 0.000935	time 0.4666 (0.4694)	loss 3.8890 (3.7401)	grad_norm 1.1844 (nan)	mem 14851MB
[2022-11-06 03:54:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][750/1251]	eta 0:03:55 lr 0.000935	time 0.4753 (0.4693)	loss 4.0822 (3.7430)	grad_norm 1.2938 (nan)	mem 14851MB
[2022-11-06 03:55:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][800/1251]	eta 0:03:31 lr 0.000935	time 0.4637 (0.4693)	loss 3.7982 (3.7387)	grad_norm 1.0751 (nan)	mem 14851MB
[2022-11-06 03:55:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][850/1251]	eta 0:03:08 lr 0.000935	time 0.4705 (0.4692)	loss 4.1795 (3.7333)	grad_norm 1.1118 (nan)	mem 14851MB
[2022-11-06 03:55:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][900/1251]	eta 0:02:44 lr 0.000934	time 0.4602 (0.4691)	loss 2.5187 (3.7371)	grad_norm 1.1322 (nan)	mem 14851MB
[2022-11-06 03:56:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][950/1251]	eta 0:02:21 lr 0.000934	time 0.4698 (0.4689)	loss 3.3238 (3.7353)	grad_norm 1.1361 (nan)	mem 14851MB
[2022-11-06 03:56:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][1000/1251]	eta 0:01:57 lr 0.000934	time 0.4731 (0.4689)	loss 4.4467 (3.7341)	grad_norm 1.2281 (nan)	mem 14851MB
[2022-11-06 03:57:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][1050/1251]	eta 0:01:34 lr 0.000934	time 0.4626 (0.4689)	loss 3.7633 (3.7343)	grad_norm 1.3941 (nan)	mem 14851MB
[2022-11-06 03:57:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][1100/1251]	eta 0:01:10 lr 0.000934	time 0.4590 (0.4687)	loss 3.7163 (3.7340)	grad_norm 1.3123 (nan)	mem 14851MB
[2022-11-06 03:57:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][1150/1251]	eta 0:00:47 lr 0.000934	time 0.4598 (0.4687)	loss 4.4553 (3.7354)	grad_norm 1.2011 (nan)	mem 14851MB
[2022-11-06 03:58:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][1200/1251]	eta 0:00:23 lr 0.000934	time 0.4701 (0.4686)	loss 4.2908 (3.7355)	grad_norm 1.2473 (nan)	mem 14851MB
[2022-11-06 03:58:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [49/300][1250/1251]	eta 0:00:00 lr 0.000934	time 0.4571 (0.4684)	loss 4.1495 (3.7383)	grad_norm 1.1248 (nan)	mem 14851MB
[2022-11-06 03:58:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 49 training takes 0:09:46
[2022-11-06 03:58:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_49.pth saving......
[2022-11-06 03:58:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_49.pth saved !!!
[2022-11-06 03:58:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.504 (1.504)	Loss 1.1959 (1.1959)	Acc@1 71.680 (71.680)	Acc@5 92.480 (92.480)	Mem 14851MB
[2022-11-06 03:58:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.178 Acc@5 91.816
[2022-11-06 03:58:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.2%
[2022-11-06 03:58:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.790 (1.790)	Loss 1.0675 (1.0675)	Acc@1 74.121 (74.121)	Acc@5 93.359 (93.359)	Mem 14851MB
[2022-11-06 03:58:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.872 Acc@5 92.406
[2022-11-06 03:58:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 74.9%
[2022-11-06 03:58:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 74.87% at 49 epoch
[2022-11-06 03:59:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][0/1251]	eta 0:46:07 lr 0.000934	time 2.2124 (2.2124)	loss 3.2568 (3.2568)	grad_norm 1.0693 (1.0693)	mem 14851MB
[2022-11-06 03:59:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][50/1251]	eta 0:10:04 lr 0.000934	time 0.4739 (0.5037)	loss 4.0915 (3.6154)	grad_norm 1.2522 (1.2338)	mem 14851MB
[2022-11-06 03:59:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][100/1251]	eta 0:09:21 lr 0.000933	time 0.4640 (0.4876)	loss 3.6961 (3.6535)	grad_norm 1.3441 (1.2470)	mem 14851MB
[2022-11-06 04:00:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][150/1251]	eta 0:08:50 lr 0.000933	time 0.4635 (0.4814)	loss 3.8352 (3.7300)	grad_norm 1.5129 (1.2575)	mem 14851MB
[2022-11-06 04:00:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][200/1251]	eta 0:08:22 lr 0.000933	time 0.4638 (0.4782)	loss 3.7015 (3.7349)	grad_norm 1.0717 (1.2434)	mem 14851MB
[2022-11-06 04:00:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][250/1251]	eta 0:07:56 lr 0.000933	time 0.4582 (0.4761)	loss 3.9156 (3.7415)	grad_norm 1.1348 (1.2464)	mem 14851MB
[2022-11-06 04:01:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][300/1251]	eta 0:07:31 lr 0.000933	time 0.4725 (0.4745)	loss 3.0478 (3.7311)	grad_norm 1.1757 (1.2488)	mem 14851MB
[2022-11-06 04:01:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][350/1251]	eta 0:07:06 lr 0.000933	time 0.4708 (0.4733)	loss 3.0621 (3.7664)	grad_norm 1.1910 (1.2500)	mem 14851MB
[2022-11-06 04:02:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][400/1251]	eta 0:06:41 lr 0.000933	time 0.4645 (0.4723)	loss 3.7286 (3.7675)	grad_norm 1.2703 (1.2505)	mem 14851MB
[2022-11-06 04:02:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][450/1251]	eta 0:06:17 lr 0.000933	time 0.4622 (0.4716)	loss 3.2815 (3.7719)	grad_norm 1.1385 (1.2513)	mem 14851MB
[2022-11-06 04:02:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][500/1251]	eta 0:05:53 lr 0.000933	time 0.4611 (0.4710)	loss 4.1183 (3.7670)	grad_norm 1.2310 (1.2470)	mem 14851MB
[2022-11-06 04:03:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][550/1251]	eta 0:05:30 lr 0.000933	time 0.4663 (0.4712)	loss 3.8837 (3.7726)	grad_norm 1.1824 (1.2484)	mem 14851MB
[2022-11-06 04:03:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][600/1251]	eta 0:05:06 lr 0.000932	time 0.4544 (0.4709)	loss 3.3134 (3.7702)	grad_norm 1.2220 (1.2470)	mem 14851MB
[2022-11-06 04:04:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][650/1251]	eta 0:04:42 lr 0.000932	time 0.4714 (0.4705)	loss 3.0238 (3.7641)	grad_norm 1.1127 (1.2494)	mem 14851MB
[2022-11-06 04:04:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][700/1251]	eta 0:04:19 lr 0.000932	time 0.4664 (0.4703)	loss 3.5996 (3.7644)	grad_norm 1.2962 (1.2485)	mem 14851MB
[2022-11-06 04:04:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][750/1251]	eta 0:03:55 lr 0.000932	time 0.4655 (0.4699)	loss 2.4536 (3.7666)	grad_norm 1.2992 (1.2485)	mem 14851MB
[2022-11-06 04:05:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][800/1251]	eta 0:03:31 lr 0.000932	time 0.4682 (0.4700)	loss 2.6351 (3.7752)	grad_norm 1.2059 (1.2493)	mem 14851MB
[2022-11-06 04:05:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][850/1251]	eta 0:03:08 lr 0.000932	time 0.4775 (0.4700)	loss 3.8682 (3.7761)	grad_norm 1.2842 (1.2491)	mem 14851MB
[2022-11-06 04:06:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][900/1251]	eta 0:02:44 lr 0.000932	time 0.4939 (0.4698)	loss 4.4544 (3.7765)	grad_norm 1.2617 (1.2494)	mem 14851MB
[2022-11-06 04:06:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][950/1251]	eta 0:02:21 lr 0.000932	time 0.4620 (0.4697)	loss 4.0413 (3.7729)	grad_norm 1.0480 (1.2484)	mem 14851MB
[2022-11-06 04:06:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][1000/1251]	eta 0:01:57 lr 0.000932	time 0.4617 (0.4695)	loss 3.4997 (3.7716)	grad_norm 1.2265 (1.2479)	mem 14851MB
[2022-11-06 04:07:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][1050/1251]	eta 0:01:34 lr 0.000931	time 0.4645 (0.4697)	loss 4.0615 (3.7681)	grad_norm 1.3329 (1.2478)	mem 14851MB
[2022-11-06 04:07:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][1100/1251]	eta 0:01:10 lr 0.000931	time 0.4648 (0.4696)	loss 3.6756 (3.7656)	grad_norm 1.2672 (1.2486)	mem 14851MB
[2022-11-06 04:07:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][1150/1251]	eta 0:00:47 lr 0.000931	time 0.4578 (0.4695)	loss 4.2994 (3.7693)	grad_norm 1.1209 (1.2474)	mem 14851MB
[2022-11-06 04:08:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][1200/1251]	eta 0:00:23 lr 0.000931	time 0.4750 (0.4695)	loss 3.4979 (3.7697)	grad_norm 1.2785 (1.2466)	mem 14851MB
[2022-11-06 04:08:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [50/300][1250/1251]	eta 0:00:00 lr 0.000931	time 0.4571 (0.4692)	loss 4.1902 (3.7723)	grad_norm 1.1837 (1.2449)	mem 14851MB
[2022-11-06 04:08:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 50 training takes 0:09:47
[2022-11-06 04:08:46 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_50.pth saving......
[2022-11-06 04:08:47 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_50.pth saved !!!
[2022-11-06 04:08:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.544 (1.544)	Loss 1.2046 (1.2046)	Acc@1 73.242 (73.242)	Acc@5 91.797 (91.797)	Mem 14851MB
[2022-11-06 04:08:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.022 Acc@5 91.654
[2022-11-06 04:08:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.0%
[2022-11-06 04:08:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.650 (1.650)	Loss 1.0808 (1.0808)	Acc@1 73.730 (73.730)	Acc@5 92.383 (92.383)	Mem 14851MB
[2022-11-06 04:09:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.056 Acc@5 92.472
[2022-11-06 04:09:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 75.1%
[2022-11-06 04:09:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 75.06% at 50 epoch
[2022-11-06 04:09:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][0/1251]	eta 0:39:47 lr 0.000931	time 1.9086 (1.9086)	loss 2.3463 (2.3463)	grad_norm 1.1660 (1.1660)	mem 14851MB
[2022-11-06 04:09:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][50/1251]	eta 0:10:04 lr 0.000931	time 0.4740 (0.5033)	loss 4.0102 (3.7529)	grad_norm 1.3582 (1.2294)	mem 14851MB
[2022-11-06 04:09:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][100/1251]	eta 0:09:20 lr 0.000931	time 0.4663 (0.4873)	loss 2.9233 (3.7740)	grad_norm 1.0871 (1.2390)	mem 14851MB
[2022-11-06 04:10:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][150/1251]	eta 0:08:48 lr 0.000931	time 0.4730 (0.4805)	loss 4.2379 (3.7520)	grad_norm 1.2898 (1.2296)	mem 14851MB
[2022-11-06 04:10:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][200/1251]	eta 0:08:21 lr 0.000931	time 0.4757 (0.4768)	loss 4.4006 (3.7706)	grad_norm 1.2313 (1.2327)	mem 14851MB
[2022-11-06 04:11:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][250/1251]	eta 0:07:55 lr 0.000931	time 0.4692 (0.4747)	loss 2.8462 (3.7606)	grad_norm 1.1822 (1.2330)	mem 14851MB
[2022-11-06 04:11:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][300/1251]	eta 0:07:29 lr 0.000930	time 0.4667 (0.4732)	loss 3.8737 (3.7631)	grad_norm 1.5432 (1.2344)	mem 14851MB
[2022-11-06 04:11:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][350/1251]	eta 0:07:05 lr 0.000930	time 0.4657 (0.4723)	loss 3.2696 (3.7481)	grad_norm 1.2323 (1.2339)	mem 14851MB
[2022-11-06 04:12:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][400/1251]	eta 0:06:41 lr 0.000930	time 0.4642 (0.4716)	loss 3.8563 (3.7405)	grad_norm 1.1046 (1.2309)	mem 14851MB
[2022-11-06 04:12:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][450/1251]	eta 0:06:17 lr 0.000930	time 0.4595 (0.4707)	loss 3.9538 (3.7444)	grad_norm 1.0587 (1.2319)	mem 14851MB
[2022-11-06 04:13:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][500/1251]	eta 0:05:53 lr 0.000930	time 0.4731 (0.4702)	loss 4.4025 (3.7454)	grad_norm 1.2781 (1.2348)	mem 14851MB
[2022-11-06 04:13:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][550/1251]	eta 0:05:29 lr 0.000930	time 0.4584 (0.4704)	loss 2.7960 (3.7457)	grad_norm 1.1268 (1.2381)	mem 14851MB
[2022-11-06 04:13:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][600/1251]	eta 0:05:06 lr 0.000930	time 0.4611 (0.4701)	loss 3.5649 (3.7443)	grad_norm 1.2135 (1.2373)	mem 14851MB
[2022-11-06 04:14:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][650/1251]	eta 0:04:42 lr 0.000930	time 0.4772 (0.4698)	loss 3.7739 (3.7453)	grad_norm 1.3132 (1.2343)	mem 14851MB
[2022-11-06 04:14:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][700/1251]	eta 0:04:18 lr 0.000930	time 0.4649 (0.4697)	loss 2.8961 (3.7459)	grad_norm 1.1354 (1.2356)	mem 14851MB
[2022-11-06 04:14:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][750/1251]	eta 0:03:55 lr 0.000929	time 0.4667 (0.4694)	loss 3.6642 (3.7385)	grad_norm 1.1602 (1.2364)	mem 14851MB
[2022-11-06 04:15:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][800/1251]	eta 0:03:31 lr 0.000929	time 0.4702 (0.4696)	loss 3.2187 (3.7380)	grad_norm 1.3302 (1.2388)	mem 14851MB
[2022-11-06 04:15:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][850/1251]	eta 0:03:08 lr 0.000929	time 0.4697 (0.4696)	loss 3.8056 (3.7405)	grad_norm 1.3874 (1.2401)	mem 14851MB
[2022-11-06 04:16:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][900/1251]	eta 0:02:44 lr 0.000929	time 0.4620 (0.4694)	loss 3.6052 (3.7360)	grad_norm 1.0999 (1.2398)	mem 14851MB
[2022-11-06 04:16:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][950/1251]	eta 0:02:21 lr 0.000929	time 0.4629 (0.4693)	loss 3.1225 (3.7371)	grad_norm 1.1760 (1.2402)	mem 14851MB
[2022-11-06 04:16:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][1000/1251]	eta 0:01:57 lr 0.000929	time 0.4723 (0.4692)	loss 3.0413 (3.7314)	grad_norm 1.2135 (1.2422)	mem 14851MB
[2022-11-06 04:17:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][1050/1251]	eta 0:01:34 lr 0.000929	time 0.4591 (0.4694)	loss 4.3282 (3.7269)	grad_norm 1.1252 (1.2407)	mem 14851MB
[2022-11-06 04:17:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][1100/1251]	eta 0:01:10 lr 0.000929	time 0.4610 (0.4693)	loss 4.0128 (3.7325)	grad_norm 1.2728 (1.2411)	mem 14851MB
[2022-11-06 04:18:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][1150/1251]	eta 0:00:47 lr 0.000929	time 0.4648 (0.4692)	loss 3.6458 (3.7319)	grad_norm 1.2659 (1.2406)	mem 14851MB
[2022-11-06 04:18:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][1200/1251]	eta 0:00:23 lr 0.000929	time 0.4657 (0.4691)	loss 3.6520 (3.7319)	grad_norm 1.2017 (1.2408)	mem 14851MB
[2022-11-06 04:18:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [51/300][1250/1251]	eta 0:00:00 lr 0.000928	time 0.4594 (0.4689)	loss 4.1189 (3.7320)	grad_norm 1.0641 (1.2396)	mem 14851MB
[2022-11-06 04:18:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 51 training takes 0:09:46
[2022-11-06 04:18:51 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_51.pth saving......
[2022-11-06 04:18:52 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_51.pth saved !!!
[2022-11-06 04:18:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.637 (1.637)	Loss 1.1000 (1.1000)	Acc@1 74.121 (74.121)	Acc@5 92.773 (92.773)	Mem 14851MB
[2022-11-06 04:19:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.072 Acc@5 91.722
[2022-11-06 04:19:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.1%
[2022-11-06 04:19:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.617 (1.617)	Loss 1.0650 (1.0650)	Acc@1 75.781 (75.781)	Acc@5 92.676 (92.676)	Mem 14851MB
[2022-11-06 04:19:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.218 Acc@5 92.552
[2022-11-06 04:19:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 75.2%
[2022-11-06 04:19:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 75.22% at 51 epoch
[2022-11-06 04:19:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][0/1251]	eta 0:42:26 lr 0.000928	time 2.0358 (2.0358)	loss 3.5251 (3.5251)	grad_norm 1.1857 (1.1857)	mem 14851MB
[2022-11-06 04:19:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][50/1251]	eta 0:10:03 lr 0.000928	time 0.4760 (0.5023)	loss 4.1213 (3.7213)	grad_norm 1.3057 (1.2517)	mem 14851MB
[2022-11-06 04:19:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][100/1251]	eta 0:09:20 lr 0.000928	time 0.4624 (0.4867)	loss 4.3461 (3.6914)	grad_norm 1.4073 (1.2510)	mem 14851MB
[2022-11-06 04:20:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][150/1251]	eta 0:08:48 lr 0.000928	time 0.4750 (0.4802)	loss 3.9683 (3.7838)	grad_norm 1.1884 (1.2416)	mem 14851MB
[2022-11-06 04:20:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][200/1251]	eta 0:08:20 lr 0.000928	time 0.4623 (0.4764)	loss 3.8672 (3.7806)	grad_norm 1.2299 (1.2443)	mem 14851MB
[2022-11-06 04:21:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][250/1251]	eta 0:07:55 lr 0.000928	time 0.4597 (0.4747)	loss 3.8405 (3.7693)	grad_norm 1.1570 (1.2412)	mem 14851MB
[2022-11-06 04:21:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][300/1251]	eta 0:07:30 lr 0.000928	time 0.4574 (0.4733)	loss 4.4523 (3.7616)	grad_norm 1.0573 (1.2435)	mem 14851MB
[2022-11-06 04:21:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][350/1251]	eta 0:07:05 lr 0.000928	time 0.4578 (0.4721)	loss 4.6934 (3.7582)	grad_norm 1.2907 (1.2438)	mem 14851MB
[2022-11-06 04:22:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][400/1251]	eta 0:06:41 lr 0.000928	time 0.4655 (0.4714)	loss 4.1627 (3.7796)	grad_norm 1.0640 (1.2413)	mem 14851MB
[2022-11-06 04:22:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][450/1251]	eta 0:06:17 lr 0.000927	time 0.4607 (0.4707)	loss 4.5734 (3.7832)	grad_norm 1.1908 (1.2414)	mem 14851MB
[2022-11-06 04:23:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][500/1251]	eta 0:05:53 lr 0.000927	time 0.4651 (0.4702)	loss 4.3865 (3.7937)	grad_norm 1.2410 (1.2417)	mem 14851MB
[2022-11-06 04:23:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][550/1251]	eta 0:05:29 lr 0.000927	time 0.4753 (0.4706)	loss 3.7649 (3.7886)	grad_norm 1.3474 (1.2453)	mem 14851MB
[2022-11-06 04:23:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][600/1251]	eta 0:05:06 lr 0.000927	time 0.4678 (0.4703)	loss 3.5865 (3.7808)	grad_norm 1.3412 (1.2459)	mem 14851MB
[2022-11-06 04:24:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][650/1251]	eta 0:04:42 lr 0.000927	time 0.4550 (0.4699)	loss 4.1716 (3.7741)	grad_norm 1.2404 (1.2449)	mem 14851MB
[2022-11-06 04:24:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][700/1251]	eta 0:04:18 lr 0.000927	time 0.4549 (0.4696)	loss 3.6701 (3.7668)	grad_norm 1.1805 (1.2464)	mem 14851MB
[2022-11-06 04:25:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][750/1251]	eta 0:03:55 lr 0.000927	time 0.4729 (0.4693)	loss 2.8354 (3.7688)	grad_norm 1.2695 (1.2456)	mem 14851MB
[2022-11-06 04:25:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][800/1251]	eta 0:03:31 lr 0.000927	time 0.4557 (0.4695)	loss 4.3766 (3.7610)	grad_norm 1.2844 (inf)	mem 14851MB
[2022-11-06 04:25:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][850/1251]	eta 0:03:08 lr 0.000927	time 0.4576 (0.4693)	loss 4.5456 (3.7588)	grad_norm 1.1102 (inf)	mem 14851MB
[2022-11-06 04:26:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][900/1251]	eta 0:02:44 lr 0.000926	time 0.4695 (0.4691)	loss 4.1344 (3.7604)	grad_norm 1.2486 (inf)	mem 14851MB
[2022-11-06 04:26:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][950/1251]	eta 0:02:21 lr 0.000926	time 0.4572 (0.4690)	loss 3.8264 (3.7559)	grad_norm 1.2034 (inf)	mem 14851MB
[2022-11-06 04:26:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][1000/1251]	eta 0:01:57 lr 0.000926	time 0.4668 (0.4688)	loss 4.0460 (3.7562)	grad_norm 1.1646 (inf)	mem 14851MB
[2022-11-06 04:27:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][1050/1251]	eta 0:01:34 lr 0.000926	time 0.4679 (0.4690)	loss 2.8276 (3.7590)	grad_norm 1.3120 (inf)	mem 14851MB
[2022-11-06 04:27:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][1100/1251]	eta 0:01:10 lr 0.000926	time 0.4611 (0.4689)	loss 3.8868 (3.7610)	grad_norm 1.3908 (inf)	mem 14851MB
[2022-11-06 04:28:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][1150/1251]	eta 0:00:47 lr 0.000926	time 0.4562 (0.4688)	loss 3.8517 (3.7635)	grad_norm 1.3083 (inf)	mem 14851MB
[2022-11-06 04:28:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][1200/1251]	eta 0:00:23 lr 0.000926	time 0.4619 (0.4688)	loss 3.4167 (3.7615)	grad_norm 1.1674 (inf)	mem 14851MB
[2022-11-06 04:28:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [52/300][1250/1251]	eta 0:00:00 lr 0.000926	time 0.4572 (0.4685)	loss 4.1457 (3.7583)	grad_norm 1.2925 (inf)	mem 14851MB
[2022-11-06 04:28:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 52 training takes 0:09:46
[2022-11-06 04:28:56 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_52.pth saving......
[2022-11-06 04:28:56 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_52.pth saved !!!
[2022-11-06 04:28:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.544 (1.544)	Loss 1.1616 (1.1616)	Acc@1 74.414 (74.414)	Acc@5 92.773 (92.773)	Mem 14851MB
[2022-11-06 04:29:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.310 Acc@5 91.872
[2022-11-06 04:29:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.3%
[2022-11-06 04:29:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.628 (1.628)	Loss 0.9448 (0.9448)	Acc@1 76.465 (76.465)	Acc@5 94.043 (94.043)	Mem 14851MB
[2022-11-06 04:29:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.442 Acc@5 92.646
[2022-11-06 04:29:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 75.4%
[2022-11-06 04:29:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 75.44% at 52 epoch
[2022-11-06 04:29:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][0/1251]	eta 0:40:13 lr 0.000926	time 1.9292 (1.9292)	loss 4.0923 (4.0923)	grad_norm 1.1134 (1.1134)	mem 14851MB
[2022-11-06 04:29:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][50/1251]	eta 0:10:02 lr 0.000926	time 0.4663 (0.5013)	loss 4.2361 (3.6339)	grad_norm 1.1718 (1.2226)	mem 14851MB
[2022-11-06 04:30:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][100/1251]	eta 0:09:18 lr 0.000925	time 0.4698 (0.4854)	loss 3.2895 (3.7396)	grad_norm 1.2198 (1.2456)	mem 14851MB
[2022-11-06 04:30:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][150/1251]	eta 0:08:47 lr 0.000925	time 0.4642 (0.4790)	loss 4.2177 (3.7142)	grad_norm 1.3229 (1.2375)	mem 14851MB
[2022-11-06 04:30:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][200/1251]	eta 0:08:20 lr 0.000925	time 0.4816 (0.4762)	loss 4.2470 (3.7104)	grad_norm 1.2113 (1.2443)	mem 14851MB
[2022-11-06 04:31:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][250/1251]	eta 0:07:54 lr 0.000925	time 0.4516 (0.4739)	loss 4.4433 (3.7203)	grad_norm 1.0815 (1.2385)	mem 14851MB
[2022-11-06 04:31:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][300/1251]	eta 0:07:29 lr 0.000925	time 0.4604 (0.4730)	loss 2.6197 (3.7280)	grad_norm 1.0840 (1.2380)	mem 14851MB
[2022-11-06 04:31:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][350/1251]	eta 0:07:05 lr 0.000925	time 0.4610 (0.4723)	loss 3.3346 (3.7137)	grad_norm 1.1441 (1.2367)	mem 14851MB
[2022-11-06 04:32:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][400/1251]	eta 0:06:41 lr 0.000925	time 0.4670 (0.4714)	loss 3.6647 (3.7224)	grad_norm 1.2345 (1.2373)	mem 14851MB
[2022-11-06 04:32:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][450/1251]	eta 0:06:17 lr 0.000925	time 0.4683 (0.4708)	loss 3.5570 (3.7265)	grad_norm 1.3190 (1.2385)	mem 14851MB
[2022-11-06 04:33:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][500/1251]	eta 0:05:53 lr 0.000925	time 0.4590 (0.4703)	loss 3.9306 (3.7253)	grad_norm 1.3477 (1.2442)	mem 14851MB
[2022-11-06 04:33:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][550/1251]	eta 0:05:29 lr 0.000924	time 0.4716 (0.4703)	loss 4.3961 (3.7455)	grad_norm 1.3687 (1.2409)	mem 14851MB
[2022-11-06 04:33:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][600/1251]	eta 0:05:05 lr 0.000924	time 0.4698 (0.4699)	loss 4.0578 (3.7352)	grad_norm 1.3704 (1.2424)	mem 14851MB
[2022-11-06 04:34:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][650/1251]	eta 0:04:42 lr 0.000924	time 0.4662 (0.4696)	loss 4.7647 (3.7414)	grad_norm 1.3340 (1.2410)	mem 14851MB
[2022-11-06 04:34:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][700/1251]	eta 0:04:18 lr 0.000924	time 0.4633 (0.4693)	loss 3.0403 (3.7379)	grad_norm 1.1336 (1.2399)	mem 14851MB
[2022-11-06 04:35:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][750/1251]	eta 0:03:55 lr 0.000924	time 0.4672 (0.4692)	loss 4.2431 (3.7353)	grad_norm 1.3538 (1.2399)	mem 14851MB
[2022-11-06 04:35:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][800/1251]	eta 0:03:31 lr 0.000924	time 0.4728 (0.4694)	loss 3.1490 (3.7338)	grad_norm 1.3516 (1.2402)	mem 14851MB
[2022-11-06 04:35:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][850/1251]	eta 0:03:08 lr 0.000924	time 0.4745 (0.4692)	loss 4.5157 (3.7264)	grad_norm 1.2326 (1.2405)	mem 14851MB
[2022-11-06 04:36:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][900/1251]	eta 0:02:44 lr 0.000924	time 0.4781 (0.4690)	loss 3.9154 (3.7292)	grad_norm 1.2186 (nan)	mem 14851MB
[2022-11-06 04:36:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][950/1251]	eta 0:02:21 lr 0.000924	time 0.4650 (0.4689)	loss 2.9115 (3.7242)	grad_norm 1.3051 (nan)	mem 14851MB
[2022-11-06 04:37:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][1000/1251]	eta 0:01:57 lr 0.000923	time 0.4562 (0.4688)	loss 2.6435 (3.7241)	grad_norm 1.2257 (nan)	mem 14851MB
[2022-11-06 04:37:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][1050/1251]	eta 0:01:34 lr 0.000923	time 0.4601 (0.4689)	loss 3.7962 (3.7206)	grad_norm 1.1632 (nan)	mem 14851MB
[2022-11-06 04:37:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][1100/1251]	eta 0:01:10 lr 0.000923	time 0.4697 (0.4688)	loss 3.3034 (3.7182)	grad_norm 1.3434 (nan)	mem 14851MB
[2022-11-06 04:38:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][1150/1251]	eta 0:00:47 lr 0.000923	time 0.4756 (0.4687)	loss 3.3730 (3.7186)	grad_norm 1.2345 (nan)	mem 14851MB
[2022-11-06 04:38:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][1200/1251]	eta 0:00:23 lr 0.000923	time 0.4560 (0.4685)	loss 4.0425 (3.7200)	grad_norm 1.3688 (nan)	mem 14851MB
[2022-11-06 04:38:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [53/300][1250/1251]	eta 0:00:00 lr 0.000923	time 0.4620 (0.4684)	loss 3.7485 (3.7137)	grad_norm 1.2344 (nan)	mem 14851MB
[2022-11-06 04:39:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 53 training takes 0:09:46
[2022-11-06 04:39:00 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_53.pth saving......
[2022-11-06 04:39:00 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_53.pth saved !!!
[2022-11-06 04:39:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.685 (1.685)	Loss 1.2560 (1.2560)	Acc@1 69.922 (69.922)	Acc@5 90.527 (90.527)	Mem 14851MB
[2022-11-06 04:39:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.316 Acc@5 91.958
[2022-11-06 04:39:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.3%
[2022-11-06 04:39:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.535 (1.535)	Loss 1.0152 (1.0152)	Acc@1 76.172 (76.172)	Acc@5 92.676 (92.676)	Mem 14851MB
[2022-11-06 04:39:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.580 Acc@5 92.758
[2022-11-06 04:39:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 75.6%
[2022-11-06 04:39:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 75.58% at 53 epoch
[2022-11-06 04:39:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][0/1251]	eta 0:39:40 lr 0.000923	time 1.9028 (1.9028)	loss 2.6722 (2.6722)	grad_norm 1.2044 (1.2044)	mem 14851MB
[2022-11-06 04:39:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][50/1251]	eta 0:10:01 lr 0.000923	time 0.4632 (0.5006)	loss 4.5732 (3.8215)	grad_norm 1.1264 (1.2322)	mem 14851MB
[2022-11-06 04:40:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][100/1251]	eta 0:09:15 lr 0.000923	time 0.4610 (0.4827)	loss 4.0773 (3.7938)	grad_norm 1.1759 (1.2333)	mem 14851MB
[2022-11-06 04:40:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][150/1251]	eta 0:08:46 lr 0.000923	time 0.5612 (0.4778)	loss 4.5455 (3.8070)	grad_norm 1.2466 (1.2273)	mem 14851MB
[2022-11-06 04:40:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][200/1251]	eta 0:08:19 lr 0.000922	time 0.4543 (0.4756)	loss 3.7847 (3.7810)	grad_norm 1.0863 (1.2354)	mem 14851MB
[2022-11-06 04:41:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][250/1251]	eta 0:07:54 lr 0.000922	time 0.4767 (0.4741)	loss 4.1777 (3.7681)	grad_norm 1.3941 (1.2455)	mem 14851MB
[2022-11-06 04:41:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][300/1251]	eta 0:07:29 lr 0.000922	time 0.4589 (0.4724)	loss 3.5552 (3.7442)	grad_norm 1.3587 (1.2521)	mem 14851MB
[2022-11-06 04:42:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][350/1251]	eta 0:07:04 lr 0.000922	time 0.4687 (0.4714)	loss 3.6096 (3.7487)	grad_norm 1.1400 (1.2479)	mem 14851MB
[2022-11-06 04:42:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][400/1251]	eta 0:06:40 lr 0.000922	time 0.4633 (0.4706)	loss 4.0933 (3.7394)	grad_norm 1.0873 (1.2507)	mem 14851MB
[2022-11-06 04:42:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][450/1251]	eta 0:06:16 lr 0.000922	time 0.4621 (0.4700)	loss 3.9011 (3.7348)	grad_norm 1.3516 (1.2509)	mem 14851MB
[2022-11-06 04:43:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][500/1251]	eta 0:05:52 lr 0.000922	time 0.4661 (0.4696)	loss 2.5143 (3.7281)	grad_norm 1.0623 (1.2474)	mem 14851MB
[2022-11-06 04:43:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][550/1251]	eta 0:05:29 lr 0.000922	time 0.4632 (0.4698)	loss 4.1962 (3.7278)	grad_norm 1.2588 (1.2488)	mem 14851MB
[2022-11-06 04:44:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][600/1251]	eta 0:05:05 lr 0.000922	time 0.4747 (0.4694)	loss 3.5385 (3.7281)	grad_norm 1.1883 (1.2467)	mem 14851MB
[2022-11-06 04:44:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][650/1251]	eta 0:04:41 lr 0.000921	time 0.4588 (0.4691)	loss 3.0882 (3.7354)	grad_norm 1.4435 (1.2460)	mem 14851MB
[2022-11-06 04:44:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][700/1251]	eta 0:04:18 lr 0.000921	time 0.4666 (0.4692)	loss 4.0428 (3.7427)	grad_norm 1.2632 (1.2473)	mem 14851MB
[2022-11-06 04:45:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][750/1251]	eta 0:03:55 lr 0.000921	time 0.4666 (0.4691)	loss 2.6722 (3.7428)	grad_norm 1.2268 (1.2491)	mem 14851MB
[2022-11-06 04:45:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][800/1251]	eta 0:03:31 lr 0.000921	time 0.4623 (0.4692)	loss 4.4124 (3.7499)	grad_norm 1.1882 (1.2502)	mem 14851MB
[2022-11-06 04:45:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][850/1251]	eta 0:03:08 lr 0.000921	time 0.4730 (0.4689)	loss 3.1109 (3.7547)	grad_norm 1.1366 (1.2491)	mem 14851MB
[2022-11-06 04:46:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][900/1251]	eta 0:02:44 lr 0.000921	time 0.4654 (0.4689)	loss 4.1658 (3.7570)	grad_norm 1.2931 (1.2484)	mem 14851MB
[2022-11-06 04:46:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][950/1251]	eta 0:02:21 lr 0.000921	time 0.4660 (0.4688)	loss 4.0067 (3.7544)	grad_norm 1.4869 (1.2491)	mem 14851MB
[2022-11-06 04:47:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][1000/1251]	eta 0:01:57 lr 0.000921	time 0.4538 (0.4687)	loss 3.7688 (3.7563)	grad_norm 1.0760 (1.2489)	mem 14851MB
[2022-11-06 04:47:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][1050/1251]	eta 0:01:34 lr 0.000921	time 0.4625 (0.4688)	loss 4.0929 (3.7553)	grad_norm 1.3021 (1.2482)	mem 14851MB
[2022-11-06 04:47:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][1100/1251]	eta 0:01:10 lr 0.000920	time 0.4634 (0.4686)	loss 4.5597 (3.7577)	grad_norm 1.3026 (1.2472)	mem 14851MB
[2022-11-06 04:48:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][1150/1251]	eta 0:00:47 lr 0.000920	time 0.4618 (0.4685)	loss 3.9784 (3.7541)	grad_norm 1.2512 (1.2470)	mem 14851MB
[2022-11-06 04:48:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][1200/1251]	eta 0:00:23 lr 0.000920	time 0.4705 (0.4685)	loss 4.4245 (3.7567)	grad_norm 1.3549 (1.2463)	mem 14851MB
[2022-11-06 04:49:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [54/300][1250/1251]	eta 0:00:00 lr 0.000920	time 0.4574 (0.4683)	loss 3.0250 (3.7501)	grad_norm 1.1396 (1.2446)	mem 14851MB
[2022-11-06 04:49:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 54 training takes 0:09:46
[2022-11-06 04:49:04 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_54.pth saving......
[2022-11-06 04:49:05 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_54.pth saved !!!
[2022-11-06 04:49:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.598 (1.598)	Loss 1.1830 (1.1830)	Acc@1 71.875 (71.875)	Acc@5 91.504 (91.504)	Mem 14851MB
[2022-11-06 04:49:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.220 Acc@5 92.008
[2022-11-06 04:49:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.2%
[2022-11-06 04:49:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.577 (1.577)	Loss 1.0180 (1.0180)	Acc@1 76.855 (76.855)	Acc@5 93.164 (93.164)	Mem 14851MB
[2022-11-06 04:49:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.710 Acc@5 92.846
[2022-11-06 04:49:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 75.7%
[2022-11-06 04:49:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 75.71% at 54 epoch
[2022-11-06 04:49:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][0/1251]	eta 0:42:46 lr 0.000920	time 2.0515 (2.0515)	loss 3.8948 (3.8948)	grad_norm 1.1327 (1.1327)	mem 14851MB
[2022-11-06 04:49:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][50/1251]	eta 0:10:06 lr 0.000920	time 0.4678 (0.5046)	loss 3.7265 (3.7206)	grad_norm 1.2550 (1.2306)	mem 14851MB
[2022-11-06 04:50:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][100/1251]	eta 0:09:19 lr 0.000920	time 0.4648 (0.4862)	loss 4.1652 (3.7048)	grad_norm 1.3578 (1.2526)	mem 14851MB
[2022-11-06 04:50:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][150/1251]	eta 0:08:48 lr 0.000920	time 0.4720 (0.4804)	loss 3.6002 (3.7325)	grad_norm 1.3465 (1.2668)	mem 14851MB
[2022-11-06 04:50:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][200/1251]	eta 0:08:20 lr 0.000920	time 0.4659 (0.4766)	loss 4.1603 (3.7515)	grad_norm 1.3498 (1.2606)	mem 14851MB
[2022-11-06 04:51:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][250/1251]	eta 0:07:55 lr 0.000920	time 0.4638 (0.4746)	loss 4.3087 (3.7479)	grad_norm 1.3205 (1.2587)	mem 14851MB
[2022-11-06 04:51:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][300/1251]	eta 0:07:29 lr 0.000919	time 0.4633 (0.4731)	loss 3.0359 (3.7190)	grad_norm 1.2402 (1.2522)	mem 14851MB
[2022-11-06 04:52:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][350/1251]	eta 0:07:05 lr 0.000919	time 0.4717 (0.4722)	loss 3.3565 (3.7091)	grad_norm 1.1618 (1.2522)	mem 14851MB
[2022-11-06 04:52:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][400/1251]	eta 0:06:40 lr 0.000919	time 0.4647 (0.4712)	loss 2.5468 (3.7063)	grad_norm 1.5424 (1.2526)	mem 14851MB
[2022-11-06 04:52:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][450/1251]	eta 0:06:16 lr 0.000919	time 0.4725 (0.4705)	loss 4.5105 (3.7086)	grad_norm 1.1048 (1.2512)	mem 14851MB
[2022-11-06 04:53:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][500/1251]	eta 0:05:52 lr 0.000919	time 0.4679 (0.4699)	loss 2.9284 (3.7039)	grad_norm 1.1425 (1.2538)	mem 14851MB
[2022-11-06 04:53:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][550/1251]	eta 0:05:29 lr 0.000919	time 0.4649 (0.4700)	loss 4.0230 (3.7087)	grad_norm 1.1448 (1.2516)	mem 14851MB
[2022-11-06 04:54:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][600/1251]	eta 0:05:05 lr 0.000919	time 0.4798 (0.4695)	loss 3.7665 (3.7076)	grad_norm 1.1589 (1.2477)	mem 14851MB
[2022-11-06 04:54:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][650/1251]	eta 0:04:42 lr 0.000919	time 0.4780 (0.4696)	loss 3.5867 (3.7202)	grad_norm 1.3674 (1.2454)	mem 14851MB
[2022-11-06 04:54:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][700/1251]	eta 0:04:18 lr 0.000919	time 0.4600 (0.4694)	loss 4.4660 (3.7175)	grad_norm 1.3599 (1.2476)	mem 14851MB
[2022-11-06 04:55:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][750/1251]	eta 0:03:55 lr 0.000918	time 0.4682 (0.4692)	loss 2.7125 (3.7167)	grad_norm 1.2500 (1.2479)	mem 14851MB
[2022-11-06 04:55:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][800/1251]	eta 0:03:31 lr 0.000918	time 0.4660 (0.4692)	loss 2.9040 (3.7089)	grad_norm 1.1195 (1.2479)	mem 14851MB
[2022-11-06 04:56:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][850/1251]	eta 0:03:08 lr 0.000918	time 0.4554 (0.4690)	loss 2.5818 (3.7040)	grad_norm 1.5648 (1.2508)	mem 14851MB
[2022-11-06 04:56:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][900/1251]	eta 0:02:44 lr 0.000918	time 0.4745 (0.4689)	loss 3.8188 (3.7083)	grad_norm 1.3331 (1.2493)	mem 14851MB
[2022-11-06 04:56:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][950/1251]	eta 0:02:21 lr 0.000918	time 0.4710 (0.4689)	loss 4.2191 (3.7151)	grad_norm 1.3841 (1.2495)	mem 14851MB
[2022-11-06 04:57:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][1000/1251]	eta 0:01:57 lr 0.000918	time 0.4735 (0.4687)	loss 4.2189 (3.7173)	grad_norm 1.1107 (1.2493)	mem 14851MB
[2022-11-06 04:57:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][1050/1251]	eta 0:01:34 lr 0.000918	time 0.4570 (0.4687)	loss 2.9896 (3.7159)	grad_norm 1.4772 (1.2498)	mem 14851MB
[2022-11-06 04:57:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][1100/1251]	eta 0:01:10 lr 0.000918	time 0.4716 (0.4685)	loss 3.9285 (3.7121)	grad_norm 1.2366 (1.2485)	mem 14851MB
[2022-11-06 04:58:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][1150/1251]	eta 0:00:47 lr 0.000918	time 0.4617 (0.4686)	loss 4.5063 (3.7163)	grad_norm 1.3650 (1.2468)	mem 14851MB
[2022-11-06 04:58:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][1200/1251]	eta 0:00:23 lr 0.000917	time 0.4642 (0.4685)	loss 2.6740 (3.7070)	grad_norm 1.0380 (1.2465)	mem 14851MB
[2022-11-06 04:59:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [55/300][1250/1251]	eta 0:00:00 lr 0.000917	time 0.4571 (0.4683)	loss 3.9178 (3.7065)	grad_norm 1.2416 (1.2451)	mem 14851MB
[2022-11-06 04:59:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 55 training takes 0:09:46
[2022-11-06 04:59:08 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_55.pth saving......
[2022-11-06 04:59:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_55.pth saved !!!
[2022-11-06 04:59:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.493 (1.493)	Loss 1.1657 (1.1657)	Acc@1 73.730 (73.730)	Acc@5 92.383 (92.383)	Mem 14851MB
[2022-11-06 04:59:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.332 Acc@5 91.862
[2022-11-06 04:59:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.3%
[2022-11-06 04:59:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.638 (1.638)	Loss 1.0522 (1.0522)	Acc@1 75.684 (75.684)	Acc@5 93.262 (93.262)	Mem 14851MB
[2022-11-06 04:59:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.868 Acc@5 92.926
[2022-11-06 04:59:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 75.9%
[2022-11-06 04:59:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 75.87% at 55 epoch
[2022-11-06 04:59:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][0/1251]	eta 0:41:27 lr 0.000917	time 1.9887 (1.9887)	loss 3.4788 (3.4788)	grad_norm 1.1379 (1.1379)	mem 14851MB
[2022-11-06 04:59:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][50/1251]	eta 0:10:00 lr 0.000917	time 0.4730 (0.5003)	loss 3.1205 (3.6013)	grad_norm 1.1813 (1.2257)	mem 14851MB
[2022-11-06 05:00:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][100/1251]	eta 0:09:18 lr 0.000917	time 0.4685 (0.4852)	loss 2.8224 (3.6709)	grad_norm 1.1739 (1.2348)	mem 14851MB
[2022-11-06 05:00:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][150/1251]	eta 0:08:49 lr 0.000917	time 0.4608 (0.4807)	loss 3.4655 (3.7093)	grad_norm 1.1938 (1.2443)	mem 14851MB
[2022-11-06 05:01:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][200/1251]	eta 0:08:20 lr 0.000917	time 0.4679 (0.4766)	loss 3.0796 (3.6863)	grad_norm 1.2257 (1.2521)	mem 14851MB
[2022-11-06 05:01:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][250/1251]	eta 0:07:54 lr 0.000917	time 0.4635 (0.4741)	loss 3.5040 (3.6997)	grad_norm 1.1107 (1.2488)	mem 14851MB
[2022-11-06 05:01:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][300/1251]	eta 0:07:29 lr 0.000917	time 0.4650 (0.4729)	loss 4.1638 (3.7117)	grad_norm 1.2258 (1.2461)	mem 14851MB
[2022-11-06 05:02:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][350/1251]	eta 0:07:05 lr 0.000916	time 0.4618 (0.4719)	loss 3.8285 (3.7211)	grad_norm 1.1651 (1.2510)	mem 14851MB
[2022-11-06 05:02:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][400/1251]	eta 0:06:41 lr 0.000916	time 0.4710 (0.4714)	loss 4.3726 (3.7260)	grad_norm 1.1466 (1.2460)	mem 14851MB
[2022-11-06 05:02:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][450/1251]	eta 0:06:17 lr 0.000916	time 0.4685 (0.4708)	loss 3.6009 (3.7171)	grad_norm 1.0739 (1.2501)	mem 14851MB
[2022-11-06 05:03:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][500/1251]	eta 0:05:53 lr 0.000916	time 0.4630 (0.4702)	loss 4.2468 (3.7227)	grad_norm 1.1953 (1.2505)	mem 14851MB
[2022-11-06 05:03:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][550/1251]	eta 0:05:29 lr 0.000916	time 0.4681 (0.4701)	loss 3.2447 (3.7263)	grad_norm 1.1556 (1.2507)	mem 14851MB
[2022-11-06 05:04:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][600/1251]	eta 0:05:06 lr 0.000916	time 0.4525 (0.4701)	loss 4.0980 (3.7251)	grad_norm 1.2086 (1.2527)	mem 14851MB
[2022-11-06 05:04:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][650/1251]	eta 0:04:42 lr 0.000916	time 0.4611 (0.4699)	loss 3.4544 (3.7363)	grad_norm 1.2836 (1.2528)	mem 14851MB
[2022-11-06 05:04:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][700/1251]	eta 0:04:18 lr 0.000916	time 0.4612 (0.4696)	loss 2.8069 (3.7312)	grad_norm 1.2928 (1.2521)	mem 14851MB
[2022-11-06 05:05:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][750/1251]	eta 0:03:55 lr 0.000916	time 0.4652 (0.4693)	loss 4.3151 (3.7215)	grad_norm 1.2798 (1.2516)	mem 14851MB
[2022-11-06 05:05:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][800/1251]	eta 0:03:31 lr 0.000915	time 0.4705 (0.4694)	loss 4.0833 (3.7198)	grad_norm 1.0900 (1.2524)	mem 14851MB
[2022-11-06 05:06:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][850/1251]	eta 0:03:08 lr 0.000915	time 0.4725 (0.4694)	loss 4.1324 (3.7166)	grad_norm 1.2105 (1.2529)	mem 14851MB
[2022-11-06 05:06:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][900/1251]	eta 0:02:44 lr 0.000915	time 0.4611 (0.4692)	loss 3.8234 (3.7216)	grad_norm 1.2719 (1.2522)	mem 14851MB
[2022-11-06 05:06:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][950/1251]	eta 0:02:21 lr 0.000915	time 0.4639 (0.4690)	loss 4.0199 (3.7252)	grad_norm 1.5078 (1.2515)	mem 14851MB
[2022-11-06 05:07:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][1000/1251]	eta 0:01:57 lr 0.000915	time 0.4598 (0.4690)	loss 4.2039 (3.7322)	grad_norm 1.2990 (nan)	mem 14851MB
[2022-11-06 05:07:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][1050/1251]	eta 0:01:34 lr 0.000915	time 0.4598 (0.4690)	loss 3.3434 (3.7247)	grad_norm 1.2008 (nan)	mem 14851MB
[2022-11-06 05:08:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][1100/1251]	eta 0:01:10 lr 0.000915	time 0.4767 (0.4690)	loss 2.8142 (3.7263)	grad_norm 1.2501 (nan)	mem 14851MB
[2022-11-06 05:08:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][1150/1251]	eta 0:00:47 lr 0.000915	time 0.4793 (0.4689)	loss 3.8504 (3.7252)	grad_norm 1.2420 (nan)	mem 14851MB
[2022-11-06 05:08:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][1200/1251]	eta 0:00:23 lr 0.000915	time 0.4663 (0.4687)	loss 2.9984 (3.7278)	grad_norm 1.3628 (nan)	mem 14851MB
[2022-11-06 05:09:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [56/300][1250/1251]	eta 0:00:00 lr 0.000914	time 0.4566 (0.4685)	loss 4.0086 (3.7272)	grad_norm 1.1242 (nan)	mem 14851MB
[2022-11-06 05:09:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 56 training takes 0:09:46
[2022-11-06 05:09:12 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_56.pth saving......
[2022-11-06 05:09:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_56.pth saved !!!
[2022-11-06 05:09:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.698 (1.698)	Loss 1.1325 (1.1325)	Acc@1 73.730 (73.730)	Acc@5 92.578 (92.578)	Mem 14851MB
[2022-11-06 05:09:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.390 Acc@5 92.060
[2022-11-06 05:09:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.4%
[2022-11-06 05:09:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.675 (1.675)	Loss 1.0319 (1.0319)	Acc@1 76.074 (76.074)	Acc@5 92.871 (92.871)	Mem 14851MB
[2022-11-06 05:09:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.956 Acc@5 92.992
[2022-11-06 05:09:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.0%
[2022-11-06 05:09:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 75.96% at 56 epoch
[2022-11-06 05:09:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][0/1251]	eta 0:39:59 lr 0.000914	time 1.9178 (1.9178)	loss 2.6954 (2.6954)	grad_norm 1.2411 (1.2411)	mem 14851MB
[2022-11-06 05:09:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][50/1251]	eta 0:09:59 lr 0.000914	time 0.4649 (0.4988)	loss 4.1496 (3.5295)	grad_norm 1.1736 (1.2455)	mem 14851MB
[2022-11-06 05:10:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][100/1251]	eta 0:09:20 lr 0.000914	time 0.4669 (0.4870)	loss 4.1061 (3.6247)	grad_norm 1.2253 (1.2529)	mem 14851MB
[2022-11-06 05:10:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][150/1251]	eta 0:08:47 lr 0.000914	time 0.4701 (0.4795)	loss 3.6378 (3.6616)	grad_norm 1.1481 (1.2450)	mem 14851MB
[2022-11-06 05:11:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][200/1251]	eta 0:08:20 lr 0.000914	time 0.4644 (0.4762)	loss 3.0271 (3.6749)	grad_norm 1.1644 (1.2407)	mem 14851MB
[2022-11-06 05:11:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][250/1251]	eta 0:07:54 lr 0.000914	time 0.4646 (0.4743)	loss 4.1542 (3.6587)	grad_norm 1.4481 (1.2455)	mem 14851MB
[2022-11-06 05:11:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][300/1251]	eta 0:07:29 lr 0.000914	time 0.4723 (0.4729)	loss 2.9883 (3.6605)	grad_norm 1.1447 (1.2449)	mem 14851MB
[2022-11-06 05:12:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][350/1251]	eta 0:07:05 lr 0.000914	time 0.4711 (0.4721)	loss 4.4430 (3.6701)	grad_norm 1.2227 (1.2450)	mem 14851MB
[2022-11-06 05:12:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][400/1251]	eta 0:06:41 lr 0.000913	time 0.4735 (0.4714)	loss 4.1343 (3.6898)	grad_norm 1.2428 (1.2450)	mem 14851MB
[2022-11-06 05:13:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][450/1251]	eta 0:06:17 lr 0.000913	time 0.4601 (0.4708)	loss 4.0305 (3.6977)	grad_norm 1.4150 (1.2502)	mem 14851MB
[2022-11-06 05:13:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][500/1251]	eta 0:05:53 lr 0.000913	time 0.4507 (0.4702)	loss 4.3456 (3.6920)	grad_norm 1.2160 (1.2516)	mem 14851MB
[2022-11-06 05:13:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][550/1251]	eta 0:05:29 lr 0.000913	time 0.4682 (0.4704)	loss 4.1308 (3.6928)	grad_norm 1.2946 (1.2511)	mem 14851MB
[2022-11-06 05:14:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][600/1251]	eta 0:05:06 lr 0.000913	time 0.4638 (0.4701)	loss 2.6803 (3.6815)	grad_norm 1.0045 (1.2464)	mem 14851MB
[2022-11-06 05:14:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][650/1251]	eta 0:04:42 lr 0.000913	time 0.4699 (0.4698)	loss 2.7933 (3.6814)	grad_norm 1.3695 (1.2444)	mem 14851MB
[2022-11-06 05:14:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][700/1251]	eta 0:04:18 lr 0.000913	time 0.4629 (0.4695)	loss 4.8330 (3.6932)	grad_norm 1.2657 (1.2449)	mem 14851MB
[2022-11-06 05:15:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][750/1251]	eta 0:03:55 lr 0.000913	time 0.4685 (0.4693)	loss 3.1207 (3.6931)	grad_norm 1.2243 (1.2466)	mem 14851MB
[2022-11-06 05:15:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][800/1251]	eta 0:03:31 lr 0.000913	time 0.4668 (0.4695)	loss 3.6184 (3.6973)	grad_norm 1.1602 (1.2436)	mem 14851MB
[2022-11-06 05:16:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][850/1251]	eta 0:03:08 lr 0.000912	time 0.4614 (0.4695)	loss 3.3374 (3.6948)	grad_norm 1.1925 (1.2421)	mem 14851MB
[2022-11-06 05:16:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][900/1251]	eta 0:02:44 lr 0.000912	time 0.4684 (0.4693)	loss 3.8310 (3.6865)	grad_norm 1.1967 (1.2442)	mem 14851MB
[2022-11-06 05:16:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][950/1251]	eta 0:02:21 lr 0.000912	time 0.4700 (0.4691)	loss 4.3208 (3.6969)	grad_norm 1.1381 (1.2448)	mem 14851MB
[2022-11-06 05:17:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][1000/1251]	eta 0:01:57 lr 0.000912	time 0.4512 (0.4690)	loss 3.5816 (3.7033)	grad_norm 1.3978 (1.2448)	mem 14851MB
[2022-11-06 05:17:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][1050/1251]	eta 0:01:34 lr 0.000912	time 0.4711 (0.4690)	loss 3.6361 (3.6996)	grad_norm 1.1443 (1.2456)	mem 14851MB
[2022-11-06 05:18:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][1100/1251]	eta 0:01:10 lr 0.000912	time 0.4593 (0.4689)	loss 3.8387 (3.6991)	grad_norm 1.2675 (1.2443)	mem 14851MB
[2022-11-06 05:18:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][1150/1251]	eta 0:00:47 lr 0.000912	time 0.4604 (0.4688)	loss 4.3500 (3.6990)	grad_norm 1.1810 (1.2443)	mem 14851MB
[2022-11-06 05:18:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][1200/1251]	eta 0:00:23 lr 0.000912	time 0.4630 (0.4687)	loss 3.9012 (3.6975)	grad_norm 1.0946 (1.2458)	mem 14851MB
[2022-11-06 05:19:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [57/300][1250/1251]	eta 0:00:00 lr 0.000911	time 0.4582 (0.4685)	loss 3.5796 (3.6923)	grad_norm 1.3682 (1.2457)	mem 14851MB
[2022-11-06 05:19:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 57 training takes 0:09:46
[2022-11-06 05:19:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_57.pth saving......
[2022-11-06 05:19:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_57.pth saved !!!
[2022-11-06 05:19:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.611 (1.611)	Loss 1.1386 (1.1386)	Acc@1 72.852 (72.852)	Acc@5 92.480 (92.480)	Mem 14851MB
[2022-11-06 05:19:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.836 Acc@5 92.254
[2022-11-06 05:19:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.8%
[2022-11-06 05:19:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.756 (1.756)	Loss 1.0025 (1.0025)	Acc@1 76.270 (76.270)	Acc@5 92.578 (92.578)	Mem 14851MB
[2022-11-06 05:19:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.044 Acc@5 93.052
[2022-11-06 05:19:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.0%
[2022-11-06 05:19:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.04% at 57 epoch
[2022-11-06 05:19:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][0/1251]	eta 0:43:18 lr 0.000911	time 2.0769 (2.0769)	loss 3.9537 (3.9537)	grad_norm 1.0955 (1.0955)	mem 14851MB
[2022-11-06 05:20:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][50/1251]	eta 0:10:07 lr 0.000911	time 0.4746 (0.5056)	loss 3.9828 (3.5225)	grad_norm 1.1926 (1.2357)	mem 14851MB
[2022-11-06 05:20:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][100/1251]	eta 0:09:20 lr 0.000911	time 0.4635 (0.4869)	loss 4.4406 (3.6291)	grad_norm 1.3417 (1.2377)	mem 14851MB
[2022-11-06 05:20:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][150/1251]	eta 0:08:48 lr 0.000911	time 0.4706 (0.4798)	loss 3.6985 (3.6451)	grad_norm 1.1567 (1.2336)	mem 14851MB
[2022-11-06 05:21:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][200/1251]	eta 0:08:20 lr 0.000911	time 0.4624 (0.4764)	loss 3.2175 (3.6660)	grad_norm 1.1153 (1.2358)	mem 14851MB
[2022-11-06 05:21:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][250/1251]	eta 0:07:54 lr 0.000911	time 0.4610 (0.4745)	loss 4.8733 (3.6580)	grad_norm 1.2801 (1.2450)	mem 14851MB
[2022-11-06 05:21:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][300/1251]	eta 0:07:29 lr 0.000911	time 0.4602 (0.4730)	loss 4.0315 (3.6734)	grad_norm 1.2287 (1.2472)	mem 14851MB
[2022-11-06 05:22:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][350/1251]	eta 0:07:05 lr 0.000911	time 0.4779 (0.4718)	loss 4.5229 (3.6917)	grad_norm 1.3617 (1.2448)	mem 14851MB
[2022-11-06 05:22:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][400/1251]	eta 0:06:40 lr 0.000911	time 0.4519 (0.4712)	loss 3.6158 (3.6959)	grad_norm 1.1186 (1.2478)	mem 14851MB
[2022-11-06 05:23:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][450/1251]	eta 0:06:16 lr 0.000910	time 0.4720 (0.4705)	loss 3.3621 (3.7156)	grad_norm 1.2677 (1.2499)	mem 14851MB
[2022-11-06 05:23:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][500/1251]	eta 0:05:52 lr 0.000910	time 0.4710 (0.4700)	loss 2.6274 (3.7058)	grad_norm 1.2156 (1.2502)	mem 14851MB
[2022-11-06 05:23:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][550/1251]	eta 0:05:29 lr 0.000910	time 0.4623 (0.4701)	loss 3.5257 (3.7190)	grad_norm 1.2797 (1.2515)	mem 14851MB
[2022-11-06 05:24:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][600/1251]	eta 0:05:05 lr 0.000910	time 0.4575 (0.4697)	loss 4.0497 (3.7267)	grad_norm 1.2298 (1.2503)	mem 14851MB
[2022-11-06 05:24:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][650/1251]	eta 0:04:42 lr 0.000910	time 0.4618 (0.4695)	loss 2.5901 (3.7287)	grad_norm 1.2644 (1.2503)	mem 14851MB
[2022-11-06 05:25:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][700/1251]	eta 0:04:18 lr 0.000910	time 0.4528 (0.4695)	loss 4.0079 (3.7266)	grad_norm 1.2221 (1.2499)	mem 14851MB
[2022-11-06 05:25:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][750/1251]	eta 0:03:55 lr 0.000910	time 0.4642 (0.4693)	loss 3.8716 (3.7290)	grad_norm 1.1944 (1.2485)	mem 14851MB
[2022-11-06 05:25:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][800/1251]	eta 0:03:31 lr 0.000910	time 0.4576 (0.4692)	loss 2.6698 (3.7242)	grad_norm 1.0852 (1.2485)	mem 14851MB
[2022-11-06 05:26:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][850/1251]	eta 0:03:08 lr 0.000909	time 0.4583 (0.4689)	loss 4.0941 (3.7227)	grad_norm 1.1360 (1.2478)	mem 14851MB
[2022-11-06 05:26:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][900/1251]	eta 0:02:44 lr 0.000909	time 0.4630 (0.4688)	loss 3.7044 (3.7231)	grad_norm 1.1841 (1.2469)	mem 14851MB
[2022-11-06 05:27:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][950/1251]	eta 0:02:21 lr 0.000909	time 0.4599 (0.4687)	loss 3.8017 (3.7221)	grad_norm 1.1363 (1.2454)	mem 14851MB
[2022-11-06 05:27:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][1000/1251]	eta 0:01:57 lr 0.000909	time 0.4590 (0.4686)	loss 3.4295 (3.7186)	grad_norm 1.1165 (1.2448)	mem 14851MB
[2022-11-06 05:27:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][1050/1251]	eta 0:01:34 lr 0.000909	time 0.4642 (0.4688)	loss 4.2061 (3.7198)	grad_norm 1.2885 (1.2446)	mem 14851MB
[2022-11-06 05:28:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][1100/1251]	eta 0:01:10 lr 0.000909	time 0.4631 (0.4686)	loss 3.9683 (3.7171)	grad_norm 1.1938 (1.2447)	mem 14851MB
[2022-11-06 05:28:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][1150/1251]	eta 0:00:47 lr 0.000909	time 0.4647 (0.4686)	loss 4.0286 (3.7128)	grad_norm 1.2169 (1.2458)	mem 14851MB
[2022-11-06 05:28:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][1200/1251]	eta 0:00:23 lr 0.000909	time 0.4669 (0.4685)	loss 3.9842 (3.7120)	grad_norm 1.1694 (nan)	mem 14851MB
[2022-11-06 05:29:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [58/300][1250/1251]	eta 0:00:00 lr 0.000908	time 0.4587 (0.4683)	loss 2.7339 (3.7104)	grad_norm 1.1781 (nan)	mem 14851MB
[2022-11-06 05:29:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 58 training takes 0:09:46
[2022-11-06 05:29:21 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_58.pth saving......
[2022-11-06 05:29:22 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_58.pth saved !!!
[2022-11-06 05:29:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.562 (1.562)	Loss 1.1629 (1.1629)	Acc@1 72.559 (72.559)	Acc@5 91.406 (91.406)	Mem 14851MB
[2022-11-06 05:29:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.942 Acc@5 92.086
[2022-11-06 05:29:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.9%
[2022-11-06 05:29:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.541 (1.541)	Loss 1.0116 (1.0116)	Acc@1 76.562 (76.562)	Acc@5 92.871 (92.871)	Mem 14851MB
[2022-11-06 05:29:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.170 Acc@5 93.080
[2022-11-06 05:29:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.2%
[2022-11-06 05:29:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.17% at 58 epoch
[2022-11-06 05:29:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][0/1251]	eta 0:40:57 lr 0.000908	time 1.9644 (1.9644)	loss 4.1301 (4.1301)	grad_norm 1.1682 (1.1682)	mem 14851MB
[2022-11-06 05:30:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][50/1251]	eta 0:10:03 lr 0.000908	time 0.4733 (0.5026)	loss 3.7132 (3.7058)	grad_norm 1.4106 (1.2260)	mem 14851MB
[2022-11-06 05:30:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][100/1251]	eta 0:09:18 lr 0.000908	time 0.4630 (0.4854)	loss 3.0670 (3.6419)	grad_norm 1.0641 (1.2447)	mem 14851MB
[2022-11-06 05:30:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][150/1251]	eta 0:08:48 lr 0.000908	time 0.4653 (0.4798)	loss 4.1162 (3.6278)	grad_norm 1.2132 (1.2551)	mem 14851MB
[2022-11-06 05:31:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][200/1251]	eta 0:08:20 lr 0.000908	time 0.4564 (0.4760)	loss 4.1315 (3.6590)	grad_norm 1.2537 (1.2511)	mem 14851MB
[2022-11-06 05:31:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][250/1251]	eta 0:07:54 lr 0.000908	time 0.4543 (0.4741)	loss 4.2821 (3.6691)	grad_norm 1.2897 (1.2570)	mem 14851MB
[2022-11-06 05:32:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][300/1251]	eta 0:07:29 lr 0.000908	time 0.4637 (0.4726)	loss 3.8322 (3.6747)	grad_norm 1.3672 (1.2524)	mem 14851MB
[2022-11-06 05:32:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][350/1251]	eta 0:07:04 lr 0.000908	time 0.4645 (0.4715)	loss 3.4131 (3.6867)	grad_norm 1.1384 (1.2503)	mem 14851MB
[2022-11-06 05:32:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][400/1251]	eta 0:06:40 lr 0.000908	time 0.4649 (0.4706)	loss 3.1778 (3.6800)	grad_norm 1.2026 (1.2480)	mem 14851MB
[2022-11-06 05:33:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][450/1251]	eta 0:06:16 lr 0.000907	time 0.4663 (0.4702)	loss 3.9739 (3.6652)	grad_norm 1.1899 (1.2509)	mem 14851MB
[2022-11-06 05:33:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][500/1251]	eta 0:05:52 lr 0.000907	time 0.4739 (0.4697)	loss 3.5266 (3.6767)	grad_norm 1.1389 (1.2477)	mem 14851MB
[2022-11-06 05:33:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][550/1251]	eta 0:05:29 lr 0.000907	time 0.4713 (0.4699)	loss 4.1216 (3.6929)	grad_norm 1.3517 (1.2477)	mem 14851MB
[2022-11-06 05:34:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][600/1251]	eta 0:05:05 lr 0.000907	time 0.4611 (0.4696)	loss 4.3660 (3.6923)	grad_norm 1.0699 (1.2484)	mem 14851MB
[2022-11-06 05:34:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][650/1251]	eta 0:04:42 lr 0.000907	time 0.4719 (0.4694)	loss 3.0712 (3.6875)	grad_norm 1.1942 (1.2492)	mem 14851MB
[2022-11-06 05:35:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][700/1251]	eta 0:04:18 lr 0.000907	time 0.4596 (0.4691)	loss 3.7203 (3.6892)	grad_norm 1.5948 (1.2482)	mem 14851MB
[2022-11-06 05:35:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][750/1251]	eta 0:03:54 lr 0.000907	time 0.4786 (0.4689)	loss 3.7219 (3.6854)	grad_norm 1.1503 (1.2456)	mem 14851MB
[2022-11-06 05:35:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][800/1251]	eta 0:03:31 lr 0.000907	time 0.4652 (0.4690)	loss 2.9760 (3.6910)	grad_norm 1.1666 (1.2457)	mem 14851MB
[2022-11-06 05:36:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][850/1251]	eta 0:03:08 lr 0.000906	time 0.4666 (0.4690)	loss 3.9664 (3.6933)	grad_norm 1.2755 (1.2476)	mem 14851MB
[2022-11-06 05:36:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][900/1251]	eta 0:02:44 lr 0.000906	time 0.4885 (0.4689)	loss 3.8215 (3.6983)	grad_norm 1.2953 (1.2482)	mem 14851MB
[2022-11-06 05:37:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][950/1251]	eta 0:02:21 lr 0.000906	time 0.4622 (0.4687)	loss 3.4706 (3.7059)	grad_norm 1.1241 (1.2477)	mem 14851MB
[2022-11-06 05:37:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][1000/1251]	eta 0:01:57 lr 0.000906	time 0.4555 (0.4686)	loss 2.8374 (3.7066)	grad_norm 1.0313 (1.2459)	mem 14851MB
[2022-11-06 05:37:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][1050/1251]	eta 0:01:34 lr 0.000906	time 0.4689 (0.4688)	loss 3.2216 (3.7070)	grad_norm 1.3070 (1.2463)	mem 14851MB
[2022-11-06 05:38:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][1100/1251]	eta 0:01:10 lr 0.000906	time 0.4574 (0.4687)	loss 4.0351 (3.7092)	grad_norm 1.2743 (1.2474)	mem 14851MB
[2022-11-06 05:38:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][1150/1251]	eta 0:00:47 lr 0.000906	time 0.4601 (0.4687)	loss 3.9518 (3.7085)	grad_norm 1.2266 (1.2473)	mem 14851MB
[2022-11-06 05:39:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][1200/1251]	eta 0:00:23 lr 0.000906	time 0.4688 (0.4686)	loss 3.9747 (3.7049)	grad_norm 1.2950 (1.2472)	mem 14851MB
[2022-11-06 05:39:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [59/300][1250/1251]	eta 0:00:00 lr 0.000905	time 0.4569 (0.4684)	loss 4.2403 (3.7059)	grad_norm 1.2585 (1.2490)	mem 14851MB
[2022-11-06 05:39:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 59 training takes 0:09:46
[2022-11-06 05:39:25 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_59.pth saving......
[2022-11-06 05:39:26 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_59.pth saved !!!
[2022-11-06 05:39:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.568 (1.568)	Loss 1.1317 (1.1317)	Acc@1 73.242 (73.242)	Acc@5 93.262 (93.262)	Mem 14851MB
[2022-11-06 05:39:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.826 Acc@5 92.300
[2022-11-06 05:39:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.8%
[2022-11-06 05:39:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.658 (1.658)	Loss 0.9818 (0.9818)	Acc@1 75.293 (75.293)	Acc@5 93.945 (93.945)	Mem 14851MB
[2022-11-06 05:39:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.286 Acc@5 93.140
[2022-11-06 05:39:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.3%
[2022-11-06 05:39:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.29% at 59 epoch
[2022-11-06 05:39:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][0/1251]	eta 0:40:21 lr 0.000905	time 1.9354 (1.9354)	loss 3.5298 (3.5298)	grad_norm 1.3803 (1.3803)	mem 14851MB
[2022-11-06 05:40:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][50/1251]	eta 0:10:07 lr 0.000905	time 0.4617 (0.5054)	loss 3.8919 (3.6904)	grad_norm 1.3373 (1.2533)	mem 14851MB
[2022-11-06 05:40:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][100/1251]	eta 0:09:20 lr 0.000905	time 0.4712 (0.4873)	loss 4.3036 (3.6716)	grad_norm 1.3367 (1.2540)	mem 14851MB
[2022-11-06 05:40:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][150/1251]	eta 0:08:49 lr 0.000905	time 0.4649 (0.4808)	loss 3.4056 (3.6903)	grad_norm 1.1874 (1.2504)	mem 14851MB
[2022-11-06 05:41:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][200/1251]	eta 0:08:21 lr 0.000905	time 0.4649 (0.4771)	loss 3.8340 (3.7006)	grad_norm 1.5300 (1.2444)	mem 14851MB
[2022-11-06 05:41:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][250/1251]	eta 0:07:54 lr 0.000905	time 0.4706 (0.4745)	loss 4.1246 (3.6975)	grad_norm 1.2902 (1.2438)	mem 14851MB
[2022-11-06 05:42:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][300/1251]	eta 0:07:29 lr 0.000905	time 0.4684 (0.4730)	loss 4.4923 (3.6956)	grad_norm 1.1689 (1.2393)	mem 14851MB
[2022-11-06 05:42:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][350/1251]	eta 0:07:05 lr 0.000905	time 0.4669 (0.4724)	loss 2.5922 (3.6891)	grad_norm 1.3429 (1.2396)	mem 14851MB
[2022-11-06 05:42:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][400/1251]	eta 0:06:41 lr 0.000904	time 0.4621 (0.4715)	loss 4.0782 (3.7023)	grad_norm 1.3545 (1.2465)	mem 14851MB
[2022-11-06 05:43:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][450/1251]	eta 0:06:17 lr 0.000904	time 0.4640 (0.4707)	loss 4.0917 (3.7000)	grad_norm 1.1244 (1.2441)	mem 14851MB
[2022-11-06 05:43:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][500/1251]	eta 0:05:53 lr 0.000904	time 0.4632 (0.4702)	loss 3.7628 (3.7006)	grad_norm 1.2563 (1.2450)	mem 14851MB
[2022-11-06 05:44:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][550/1251]	eta 0:05:29 lr 0.000904	time 0.4640 (0.4704)	loss 3.9941 (3.7123)	grad_norm 1.1802 (1.2455)	mem 14851MB
[2022-11-06 05:44:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][600/1251]	eta 0:05:06 lr 0.000904	time 0.4624 (0.4702)	loss 3.7959 (3.7178)	grad_norm 1.2374 (1.2452)	mem 14851MB
[2022-11-06 05:44:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][650/1251]	eta 0:04:42 lr 0.000904	time 0.4538 (0.4698)	loss 3.5226 (3.7240)	grad_norm 1.1678 (1.2477)	mem 14851MB
[2022-11-06 05:45:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][700/1251]	eta 0:04:18 lr 0.000904	time 0.4613 (0.4694)	loss 3.7696 (3.7261)	grad_norm 1.1138 (1.2476)	mem 14851MB
[2022-11-06 05:45:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][750/1251]	eta 0:03:55 lr 0.000904	time 0.4597 (0.4691)	loss 4.5989 (3.7249)	grad_norm 1.3581 (1.2468)	mem 14851MB
[2022-11-06 05:45:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][800/1251]	eta 0:03:31 lr 0.000904	time 0.4724 (0.4693)	loss 2.6696 (3.7200)	grad_norm 1.2885 (1.2480)	mem 14851MB
[2022-11-06 05:46:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][850/1251]	eta 0:03:08 lr 0.000903	time 0.4613 (0.4692)	loss 2.9691 (3.7173)	grad_norm 1.1176 (1.2484)	mem 14851MB
[2022-11-06 05:46:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][900/1251]	eta 0:02:44 lr 0.000903	time 0.4661 (0.4691)	loss 2.6128 (3.7120)	grad_norm 1.3557 (1.2484)	mem 14851MB
[2022-11-06 05:47:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][950/1251]	eta 0:02:21 lr 0.000903	time 0.4632 (0.4689)	loss 3.8354 (3.7067)	grad_norm 1.3586 (1.2486)	mem 14851MB
[2022-11-06 05:47:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][1000/1251]	eta 0:01:57 lr 0.000903	time 0.4653 (0.4688)	loss 2.9379 (3.7034)	grad_norm 1.2036 (1.2492)	mem 14851MB
[2022-11-06 05:47:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][1050/1251]	eta 0:01:34 lr 0.000903	time 0.4605 (0.4689)	loss 4.2418 (3.7075)	grad_norm 1.2985 (1.2494)	mem 14851MB
[2022-11-06 05:48:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][1100/1251]	eta 0:01:10 lr 0.000903	time 0.4652 (0.4688)	loss 2.9267 (3.7098)	grad_norm 1.5231 (1.2490)	mem 14851MB
[2022-11-06 05:48:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][1150/1251]	eta 0:00:47 lr 0.000903	time 0.4617 (0.4686)	loss 4.1504 (3.7072)	grad_norm 1.2983 (1.2484)	mem 14851MB
[2022-11-06 05:49:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][1200/1251]	eta 0:00:23 lr 0.000903	time 0.4691 (0.4686)	loss 4.3826 (3.7102)	grad_norm 1.1644 (1.2488)	mem 14851MB
[2022-11-06 05:49:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [60/300][1250/1251]	eta 0:00:00 lr 0.000902	time 0.4625 (0.4684)	loss 3.9508 (3.7090)	grad_norm 1.1314 (1.2480)	mem 14851MB
[2022-11-06 05:49:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 60 training takes 0:09:46
[2022-11-06 05:49:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_60.pth saving......
[2022-11-06 05:49:30 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_60.pth saved !!!
[2022-11-06 05:49:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.622 (1.622)	Loss 1.0595 (1.0595)	Acc@1 74.902 (74.902)	Acc@5 93.262 (93.262)	Mem 14851MB
[2022-11-06 05:49:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.788 Acc@5 92.234
[2022-11-06 05:49:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.8%
[2022-11-06 05:49:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.593 (1.593)	Loss 1.0450 (1.0450)	Acc@1 76.074 (76.074)	Acc@5 91.797 (91.797)	Mem 14851MB
[2022-11-06 05:49:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.374 Acc@5 93.204
[2022-11-06 05:49:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.4%
[2022-11-06 05:49:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.37% at 60 epoch
[2022-11-06 05:49:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][0/1251]	eta 0:41:01 lr 0.000902	time 1.9676 (1.9676)	loss 4.3459 (4.3459)	grad_norm 1.2431 (1.2431)	mem 14851MB
[2022-11-06 05:50:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][50/1251]	eta 0:10:05 lr 0.000902	time 0.4595 (0.5038)	loss 3.3789 (3.6187)	grad_norm 1.2312 (1.2511)	mem 14851MB
[2022-11-06 05:50:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][100/1251]	eta 0:09:19 lr 0.000902	time 0.4621 (0.4864)	loss 4.4040 (3.6488)	grad_norm 1.2994 (1.2541)	mem 14851MB
[2022-11-06 05:51:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][150/1251]	eta 0:08:48 lr 0.000902	time 0.4644 (0.4796)	loss 4.3632 (3.6605)	grad_norm 1.1932 (1.2490)	mem 14851MB
[2022-11-06 05:51:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][200/1251]	eta 0:08:20 lr 0.000902	time 0.4618 (0.4762)	loss 3.6742 (3.6551)	grad_norm 1.2579 (1.2516)	mem 14851MB
[2022-11-06 05:51:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][250/1251]	eta 0:07:54 lr 0.000902	time 0.4614 (0.4743)	loss 3.9965 (3.6670)	grad_norm 1.3127 (1.2518)	mem 14851MB
[2022-11-06 05:52:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][300/1251]	eta 0:07:29 lr 0.000902	time 0.4664 (0.4728)	loss 4.3109 (3.6730)	grad_norm 1.2306 (1.2581)	mem 14851MB
[2022-11-06 05:52:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][350/1251]	eta 0:07:04 lr 0.000902	time 0.4633 (0.4716)	loss 3.5066 (3.6743)	grad_norm 1.4180 (1.2617)	mem 14851MB
[2022-11-06 05:52:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][400/1251]	eta 0:06:40 lr 0.000901	time 0.4603 (0.4707)	loss 4.1894 (3.6834)	grad_norm 1.2393 (1.2589)	mem 14851MB
[2022-11-06 05:53:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][450/1251]	eta 0:06:16 lr 0.000901	time 0.4677 (0.4702)	loss 3.7397 (3.6785)	grad_norm 1.4132 (1.2582)	mem 14851MB
[2022-11-06 05:53:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][500/1251]	eta 0:05:52 lr 0.000901	time 0.4597 (0.4699)	loss 2.6499 (3.6813)	grad_norm 1.1978 (1.2568)	mem 14851MB
[2022-11-06 05:54:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][550/1251]	eta 0:05:29 lr 0.000901	time 0.4603 (0.4699)	loss 4.6549 (3.6921)	grad_norm 1.4454 (1.2572)	mem 14851MB
[2022-11-06 05:54:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][600/1251]	eta 0:05:05 lr 0.000901	time 0.4687 (0.4695)	loss 3.2835 (3.6965)	grad_norm 1.1620 (1.2562)	mem 14851MB
[2022-11-06 05:54:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][650/1251]	eta 0:04:41 lr 0.000901	time 0.4633 (0.4691)	loss 2.9631 (3.6968)	grad_norm 1.3320 (1.2591)	mem 14851MB
[2022-11-06 05:55:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][700/1251]	eta 0:04:18 lr 0.000901	time 0.4658 (0.4690)	loss 2.7098 (3.6932)	grad_norm 1.1738 (1.2600)	mem 14851MB
[2022-11-06 05:55:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][750/1251]	eta 0:03:54 lr 0.000901	time 0.4753 (0.4687)	loss 4.3845 (3.6908)	grad_norm 1.2872 (1.2579)	mem 14851MB
[2022-11-06 05:56:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][800/1251]	eta 0:03:31 lr 0.000900	time 0.5412 (0.4689)	loss 3.9745 (3.6995)	grad_norm 1.2518 (1.2574)	mem 14851MB
[2022-11-06 05:56:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][850/1251]	eta 0:03:07 lr 0.000900	time 0.4582 (0.4687)	loss 2.9542 (3.6999)	grad_norm 1.1533 (1.2556)	mem 14851MB
[2022-11-06 05:56:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][900/1251]	eta 0:02:44 lr 0.000900	time 0.4648 (0.4684)	loss 4.2853 (3.7090)	grad_norm 1.1963 (1.2551)	mem 14851MB
[2022-11-06 05:57:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][950/1251]	eta 0:02:20 lr 0.000900	time 0.4634 (0.4682)	loss 3.0363 (3.7104)	grad_norm 1.2829 (1.2555)	mem 14851MB
[2022-11-06 05:57:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][1000/1251]	eta 0:01:57 lr 0.000900	time 0.4654 (0.4682)	loss 3.8557 (3.7091)	grad_norm 1.1235 (1.2566)	mem 14851MB
[2022-11-06 05:57:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][1050/1251]	eta 0:01:34 lr 0.000900	time 0.4588 (0.4683)	loss 2.4904 (3.7119)	grad_norm 1.2434 (1.2578)	mem 14851MB
[2022-11-06 05:58:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][1100/1251]	eta 0:01:10 lr 0.000900	time 0.4706 (0.4682)	loss 3.7632 (3.7156)	grad_norm 1.2002 (1.2581)	mem 14851MB
[2022-11-06 05:58:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][1150/1251]	eta 0:00:47 lr 0.000900	time 0.4645 (0.4680)	loss 3.8334 (3.7115)	grad_norm 1.1672 (1.2600)	mem 14851MB
[2022-11-06 05:59:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][1200/1251]	eta 0:00:23 lr 0.000899	time 0.4644 (0.4679)	loss 2.8004 (3.7118)	grad_norm 1.1295 (1.2588)	mem 14851MB
[2022-11-06 05:59:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [61/300][1250/1251]	eta 0:00:00 lr 0.000899	time 0.4572 (0.4677)	loss 3.5779 (3.7068)	grad_norm 1.2004 (1.2578)	mem 14851MB
[2022-11-06 05:59:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 61 training takes 0:09:45
[2022-11-06 05:59:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_61.pth saving......
[2022-11-06 05:59:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_61.pth saved !!!
[2022-11-06 05:59:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.500 (1.500)	Loss 1.1406 (1.1406)	Acc@1 72.266 (72.266)	Acc@5 91.504 (91.504)	Mem 14851MB
[2022-11-06 05:59:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.918 Acc@5 92.190
[2022-11-06 05:59:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.9%
[2022-11-06 05:59:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.650 (1.650)	Loss 0.9807 (0.9807)	Acc@1 76.367 (76.367)	Acc@5 93.359 (93.359)	Mem 14851MB
[2022-11-06 05:59:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.468 Acc@5 93.298
[2022-11-06 05:59:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.5%
[2022-11-06 05:59:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.47% at 61 epoch
[2022-11-06 05:59:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][0/1251]	eta 0:40:52 lr 0.000899	time 1.9603 (1.9603)	loss 4.2297 (4.2297)	grad_norm 1.2056 (1.2056)	mem 14851MB
[2022-11-06 06:00:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][50/1251]	eta 0:10:02 lr 0.000899	time 0.4782 (0.5015)	loss 3.1922 (3.6617)	grad_norm 1.3597 (1.2558)	mem 14851MB
[2022-11-06 06:00:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][100/1251]	eta 0:09:18 lr 0.000899	time 0.4592 (0.4852)	loss 4.2109 (3.6238)	grad_norm 1.4489 (1.2504)	mem 14851MB
[2022-11-06 06:01:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][150/1251]	eta 0:08:46 lr 0.000899	time 0.4747 (0.4786)	loss 4.4491 (3.6067)	grad_norm 1.1900 (1.2600)	mem 14851MB
[2022-11-06 06:01:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][200/1251]	eta 0:08:20 lr 0.000899	time 0.4734 (0.4761)	loss 3.9793 (3.6045)	grad_norm 1.1478 (1.2567)	mem 14851MB
[2022-11-06 06:01:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][250/1251]	eta 0:07:54 lr 0.000899	time 0.4595 (0.4740)	loss 3.3046 (3.6229)	grad_norm 1.2998 (nan)	mem 14851MB
[2022-11-06 06:02:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][300/1251]	eta 0:07:29 lr 0.000899	time 0.4677 (0.4728)	loss 3.8330 (3.6251)	grad_norm 1.5097 (nan)	mem 14851MB
[2022-11-06 06:02:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][350/1251]	eta 0:07:04 lr 0.000898	time 0.4629 (0.4715)	loss 4.2715 (3.6308)	grad_norm 1.2996 (nan)	mem 14851MB
[2022-11-06 06:02:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][400/1251]	eta 0:06:40 lr 0.000898	time 0.4629 (0.4707)	loss 4.1660 (3.6476)	grad_norm 1.2457 (nan)	mem 14851MB
[2022-11-06 06:03:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][450/1251]	eta 0:06:16 lr 0.000898	time 0.4567 (0.4702)	loss 2.8962 (3.6359)	grad_norm 1.2705 (nan)	mem 14851MB
[2022-11-06 06:03:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][500/1251]	eta 0:05:52 lr 0.000898	time 0.4635 (0.4698)	loss 3.4379 (3.6470)	grad_norm 1.3253 (nan)	mem 14851MB
[2022-11-06 06:04:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][550/1251]	eta 0:05:29 lr 0.000898	time 0.4630 (0.4698)	loss 4.0222 (3.6527)	grad_norm 1.4003 (nan)	mem 14851MB
[2022-11-06 06:04:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][600/1251]	eta 0:05:05 lr 0.000898	time 0.4632 (0.4694)	loss 3.0035 (3.6604)	grad_norm 1.2817 (nan)	mem 14851MB
[2022-11-06 06:04:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][650/1251]	eta 0:04:41 lr 0.000898	time 0.4695 (0.4691)	loss 4.0269 (3.6603)	grad_norm 1.3933 (nan)	mem 14851MB
[2022-11-06 06:05:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][700/1251]	eta 0:04:18 lr 0.000898	time 0.4720 (0.4689)	loss 4.0715 (3.6616)	grad_norm 1.3363 (nan)	mem 14851MB
[2022-11-06 06:05:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][750/1251]	eta 0:03:54 lr 0.000897	time 0.4611 (0.4688)	loss 4.4000 (3.6613)	grad_norm 1.2528 (nan)	mem 14851MB
[2022-11-06 06:06:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][800/1251]	eta 0:03:31 lr 0.000897	time 0.4573 (0.4690)	loss 2.7098 (3.6616)	grad_norm 1.5722 (nan)	mem 14851MB
[2022-11-06 06:06:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][850/1251]	eta 0:03:07 lr 0.000897	time 0.4677 (0.4687)	loss 3.8443 (3.6627)	grad_norm 1.2197 (nan)	mem 14851MB
[2022-11-06 06:06:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][900/1251]	eta 0:02:44 lr 0.000897	time 0.5297 (0.4686)	loss 3.2813 (3.6558)	grad_norm 1.4422 (nan)	mem 14851MB
[2022-11-06 06:07:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][950/1251]	eta 0:02:20 lr 0.000897	time 0.4656 (0.4684)	loss 4.1490 (3.6569)	grad_norm 1.2241 (nan)	mem 14851MB
[2022-11-06 06:07:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][1000/1251]	eta 0:01:57 lr 0.000897	time 0.4604 (0.4683)	loss 3.3708 (3.6569)	grad_norm 1.1072 (nan)	mem 14851MB
[2022-11-06 06:08:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][1050/1251]	eta 0:01:34 lr 0.000897	time 0.4601 (0.4685)	loss 4.2645 (3.6624)	grad_norm 1.3559 (nan)	mem 14851MB
[2022-11-06 06:08:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][1100/1251]	eta 0:01:10 lr 0.000897	time 0.4718 (0.4683)	loss 4.0171 (3.6652)	grad_norm 1.2969 (nan)	mem 14851MB
[2022-11-06 06:08:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][1150/1251]	eta 0:00:47 lr 0.000896	time 0.4622 (0.4683)	loss 4.2633 (3.6671)	grad_norm 1.2974 (nan)	mem 14851MB
[2022-11-06 06:09:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][1200/1251]	eta 0:00:23 lr 0.000896	time 0.4614 (0.4681)	loss 4.2406 (3.6703)	grad_norm 1.1435 (nan)	mem 14851MB
[2022-11-06 06:09:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [62/300][1250/1251]	eta 0:00:00 lr 0.000896	time 0.4573 (0.4680)	loss 4.1201 (3.6689)	grad_norm 1.2330 (nan)	mem 14851MB
[2022-11-06 06:09:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 62 training takes 0:09:45
[2022-11-06 06:09:36 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_62.pth saving......
[2022-11-06 06:09:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_62.pth saved !!!
[2022-11-06 06:09:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.608 (1.608)	Loss 1.0845 (1.0845)	Acc@1 74.609 (74.609)	Acc@5 93.359 (93.359)	Mem 14851MB
[2022-11-06 06:09:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.878 Acc@5 92.170
[2022-11-06 06:09:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.9%
[2022-11-06 06:09:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.574 (1.574)	Loss 1.0047 (1.0047)	Acc@1 75.781 (75.781)	Acc@5 94.043 (94.043)	Mem 14851MB
[2022-11-06 06:09:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.578 Acc@5 93.374
[2022-11-06 06:09:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.6%
[2022-11-06 06:09:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.58% at 62 epoch
[2022-11-06 06:09:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][0/1251]	eta 0:40:57 lr 0.000896	time 1.9643 (1.9643)	loss 3.9134 (3.9134)	grad_norm 1.1742 (1.1742)	mem 14851MB
[2022-11-06 06:10:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][50/1251]	eta 0:10:01 lr 0.000896	time 0.4685 (0.5010)	loss 3.5216 (3.6378)	grad_norm 1.3440 (1.2644)	mem 14851MB
[2022-11-06 06:10:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][100/1251]	eta 0:09:19 lr 0.000896	time 0.4677 (0.4858)	loss 4.2245 (3.6633)	grad_norm 1.3287 (1.2546)	mem 14851MB
[2022-11-06 06:11:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][150/1251]	eta 0:08:48 lr 0.000896	time 0.4639 (0.4802)	loss 3.6046 (3.6690)	grad_norm 1.2228 (1.2510)	mem 14851MB
[2022-11-06 06:11:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][200/1251]	eta 0:08:21 lr 0.000896	time 0.4647 (0.4768)	loss 3.4082 (3.6461)	grad_norm 1.2402 (1.2492)	mem 14851MB
[2022-11-06 06:11:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][250/1251]	eta 0:07:54 lr 0.000895	time 0.4714 (0.4744)	loss 2.9343 (3.6522)	grad_norm 1.2684 (1.2478)	mem 14851MB
[2022-11-06 06:12:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][300/1251]	eta 0:07:29 lr 0.000895	time 0.4549 (0.4731)	loss 2.9408 (3.6349)	grad_norm 1.2651 (1.2489)	mem 14851MB
[2022-11-06 06:12:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][350/1251]	eta 0:07:05 lr 0.000895	time 0.4643 (0.4721)	loss 4.0780 (3.6456)	grad_norm 1.1764 (1.2469)	mem 14851MB
[2022-11-06 06:13:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][400/1251]	eta 0:06:41 lr 0.000895	time 0.4613 (0.4712)	loss 3.4645 (3.6574)	grad_norm 1.2080 (1.2506)	mem 14851MB
[2022-11-06 06:13:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][450/1251]	eta 0:06:16 lr 0.000895	time 0.4619 (0.4706)	loss 3.9881 (3.6806)	grad_norm 1.1826 (1.2491)	mem 14851MB
[2022-11-06 06:13:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][500/1251]	eta 0:05:53 lr 0.000895	time 0.4667 (0.4702)	loss 3.7969 (3.6777)	grad_norm 1.2667 (1.2467)	mem 14851MB
[2022-11-06 06:14:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][550/1251]	eta 0:05:29 lr 0.000895	time 0.4531 (0.4699)	loss 4.7599 (3.6890)	grad_norm 1.3647 (1.2488)	mem 14851MB
[2022-11-06 06:14:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][600/1251]	eta 0:05:05 lr 0.000895	time 0.4583 (0.4699)	loss 2.8889 (3.6798)	grad_norm 1.3893 (1.2517)	mem 14851MB
[2022-11-06 06:15:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][650/1251]	eta 0:04:42 lr 0.000894	time 0.4681 (0.4698)	loss 4.0954 (3.6922)	grad_norm 1.2646 (1.2525)	mem 14851MB
[2022-11-06 06:15:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][700/1251]	eta 0:04:18 lr 0.000894	time 0.4503 (0.4694)	loss 3.8061 (3.6911)	grad_norm 1.2412 (1.2528)	mem 14851MB
[2022-11-06 06:15:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][750/1251]	eta 0:03:55 lr 0.000894	time 0.4678 (0.4691)	loss 3.4769 (3.6816)	grad_norm 1.2744 (1.2536)	mem 14851MB
[2022-11-06 06:16:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][800/1251]	eta 0:03:31 lr 0.000894	time 0.4710 (0.4690)	loss 4.2778 (3.6772)	grad_norm 1.2119 (1.2545)	mem 14851MB
[2022-11-06 06:16:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][850/1251]	eta 0:03:08 lr 0.000894	time 0.4649 (0.4691)	loss 3.4360 (3.6746)	grad_norm 1.2371 (1.2548)	mem 14851MB
[2022-11-06 06:16:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][900/1251]	eta 0:02:44 lr 0.000894	time 0.4614 (0.4689)	loss 3.5886 (3.6718)	grad_norm 1.3332 (1.2547)	mem 14851MB
[2022-11-06 06:17:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][950/1251]	eta 0:02:21 lr 0.000894	time 0.4641 (0.4689)	loss 3.0424 (3.6700)	grad_norm 1.1329 (1.2537)	mem 14851MB
[2022-11-06 06:17:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][1000/1251]	eta 0:01:57 lr 0.000894	time 0.4698 (0.4687)	loss 4.0760 (3.6713)	grad_norm 1.5401 (1.2555)	mem 14851MB
[2022-11-06 06:18:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][1050/1251]	eta 0:01:34 lr 0.000893	time 0.4581 (0.4687)	loss 2.9373 (3.6659)	grad_norm 1.2097 (1.2567)	mem 14851MB
[2022-11-06 06:18:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][1100/1251]	eta 0:01:10 lr 0.000893	time 0.4561 (0.4687)	loss 3.6648 (3.6636)	grad_norm 1.1685 (1.2568)	mem 14851MB
[2022-11-06 06:18:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][1150/1251]	eta 0:00:47 lr 0.000893	time 0.4671 (0.4687)	loss 3.9034 (3.6666)	grad_norm 1.2470 (1.2573)	mem 14851MB
[2022-11-06 06:19:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][1200/1251]	eta 0:00:23 lr 0.000893	time 0.4721 (0.4686)	loss 3.2021 (3.6703)	grad_norm 1.3709 (nan)	mem 14851MB
[2022-11-06 06:19:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [63/300][1250/1251]	eta 0:00:00 lr 0.000893	time 0.4568 (0.4685)	loss 2.5044 (3.6663)	grad_norm 1.2749 (nan)	mem 14851MB
[2022-11-06 06:19:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 63 training takes 0:09:46
[2022-11-06 06:19:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_63.pth saving......
[2022-11-06 06:19:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_63.pth saved !!!
[2022-11-06 06:19:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.468 (1.468)	Loss 1.1536 (1.1536)	Acc@1 72.656 (72.656)	Acc@5 92.676 (92.676)	Mem 14851MB
[2022-11-06 06:19:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.194 Acc@5 92.202
[2022-11-06 06:19:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.2%
[2022-11-06 06:19:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.683 (1.683)	Loss 0.9600 (0.9600)	Acc@1 78.613 (78.613)	Acc@5 94.141 (94.141)	Mem 14851MB
[2022-11-06 06:19:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.654 Acc@5 93.450
[2022-11-06 06:19:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.7%
[2022-11-06 06:19:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.65% at 63 epoch
[2022-11-06 06:20:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][0/1251]	eta 0:42:21 lr 0.000893	time 2.0316 (2.0316)	loss 3.8266 (3.8266)	grad_norm 1.3287 (1.3287)	mem 14851MB
[2022-11-06 06:20:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][50/1251]	eta 0:09:59 lr 0.000893	time 0.4565 (0.4991)	loss 3.2440 (3.6338)	grad_norm 1.0899 (1.2699)	mem 14851MB
[2022-11-06 06:20:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][100/1251]	eta 0:09:17 lr 0.000893	time 0.4715 (0.4848)	loss 3.9129 (3.6680)	grad_norm 1.4042 (1.2629)	mem 14851MB
[2022-11-06 06:21:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][150/1251]	eta 0:08:48 lr 0.000893	time 0.4566 (0.4797)	loss 4.0607 (3.6305)	grad_norm 1.1584 (1.2586)	mem 14851MB
[2022-11-06 06:21:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][200/1251]	eta 0:08:20 lr 0.000892	time 0.4682 (0.4760)	loss 3.4752 (3.6609)	grad_norm 1.2633 (1.2573)	mem 14851MB
[2022-11-06 06:21:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][250/1251]	eta 0:07:54 lr 0.000892	time 0.4643 (0.4741)	loss 3.5241 (3.6589)	grad_norm 1.2002 (1.2533)	mem 14851MB
[2022-11-06 06:22:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][300/1251]	eta 0:07:29 lr 0.000892	time 0.4564 (0.4727)	loss 3.2412 (3.6608)	grad_norm 1.2227 (1.2577)	mem 14851MB
[2022-11-06 06:22:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][350/1251]	eta 0:07:05 lr 0.000892	time 0.4735 (0.4717)	loss 2.5259 (3.6697)	grad_norm 1.1759 (1.2574)	mem 14851MB
[2022-11-06 06:23:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][400/1251]	eta 0:06:40 lr 0.000892	time 0.4641 (0.4708)	loss 3.7206 (3.6612)	grad_norm 1.4500 (1.2545)	mem 14851MB
[2022-11-06 06:23:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][450/1251]	eta 0:06:16 lr 0.000892	time 0.4671 (0.4704)	loss 3.8759 (3.6584)	grad_norm 1.1502 (1.2546)	mem 14851MB
[2022-11-06 06:23:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][500/1251]	eta 0:05:52 lr 0.000892	time 0.4674 (0.4698)	loss 3.1891 (3.6609)	grad_norm 1.2172 (1.2530)	mem 14851MB
[2022-11-06 06:24:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][550/1251]	eta 0:05:29 lr 0.000892	time 0.4664 (0.4697)	loss 4.6892 (3.6677)	grad_norm 1.1381 (1.2502)	mem 14851MB
[2022-11-06 06:24:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][600/1251]	eta 0:05:05 lr 0.000891	time 0.4783 (0.4696)	loss 4.6162 (3.6691)	grad_norm 1.2936 (1.2479)	mem 14851MB
[2022-11-06 06:25:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][650/1251]	eta 0:04:42 lr 0.000891	time 0.4537 (0.4693)	loss 2.5676 (3.6628)	grad_norm 1.3266 (1.2480)	mem 14851MB
[2022-11-06 06:25:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][700/1251]	eta 0:04:18 lr 0.000891	time 0.4529 (0.4692)	loss 3.7545 (3.6644)	grad_norm 1.2866 (1.2523)	mem 14851MB
[2022-11-06 06:25:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][750/1251]	eta 0:03:54 lr 0.000891	time 0.4526 (0.4690)	loss 4.0408 (3.6642)	grad_norm 1.2017 (1.2527)	mem 14851MB
[2022-11-06 06:26:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][800/1251]	eta 0:03:31 lr 0.000891	time 0.4707 (0.4691)	loss 4.4570 (3.6700)	grad_norm 1.4142 (1.2506)	mem 14851MB
[2022-11-06 06:26:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][850/1251]	eta 0:03:08 lr 0.000891	time 0.4545 (0.4692)	loss 3.8003 (3.6708)	grad_norm 1.2941 (1.2508)	mem 14851MB
[2022-11-06 06:27:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][900/1251]	eta 0:02:44 lr 0.000891	time 0.4643 (0.4690)	loss 3.4153 (3.6782)	grad_norm 1.2922 (1.2513)	mem 14851MB
[2022-11-06 06:27:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][950/1251]	eta 0:02:21 lr 0.000890	time 0.4681 (0.4688)	loss 3.4313 (3.6854)	grad_norm 1.2347 (1.2508)	mem 14851MB
[2022-11-06 06:27:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][1000/1251]	eta 0:01:57 lr 0.000890	time 0.4614 (0.4687)	loss 3.3723 (3.6794)	grad_norm 1.2096 (1.2524)	mem 14851MB
[2022-11-06 06:28:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][1050/1251]	eta 0:01:34 lr 0.000890	time 0.4596 (0.4687)	loss 2.7660 (3.6762)	grad_norm 1.1308 (1.2537)	mem 14851MB
[2022-11-06 06:28:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][1100/1251]	eta 0:01:10 lr 0.000890	time 0.4666 (0.4688)	loss 3.3298 (3.6777)	grad_norm 1.1535 (1.2548)	mem 14851MB
[2022-11-06 06:28:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][1150/1251]	eta 0:00:47 lr 0.000890	time 0.4686 (0.4686)	loss 3.3095 (3.6785)	grad_norm 1.1362 (1.2540)	mem 14851MB
[2022-11-06 06:29:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][1200/1251]	eta 0:00:23 lr 0.000890	time 0.4601 (0.4685)	loss 3.8269 (3.6766)	grad_norm 1.2696 (1.2544)	mem 14851MB
[2022-11-06 06:29:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [64/300][1250/1251]	eta 0:00:00 lr 0.000890	time 0.4570 (0.4683)	loss 3.6009 (3.6727)	grad_norm 1.2698 (1.2545)	mem 14851MB
[2022-11-06 06:29:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 64 training takes 0:09:45
[2022-11-06 06:29:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_64.pth saving......
[2022-11-06 06:29:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_64.pth saved !!!
[2022-11-06 06:29:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.653 (1.653)	Loss 1.0762 (1.0762)	Acc@1 75.293 (75.293)	Acc@5 92.871 (92.871)	Mem 14851MB
[2022-11-06 06:29:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.090 Acc@5 92.232
[2022-11-06 06:29:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.1%
[2022-11-06 06:29:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.581 (1.581)	Loss 1.0008 (1.0008)	Acc@1 77.539 (77.539)	Acc@5 93.066 (93.066)	Mem 14851MB
[2022-11-06 06:30:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.772 Acc@5 93.488
[2022-11-06 06:30:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.8%
[2022-11-06 06:30:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.77% at 64 epoch
[2022-11-06 06:30:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][0/1251]	eta 0:40:16 lr 0.000890	time 1.9313 (1.9313)	loss 2.9177 (2.9177)	grad_norm 1.2793 (1.2793)	mem 14851MB
[2022-11-06 06:30:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][50/1251]	eta 0:10:01 lr 0.000890	time 0.4672 (0.5005)	loss 3.3318 (3.6316)	grad_norm 1.2882 (1.2222)	mem 14851MB
[2022-11-06 06:30:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][100/1251]	eta 0:09:19 lr 0.000889	time 0.4668 (0.4859)	loss 4.1565 (3.5956)	grad_norm 1.2356 (1.2302)	mem 14851MB
[2022-11-06 06:31:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][150/1251]	eta 0:08:48 lr 0.000889	time 0.4658 (0.4796)	loss 3.8580 (3.6321)	grad_norm 1.1675 (1.2353)	mem 14851MB
[2022-11-06 06:31:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][200/1251]	eta 0:08:20 lr 0.000889	time 0.4613 (0.4760)	loss 4.1704 (3.6158)	grad_norm 1.3172 (1.2420)	mem 14851MB
[2022-11-06 06:32:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][250/1251]	eta 0:07:55 lr 0.000889	time 0.4547 (0.4746)	loss 4.5616 (3.6187)	grad_norm 1.4532 (1.2513)	mem 14851MB
[2022-11-06 06:32:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][300/1251]	eta 0:07:29 lr 0.000889	time 0.4527 (0.4730)	loss 3.5721 (3.6251)	grad_norm 1.1304 (1.2555)	mem 14851MB
[2022-11-06 06:32:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][350/1251]	eta 0:07:05 lr 0.000889	time 0.4639 (0.4721)	loss 3.0762 (3.6439)	grad_norm 1.1195 (1.2522)	mem 14851MB
[2022-11-06 06:33:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][400/1251]	eta 0:06:41 lr 0.000889	time 0.4606 (0.4713)	loss 2.7730 (3.6441)	grad_norm 1.2537 (1.2507)	mem 14851MB
[2022-11-06 06:33:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][450/1251]	eta 0:06:16 lr 0.000889	time 0.4586 (0.4706)	loss 3.8307 (3.6442)	grad_norm 1.2219 (1.2479)	mem 14851MB
[2022-11-06 06:33:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][500/1251]	eta 0:05:53 lr 0.000888	time 0.4694 (0.4701)	loss 2.6805 (3.6533)	grad_norm 1.0993 (1.2485)	mem 14851MB
[2022-11-06 06:34:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][550/1251]	eta 0:05:29 lr 0.000888	time 0.4615 (0.4700)	loss 3.5594 (3.6603)	grad_norm 1.1996 (1.2492)	mem 14851MB
[2022-11-06 06:34:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][600/1251]	eta 0:05:05 lr 0.000888	time 0.4606 (0.4700)	loss 3.9520 (3.6638)	grad_norm 1.2512 (1.2508)	mem 14851MB
[2022-11-06 06:35:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][650/1251]	eta 0:04:42 lr 0.000888	time 0.4625 (0.4698)	loss 4.0669 (3.6632)	grad_norm 1.2105 (1.2516)	mem 14851MB
[2022-11-06 06:35:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][700/1251]	eta 0:04:18 lr 0.000888	time 0.5356 (0.4695)	loss 4.1140 (3.6737)	grad_norm 1.2790 (1.2506)	mem 14851MB
[2022-11-06 06:35:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][750/1251]	eta 0:03:55 lr 0.000888	time 0.4650 (0.4692)	loss 2.9539 (3.6827)	grad_norm 1.2967 (1.2519)	mem 14851MB
[2022-11-06 06:36:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][800/1251]	eta 0:03:31 lr 0.000888	time 0.4715 (0.4693)	loss 3.1504 (3.6878)	grad_norm 1.0321 (inf)	mem 14851MB
[2022-11-06 06:36:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][850/1251]	eta 0:03:08 lr 0.000887	time 0.4553 (0.4691)	loss 3.8210 (3.6865)	grad_norm 1.2514 (inf)	mem 14851MB
[2022-11-06 06:37:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][900/1251]	eta 0:02:44 lr 0.000887	time 0.4623 (0.4690)	loss 2.8462 (3.6847)	grad_norm 1.0506 (inf)	mem 14851MB
[2022-11-06 06:37:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][950/1251]	eta 0:02:21 lr 0.000887	time 0.4642 (0.4688)	loss 4.0953 (3.6853)	grad_norm 1.2430 (inf)	mem 14851MB
[2022-11-06 06:37:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][1000/1251]	eta 0:01:57 lr 0.000887	time 0.4633 (0.4686)	loss 2.8249 (3.6860)	grad_norm 1.3417 (inf)	mem 14851MB
[2022-11-06 06:38:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][1050/1251]	eta 0:01:34 lr 0.000887	time 0.4615 (0.4687)	loss 4.1113 (3.6837)	grad_norm 1.3084 (inf)	mem 14851MB
[2022-11-06 06:38:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][1100/1251]	eta 0:01:10 lr 0.000887	time 0.4625 (0.4686)	loss 3.7443 (3.6819)	grad_norm 1.1174 (nan)	mem 14851MB
[2022-11-06 06:39:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][1150/1251]	eta 0:00:47 lr 0.000887	time 0.4738 (0.4685)	loss 3.2599 (3.6850)	grad_norm 1.1696 (nan)	mem 14851MB
[2022-11-06 06:39:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][1200/1251]	eta 0:00:23 lr 0.000887	time 0.5311 (0.4684)	loss 4.4184 (3.6883)	grad_norm 1.0675 (nan)	mem 14851MB
[2022-11-06 06:39:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [65/300][1250/1251]	eta 0:00:00 lr 0.000886	time 0.4578 (0.4682)	loss 3.7169 (3.6891)	grad_norm 1.1447 (nan)	mem 14851MB
[2022-11-06 06:39:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 65 training takes 0:09:45
[2022-11-06 06:39:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_65.pth saving......
[2022-11-06 06:39:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_65.pth saved !!!
[2022-11-06 06:39:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.608 (1.608)	Loss 1.1287 (1.1287)	Acc@1 72.754 (72.754)	Acc@5 92.969 (92.969)	Mem 14851MB
[2022-11-06 06:39:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.122 Acc@5 92.372
[2022-11-06 06:39:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.1%
[2022-11-06 06:39:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.674 (1.674)	Loss 0.9890 (0.9890)	Acc@1 76.562 (76.562)	Acc@5 93.652 (93.652)	Mem 14851MB
[2022-11-06 06:40:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.876 Acc@5 93.534
[2022-11-06 06:40:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.9%
[2022-11-06 06:40:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.88% at 65 epoch
[2022-11-06 06:40:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][0/1251]	eta 0:43:05 lr 0.000886	time 2.0670 (2.0670)	loss 3.1456 (3.1456)	grad_norm 1.4715 (1.4715)	mem 14851MB
[2022-11-06 06:40:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][50/1251]	eta 0:10:00 lr 0.000886	time 0.4646 (0.5004)	loss 4.1343 (3.7847)	grad_norm 1.3330 (1.2753)	mem 14851MB
[2022-11-06 06:40:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][100/1251]	eta 0:09:18 lr 0.000886	time 0.4578 (0.4854)	loss 3.3404 (3.6667)	grad_norm 1.2180 (1.2719)	mem 14851MB
[2022-11-06 06:41:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][150/1251]	eta 0:08:48 lr 0.000886	time 0.4728 (0.4800)	loss 3.9801 (3.6945)	grad_norm 1.1222 (1.2681)	mem 14851MB
[2022-11-06 06:41:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][200/1251]	eta 0:08:20 lr 0.000886	time 0.4672 (0.4765)	loss 3.1845 (3.6799)	grad_norm 1.4034 (1.2585)	mem 14851MB
[2022-11-06 06:42:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][250/1251]	eta 0:07:54 lr 0.000886	time 0.4662 (0.4744)	loss 2.7905 (3.6600)	grad_norm 1.1539 (1.2622)	mem 14851MB
[2022-11-06 06:42:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][300/1251]	eta 0:07:30 lr 0.000886	time 0.4662 (0.4733)	loss 3.9873 (3.6803)	grad_norm 1.5488 (1.2678)	mem 14851MB
[2022-11-06 06:42:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][350/1251]	eta 0:07:05 lr 0.000885	time 0.4638 (0.4720)	loss 2.8790 (3.6692)	grad_norm 1.3143 (1.2670)	mem 14851MB
[2022-11-06 06:43:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][400/1251]	eta 0:06:40 lr 0.000885	time 0.4618 (0.4712)	loss 3.8588 (3.6817)	grad_norm 1.1370 (1.2715)	mem 14851MB
[2022-11-06 06:43:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][450/1251]	eta 0:06:16 lr 0.000885	time 0.4653 (0.4705)	loss 3.1232 (3.6668)	grad_norm 1.0629 (1.2669)	mem 14851MB
[2022-11-06 06:44:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][500/1251]	eta 0:05:52 lr 0.000885	time 0.4606 (0.4700)	loss 3.7972 (3.6574)	grad_norm 1.3203 (1.2657)	mem 14851MB
[2022-11-06 06:44:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][550/1251]	eta 0:05:29 lr 0.000885	time 0.4688 (0.4699)	loss 4.0498 (3.6650)	grad_norm 1.3642 (1.2638)	mem 14851MB
[2022-11-06 06:44:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][600/1251]	eta 0:05:05 lr 0.000885	time 0.4587 (0.4698)	loss 3.7767 (3.6691)	grad_norm 1.2575 (1.2639)	mem 14851MB
[2022-11-06 06:45:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][650/1251]	eta 0:04:42 lr 0.000885	time 0.4739 (0.4696)	loss 4.0556 (3.6716)	grad_norm 1.2656 (1.2637)	mem 14851MB
[2022-11-06 06:45:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][700/1251]	eta 0:04:18 lr 0.000885	time 0.4640 (0.4692)	loss 3.1182 (3.6739)	grad_norm 1.1854 (1.2605)	mem 14851MB
[2022-11-06 06:45:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][750/1251]	eta 0:03:54 lr 0.000884	time 0.4676 (0.4689)	loss 3.3309 (3.6753)	grad_norm 1.2647 (1.2591)	mem 14851MB
[2022-11-06 06:46:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][800/1251]	eta 0:03:31 lr 0.000884	time 0.4685 (0.4689)	loss 3.7791 (3.6677)	grad_norm 1.3042 (1.2589)	mem 14851MB
[2022-11-06 06:46:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][850/1251]	eta 0:03:07 lr 0.000884	time 0.4680 (0.4688)	loss 3.9270 (3.6671)	grad_norm 1.1826 (1.2596)	mem 14851MB
[2022-11-06 06:47:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][900/1251]	eta 0:02:44 lr 0.000884	time 0.4667 (0.4687)	loss 2.8349 (3.6676)	grad_norm 1.1012 (1.2590)	mem 14851MB
[2022-11-06 06:47:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][950/1251]	eta 0:02:21 lr 0.000884	time 0.4675 (0.4686)	loss 2.6718 (3.6679)	grad_norm 1.1506 (1.2586)	mem 14851MB
[2022-11-06 06:47:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][1000/1251]	eta 0:01:57 lr 0.000884	time 0.4542 (0.4684)	loss 2.7926 (3.6665)	grad_norm 1.2282 (1.2580)	mem 14851MB
[2022-11-06 06:48:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][1050/1251]	eta 0:01:34 lr 0.000884	time 0.4554 (0.4685)	loss 3.4066 (3.6721)	grad_norm 1.4494 (1.2594)	mem 14851MB
[2022-11-06 06:48:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][1100/1251]	eta 0:01:10 lr 0.000883	time 0.4746 (0.4684)	loss 3.8558 (3.6716)	grad_norm 1.0729 (1.2604)	mem 14851MB
[2022-11-06 06:49:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][1150/1251]	eta 0:00:47 lr 0.000883	time 0.4618 (0.4683)	loss 4.1070 (3.6733)	grad_norm 1.3379 (1.2605)	mem 14851MB
[2022-11-06 06:49:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][1200/1251]	eta 0:00:23 lr 0.000883	time 0.4645 (0.4682)	loss 4.0608 (3.6722)	grad_norm 1.1262 (1.2603)	mem 14851MB
[2022-11-06 06:49:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [66/300][1250/1251]	eta 0:00:00 lr 0.000883	time 0.4571 (0.4680)	loss 3.7477 (3.6764)	grad_norm 1.2411 (1.2596)	mem 14851MB
[2022-11-06 06:49:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 66 training takes 0:09:45
[2022-11-06 06:49:52 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_66.pth saving......
[2022-11-06 06:49:53 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_66.pth saved !!!
[2022-11-06 06:49:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.638 (1.638)	Loss 1.1649 (1.1649)	Acc@1 73.828 (73.828)	Acc@5 91.602 (91.602)	Mem 14851MB
[2022-11-06 06:50:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.068 Acc@5 92.422
[2022-11-06 06:50:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.1%
[2022-11-06 06:50:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.578 (1.578)	Loss 0.9911 (0.9911)	Acc@1 76.367 (76.367)	Acc@5 94.043 (94.043)	Mem 14851MB
[2022-11-06 06:50:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.944 Acc@5 93.612
[2022-11-06 06:50:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.9%
[2022-11-06 06:50:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.94% at 66 epoch
[2022-11-06 06:50:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][0/1251]	eta 0:40:47 lr 0.000883	time 1.9568 (1.9568)	loss 3.7938 (3.7938)	grad_norm 1.2830 (1.2830)	mem 14851MB
[2022-11-06 06:50:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][50/1251]	eta 0:10:01 lr 0.000883	time 0.4715 (0.5004)	loss 4.8289 (3.6224)	grad_norm 1.2227 (1.2785)	mem 14851MB
[2022-11-06 06:50:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][100/1251]	eta 0:09:18 lr 0.000883	time 0.4659 (0.4857)	loss 3.1971 (3.6126)	grad_norm 1.3523 (1.2653)	mem 14851MB
[2022-11-06 06:51:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][150/1251]	eta 0:08:47 lr 0.000883	time 0.4695 (0.4793)	loss 2.6629 (3.6105)	grad_norm 1.1967 (1.2698)	mem 14851MB
[2022-11-06 06:51:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][200/1251]	eta 0:08:19 lr 0.000883	time 0.4540 (0.4757)	loss 4.4217 (3.6381)	grad_norm 1.2569 (1.2734)	mem 14851MB
[2022-11-06 06:52:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][250/1251]	eta 0:07:54 lr 0.000882	time 0.4590 (0.4735)	loss 4.0778 (3.6338)	grad_norm 1.2402 (1.2638)	mem 14851MB
[2022-11-06 06:52:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][300/1251]	eta 0:07:28 lr 0.000882	time 0.4548 (0.4720)	loss 4.0030 (3.6509)	grad_norm 1.5095 (1.2625)	mem 14851MB
[2022-11-06 06:52:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][350/1251]	eta 0:07:04 lr 0.000882	time 0.4646 (0.4711)	loss 4.1922 (3.6579)	grad_norm 1.1580 (1.2629)	mem 14851MB
[2022-11-06 06:53:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][400/1251]	eta 0:06:40 lr 0.000882	time 0.4626 (0.4707)	loss 3.0373 (3.6524)	grad_norm 1.2643 (1.2596)	mem 14851MB
[2022-11-06 06:53:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][450/1251]	eta 0:06:16 lr 0.000882	time 0.4704 (0.4700)	loss 3.9383 (3.6520)	grad_norm 1.1858 (1.2613)	mem 14851MB
[2022-11-06 06:54:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][500/1251]	eta 0:05:52 lr 0.000882	time 0.4624 (0.4696)	loss 3.9493 (3.6548)	grad_norm 1.0836 (1.2600)	mem 14851MB
[2022-11-06 06:54:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][550/1251]	eta 0:05:29 lr 0.000882	time 0.4606 (0.4695)	loss 3.8445 (3.6663)	grad_norm 1.3008 (1.2609)	mem 14851MB
[2022-11-06 06:54:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][600/1251]	eta 0:05:05 lr 0.000881	time 0.4620 (0.4693)	loss 4.0450 (3.6625)	grad_norm 1.1142 (1.2618)	mem 14851MB
[2022-11-06 06:55:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][650/1251]	eta 0:04:41 lr 0.000881	time 0.4679 (0.4690)	loss 2.5025 (3.6587)	grad_norm 1.3856 (1.2622)	mem 14851MB
[2022-11-06 06:55:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][700/1251]	eta 0:04:18 lr 0.000881	time 0.4628 (0.4688)	loss 3.7800 (3.6626)	grad_norm 1.0584 (1.2618)	mem 14851MB
[2022-11-06 06:56:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][750/1251]	eta 0:03:54 lr 0.000881	time 0.4556 (0.4686)	loss 3.8578 (3.6590)	grad_norm 1.5796 (1.2623)	mem 14851MB
[2022-11-06 06:56:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][800/1251]	eta 0:03:31 lr 0.000881	time 0.4627 (0.4687)	loss 2.5612 (3.6605)	grad_norm 1.3407 (1.2652)	mem 14851MB
[2022-11-06 06:56:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][850/1251]	eta 0:03:07 lr 0.000881	time 0.4757 (0.4686)	loss 3.1078 (3.6660)	grad_norm 1.2511 (1.2626)	mem 14851MB
[2022-11-06 06:57:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][900/1251]	eta 0:02:44 lr 0.000881	time 0.4602 (0.4685)	loss 2.6440 (3.6748)	grad_norm 1.1797 (1.2623)	mem 14851MB
[2022-11-06 06:57:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][950/1251]	eta 0:02:20 lr 0.000881	time 0.4660 (0.4683)	loss 3.5846 (3.6718)	grad_norm 1.3057 (1.2630)	mem 14851MB
[2022-11-06 06:57:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][1000/1251]	eta 0:01:57 lr 0.000880	time 0.4660 (0.4682)	loss 3.1449 (3.6747)	grad_norm 1.1010 (nan)	mem 14851MB
[2022-11-06 06:58:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][1050/1251]	eta 0:01:34 lr 0.000880	time 0.4707 (0.4683)	loss 2.9767 (3.6798)	grad_norm 1.3660 (nan)	mem 14851MB
[2022-11-06 06:58:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][1100/1251]	eta 0:01:10 lr 0.000880	time 0.5433 (0.4683)	loss 4.0813 (3.6829)	grad_norm 1.2055 (nan)	mem 14851MB
[2022-11-06 06:59:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][1150/1251]	eta 0:00:47 lr 0.000880	time 0.4655 (0.4681)	loss 4.2805 (3.6854)	grad_norm 1.1757 (nan)	mem 14851MB
[2022-11-06 06:59:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][1200/1251]	eta 0:00:23 lr 0.000880	time 0.4618 (0.4681)	loss 4.2158 (3.6842)	grad_norm 1.2731 (nan)	mem 14851MB
[2022-11-06 06:59:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [67/300][1250/1251]	eta 0:00:00 lr 0.000880	time 0.4579 (0.4679)	loss 4.0454 (3.6817)	grad_norm 1.1955 (nan)	mem 14851MB
[2022-11-06 06:59:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 67 training takes 0:09:45
[2022-11-06 06:59:56 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_67.pth saving......
[2022-11-06 06:59:57 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_67.pth saved !!!
[2022-11-06 06:59:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.541 (1.541)	Loss 1.0467 (1.0467)	Acc@1 75.879 (75.879)	Acc@5 93.555 (93.555)	Mem 14851MB
[2022-11-06 07:00:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.260 Acc@5 92.476
[2022-11-06 07:00:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.3%
[2022-11-06 07:00:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.708 (1.708)	Loss 0.9725 (0.9725)	Acc@1 77.148 (77.148)	Acc@5 93.457 (93.457)	Mem 14851MB
[2022-11-06 07:00:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.034 Acc@5 93.666
[2022-11-06 07:00:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.0%
[2022-11-06 07:00:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.03% at 67 epoch
[2022-11-06 07:00:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][0/1251]	eta 0:42:15 lr 0.000880	time 2.0272 (2.0272)	loss 4.2993 (4.2993)	grad_norm 1.3277 (1.3277)	mem 14851MB
[2022-11-06 07:00:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][50/1251]	eta 0:10:04 lr 0.000880	time 0.4615 (0.5035)	loss 4.0389 (3.6228)	grad_norm 1.4726 (1.2504)	mem 14851MB
[2022-11-06 07:01:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][100/1251]	eta 0:09:19 lr 0.000879	time 0.4752 (0.4861)	loss 2.3682 (3.6625)	grad_norm 1.2414 (1.2654)	mem 14851MB
[2022-11-06 07:01:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][150/1251]	eta 0:08:47 lr 0.000879	time 0.4657 (0.4793)	loss 3.2722 (3.6373)	grad_norm 1.2696 (1.2532)	mem 14851MB
[2022-11-06 07:01:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][200/1251]	eta 0:08:20 lr 0.000879	time 0.4645 (0.4757)	loss 2.5928 (3.6313)	grad_norm 1.1792 (1.2546)	mem 14851MB
[2022-11-06 07:02:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][250/1251]	eta 0:07:54 lr 0.000879	time 0.4539 (0.4743)	loss 3.6170 (3.6223)	grad_norm 1.2066 (1.2553)	mem 14851MB
[2022-11-06 07:02:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][300/1251]	eta 0:07:29 lr 0.000879	time 0.4696 (0.4731)	loss 4.1636 (3.6356)	grad_norm 1.3491 (1.2538)	mem 14851MB
[2022-11-06 07:03:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][350/1251]	eta 0:07:05 lr 0.000879	time 0.4753 (0.4721)	loss 3.9040 (3.6260)	grad_norm 1.2330 (1.2577)	mem 14851MB
[2022-11-06 07:03:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][400/1251]	eta 0:06:41 lr 0.000879	time 0.4662 (0.4714)	loss 4.1759 (3.6461)	grad_norm 1.1890 (1.2612)	mem 14851MB
[2022-11-06 07:03:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][450/1251]	eta 0:06:17 lr 0.000878	time 0.4787 (0.4709)	loss 4.2111 (3.6380)	grad_norm 1.3382 (1.2629)	mem 14851MB
[2022-11-06 07:04:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][500/1251]	eta 0:05:53 lr 0.000878	time 0.4627 (0.4705)	loss 3.8029 (3.6541)	grad_norm 1.2228 (1.2621)	mem 14851MB
[2022-11-06 07:04:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][550/1251]	eta 0:05:29 lr 0.000878	time 0.4682 (0.4705)	loss 2.5478 (3.6520)	grad_norm 1.2041 (1.2612)	mem 14851MB
[2022-11-06 07:04:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][600/1251]	eta 0:05:06 lr 0.000878	time 0.4735 (0.4703)	loss 3.2259 (3.6547)	grad_norm 1.3560 (1.2634)	mem 14851MB
[2022-11-06 07:05:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][650/1251]	eta 0:04:42 lr 0.000878	time 0.4619 (0.4700)	loss 4.1522 (3.6561)	grad_norm 1.4226 (1.2644)	mem 14851MB
[2022-11-06 07:05:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][700/1251]	eta 0:04:18 lr 0.000878	time 0.4592 (0.4697)	loss 4.1372 (3.6609)	grad_norm 1.2748 (1.2650)	mem 14851MB
[2022-11-06 07:06:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][750/1251]	eta 0:03:55 lr 0.000878	time 0.4646 (0.4695)	loss 3.3538 (3.6553)	grad_norm 1.2782 (1.2671)	mem 14851MB
[2022-11-06 07:06:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][800/1251]	eta 0:03:31 lr 0.000878	time 0.4667 (0.4694)	loss 2.6436 (3.6631)	grad_norm 1.2358 (1.2671)	mem 14851MB
[2022-11-06 07:06:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][850/1251]	eta 0:03:08 lr 0.000877	time 0.4644 (0.4693)	loss 4.1712 (3.6585)	grad_norm 1.2124 (1.2662)	mem 14851MB
[2022-11-06 07:07:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][900/1251]	eta 0:02:44 lr 0.000877	time 0.4748 (0.4691)	loss 3.6003 (3.6574)	grad_norm 1.1909 (1.2681)	mem 14851MB
[2022-11-06 07:07:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][950/1251]	eta 0:02:21 lr 0.000877	time 0.4757 (0.4690)	loss 3.8092 (3.6612)	grad_norm 1.2020 (1.2677)	mem 14851MB
[2022-11-06 07:08:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][1000/1251]	eta 0:01:57 lr 0.000877	time 0.4647 (0.4690)	loss 4.3029 (3.6522)	grad_norm 1.2587 (1.2671)	mem 14851MB
[2022-11-06 07:08:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][1050/1251]	eta 0:01:34 lr 0.000877	time 0.4669 (0.4689)	loss 4.3551 (3.6508)	grad_norm 1.2629 (1.2685)	mem 14851MB
[2022-11-06 07:08:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][1100/1251]	eta 0:01:10 lr 0.000877	time 0.4568 (0.4688)	loss 2.9414 (3.6493)	grad_norm 1.4149 (1.2685)	mem 14851MB
[2022-11-06 07:09:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][1150/1251]	eta 0:00:47 lr 0.000877	time 0.4667 (0.4687)	loss 3.9927 (3.6496)	grad_norm 1.1772 (1.2685)	mem 14851MB
[2022-11-06 07:09:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][1200/1251]	eta 0:00:23 lr 0.000876	time 0.4692 (0.4687)	loss 2.5993 (3.6465)	grad_norm 1.4753 (1.2679)	mem 14851MB
[2022-11-06 07:10:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [68/300][1250/1251]	eta 0:00:00 lr 0.000876	time 0.4577 (0.4685)	loss 3.0290 (3.6462)	grad_norm 1.1586 (1.2667)	mem 14851MB
[2022-11-06 07:10:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 68 training takes 0:09:46
[2022-11-06 07:10:01 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_68.pth saving......
[2022-11-06 07:10:01 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_68.pth saved !!!
[2022-11-06 07:10:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.498 (1.498)	Loss 1.1037 (1.1037)	Acc@1 74.121 (74.121)	Acc@5 91.895 (91.895)	Mem 14851MB
[2022-11-06 07:10:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.502 Acc@5 92.542
[2022-11-06 07:10:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.5%
[2022-11-06 07:10:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.606 (1.606)	Loss 0.9167 (0.9167)	Acc@1 77.637 (77.637)	Acc@5 94.141 (94.141)	Mem 14851MB
[2022-11-06 07:10:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.166 Acc@5 93.704
[2022-11-06 07:10:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.2%
[2022-11-06 07:10:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.17% at 68 epoch
[2022-11-06 07:10:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][0/1251]	eta 0:40:13 lr 0.000876	time 1.9294 (1.9294)	loss 3.2827 (3.2827)	grad_norm 1.2909 (1.2909)	mem 14851MB
[2022-11-06 07:10:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][50/1251]	eta 0:10:00 lr 0.000876	time 0.4570 (0.4998)	loss 4.1930 (3.6577)	grad_norm 1.2805 (1.2648)	mem 14851MB
[2022-11-06 07:11:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][100/1251]	eta 0:09:18 lr 0.000876	time 0.4610 (0.4854)	loss 4.1796 (3.6590)	grad_norm 1.1497 (1.2682)	mem 14851MB
[2022-11-06 07:11:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][150/1251]	eta 0:08:47 lr 0.000876	time 0.4536 (0.4788)	loss 3.1891 (3.6724)	grad_norm 1.2585 (1.2678)	mem 14851MB
[2022-11-06 07:11:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][200/1251]	eta 0:08:19 lr 0.000876	time 0.4720 (0.4755)	loss 3.9279 (3.6305)	grad_norm 1.2764 (1.2624)	mem 14851MB
[2022-11-06 07:12:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][250/1251]	eta 0:07:54 lr 0.000876	time 0.4625 (0.4738)	loss 3.3312 (3.6097)	grad_norm 1.2631 (1.2627)	mem 14851MB
[2022-11-06 07:12:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][300/1251]	eta 0:07:29 lr 0.000875	time 0.4654 (0.4728)	loss 3.9050 (3.6143)	grad_norm 1.1349 (1.2622)	mem 14851MB
[2022-11-06 07:13:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][350/1251]	eta 0:07:05 lr 0.000875	time 0.4623 (0.4721)	loss 3.6456 (3.6128)	grad_norm 1.1900 (1.2629)	mem 14851MB
[2022-11-06 07:13:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][400/1251]	eta 0:06:41 lr 0.000875	time 0.4643 (0.4715)	loss 3.6557 (3.6113)	grad_norm 1.1529 (1.2663)	mem 14851MB
[2022-11-06 07:13:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][450/1251]	eta 0:06:17 lr 0.000875	time 0.4633 (0.4711)	loss 2.8793 (3.6049)	grad_norm 1.4642 (1.2678)	mem 14851MB
[2022-11-06 07:14:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][500/1251]	eta 0:05:53 lr 0.000875	time 0.4695 (0.4705)	loss 3.9472 (3.6035)	grad_norm 1.1490 (1.2632)	mem 14851MB
[2022-11-06 07:14:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][550/1251]	eta 0:05:29 lr 0.000875	time 0.4746 (0.4704)	loss 4.1463 (3.6124)	grad_norm 1.3255 (1.2620)	mem 14851MB
[2022-11-06 07:15:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][600/1251]	eta 0:05:05 lr 0.000875	time 0.4664 (0.4700)	loss 4.0239 (3.6269)	grad_norm 1.2034 (1.2638)	mem 14851MB
[2022-11-06 07:15:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][650/1251]	eta 0:04:42 lr 0.000875	time 0.4682 (0.4697)	loss 3.6890 (3.6282)	grad_norm 1.2934 (1.2627)	mem 14851MB
[2022-11-06 07:15:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][700/1251]	eta 0:04:18 lr 0.000874	time 0.4693 (0.4696)	loss 3.5300 (3.6271)	grad_norm 1.1376 (1.2662)	mem 14851MB
[2022-11-06 07:16:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][750/1251]	eta 0:03:55 lr 0.000874	time 0.4589 (0.4694)	loss 3.2416 (3.6295)	grad_norm 1.0932 (1.2673)	mem 14851MB
[2022-11-06 07:16:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][800/1251]	eta 0:03:31 lr 0.000874	time 0.4635 (0.4696)	loss 3.9115 (3.6307)	grad_norm 1.3860 (1.2658)	mem 14851MB
[2022-11-06 07:16:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][850/1251]	eta 0:03:08 lr 0.000874	time 0.4874 (0.4694)	loss 2.8669 (3.6267)	grad_norm 1.1997 (1.2656)	mem 14851MB
[2022-11-06 07:17:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][900/1251]	eta 0:02:44 lr 0.000874	time 0.4615 (0.4692)	loss 3.3284 (3.6327)	grad_norm 1.1535 (1.2660)	mem 14851MB
[2022-11-06 07:17:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][950/1251]	eta 0:02:21 lr 0.000874	time 0.4700 (0.4691)	loss 3.3210 (3.6398)	grad_norm 1.2781 (1.2648)	mem 14851MB
[2022-11-06 07:18:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][1000/1251]	eta 0:01:57 lr 0.000874	time 0.4709 (0.4691)	loss 3.7380 (3.6411)	grad_norm 1.3456 (1.2647)	mem 14851MB
[2022-11-06 07:18:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][1050/1251]	eta 0:01:34 lr 0.000873	time 0.4593 (0.4693)	loss 4.2752 (3.6473)	grad_norm 1.3187 (1.2644)	mem 14851MB
[2022-11-06 07:18:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][1100/1251]	eta 0:01:10 lr 0.000873	time 0.4810 (0.4692)	loss 3.5711 (3.6447)	grad_norm 1.1461 (1.2635)	mem 14851MB
[2022-11-06 07:19:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][1150/1251]	eta 0:00:47 lr 0.000873	time 0.4599 (0.4690)	loss 2.9836 (3.6455)	grad_norm 1.3074 (1.2621)	mem 14851MB
[2022-11-06 07:19:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][1200/1251]	eta 0:00:23 lr 0.000873	time 0.4628 (0.4688)	loss 2.6095 (3.6419)	grad_norm 1.4553 (1.2629)	mem 14851MB
[2022-11-06 07:20:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [69/300][1250/1251]	eta 0:00:00 lr 0.000873	time 0.4578 (0.4687)	loss 3.9481 (3.6455)	grad_norm 1.4370 (1.2635)	mem 14851MB
[2022-11-06 07:20:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 69 training takes 0:09:46
[2022-11-06 07:20:05 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_69.pth saving......
[2022-11-06 07:20:06 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_69.pth saved !!!
[2022-11-06 07:20:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.597 (1.597)	Loss 1.0950 (1.0950)	Acc@1 75.098 (75.098)	Acc@5 92.090 (92.090)	Mem 14851MB
[2022-11-06 07:20:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.378 Acc@5 92.334
[2022-11-06 07:20:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.4%
[2022-11-06 07:20:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.651 (1.651)	Loss 0.9582 (0.9582)	Acc@1 76.660 (76.660)	Acc@5 94.141 (94.141)	Mem 14851MB
[2022-11-06 07:20:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.252 Acc@5 93.732
[2022-11-06 07:20:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.3%
[2022-11-06 07:20:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.25% at 69 epoch
[2022-11-06 07:20:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][0/1251]	eta 0:39:57 lr 0.000873	time 1.9166 (1.9166)	loss 4.1993 (4.1993)	grad_norm 1.2157 (1.2157)	mem 14851MB
[2022-11-06 07:20:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][50/1251]	eta 0:09:57 lr 0.000873	time 0.4642 (0.4975)	loss 3.4277 (3.6324)	grad_norm 1.4157 (1.2670)	mem 14851MB
[2022-11-06 07:21:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][100/1251]	eta 0:09:15 lr 0.000873	time 0.4570 (0.4830)	loss 3.0721 (3.6353)	grad_norm 1.3444 (nan)	mem 14851MB
[2022-11-06 07:21:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][150/1251]	eta 0:08:46 lr 0.000872	time 0.4710 (0.4778)	loss 3.7995 (3.6402)	grad_norm 1.2198 (nan)	mem 14851MB
[2022-11-06 07:21:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][200/1251]	eta 0:08:19 lr 0.000872	time 0.4542 (0.4754)	loss 3.0692 (3.6535)	grad_norm 1.2631 (nan)	mem 14851MB
[2022-11-06 07:22:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][250/1251]	eta 0:07:54 lr 0.000872	time 0.4532 (0.4739)	loss 3.5939 (3.6463)	grad_norm 1.2781 (nan)	mem 14851MB
[2022-11-06 07:22:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][300/1251]	eta 0:07:29 lr 0.000872	time 0.4605 (0.4728)	loss 4.3820 (3.6444)	grad_norm 1.4970 (nan)	mem 14851MB
[2022-11-06 07:23:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][350/1251]	eta 0:07:05 lr 0.000872	time 0.4635 (0.4718)	loss 4.0042 (3.6389)	grad_norm 1.1881 (nan)	mem 14851MB
[2022-11-06 07:23:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][400/1251]	eta 0:06:40 lr 0.000872	time 0.4616 (0.4709)	loss 4.1223 (3.6349)	grad_norm 1.5325 (nan)	mem 14851MB
[2022-11-06 07:23:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][450/1251]	eta 0:06:16 lr 0.000872	time 0.4694 (0.4703)	loss 2.5490 (3.6403)	grad_norm 1.2556 (nan)	mem 14851MB
[2022-11-06 07:24:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][500/1251]	eta 0:05:52 lr 0.000871	time 0.4667 (0.4698)	loss 3.2483 (3.6359)	grad_norm 1.1460 (nan)	mem 14851MB
[2022-11-06 07:24:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][550/1251]	eta 0:05:29 lr 0.000871	time 0.4734 (0.4699)	loss 3.1348 (3.6283)	grad_norm 1.1990 (nan)	mem 14851MB
[2022-11-06 07:25:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][600/1251]	eta 0:05:05 lr 0.000871	time 0.4668 (0.4696)	loss 3.7749 (3.6256)	grad_norm 1.2697 (nan)	mem 14851MB
[2022-11-06 07:25:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][650/1251]	eta 0:04:42 lr 0.000871	time 0.4639 (0.4695)	loss 4.1821 (3.6317)	grad_norm 1.1800 (nan)	mem 14851MB
[2022-11-06 07:25:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][700/1251]	eta 0:04:18 lr 0.000871	time 0.4581 (0.4694)	loss 3.3531 (3.6321)	grad_norm 1.4476 (nan)	mem 14851MB
[2022-11-06 07:26:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][750/1251]	eta 0:03:55 lr 0.000871	time 0.4762 (0.4692)	loss 4.6449 (3.6330)	grad_norm 1.2755 (nan)	mem 14851MB
[2022-11-06 07:26:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][800/1251]	eta 0:03:31 lr 0.000871	time 0.4594 (0.4693)	loss 4.0072 (3.6334)	grad_norm 1.2663 (nan)	mem 14851MB
[2022-11-06 07:27:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][850/1251]	eta 0:03:08 lr 0.000870	time 0.4666 (0.4693)	loss 2.9899 (3.6394)	grad_norm 1.0958 (nan)	mem 14851MB
[2022-11-06 07:27:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][900/1251]	eta 0:02:44 lr 0.000870	time 0.5445 (0.4691)	loss 4.4283 (3.6450)	grad_norm 1.2067 (nan)	mem 14851MB
[2022-11-06 07:27:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][950/1251]	eta 0:02:21 lr 0.000870	time 0.4697 (0.4690)	loss 4.1841 (3.6469)	grad_norm 1.1745 (nan)	mem 14851MB
[2022-11-06 07:28:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][1000/1251]	eta 0:01:57 lr 0.000870	time 0.4649 (0.4688)	loss 4.0171 (3.6448)	grad_norm 1.2179 (nan)	mem 14851MB
[2022-11-06 07:28:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][1050/1251]	eta 0:01:34 lr 0.000870	time 0.4668 (0.4689)	loss 3.7059 (3.6489)	grad_norm 1.3178 (nan)	mem 14851MB
[2022-11-06 07:28:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][1100/1251]	eta 0:01:10 lr 0.000870	time 0.4681 (0.4687)	loss 4.4157 (3.6532)	grad_norm 1.3264 (nan)	mem 14851MB
[2022-11-06 07:29:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][1150/1251]	eta 0:00:47 lr 0.000870	time 0.4651 (0.4688)	loss 4.1122 (3.6554)	grad_norm 1.4442 (nan)	mem 14851MB
[2022-11-06 07:29:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][1200/1251]	eta 0:00:23 lr 0.000870	time 0.4635 (0.4687)	loss 3.5603 (3.6526)	grad_norm 1.2061 (nan)	mem 14851MB
[2022-11-06 07:30:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [70/300][1250/1251]	eta 0:00:00 lr 0.000869	time 0.4567 (0.4684)	loss 3.9879 (3.6503)	grad_norm 1.1958 (nan)	mem 14851MB
[2022-11-06 07:30:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 70 training takes 0:09:46
[2022-11-06 07:30:10 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_70.pth saving......
[2022-11-06 07:30:10 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_70.pth saved !!!
[2022-11-06 07:30:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.536 (1.536)	Loss 1.0656 (1.0656)	Acc@1 75.977 (75.977)	Acc@5 92.969 (92.969)	Mem 14851MB
[2022-11-06 07:30:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.660 Acc@5 92.434
[2022-11-06 07:30:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.7%
[2022-11-06 07:30:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.574 (1.574)	Loss 0.9533 (0.9533)	Acc@1 76.660 (76.660)	Acc@5 94.336 (94.336)	Mem 14851MB
[2022-11-06 07:30:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.320 Acc@5 93.778
[2022-11-06 07:30:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.3%
[2022-11-06 07:30:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.32% at 70 epoch
[2022-11-06 07:30:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][0/1251]	eta 0:40:39 lr 0.000869	time 1.9497 (1.9497)	loss 2.9886 (2.9886)	grad_norm 1.1401 (1.1401)	mem 14851MB
[2022-11-06 07:30:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][50/1251]	eta 0:10:02 lr 0.000869	time 0.4551 (0.5018)	loss 3.8329 (3.6188)	grad_norm 1.3436 (1.2620)	mem 14851MB
[2022-11-06 07:31:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][100/1251]	eta 0:09:17 lr 0.000869	time 0.4602 (0.4843)	loss 4.0891 (3.6356)	grad_norm 1.3544 (1.2575)	mem 14851MB
[2022-11-06 07:31:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][150/1251]	eta 0:08:47 lr 0.000869	time 0.4581 (0.4791)	loss 4.3020 (3.6359)	grad_norm 1.3835 (1.2554)	mem 14851MB
[2022-11-06 07:32:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][200/1251]	eta 0:08:20 lr 0.000869	time 0.4613 (0.4760)	loss 3.8622 (3.6198)	grad_norm 1.1609 (1.2554)	mem 14851MB
[2022-11-06 07:32:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][250/1251]	eta 0:07:54 lr 0.000869	time 0.4636 (0.4742)	loss 3.4794 (3.6021)	grad_norm 1.4584 (1.2545)	mem 14851MB
[2022-11-06 07:32:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][300/1251]	eta 0:07:29 lr 0.000869	time 0.4548 (0.4726)	loss 4.2984 (3.5929)	grad_norm 1.4164 (1.2540)	mem 14851MB
[2022-11-06 07:33:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][350/1251]	eta 0:07:05 lr 0.000868	time 0.4682 (0.4719)	loss 3.5432 (3.5863)	grad_norm 1.1446 (1.2577)	mem 14851MB
[2022-11-06 07:33:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][400/1251]	eta 0:06:40 lr 0.000868	time 0.4650 (0.4709)	loss 3.3323 (3.5861)	grad_norm 1.1803 (1.2571)	mem 14851MB
[2022-11-06 07:33:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][450/1251]	eta 0:06:16 lr 0.000868	time 0.4553 (0.4702)	loss 2.8386 (3.5812)	grad_norm 1.3501 (1.2591)	mem 14851MB
[2022-11-06 07:34:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][500/1251]	eta 0:05:52 lr 0.000868	time 0.4728 (0.4696)	loss 3.4125 (3.5949)	grad_norm 1.1917 (1.2560)	mem 14851MB
[2022-11-06 07:34:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][550/1251]	eta 0:05:29 lr 0.000868	time 0.4720 (0.4694)	loss 3.6351 (3.5972)	grad_norm 1.2125 (1.2539)	mem 14851MB
[2022-11-06 07:35:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][600/1251]	eta 0:05:05 lr 0.000868	time 0.4736 (0.4693)	loss 3.0101 (3.5988)	grad_norm 1.2543 (1.2536)	mem 14851MB
[2022-11-06 07:35:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][650/1251]	eta 0:04:41 lr 0.000868	time 0.4624 (0.4692)	loss 3.6352 (3.6101)	grad_norm 1.2383 (1.2549)	mem 14851MB
[2022-11-06 07:35:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][700/1251]	eta 0:04:18 lr 0.000867	time 0.4679 (0.4691)	loss 3.7099 (3.6205)	grad_norm 1.1630 (1.2568)	mem 14851MB
[2022-11-06 07:36:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][750/1251]	eta 0:03:54 lr 0.000867	time 0.4775 (0.4689)	loss 4.1397 (3.6141)	grad_norm 1.2688 (1.2561)	mem 14851MB
[2022-11-06 07:36:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][800/1251]	eta 0:03:31 lr 0.000867	time 0.4657 (0.4688)	loss 4.5130 (3.6273)	grad_norm 1.3872 (1.2579)	mem 14851MB
[2022-11-06 07:37:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][850/1251]	eta 0:03:07 lr 0.000867	time 0.4699 (0.4688)	loss 3.3188 (3.6267)	grad_norm 1.5094 (1.2595)	mem 14851MB
[2022-11-06 07:37:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][900/1251]	eta 0:02:44 lr 0.000867	time 0.4687 (0.4686)	loss 2.6723 (3.6267)	grad_norm 1.2432 (1.2581)	mem 14851MB
[2022-11-06 07:37:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][950/1251]	eta 0:02:21 lr 0.000867	time 0.4652 (0.4685)	loss 2.6189 (3.6264)	grad_norm 1.2494 (1.2582)	mem 14851MB
[2022-11-06 07:38:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][1000/1251]	eta 0:01:57 lr 0.000867	time 0.4793 (0.4684)	loss 3.7243 (3.6311)	grad_norm 1.2505 (1.2582)	mem 14851MB
[2022-11-06 07:38:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][1050/1251]	eta 0:01:34 lr 0.000866	time 0.4652 (0.4683)	loss 3.6168 (3.6280)	grad_norm 1.1656 (1.2593)	mem 14851MB
[2022-11-06 07:39:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][1100/1251]	eta 0:01:10 lr 0.000866	time 0.4767 (0.4684)	loss 3.7800 (3.6301)	grad_norm 1.2431 (1.2587)	mem 14851MB
[2022-11-06 07:39:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][1150/1251]	eta 0:00:47 lr 0.000866	time 0.4588 (0.4683)	loss 3.6345 (3.6314)	grad_norm 1.1590 (1.2592)	mem 14851MB
[2022-11-06 07:39:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][1200/1251]	eta 0:00:23 lr 0.000866	time 0.4650 (0.4683)	loss 3.9071 (3.6273)	grad_norm 1.2714 (1.2590)	mem 14851MB
[2022-11-06 07:40:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [71/300][1250/1251]	eta 0:00:00 lr 0.000866	time 0.4572 (0.4682)	loss 2.5092 (3.6251)	grad_norm 1.2428 (1.2595)	mem 14851MB
[2022-11-06 07:40:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 71 training takes 0:09:45
[2022-11-06 07:40:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_71.pth saving......
[2022-11-06 07:40:14 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_71.pth saved !!!
[2022-11-06 07:40:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.778 (1.778)	Loss 1.1208 (1.1208)	Acc@1 74.414 (74.414)	Acc@5 90.820 (90.820)	Mem 14851MB
[2022-11-06 07:40:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.714 Acc@5 92.594
[2022-11-06 07:40:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.7%
[2022-11-06 07:40:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.546 (1.546)	Loss 0.8930 (0.8930)	Acc@1 78.711 (78.711)	Acc@5 94.629 (94.629)	Mem 14851MB
[2022-11-06 07:40:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.346 Acc@5 93.804
[2022-11-06 07:40:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.3%
[2022-11-06 07:40:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.35% at 71 epoch
[2022-11-06 07:40:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][0/1251]	eta 0:46:44 lr 0.000866	time 2.2415 (2.2415)	loss 3.9523 (3.9523)	grad_norm 1.2263 (1.2263)	mem 14851MB
[2022-11-06 07:40:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][50/1251]	eta 0:10:07 lr 0.000866	time 0.5464 (0.5055)	loss 3.0730 (3.6831)	grad_norm 1.3477 (1.2805)	mem 14851MB
[2022-11-06 07:41:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][100/1251]	eta 0:09:22 lr 0.000866	time 0.4639 (0.4888)	loss 2.4764 (3.6017)	grad_norm 1.1801 (1.2727)	mem 14851MB
[2022-11-06 07:41:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][150/1251]	eta 0:08:50 lr 0.000865	time 0.4710 (0.4816)	loss 3.1072 (3.6006)	grad_norm 1.3681 (1.2672)	mem 14851MB
[2022-11-06 07:42:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][200/1251]	eta 0:08:22 lr 0.000865	time 0.4747 (0.4782)	loss 4.2168 (3.6285)	grad_norm 1.3989 (1.2662)	mem 14851MB
[2022-11-06 07:42:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][250/1251]	eta 0:07:56 lr 0.000865	time 0.4631 (0.4758)	loss 3.8440 (3.6067)	grad_norm 1.1793 (1.2672)	mem 14851MB
[2022-11-06 07:42:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][300/1251]	eta 0:07:30 lr 0.000865	time 0.4684 (0.4742)	loss 3.9077 (3.6018)	grad_norm 1.0987 (1.2649)	mem 14851MB
[2022-11-06 07:43:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][350/1251]	eta 0:07:06 lr 0.000865	time 0.4632 (0.4729)	loss 3.7590 (3.6169)	grad_norm 1.1867 (1.2652)	mem 14851MB
[2022-11-06 07:43:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][400/1251]	eta 0:06:41 lr 0.000865	time 0.4585 (0.4723)	loss 4.5150 (3.6176)	grad_norm 1.1659 (1.2712)	mem 14851MB
[2022-11-06 07:44:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][450/1251]	eta 0:06:17 lr 0.000865	time 0.4662 (0.4716)	loss 2.5879 (3.6167)	grad_norm 1.2885 (1.2683)	mem 14851MB
[2022-11-06 07:44:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][500/1251]	eta 0:05:53 lr 0.000864	time 0.4647 (0.4709)	loss 2.5971 (3.6209)	grad_norm 1.3876 (1.2665)	mem 14851MB
[2022-11-06 07:44:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][550/1251]	eta 0:05:30 lr 0.000864	time 0.4692 (0.4709)	loss 2.8269 (3.6117)	grad_norm 1.3201 (1.2689)	mem 14851MB
[2022-11-06 07:45:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][600/1251]	eta 0:05:06 lr 0.000864	time 0.4588 (0.4706)	loss 3.8957 (3.6022)	grad_norm 1.2034 (1.2708)	mem 14851MB
[2022-11-06 07:45:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][650/1251]	eta 0:04:42 lr 0.000864	time 0.4535 (0.4703)	loss 4.0452 (3.6156)	grad_norm 1.2652 (1.2713)	mem 14851MB
[2022-11-06 07:46:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][700/1251]	eta 0:04:18 lr 0.000864	time 0.4749 (0.4699)	loss 4.4883 (3.6150)	grad_norm 1.4119 (1.2714)	mem 14851MB
[2022-11-06 07:46:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][750/1251]	eta 0:03:55 lr 0.000864	time 0.4676 (0.4697)	loss 3.8480 (3.6161)	grad_norm 1.2808 (1.2717)	mem 14851MB
[2022-11-06 07:46:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][800/1251]	eta 0:03:31 lr 0.000864	time 0.5403 (0.4698)	loss 4.0007 (3.6236)	grad_norm 1.2648 (1.2720)	mem 14851MB
[2022-11-06 07:47:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][850/1251]	eta 0:03:08 lr 0.000863	time 0.4729 (0.4697)	loss 2.7304 (3.6298)	grad_norm 1.2753 (1.2717)	mem 14851MB
[2022-11-06 07:47:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][900/1251]	eta 0:02:44 lr 0.000863	time 0.4628 (0.4695)	loss 3.8665 (3.6318)	grad_norm 1.1161 (1.2708)	mem 14851MB
[2022-11-06 07:47:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][950/1251]	eta 0:02:21 lr 0.000863	time 0.4651 (0.4694)	loss 3.0978 (3.6259)	grad_norm 1.2537 (1.2705)	mem 14851MB
[2022-11-06 07:48:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][1000/1251]	eta 0:01:57 lr 0.000863	time 0.4517 (0.4692)	loss 3.5253 (3.6271)	grad_norm 1.3475 (1.2707)	mem 14851MB
[2022-11-06 07:48:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][1050/1251]	eta 0:01:34 lr 0.000863	time 0.4671 (0.4693)	loss 2.5105 (3.6272)	grad_norm 1.3041 (1.2704)	mem 14851MB
[2022-11-06 07:49:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][1100/1251]	eta 0:01:10 lr 0.000863	time 0.4661 (0.4692)	loss 4.2831 (3.6364)	grad_norm 1.3578 (1.2720)	mem 14851MB
[2022-11-06 07:49:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][1150/1251]	eta 0:00:47 lr 0.000863	time 0.4607 (0.4692)	loss 2.5124 (3.6309)	grad_norm 1.1530 (1.2718)	mem 14851MB
[2022-11-06 07:49:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][1200/1251]	eta 0:00:23 lr 0.000862	time 0.4599 (0.4691)	loss 3.0170 (3.6314)	grad_norm 1.1490 (1.2712)	mem 14851MB
[2022-11-06 07:50:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [72/300][1250/1251]	eta 0:00:00 lr 0.000862	time 0.4572 (0.4688)	loss 3.9378 (3.6350)	grad_norm 1.3047 (1.2718)	mem 14851MB
[2022-11-06 07:50:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 72 training takes 0:09:46
[2022-11-06 07:50:18 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_72.pth saving......
[2022-11-06 07:50:19 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_72.pth saved !!!
[2022-11-06 07:50:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.692 (1.692)	Loss 1.2301 (1.2301)	Acc@1 72.754 (72.754)	Acc@5 90.820 (90.820)	Mem 14851MB
[2022-11-06 07:50:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.696 Acc@5 92.536
[2022-11-06 07:50:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.7%
[2022-11-06 07:50:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.762 (1.762)	Loss 1.0462 (1.0462)	Acc@1 76.855 (76.855)	Acc@5 92.285 (92.285)	Mem 14851MB
[2022-11-06 07:50:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.412 Acc@5 93.870
[2022-11-06 07:50:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.4%
[2022-11-06 07:50:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.41% at 72 epoch
[2022-11-06 07:50:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][0/1251]	eta 0:41:04 lr 0.000862	time 1.9700 (1.9700)	loss 3.6073 (3.6073)	grad_norm 1.3178 (1.3178)	mem 14852MB
[2022-11-06 07:51:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][50/1251]	eta 0:10:02 lr 0.000862	time 0.4633 (0.5015)	loss 3.5425 (3.4384)	grad_norm 1.5821 (1.2586)	mem 14852MB
[2022-11-06 07:51:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][100/1251]	eta 0:09:18 lr 0.000862	time 0.4599 (0.4852)	loss 3.3377 (3.4653)	grad_norm 1.1617 (1.2734)	mem 14852MB
[2022-11-06 07:51:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][150/1251]	eta 0:08:47 lr 0.000862	time 0.4558 (0.4795)	loss 3.7183 (3.5291)	grad_norm 1.2039 (1.2687)	mem 14852MB
[2022-11-06 07:52:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][200/1251]	eta 0:08:20 lr 0.000862	time 0.4642 (0.4759)	loss 2.9133 (3.5309)	grad_norm 1.2866 (1.2660)	mem 14852MB
[2022-11-06 07:52:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][250/1251]	eta 0:07:54 lr 0.000862	time 0.4711 (0.4743)	loss 3.9113 (3.5679)	grad_norm 1.2410 (1.2681)	mem 14852MB
[2022-11-06 07:52:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][300/1251]	eta 0:07:29 lr 0.000861	time 0.4690 (0.4729)	loss 3.3900 (3.5707)	grad_norm 1.2952 (1.2625)	mem 14852MB
[2022-11-06 07:53:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][350/1251]	eta 0:07:05 lr 0.000861	time 0.4576 (0.4718)	loss 3.5029 (3.5749)	grad_norm 1.2701 (1.2645)	mem 14852MB
[2022-11-06 07:53:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][400/1251]	eta 0:06:41 lr 0.000861	time 0.4647 (0.4714)	loss 3.4068 (3.5798)	grad_norm 1.4546 (nan)	mem 14852MB
[2022-11-06 07:54:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][450/1251]	eta 0:06:17 lr 0.000861	time 0.4632 (0.4707)	loss 3.6133 (3.5645)	grad_norm 1.2312 (nan)	mem 14852MB
[2022-11-06 07:54:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][500/1251]	eta 0:05:53 lr 0.000861	time 0.4649 (0.4704)	loss 3.9424 (3.5614)	grad_norm 1.1678 (nan)	mem 14852MB
[2022-11-06 07:54:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][550/1251]	eta 0:05:29 lr 0.000861	time 0.4548 (0.4702)	loss 2.7622 (3.5539)	grad_norm 1.3043 (nan)	mem 14852MB
[2022-11-06 07:55:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][600/1251]	eta 0:05:06 lr 0.000861	time 0.4678 (0.4701)	loss 3.3706 (3.5575)	grad_norm 1.3316 (nan)	mem 14852MB
[2022-11-06 07:55:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][650/1251]	eta 0:04:42 lr 0.000860	time 0.4622 (0.4697)	loss 4.3707 (3.5615)	grad_norm 1.3963 (nan)	mem 14852MB
[2022-11-06 07:56:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][700/1251]	eta 0:04:18 lr 0.000860	time 0.4797 (0.4695)	loss 3.9981 (3.5664)	grad_norm 1.2078 (nan)	mem 14852MB
[2022-11-06 07:56:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][750/1251]	eta 0:03:55 lr 0.000860	time 0.4726 (0.4692)	loss 2.8187 (3.5666)	grad_norm 1.4618 (nan)	mem 14852MB
[2022-11-06 07:56:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][800/1251]	eta 0:03:31 lr 0.000860	time 0.4655 (0.4691)	loss 3.9978 (3.5677)	grad_norm 1.1428 (nan)	mem 14852MB
[2022-11-06 07:57:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][850/1251]	eta 0:03:08 lr 0.000860	time 0.4707 (0.4691)	loss 3.5425 (3.5748)	grad_norm 1.2496 (nan)	mem 14852MB
[2022-11-06 07:57:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][900/1251]	eta 0:02:44 lr 0.000860	time 0.4640 (0.4689)	loss 4.2192 (3.5822)	grad_norm 1.3896 (nan)	mem 14852MB
[2022-11-06 07:58:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][950/1251]	eta 0:02:21 lr 0.000860	time 0.4554 (0.4687)	loss 3.3365 (3.5858)	grad_norm 1.4302 (nan)	mem 14852MB
[2022-11-06 07:58:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][1000/1251]	eta 0:01:57 lr 0.000859	time 0.5414 (0.4688)	loss 4.1380 (3.5837)	grad_norm 1.2998 (nan)	mem 14852MB
[2022-11-06 07:58:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][1050/1251]	eta 0:01:34 lr 0.000859	time 0.4668 (0.4687)	loss 4.4017 (3.5841)	grad_norm 1.3980 (nan)	mem 14852MB
[2022-11-06 07:59:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][1100/1251]	eta 0:01:10 lr 0.000859	time 0.4631 (0.4688)	loss 3.8744 (3.5834)	grad_norm 1.4772 (nan)	mem 14852MB
[2022-11-06 07:59:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][1150/1251]	eta 0:00:47 lr 0.000859	time 0.4756 (0.4686)	loss 3.6868 (3.5870)	grad_norm 1.2205 (nan)	mem 14852MB
[2022-11-06 07:59:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][1200/1251]	eta 0:00:23 lr 0.000859	time 0.4735 (0.4685)	loss 3.1423 (3.5848)	grad_norm 1.2394 (nan)	mem 14852MB
[2022-11-06 08:00:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [73/300][1250/1251]	eta 0:00:00 lr 0.000859	time 0.4569 (0.4684)	loss 3.4850 (3.5856)	grad_norm 1.3602 (nan)	mem 14852MB
[2022-11-06 08:00:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 73 training takes 0:09:46
[2022-11-06 08:00:22 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_73.pth saving......
[2022-11-06 08:00:23 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_73.pth saved !!!
[2022-11-06 08:00:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.585 (1.585)	Loss 1.1491 (1.1491)	Acc@1 74.023 (74.023)	Acc@5 91.992 (91.992)	Mem 14852MB
[2022-11-06 08:00:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.540 Acc@5 92.642
[2022-11-06 08:00:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.5%
[2022-11-06 08:00:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.597 (1.597)	Loss 0.8992 (0.8992)	Acc@1 77.637 (77.637)	Acc@5 94.434 (94.434)	Mem 14852MB
[2022-11-06 08:00:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.500 Acc@5 93.892
[2022-11-06 08:00:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.5%
[2022-11-06 08:00:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.50% at 73 epoch
[2022-11-06 08:00:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][0/1251]	eta 0:40:41 lr 0.000859	time 1.9520 (1.9520)	loss 3.9657 (3.9657)	grad_norm 1.4092 (1.4092)	mem 14852MB
[2022-11-06 08:01:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][50/1251]	eta 0:10:06 lr 0.000859	time 0.4661 (0.5049)	loss 2.8436 (3.4807)	grad_norm 1.2153 (1.2750)	mem 14852MB
[2022-11-06 08:01:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][100/1251]	eta 0:09:19 lr 0.000858	time 0.4637 (0.4862)	loss 3.7232 (3.5973)	grad_norm 1.4701 (1.2688)	mem 14852MB
[2022-11-06 08:01:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][150/1251]	eta 0:08:48 lr 0.000858	time 0.4606 (0.4798)	loss 4.0093 (3.6149)	grad_norm 1.3341 (1.2671)	mem 14852MB
[2022-11-06 08:02:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][200/1251]	eta 0:08:20 lr 0.000858	time 0.4528 (0.4762)	loss 2.5650 (3.6181)	grad_norm 1.3392 (1.2755)	mem 14852MB
[2022-11-06 08:02:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][250/1251]	eta 0:07:54 lr 0.000858	time 0.4601 (0.4742)	loss 2.8221 (3.5759)	grad_norm 1.2513 (1.2761)	mem 14852MB
[2022-11-06 08:03:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][300/1251]	eta 0:07:29 lr 0.000858	time 0.4746 (0.4728)	loss 4.2457 (3.5710)	grad_norm 1.2096 (1.2720)	mem 14852MB
[2022-11-06 08:03:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][350/1251]	eta 0:07:05 lr 0.000858	time 0.4613 (0.4718)	loss 3.4256 (3.5821)	grad_norm 1.2298 (1.2709)	mem 14852MB
[2022-11-06 08:03:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][400/1251]	eta 0:06:41 lr 0.000858	time 0.4637 (0.4713)	loss 3.9959 (3.6048)	grad_norm 1.2146 (1.2713)	mem 14852MB
[2022-11-06 08:04:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][450/1251]	eta 0:06:17 lr 0.000857	time 0.4735 (0.4709)	loss 4.0449 (3.6247)	grad_norm 1.2215 (1.2725)	mem 14852MB
[2022-11-06 08:04:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][500/1251]	eta 0:05:53 lr 0.000857	time 0.4668 (0.4708)	loss 2.9897 (3.6154)	grad_norm 1.2140 (1.2745)	mem 14852MB
[2022-11-06 08:05:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][550/1251]	eta 0:05:29 lr 0.000857	time 0.4630 (0.4707)	loss 4.0552 (3.6269)	grad_norm 1.2302 (1.2772)	mem 14852MB
[2022-11-06 08:05:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][600/1251]	eta 0:05:06 lr 0.000857	time 0.4652 (0.4702)	loss 2.8539 (3.6249)	grad_norm 1.2910 (1.2773)	mem 14852MB
[2022-11-06 08:05:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][650/1251]	eta 0:04:42 lr 0.000857	time 0.4626 (0.4699)	loss 4.2919 (3.6154)	grad_norm 1.2977 (1.2790)	mem 14852MB
[2022-11-06 08:06:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][700/1251]	eta 0:04:18 lr 0.000857	time 0.4744 (0.4697)	loss 4.2918 (3.6096)	grad_norm 1.3052 (1.2790)	mem 14852MB
[2022-11-06 08:06:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][750/1251]	eta 0:03:55 lr 0.000856	time 0.4612 (0.4697)	loss 3.8466 (3.6047)	grad_norm 1.2029 (1.2798)	mem 14852MB
[2022-11-06 08:06:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][800/1251]	eta 0:03:31 lr 0.000856	time 0.4680 (0.4695)	loss 4.5460 (3.6094)	grad_norm 1.1525 (1.2794)	mem 14852MB
[2022-11-06 08:07:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][850/1251]	eta 0:03:08 lr 0.000856	time 0.4601 (0.4694)	loss 2.9995 (3.6027)	grad_norm 1.1359 (1.2791)	mem 14852MB
[2022-11-06 08:07:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][900/1251]	eta 0:02:44 lr 0.000856	time 0.4559 (0.4692)	loss 4.3741 (3.6125)	grad_norm 1.2463 (1.2801)	mem 14852MB
[2022-11-06 08:08:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][950/1251]	eta 0:02:21 lr 0.000856	time 0.4783 (0.4690)	loss 3.3133 (3.6175)	grad_norm 1.2170 (1.2798)	mem 14852MB
[2022-11-06 08:08:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][1000/1251]	eta 0:01:57 lr 0.000856	time 0.4539 (0.4691)	loss 3.7351 (3.6146)	grad_norm 1.2036 (1.2779)	mem 14852MB
[2022-11-06 08:08:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][1050/1251]	eta 0:01:34 lr 0.000856	time 0.4707 (0.4689)	loss 2.8850 (3.6123)	grad_norm 1.2056 (1.2778)	mem 14852MB
[2022-11-06 08:09:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][1100/1251]	eta 0:01:10 lr 0.000855	time 0.4758 (0.4687)	loss 4.1755 (3.6182)	grad_norm 1.2651 (1.2781)	mem 14852MB
[2022-11-06 08:09:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][1150/1251]	eta 0:00:47 lr 0.000855	time 0.4613 (0.4686)	loss 4.2726 (3.6203)	grad_norm 1.1772 (1.2759)	mem 14852MB
[2022-11-06 08:10:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][1200/1251]	eta 0:00:23 lr 0.000855	time 0.4616 (0.4686)	loss 3.1084 (3.6159)	grad_norm 1.2128 (1.2760)	mem 14852MB
[2022-11-06 08:10:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [74/300][1250/1251]	eta 0:00:00 lr 0.000855	time 0.4584 (0.4684)	loss 4.0631 (3.6205)	grad_norm 1.1683 (nan)	mem 14852MB
[2022-11-06 08:10:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 74 training takes 0:09:46
[2022-11-06 08:10:27 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_74.pth saving......
[2022-11-06 08:10:27 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_74.pth saved !!!
[2022-11-06 08:10:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.569 (1.569)	Loss 1.0384 (1.0384)	Acc@1 75.391 (75.391)	Acc@5 92.676 (92.676)	Mem 14852MB
[2022-11-06 08:10:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.648 Acc@5 92.580
[2022-11-06 08:10:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.6%
[2022-11-06 08:10:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.659 (1.659)	Loss 0.9142 (0.9142)	Acc@1 79.883 (79.883)	Acc@5 94.238 (94.238)	Mem 14852MB
[2022-11-06 08:10:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.520 Acc@5 93.920
[2022-11-06 08:10:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.5%
[2022-11-06 08:10:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.52% at 74 epoch
[2022-11-06 08:10:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][0/1251]	eta 0:41:13 lr 0.000855	time 1.9775 (1.9775)	loss 3.9492 (3.9492)	grad_norm 1.1159 (1.1159)	mem 14852MB
[2022-11-06 08:11:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][50/1251]	eta 0:10:07 lr 0.000855	time 0.4743 (0.5058)	loss 3.8471 (3.6312)	grad_norm 1.1613 (1.2336)	mem 14852MB
[2022-11-06 08:11:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][100/1251]	eta 0:09:19 lr 0.000855	time 0.4573 (0.4864)	loss 3.7522 (3.5950)	grad_norm 1.2708 (1.2716)	mem 14852MB
[2022-11-06 08:11:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][150/1251]	eta 0:08:47 lr 0.000855	time 0.4590 (0.4793)	loss 3.0292 (3.6252)	grad_norm 1.2174 (1.2649)	mem 14852MB
[2022-11-06 08:12:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][200/1251]	eta 0:08:20 lr 0.000854	time 0.4536 (0.4765)	loss 3.8826 (3.6310)	grad_norm 1.2068 (1.2589)	mem 14852MB
[2022-11-06 08:12:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][250/1251]	eta 0:07:54 lr 0.000854	time 0.4633 (0.4741)	loss 3.6140 (3.6214)	grad_norm 1.2154 (nan)	mem 14852MB
[2022-11-06 08:13:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][300/1251]	eta 0:07:29 lr 0.000854	time 0.4603 (0.4725)	loss 3.5235 (3.6389)	grad_norm 1.2264 (nan)	mem 14852MB
[2022-11-06 08:13:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][350/1251]	eta 0:07:04 lr 0.000854	time 0.4578 (0.4714)	loss 3.6945 (3.6361)	grad_norm 1.1019 (nan)	mem 14852MB
[2022-11-06 08:13:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][400/1251]	eta 0:06:40 lr 0.000854	time 0.4781 (0.4709)	loss 4.3529 (3.6360)	grad_norm 1.1056 (nan)	mem 14852MB
[2022-11-06 08:14:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][450/1251]	eta 0:06:16 lr 0.000854	time 0.4581 (0.4704)	loss 2.6724 (3.6340)	grad_norm 1.1073 (nan)	mem 14852MB
[2022-11-06 08:14:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][500/1251]	eta 0:05:53 lr 0.000854	time 0.4597 (0.4702)	loss 3.4258 (3.6376)	grad_norm 1.2137 (nan)	mem 14852MB
[2022-11-06 08:15:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][550/1251]	eta 0:05:29 lr 0.000853	time 0.4622 (0.4702)	loss 2.9676 (3.6376)	grad_norm 1.2900 (nan)	mem 14852MB
[2022-11-06 08:15:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][600/1251]	eta 0:05:05 lr 0.000853	time 0.4704 (0.4698)	loss 3.1894 (3.6452)	grad_norm 1.2297 (nan)	mem 14852MB
[2022-11-06 08:15:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][650/1251]	eta 0:04:42 lr 0.000853	time 0.4689 (0.4694)	loss 3.7525 (3.6474)	grad_norm 1.3382 (nan)	mem 14852MB
[2022-11-06 08:16:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][700/1251]	eta 0:04:18 lr 0.000853	time 0.4652 (0.4693)	loss 3.6621 (3.6504)	grad_norm 1.3192 (nan)	mem 14852MB
[2022-11-06 08:16:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][750/1251]	eta 0:03:55 lr 0.000853	time 0.4583 (0.4692)	loss 3.8902 (3.6378)	grad_norm 1.3757 (nan)	mem 14852MB
[2022-11-06 08:17:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][800/1251]	eta 0:03:31 lr 0.000853	time 0.4614 (0.4693)	loss 2.9985 (3.6438)	grad_norm 1.3134 (nan)	mem 14852MB
[2022-11-06 08:17:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][850/1251]	eta 0:03:08 lr 0.000853	time 0.4619 (0.4691)	loss 3.9663 (3.6417)	grad_norm 1.1981 (nan)	mem 14852MB
[2022-11-06 08:17:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][900/1251]	eta 0:02:44 lr 0.000852	time 0.4621 (0.4689)	loss 3.8684 (3.6398)	grad_norm 1.2325 (nan)	mem 14852MB
[2022-11-06 08:18:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][950/1251]	eta 0:02:21 lr 0.000852	time 0.4638 (0.4688)	loss 4.0608 (3.6379)	grad_norm 1.2827 (nan)	mem 14852MB
[2022-11-06 08:18:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][1000/1251]	eta 0:01:57 lr 0.000852	time 0.4751 (0.4687)	loss 4.1833 (3.6425)	grad_norm 1.3213 (nan)	mem 14852MB
[2022-11-06 08:18:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][1050/1251]	eta 0:01:34 lr 0.000852	time 0.4635 (0.4688)	loss 4.4680 (3.6493)	grad_norm 1.2362 (nan)	mem 14852MB
[2022-11-06 08:19:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][1100/1251]	eta 0:01:10 lr 0.000852	time 0.4694 (0.4687)	loss 3.6548 (3.6458)	grad_norm 1.2389 (nan)	mem 14852MB
[2022-11-06 08:19:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][1150/1251]	eta 0:00:47 lr 0.000852	time 0.4666 (0.4685)	loss 3.3030 (3.6414)	grad_norm 1.5911 (nan)	mem 14852MB
[2022-11-06 08:20:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][1200/1251]	eta 0:00:23 lr 0.000851	time 0.4651 (0.4684)	loss 4.1612 (3.6382)	grad_norm 1.2529 (nan)	mem 14852MB
[2022-11-06 08:20:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [75/300][1250/1251]	eta 0:00:00 lr 0.000851	time 0.4569 (0.4683)	loss 3.9384 (3.6344)	grad_norm 1.2718 (nan)	mem 14852MB
[2022-11-06 08:20:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 75 training takes 0:09:45
[2022-11-06 08:20:31 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_75.pth saving......
[2022-11-06 08:20:31 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_75.pth saved !!!
[2022-11-06 08:20:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.795 (1.795)	Loss 1.1277 (1.1277)	Acc@1 75.879 (75.879)	Acc@5 91.309 (91.309)	Mem 14852MB
[2022-11-06 08:20:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.962 Acc@5 92.602
[2022-11-06 08:20:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.0%
[2022-11-06 08:20:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.531 (1.531)	Loss 0.9198 (0.9198)	Acc@1 78.320 (78.320)	Acc@5 94.727 (94.727)	Mem 14852MB
[2022-11-06 08:20:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.562 Acc@5 93.966
[2022-11-06 08:20:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.6%
[2022-11-06 08:20:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.56% at 75 epoch
[2022-11-06 08:20:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][0/1251]	eta 0:39:42 lr 0.000851	time 1.9049 (1.9049)	loss 3.8340 (3.8340)	grad_norm 1.1030 (1.1030)	mem 14852MB
[2022-11-06 08:21:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][50/1251]	eta 0:10:04 lr 0.000851	time 0.4807 (0.5031)	loss 3.2871 (3.6630)	grad_norm 1.2842 (1.2682)	mem 14852MB
[2022-11-06 08:21:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][100/1251]	eta 0:09:19 lr 0.000851	time 0.4627 (0.4857)	loss 3.8860 (3.6597)	grad_norm 1.3627 (1.2698)	mem 14852MB
[2022-11-06 08:22:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][150/1251]	eta 0:08:47 lr 0.000851	time 0.4723 (0.4794)	loss 3.5059 (3.6720)	grad_norm 1.1788 (1.2663)	mem 14852MB
[2022-11-06 08:22:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][200/1251]	eta 0:08:20 lr 0.000851	time 0.4614 (0.4761)	loss 2.6881 (3.6595)	grad_norm 1.2470 (1.2728)	mem 14852MB
[2022-11-06 08:22:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][250/1251]	eta 0:07:54 lr 0.000851	time 0.4733 (0.4741)	loss 2.6021 (3.6378)	grad_norm 1.1294 (1.2717)	mem 14852MB
[2022-11-06 08:23:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][300/1251]	eta 0:07:29 lr 0.000850	time 0.4668 (0.4729)	loss 3.1990 (3.6186)	grad_norm 1.2560 (1.2669)	mem 14852MB
[2022-11-06 08:23:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][350/1251]	eta 0:07:05 lr 0.000850	time 0.4651 (0.4718)	loss 3.5758 (3.6199)	grad_norm 1.2202 (1.2661)	mem 14852MB
[2022-11-06 08:23:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][400/1251]	eta 0:06:40 lr 0.000850	time 0.5520 (0.4712)	loss 3.0036 (3.5983)	grad_norm 1.3810 (1.2678)	mem 14852MB
[2022-11-06 08:24:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][450/1251]	eta 0:06:16 lr 0.000850	time 0.4673 (0.4705)	loss 3.3780 (3.6086)	grad_norm 1.2001 (1.2700)	mem 14852MB
[2022-11-06 08:24:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][500/1251]	eta 0:05:52 lr 0.000850	time 0.4596 (0.4699)	loss 2.9058 (3.6075)	grad_norm 1.5368 (1.2737)	mem 14852MB
[2022-11-06 08:25:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][550/1251]	eta 0:05:29 lr 0.000850	time 0.4624 (0.4701)	loss 3.7339 (3.6102)	grad_norm 1.4304 (1.2750)	mem 14852MB
[2022-11-06 08:25:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][600/1251]	eta 0:05:05 lr 0.000850	time 0.4537 (0.4699)	loss 3.9654 (3.6067)	grad_norm 1.2004 (1.2765)	mem 14852MB
[2022-11-06 08:25:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][650/1251]	eta 0:04:42 lr 0.000849	time 0.4550 (0.4696)	loss 4.0118 (3.6113)	grad_norm 1.3269 (1.2746)	mem 14852MB
[2022-11-06 08:26:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][700/1251]	eta 0:04:18 lr 0.000849	time 0.4651 (0.4693)	loss 4.3660 (3.6164)	grad_norm 1.2917 (1.2761)	mem 14852MB
[2022-11-06 08:26:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][750/1251]	eta 0:03:55 lr 0.000849	time 0.4696 (0.4691)	loss 3.6053 (3.6203)	grad_norm 1.1629 (1.2745)	mem 14852MB
[2022-11-06 08:27:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][800/1251]	eta 0:03:31 lr 0.000849	time 0.4637 (0.4693)	loss 3.6899 (3.6204)	grad_norm 1.3636 (1.2738)	mem 14852MB
[2022-11-06 08:27:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][850/1251]	eta 0:03:08 lr 0.000849	time 0.4607 (0.4690)	loss 3.7911 (3.6253)	grad_norm 1.0906 (1.2727)	mem 14852MB
[2022-11-06 08:27:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][900/1251]	eta 0:02:44 lr 0.000849	time 0.4664 (0.4689)	loss 4.0052 (3.6238)	grad_norm 1.2819 (1.2724)	mem 14852MB
[2022-11-06 08:28:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][950/1251]	eta 0:02:21 lr 0.000849	time 0.4739 (0.4688)	loss 3.8662 (3.6133)	grad_norm 1.1720 (1.2739)	mem 14852MB
[2022-11-06 08:28:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][1000/1251]	eta 0:01:57 lr 0.000848	time 0.4514 (0.4687)	loss 4.1442 (3.6178)	grad_norm 1.4042 (1.2747)	mem 14852MB
[2022-11-06 08:29:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][1050/1251]	eta 0:01:34 lr 0.000848	time 0.4603 (0.4687)	loss 4.3144 (3.6214)	grad_norm 1.5086 (1.2757)	mem 14852MB
[2022-11-06 08:29:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][1100/1251]	eta 0:01:10 lr 0.000848	time 0.4649 (0.4687)	loss 3.4133 (3.6224)	grad_norm 1.2685 (1.2755)	mem 14852MB
[2022-11-06 08:29:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][1150/1251]	eta 0:00:47 lr 0.000848	time 0.4545 (0.4685)	loss 2.5037 (3.6155)	grad_norm 1.8543 (1.2766)	mem 14852MB
[2022-11-06 08:30:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][1200/1251]	eta 0:00:23 lr 0.000848	time 0.4752 (0.4685)	loss 4.0535 (3.6125)	grad_norm 1.1886 (1.2764)	mem 14852MB
[2022-11-06 08:30:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [76/300][1250/1251]	eta 0:00:00 lr 0.000848	time 0.4574 (0.4683)	loss 3.4627 (3.6090)	grad_norm 1.1557 (1.2760)	mem 14852MB
[2022-11-06 08:30:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 76 training takes 0:09:45
[2022-11-06 08:30:35 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_76.pth saving......
[2022-11-06 08:30:35 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_76.pth saved !!!
[2022-11-06 08:30:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.538 (1.538)	Loss 1.1515 (1.1515)	Acc@1 73.535 (73.535)	Acc@5 92.383 (92.383)	Mem 14852MB
[2022-11-06 08:30:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.908 Acc@5 92.802
[2022-11-06 08:30:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.9%
[2022-11-06 08:30:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.619 (1.619)	Loss 0.9165 (0.9165)	Acc@1 78.418 (78.418)	Acc@5 94.434 (94.434)	Mem 14852MB
[2022-11-06 08:30:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.604 Acc@5 94.006
[2022-11-06 08:30:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.6%
[2022-11-06 08:30:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.60% at 76 epoch
[2022-11-06 08:30:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][0/1251]	eta 0:41:49 lr 0.000848	time 2.0057 (2.0057)	loss 2.3817 (2.3817)	grad_norm 1.0439 (1.0439)	mem 14852MB
[2022-11-06 08:31:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][50/1251]	eta 0:10:03 lr 0.000847	time 0.4623 (0.5029)	loss 3.0756 (3.5061)	grad_norm 1.2812 (1.2569)	mem 14852MB
[2022-11-06 08:31:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][100/1251]	eta 0:09:17 lr 0.000847	time 0.4681 (0.4847)	loss 3.3864 (3.6508)	grad_norm 1.2128 (1.2607)	mem 14852MB
[2022-11-06 08:32:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][150/1251]	eta 0:08:47 lr 0.000847	time 0.4714 (0.4792)	loss 3.1983 (3.6560)	grad_norm 1.1214 (1.2701)	mem 14852MB
[2022-11-06 08:32:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][200/1251]	eta 0:08:19 lr 0.000847	time 0.4696 (0.4754)	loss 2.6077 (3.6439)	grad_norm 1.3748 (1.2606)	mem 14852MB
[2022-11-06 08:32:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][250/1251]	eta 0:07:54 lr 0.000847	time 0.4674 (0.4735)	loss 3.9513 (3.6431)	grad_norm 1.1840 (1.2630)	mem 14852MB
[2022-11-06 08:33:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][300/1251]	eta 0:07:28 lr 0.000847	time 0.4618 (0.4720)	loss 3.9697 (3.6532)	grad_norm 1.3503 (1.2652)	mem 14852MB
[2022-11-06 08:33:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][350/1251]	eta 0:07:04 lr 0.000847	time 0.4625 (0.4711)	loss 3.6777 (3.6530)	grad_norm 1.3928 (1.2682)	mem 14852MB
[2022-11-06 08:34:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][400/1251]	eta 0:06:40 lr 0.000846	time 0.4654 (0.4705)	loss 3.4079 (3.6555)	grad_norm 1.2786 (1.2699)	mem 14852MB
[2022-11-06 08:34:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][450/1251]	eta 0:06:16 lr 0.000846	time 0.4671 (0.4701)	loss 4.0094 (3.6604)	grad_norm 1.3913 (1.2705)	mem 14852MB
[2022-11-06 08:34:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][500/1251]	eta 0:05:52 lr 0.000846	time 0.4658 (0.4696)	loss 2.5956 (3.6587)	grad_norm 1.3904 (1.2728)	mem 14852MB
[2022-11-06 08:35:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][550/1251]	eta 0:05:29 lr 0.000846	time 0.4656 (0.4698)	loss 4.2119 (3.6468)	grad_norm 1.3626 (1.2733)	mem 14852MB
[2022-11-06 08:35:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][600/1251]	eta 0:05:05 lr 0.000846	time 0.4631 (0.4695)	loss 3.7021 (3.6477)	grad_norm 1.2228 (1.2734)	mem 14852MB
[2022-11-06 08:35:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][650/1251]	eta 0:04:41 lr 0.000846	time 0.4751 (0.4692)	loss 3.6488 (3.6397)	grad_norm 1.2465 (1.2751)	mem 14852MB
[2022-11-06 08:36:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][700/1251]	eta 0:04:18 lr 0.000846	time 0.4676 (0.4690)	loss 4.0134 (3.6394)	grad_norm 1.1894 (1.2755)	mem 14852MB
[2022-11-06 08:36:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][750/1251]	eta 0:03:54 lr 0.000845	time 0.4598 (0.4687)	loss 4.0819 (3.6386)	grad_norm 1.2006 (1.2770)	mem 14852MB
[2022-11-06 08:37:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][800/1251]	eta 0:03:31 lr 0.000845	time 0.4625 (0.4687)	loss 2.9058 (3.6361)	grad_norm 1.1582 (1.2782)	mem 14852MB
[2022-11-06 08:37:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][850/1251]	eta 0:03:07 lr 0.000845	time 0.4660 (0.4685)	loss 3.8621 (3.6350)	grad_norm 1.3283 (1.2788)	mem 14852MB
[2022-11-06 08:37:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][900/1251]	eta 0:02:44 lr 0.000845	time 0.4512 (0.4683)	loss 2.5867 (3.6315)	grad_norm 1.2676 (1.2773)	mem 14852MB
[2022-11-06 08:38:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][950/1251]	eta 0:02:20 lr 0.000845	time 0.4845 (0.4682)	loss 3.5612 (3.6313)	grad_norm 1.1611 (1.2781)	mem 14852MB
[2022-11-06 08:38:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][1000/1251]	eta 0:01:57 lr 0.000845	time 0.4668 (0.4682)	loss 2.9781 (3.6320)	grad_norm 1.1845 (1.2779)	mem 14852MB
[2022-11-06 08:39:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][1050/1251]	eta 0:01:34 lr 0.000844	time 0.4646 (0.4682)	loss 3.9896 (3.6362)	grad_norm 1.4558 (1.2770)	mem 14852MB
[2022-11-06 08:39:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][1100/1251]	eta 0:01:10 lr 0.000844	time 0.4642 (0.4680)	loss 4.5036 (3.6366)	grad_norm 1.3976 (1.2771)	mem 14852MB
[2022-11-06 08:39:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][1150/1251]	eta 0:00:47 lr 0.000844	time 0.4640 (0.4679)	loss 3.5611 (3.6344)	grad_norm 1.2509 (1.2777)	mem 14852MB
[2022-11-06 08:40:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][1200/1251]	eta 0:00:23 lr 0.000844	time 0.4708 (0.4678)	loss 3.4696 (3.6242)	grad_norm 1.3817 (1.2760)	mem 14852MB
[2022-11-06 08:40:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [77/300][1250/1251]	eta 0:00:00 lr 0.000844	time 0.4588 (0.4676)	loss 3.5879 (3.6286)	grad_norm 1.2465 (1.2759)	mem 14852MB
[2022-11-06 08:40:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 77 training takes 0:09:45
[2022-11-06 08:40:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_77.pth saving......
[2022-11-06 08:40:39 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_77.pth saved !!!
[2022-11-06 08:40:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.616 (1.616)	Loss 1.1675 (1.1675)	Acc@1 73.145 (73.145)	Acc@5 90.918 (90.918)	Mem 14852MB
[2022-11-06 08:40:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.288 Acc@5 92.770
[2022-11-06 08:40:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.3%
[2022-11-06 08:40:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.563 (1.563)	Loss 0.9201 (0.9201)	Acc@1 78.223 (78.223)	Acc@5 94.531 (94.531)	Mem 14852MB
[2022-11-06 08:40:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.658 Acc@5 94.026
[2022-11-06 08:40:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.7%
[2022-11-06 08:40:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.66% at 77 epoch
[2022-11-06 08:40:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][0/1251]	eta 0:41:41 lr 0.000844	time 1.9997 (1.9997)	loss 4.0680 (4.0680)	grad_norm 1.3782 (1.3782)	mem 14852MB
[2022-11-06 08:41:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][50/1251]	eta 0:10:00 lr 0.000844	time 0.4624 (0.4996)	loss 2.8502 (3.6825)	grad_norm 1.2963 (1.2810)	mem 14852MB
[2022-11-06 08:41:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][100/1251]	eta 0:09:17 lr 0.000844	time 0.4647 (0.4841)	loss 4.0856 (3.6313)	grad_norm 1.3267 (1.2949)	mem 14852MB
[2022-11-06 08:42:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][150/1251]	eta 0:08:45 lr 0.000843	time 0.4665 (0.4777)	loss 2.8420 (3.6083)	grad_norm 1.2230 (1.2893)	mem 14852MB
[2022-11-06 08:42:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][200/1251]	eta 0:08:18 lr 0.000843	time 0.4557 (0.4748)	loss 4.0037 (3.5749)	grad_norm 1.2515 (1.2857)	mem 14852MB
[2022-11-06 08:42:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][250/1251]	eta 0:07:53 lr 0.000843	time 0.4657 (0.4726)	loss 3.6342 (3.5630)	grad_norm 1.1981 (1.2810)	mem 14852MB
[2022-11-06 08:43:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][300/1251]	eta 0:07:27 lr 0.000843	time 0.4633 (0.4710)	loss 2.9231 (3.5780)	grad_norm 1.2555 (1.2799)	mem 14852MB
[2022-11-06 08:43:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][350/1251]	eta 0:07:03 lr 0.000843	time 0.4710 (0.4703)	loss 2.4158 (3.5700)	grad_norm 1.2104 (1.2801)	mem 14852MB
[2022-11-06 08:44:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][400/1251]	eta 0:06:39 lr 0.000843	time 0.4667 (0.4697)	loss 4.5042 (3.5803)	grad_norm 1.5501 (1.2788)	mem 14852MB
[2022-11-06 08:44:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][450/1251]	eta 0:06:15 lr 0.000842	time 0.4570 (0.4690)	loss 3.6547 (3.5813)	grad_norm 1.3020 (1.2793)	mem 14852MB
[2022-11-06 08:44:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][500/1251]	eta 0:05:52 lr 0.000842	time 0.4604 (0.4689)	loss 4.3114 (3.5901)	grad_norm 1.2829 (1.2788)	mem 14852MB
[2022-11-06 08:45:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][550/1251]	eta 0:05:28 lr 0.000842	time 0.4577 (0.4687)	loss 3.5377 (3.5839)	grad_norm 1.3504 (inf)	mem 14852MB
[2022-11-06 08:45:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][600/1251]	eta 0:05:05 lr 0.000842	time 0.4609 (0.4685)	loss 2.7304 (3.5779)	grad_norm 1.5269 (inf)	mem 14852MB
[2022-11-06 08:46:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][650/1251]	eta 0:04:41 lr 0.000842	time 0.4610 (0.4682)	loss 3.7158 (3.5735)	grad_norm 1.2050 (inf)	mem 14852MB
[2022-11-06 08:46:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][700/1251]	eta 0:04:17 lr 0.000842	time 0.4623 (0.4680)	loss 3.8141 (3.5814)	grad_norm 1.4264 (inf)	mem 14852MB
[2022-11-06 08:46:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][750/1251]	eta 0:03:54 lr 0.000842	time 0.4638 (0.4680)	loss 3.9073 (3.5889)	grad_norm 1.6178 (inf)	mem 14852MB
[2022-11-06 08:47:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][800/1251]	eta 0:03:31 lr 0.000841	time 0.4715 (0.4680)	loss 2.7597 (3.5954)	grad_norm 1.2664 (inf)	mem 14852MB
[2022-11-06 08:47:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][850/1251]	eta 0:03:07 lr 0.000841	time 0.4586 (0.4679)	loss 3.6446 (3.5916)	grad_norm 1.1417 (inf)	mem 14852MB
[2022-11-06 08:47:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][900/1251]	eta 0:02:44 lr 0.000841	time 0.4601 (0.4678)	loss 3.8155 (3.6007)	grad_norm 1.2833 (inf)	mem 14852MB
[2022-11-06 08:48:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][950/1251]	eta 0:02:20 lr 0.000841	time 0.4539 (0.4676)	loss 3.4664 (3.6045)	grad_norm 1.1916 (inf)	mem 14852MB
[2022-11-06 08:48:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][1000/1251]	eta 0:01:57 lr 0.000841	time 0.4733 (0.4675)	loss 2.1780 (3.6070)	grad_norm 1.3354 (inf)	mem 14852MB
[2022-11-06 08:49:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][1050/1251]	eta 0:01:33 lr 0.000841	time 0.4608 (0.4676)	loss 3.3837 (3.6105)	grad_norm 1.2886 (inf)	mem 14852MB
[2022-11-06 08:49:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][1100/1251]	eta 0:01:10 lr 0.000841	time 0.4638 (0.4676)	loss 2.6273 (3.6090)	grad_norm 1.1979 (inf)	mem 14852MB
[2022-11-06 08:49:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][1150/1251]	eta 0:00:47 lr 0.000840	time 0.4635 (0.4675)	loss 3.5770 (3.6105)	grad_norm 1.3861 (inf)	mem 14852MB
[2022-11-06 08:50:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][1200/1251]	eta 0:00:23 lr 0.000840	time 0.4656 (0.4674)	loss 4.1138 (3.6100)	grad_norm 1.1928 (inf)	mem 14852MB
[2022-11-06 08:50:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [78/300][1250/1251]	eta 0:00:00 lr 0.000840	time 0.4575 (0.4672)	loss 3.8092 (3.6112)	grad_norm 1.2322 (inf)	mem 14852MB
[2022-11-06 08:50:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 78 training takes 0:09:44
[2022-11-06 08:50:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_78.pth saving......
[2022-11-06 08:50:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_78.pth saved !!!
[2022-11-06 08:50:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.583 (1.583)	Loss 1.1581 (1.1581)	Acc@1 74.023 (74.023)	Acc@5 91.895 (91.895)	Mem 14852MB
[2022-11-06 08:50:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.124 Acc@5 92.826
[2022-11-06 08:50:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.1%
[2022-11-06 08:50:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.560 (1.560)	Loss 1.0214 (1.0214)	Acc@1 75.391 (75.391)	Acc@5 92.969 (92.969)	Mem 14852MB
[2022-11-06 08:50:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.716 Acc@5 94.040
[2022-11-06 08:50:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.7%
[2022-11-06 08:50:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.72% at 78 epoch
[2022-11-06 08:51:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][0/1251]	eta 0:39:58 lr 0.000840	time 1.9172 (1.9172)	loss 2.8940 (2.8940)	grad_norm 1.1790 (1.1790)	mem 14852MB
[2022-11-06 08:51:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][50/1251]	eta 0:09:59 lr 0.000840	time 0.4699 (0.4995)	loss 2.3257 (3.5442)	grad_norm 1.3119 (1.2813)	mem 14852MB
[2022-11-06 08:51:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][100/1251]	eta 0:09:17 lr 0.000840	time 0.4643 (0.4848)	loss 3.2235 (3.5215)	grad_norm 1.2078 (1.2772)	mem 14852MB
[2022-11-06 08:52:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][150/1251]	eta 0:08:46 lr 0.000840	time 0.4677 (0.4782)	loss 3.7553 (3.5538)	grad_norm 1.1489 (1.2730)	mem 14852MB
[2022-11-06 08:52:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][200/1251]	eta 0:08:19 lr 0.000839	time 0.4592 (0.4749)	loss 3.4336 (3.5837)	grad_norm 1.4095 (1.2756)	mem 14852MB
[2022-11-06 08:52:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][250/1251]	eta 0:07:53 lr 0.000839	time 0.4663 (0.4733)	loss 2.7187 (3.5712)	grad_norm 1.2317 (1.2791)	mem 14852MB
[2022-11-06 08:53:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][300/1251]	eta 0:07:28 lr 0.000839	time 0.4619 (0.4721)	loss 3.3157 (3.5780)	grad_norm 1.3027 (1.2800)	mem 14852MB
[2022-11-06 08:53:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][350/1251]	eta 0:07:04 lr 0.000839	time 0.4640 (0.4713)	loss 3.7597 (3.5772)	grad_norm 1.2166 (1.2815)	mem 14852MB
[2022-11-06 08:54:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][400/1251]	eta 0:06:40 lr 0.000839	time 0.4753 (0.4707)	loss 2.7003 (3.5724)	grad_norm 1.4687 (1.2822)	mem 14852MB
[2022-11-06 08:54:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][450/1251]	eta 0:06:16 lr 0.000839	time 0.4593 (0.4701)	loss 4.3381 (3.5928)	grad_norm 1.2037 (1.2801)	mem 14852MB
[2022-11-06 08:54:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][500/1251]	eta 0:05:52 lr 0.000839	time 0.4625 (0.4698)	loss 3.2779 (3.5936)	grad_norm 1.1911 (1.2824)	mem 14852MB
[2022-11-06 08:55:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][550/1251]	eta 0:05:29 lr 0.000838	time 0.4701 (0.4699)	loss 4.0750 (3.6002)	grad_norm 1.2475 (1.2830)	mem 14852MB
[2022-11-06 08:55:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][600/1251]	eta 0:05:05 lr 0.000838	time 0.4778 (0.4695)	loss 3.4149 (3.6106)	grad_norm 1.4140 (1.2834)	mem 14852MB
[2022-11-06 08:56:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][650/1251]	eta 0:04:42 lr 0.000838	time 0.4555 (0.4692)	loss 3.0376 (3.6156)	grad_norm 1.2887 (1.2833)	mem 14852MB
[2022-11-06 08:56:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][700/1251]	eta 0:04:18 lr 0.000838	time 0.4593 (0.4689)	loss 4.0795 (3.6188)	grad_norm 1.3447 (1.2875)	mem 14852MB
[2022-11-06 08:56:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][750/1251]	eta 0:03:54 lr 0.000838	time 0.4735 (0.4687)	loss 3.1508 (3.6143)	grad_norm 1.2585 (1.2867)	mem 14852MB
[2022-11-06 08:57:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][800/1251]	eta 0:03:31 lr 0.000838	time 0.4542 (0.4688)	loss 3.5342 (3.6082)	grad_norm 1.3950 (1.2863)	mem 14852MB
[2022-11-06 08:57:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][850/1251]	eta 0:03:07 lr 0.000837	time 0.4686 (0.4685)	loss 3.8287 (3.6095)	grad_norm 1.3680 (1.2879)	mem 14852MB
[2022-11-06 08:58:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][900/1251]	eta 0:02:44 lr 0.000837	time 0.4662 (0.4684)	loss 3.3554 (3.6031)	grad_norm 1.2858 (1.2871)	mem 14852MB
[2022-11-06 08:58:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][950/1251]	eta 0:02:20 lr 0.000837	time 0.4653 (0.4683)	loss 3.9146 (3.5994)	grad_norm 1.2807 (1.2883)	mem 14852MB
[2022-11-06 08:58:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][1000/1251]	eta 0:01:57 lr 0.000837	time 0.4589 (0.4681)	loss 3.4384 (3.5995)	grad_norm 1.3606 (1.2872)	mem 14852MB
[2022-11-06 08:59:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][1050/1251]	eta 0:01:34 lr 0.000837	time 0.4709 (0.4683)	loss 4.1395 (3.6005)	grad_norm 1.1935 (1.2865)	mem 14852MB
[2022-11-06 08:59:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][1100/1251]	eta 0:01:10 lr 0.000837	time 0.4730 (0.4682)	loss 4.3864 (3.6036)	grad_norm 1.2472 (1.2890)	mem 14852MB
[2022-11-06 08:59:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][1150/1251]	eta 0:00:47 lr 0.000837	time 0.4625 (0.4680)	loss 4.0925 (3.6056)	grad_norm 1.1009 (1.2876)	mem 14852MB
[2022-11-06 09:00:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][1200/1251]	eta 0:00:23 lr 0.000836	time 0.4606 (0.4679)	loss 3.2453 (3.6044)	grad_norm 1.4549 (1.2891)	mem 14852MB
[2022-11-06 09:00:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [79/300][1250/1251]	eta 0:00:00 lr 0.000836	time 0.4574 (0.4679)	loss 4.2051 (3.6008)	grad_norm 1.3330 (1.2868)	mem 14852MB
[2022-11-06 09:00:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 79 training takes 0:09:45
[2022-11-06 09:00:44 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_79.pth saving......
[2022-11-06 09:00:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_79.pth saved !!!
[2022-11-06 09:00:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.692 (1.692)	Loss 0.9997 (0.9997)	Acc@1 76.270 (76.270)	Acc@5 93.750 (93.750)	Mem 14852MB
[2022-11-06 09:00:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.920 Acc@5 92.804
[2022-11-06 09:00:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.9%
[2022-11-06 09:00:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.611 (1.611)	Loss 0.8976 (0.8976)	Acc@1 79.395 (79.395)	Acc@5 94.336 (94.336)	Mem 14852MB
[2022-11-06 09:01:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.744 Acc@5 94.086
[2022-11-06 09:01:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.7%
[2022-11-06 09:01:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.74% at 79 epoch
[2022-11-06 09:01:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][0/1251]	eta 0:40:48 lr 0.000836	time 1.9571 (1.9571)	loss 2.7878 (2.7878)	grad_norm 1.2751 (1.2751)	mem 14852MB
[2022-11-06 09:01:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][50/1251]	eta 0:09:59 lr 0.000836	time 0.5527 (0.4994)	loss 3.9142 (3.4785)	grad_norm 1.1992 (nan)	mem 14852MB
[2022-11-06 09:01:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][100/1251]	eta 0:09:17 lr 0.000836	time 0.4627 (0.4845)	loss 3.9020 (3.5740)	grad_norm 1.2125 (nan)	mem 14852MB
[2022-11-06 09:02:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][150/1251]	eta 0:08:47 lr 0.000836	time 0.4808 (0.4788)	loss 3.8052 (3.5850)	grad_norm 1.1704 (nan)	mem 14852MB
[2022-11-06 09:02:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][200/1251]	eta 0:08:20 lr 0.000836	time 0.4679 (0.4760)	loss 4.0812 (3.6128)	grad_norm 1.3258 (nan)	mem 14852MB
[2022-11-06 09:03:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][250/1251]	eta 0:07:54 lr 0.000835	time 0.4737 (0.4738)	loss 3.7485 (3.6211)	grad_norm 1.3937 (nan)	mem 14852MB
[2022-11-06 09:03:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][300/1251]	eta 0:07:29 lr 0.000835	time 0.4658 (0.4725)	loss 3.0861 (3.6098)	grad_norm 1.3421 (nan)	mem 14852MB
[2022-11-06 09:03:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][350/1251]	eta 0:07:05 lr 0.000835	time 0.4699 (0.4717)	loss 3.8587 (3.6032)	grad_norm 1.3656 (nan)	mem 14852MB
[2022-11-06 09:04:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][400/1251]	eta 0:06:41 lr 0.000835	time 0.4676 (0.4713)	loss 3.7413 (3.5940)	grad_norm 1.2202 (nan)	mem 14852MB
[2022-11-06 09:04:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][450/1251]	eta 0:06:16 lr 0.000835	time 0.4738 (0.4705)	loss 3.3416 (3.5920)	grad_norm 1.3798 (nan)	mem 14852MB
[2022-11-06 09:04:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][500/1251]	eta 0:05:52 lr 0.000835	time 0.4528 (0.4700)	loss 4.0031 (3.5868)	grad_norm 1.2581 (nan)	mem 14852MB
[2022-11-06 09:05:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][550/1251]	eta 0:05:29 lr 0.000835	time 0.4631 (0.4697)	loss 3.8567 (3.5812)	grad_norm 1.1812 (nan)	mem 14852MB
[2022-11-06 09:05:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][600/1251]	eta 0:05:05 lr 0.000834	time 0.4666 (0.4694)	loss 2.9264 (3.5807)	grad_norm 1.2714 (nan)	mem 14852MB
[2022-11-06 09:06:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][650/1251]	eta 0:04:41 lr 0.000834	time 0.4576 (0.4691)	loss 3.4237 (3.5844)	grad_norm 1.4095 (nan)	mem 14852MB
[2022-11-06 09:06:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][700/1251]	eta 0:04:18 lr 0.000834	time 0.4606 (0.4690)	loss 2.8451 (3.5846)	grad_norm 1.1233 (nan)	mem 14852MB
[2022-11-06 09:06:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][750/1251]	eta 0:03:54 lr 0.000834	time 0.4696 (0.4687)	loss 3.4967 (3.5870)	grad_norm 1.3378 (nan)	mem 14852MB
[2022-11-06 09:07:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][800/1251]	eta 0:03:31 lr 0.000834	time 0.4630 (0.4688)	loss 3.9863 (3.5894)	grad_norm 1.3117 (nan)	mem 14852MB
[2022-11-06 09:07:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][850/1251]	eta 0:03:07 lr 0.000834	time 0.4588 (0.4685)	loss 3.9586 (3.5846)	grad_norm 1.1557 (nan)	mem 14852MB
[2022-11-06 09:08:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][900/1251]	eta 0:02:44 lr 0.000833	time 0.4705 (0.4686)	loss 3.8511 (3.5820)	grad_norm 1.4026 (nan)	mem 14852MB
[2022-11-06 09:08:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][950/1251]	eta 0:02:20 lr 0.000833	time 0.4676 (0.4684)	loss 4.1505 (3.5839)	grad_norm 1.1921 (nan)	mem 14852MB
[2022-11-06 09:08:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][1000/1251]	eta 0:01:57 lr 0.000833	time 0.4679 (0.4682)	loss 2.6173 (3.5834)	grad_norm 1.5237 (nan)	mem 14852MB
[2022-11-06 09:09:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][1050/1251]	eta 0:01:34 lr 0.000833	time 0.4668 (0.4681)	loss 3.6575 (3.5831)	grad_norm 1.1017 (nan)	mem 14852MB
[2022-11-06 09:09:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][1100/1251]	eta 0:01:10 lr 0.000833	time 0.4694 (0.4680)	loss 2.7950 (3.5878)	grad_norm 1.3143 (nan)	mem 14852MB
[2022-11-06 09:10:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][1150/1251]	eta 0:00:47 lr 0.000833	time 0.4728 (0.4680)	loss 2.8730 (3.5819)	grad_norm 1.2496 (nan)	mem 14852MB
[2022-11-06 09:10:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][1200/1251]	eta 0:00:23 lr 0.000833	time 0.4628 (0.4679)	loss 3.8109 (3.5866)	grad_norm 1.3010 (nan)	mem 14852MB
[2022-11-06 09:10:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [80/300][1250/1251]	eta 0:00:00 lr 0.000832	time 0.4566 (0.4677)	loss 3.6224 (3.5936)	grad_norm 1.2251 (nan)	mem 14852MB
[2022-11-06 09:10:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 80 training takes 0:09:45
[2022-11-06 09:10:48 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_80.pth saving......
[2022-11-06 09:10:48 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_80.pth saved !!!
[2022-11-06 09:10:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.470 (1.470)	Loss 1.1893 (1.1893)	Acc@1 72.754 (72.754)	Acc@5 92.285 (92.285)	Mem 14852MB
[2022-11-06 09:10:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.886 Acc@5 92.574
[2022-11-06 09:10:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.9%
[2022-11-06 09:10:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.658 (1.658)	Loss 0.9551 (0.9551)	Acc@1 77.148 (77.148)	Acc@5 93.945 (93.945)	Mem 14852MB
[2022-11-06 09:11:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.862 Acc@5 94.086
[2022-11-06 09:11:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.9%
[2022-11-06 09:11:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.86% at 80 epoch
[2022-11-06 09:11:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][0/1251]	eta 0:40:09 lr 0.000832	time 1.9264 (1.9264)	loss 2.9117 (2.9117)	grad_norm 1.2914 (1.2914)	mem 14852MB
[2022-11-06 09:11:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][50/1251]	eta 0:10:00 lr 0.000832	time 0.4629 (0.4997)	loss 4.2403 (3.5497)	grad_norm 1.0988 (1.2879)	mem 14852MB
[2022-11-06 09:11:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][100/1251]	eta 0:09:16 lr 0.000832	time 0.4648 (0.4839)	loss 3.8344 (3.5872)	grad_norm 1.2637 (1.2867)	mem 14852MB
[2022-11-06 09:12:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][150/1251]	eta 0:08:46 lr 0.000832	time 0.4628 (0.4782)	loss 3.9087 (3.5720)	grad_norm 1.3229 (1.2896)	mem 14852MB
[2022-11-06 09:12:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][200/1251]	eta 0:08:19 lr 0.000832	time 0.4663 (0.4754)	loss 3.1837 (3.5904)	grad_norm 1.5166 (1.2868)	mem 14852MB
[2022-11-06 09:13:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][250/1251]	eta 0:07:54 lr 0.000832	time 0.4880 (0.4738)	loss 3.3560 (3.5877)	grad_norm 1.2850 (1.2880)	mem 14852MB
[2022-11-06 09:13:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][300/1251]	eta 0:07:29 lr 0.000831	time 0.4653 (0.4730)	loss 3.7864 (3.5881)	grad_norm 1.3162 (1.2853)	mem 14852MB
[2022-11-06 09:13:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][350/1251]	eta 0:07:05 lr 0.000831	time 0.4672 (0.4723)	loss 3.2584 (3.5821)	grad_norm 1.1466 (1.2819)	mem 14852MB
[2022-11-06 09:14:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][400/1251]	eta 0:06:41 lr 0.000831	time 0.4636 (0.4715)	loss 4.2343 (3.5970)	grad_norm 1.3718 (1.2831)	mem 14852MB
[2022-11-06 09:14:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][450/1251]	eta 0:06:17 lr 0.000831	time 0.4614 (0.4708)	loss 2.9706 (3.5963)	grad_norm 1.3493 (1.2836)	mem 14852MB
[2022-11-06 09:15:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][500/1251]	eta 0:05:53 lr 0.000831	time 0.4692 (0.4703)	loss 4.0313 (3.6032)	grad_norm 1.2277 (1.2835)	mem 14852MB
[2022-11-06 09:15:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][550/1251]	eta 0:05:29 lr 0.000831	time 0.4601 (0.4701)	loss 4.2219 (3.5960)	grad_norm 1.2901 (1.2836)	mem 14852MB
[2022-11-06 09:15:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][600/1251]	eta 0:05:05 lr 0.000830	time 0.4611 (0.4699)	loss 3.8128 (3.6067)	grad_norm 1.2388 (1.2816)	mem 14852MB
[2022-11-06 09:16:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][650/1251]	eta 0:04:42 lr 0.000830	time 0.4576 (0.4697)	loss 3.3001 (3.6161)	grad_norm 1.1843 (1.2827)	mem 14852MB
[2022-11-06 09:16:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][700/1251]	eta 0:04:18 lr 0.000830	time 0.4572 (0.4695)	loss 4.0403 (3.6171)	grad_norm 1.2468 (1.2839)	mem 14852MB
[2022-11-06 09:16:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][750/1251]	eta 0:03:55 lr 0.000830	time 0.4598 (0.4694)	loss 3.6509 (3.6124)	grad_norm 1.3111 (1.2833)	mem 14852MB
[2022-11-06 09:17:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][800/1251]	eta 0:03:31 lr 0.000830	time 0.4597 (0.4694)	loss 4.5524 (3.6059)	grad_norm 1.3111 (1.2852)	mem 14852MB
[2022-11-06 09:17:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][850/1251]	eta 0:03:08 lr 0.000830	time 0.4693 (0.4694)	loss 3.8601 (3.6153)	grad_norm 1.3228 (1.2861)	mem 14852MB
[2022-11-06 09:18:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][900/1251]	eta 0:02:44 lr 0.000830	time 0.4713 (0.4693)	loss 3.9239 (3.6141)	grad_norm 1.2434 (1.2854)	mem 14852MB
[2022-11-06 09:18:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][950/1251]	eta 0:02:21 lr 0.000829	time 0.4663 (0.4691)	loss 4.1358 (3.6096)	grad_norm 1.3505 (1.2840)	mem 14852MB
[2022-11-06 09:18:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][1000/1251]	eta 0:01:57 lr 0.000829	time 0.4601 (0.4690)	loss 3.8363 (3.6102)	grad_norm 1.2141 (1.2838)	mem 14852MB
[2022-11-06 09:19:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][1050/1251]	eta 0:01:34 lr 0.000829	time 0.4610 (0.4690)	loss 4.1018 (3.6104)	grad_norm 1.3185 (1.2847)	mem 14852MB
[2022-11-06 09:19:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][1100/1251]	eta 0:01:10 lr 0.000829	time 0.4709 (0.4689)	loss 3.2050 (3.6032)	grad_norm 1.4933 (1.2848)	mem 14852MB
[2022-11-06 09:20:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][1150/1251]	eta 0:00:47 lr 0.000829	time 0.4581 (0.4688)	loss 4.3890 (3.5987)	grad_norm 1.3446 (1.2848)	mem 14852MB
[2022-11-06 09:20:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][1200/1251]	eta 0:00:23 lr 0.000829	time 0.4648 (0.4687)	loss 2.8565 (3.6011)	grad_norm 1.1999 (1.2858)	mem 14852MB
[2022-11-06 09:20:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [81/300][1250/1251]	eta 0:00:00 lr 0.000828	time 0.4578 (0.4685)	loss 3.9778 (3.6033)	grad_norm 1.2028 (1.2852)	mem 14852MB
[2022-11-06 09:20:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 81 training takes 0:09:46
[2022-11-06 09:20:52 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_81.pth saving......
[2022-11-06 09:20:53 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_81.pth saved !!!
[2022-11-06 09:20:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.627 (1.627)	Loss 1.1196 (1.1196)	Acc@1 74.512 (74.512)	Acc@5 91.992 (91.992)	Mem 14852MB
[2022-11-06 09:21:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.948 Acc@5 92.836
[2022-11-06 09:21:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.9%
[2022-11-06 09:21:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.750 (1.750)	Loss 0.9698 (0.9698)	Acc@1 77.832 (77.832)	Acc@5 93.262 (93.262)	Mem 14852MB
[2022-11-06 09:21:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.916 Acc@5 94.118
[2022-11-06 09:21:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.9%
[2022-11-06 09:21:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.92% at 81 epoch
[2022-11-06 09:21:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][0/1251]	eta 0:40:04 lr 0.000828	time 1.9218 (1.9218)	loss 2.1145 (2.1145)	grad_norm 1.2383 (1.2383)	mem 14852MB
[2022-11-06 09:21:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][50/1251]	eta 0:09:58 lr 0.000828	time 0.4649 (0.4983)	loss 4.0339 (3.5928)	grad_norm 1.2722 (1.2576)	mem 14852MB
[2022-11-06 09:21:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][100/1251]	eta 0:09:17 lr 0.000828	time 0.4637 (0.4841)	loss 3.7099 (3.5716)	grad_norm 1.2728 (1.2697)	mem 14852MB
[2022-11-06 09:22:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][150/1251]	eta 0:08:46 lr 0.000828	time 0.4689 (0.4781)	loss 3.5629 (3.5920)	grad_norm 1.3837 (1.2824)	mem 14852MB
[2022-11-06 09:22:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][200/1251]	eta 0:08:19 lr 0.000828	time 0.4590 (0.4755)	loss 2.7193 (3.6050)	grad_norm 1.2353 (1.2837)	mem 14852MB
[2022-11-06 09:23:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][250/1251]	eta 0:07:53 lr 0.000828	time 0.4694 (0.4735)	loss 2.8553 (3.5892)	grad_norm 1.1611 (1.2824)	mem 14852MB
[2022-11-06 09:23:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][300/1251]	eta 0:07:28 lr 0.000828	time 0.4582 (0.4720)	loss 3.7027 (3.5988)	grad_norm 1.3029 (1.2814)	mem 14852MB
[2022-11-06 09:23:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][350/1251]	eta 0:07:04 lr 0.000827	time 0.4653 (0.4714)	loss 4.1762 (3.6019)	grad_norm 1.4021 (1.2867)	mem 14852MB
[2022-11-06 09:24:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][400/1251]	eta 0:06:40 lr 0.000827	time 0.4777 (0.4706)	loss 2.7679 (3.6030)	grad_norm 1.1931 (1.2881)	mem 14852MB
[2022-11-06 09:24:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][450/1251]	eta 0:06:16 lr 0.000827	time 0.4843 (0.4701)	loss 3.8306 (3.5954)	grad_norm 1.1992 (1.2875)	mem 14852MB
[2022-11-06 09:25:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][500/1251]	eta 0:05:52 lr 0.000827	time 0.4580 (0.4696)	loss 3.6565 (3.5865)	grad_norm 1.2771 (1.2867)	mem 14852MB
[2022-11-06 09:25:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][550/1251]	eta 0:05:29 lr 0.000827	time 0.4671 (0.4695)	loss 3.4040 (3.5916)	grad_norm 1.2447 (1.2877)	mem 14852MB
[2022-11-06 09:25:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][600/1251]	eta 0:05:05 lr 0.000827	time 0.4646 (0.4694)	loss 4.1593 (3.5857)	grad_norm 1.2321 (1.2878)	mem 14852MB
[2022-11-06 09:26:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][650/1251]	eta 0:04:41 lr 0.000826	time 0.4583 (0.4690)	loss 4.2918 (3.5958)	grad_norm 1.2588 (1.2896)	mem 14852MB
[2022-11-06 09:26:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][700/1251]	eta 0:04:18 lr 0.000826	time 0.4706 (0.4690)	loss 4.1453 (3.5968)	grad_norm 1.3360 (1.2896)	mem 14852MB
[2022-11-06 09:27:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][750/1251]	eta 0:03:54 lr 0.000826	time 0.4609 (0.4688)	loss 3.9255 (3.5952)	grad_norm 1.1926 (1.2870)	mem 14852MB
[2022-11-06 09:27:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][800/1251]	eta 0:03:31 lr 0.000826	time 0.4759 (0.4688)	loss 2.7734 (3.5939)	grad_norm 1.2481 (1.2881)	mem 14852MB
[2022-11-06 09:27:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][850/1251]	eta 0:03:07 lr 0.000826	time 0.4542 (0.4687)	loss 4.1747 (3.6049)	grad_norm 1.2031 (1.2897)	mem 14852MB
[2022-11-06 09:28:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][900/1251]	eta 0:02:44 lr 0.000826	time 0.4687 (0.4687)	loss 3.6423 (3.5988)	grad_norm 1.3435 (1.2908)	mem 14852MB
[2022-11-06 09:28:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][950/1251]	eta 0:02:21 lr 0.000825	time 0.4737 (0.4686)	loss 3.7751 (3.5982)	grad_norm 1.2044 (1.2913)	mem 14852MB
[2022-11-06 09:28:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][1000/1251]	eta 0:01:57 lr 0.000825	time 0.4616 (0.4685)	loss 2.8236 (3.6007)	grad_norm 1.3099 (1.2927)	mem 14852MB
[2022-11-06 09:29:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][1050/1251]	eta 0:01:34 lr 0.000825	time 0.4612 (0.4685)	loss 4.4723 (3.5949)	grad_norm 1.3251 (1.2921)	mem 14852MB
[2022-11-06 09:29:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][1100/1251]	eta 0:01:10 lr 0.000825	time 0.4615 (0.4684)	loss 3.9611 (3.5971)	grad_norm 1.3913 (1.2921)	mem 14852MB
[2022-11-06 09:30:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][1150/1251]	eta 0:00:47 lr 0.000825	time 0.4624 (0.4683)	loss 2.6323 (3.5951)	grad_norm 1.2231 (1.2919)	mem 14852MB
[2022-11-06 09:30:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][1200/1251]	eta 0:00:23 lr 0.000825	time 0.4669 (0.4684)	loss 4.5601 (3.5900)	grad_norm 1.4675 (1.2919)	mem 14852MB
[2022-11-06 09:30:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [82/300][1250/1251]	eta 0:00:00 lr 0.000825	time 0.4566 (0.4682)	loss 3.8515 (3.5902)	grad_norm 1.2051 (1.2924)	mem 14852MB
[2022-11-06 09:30:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 82 training takes 0:09:45
[2022-11-06 09:30:56 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_82.pth saving......
[2022-11-06 09:30:57 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_82.pth saved !!!
[2022-11-06 09:30:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.495 (1.495)	Loss 1.0950 (1.0950)	Acc@1 73.926 (73.926)	Acc@5 92.871 (92.871)	Mem 14852MB
[2022-11-06 09:31:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.090 Acc@5 92.988
[2022-11-06 09:31:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.1%
[2022-11-06 09:31:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.640 (1.640)	Loss 0.8820 (0.8820)	Acc@1 79.395 (79.395)	Acc@5 94.727 (94.727)	Mem 14852MB
[2022-11-06 09:31:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.980 Acc@5 94.138
[2022-11-06 09:31:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.0%
[2022-11-06 09:31:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.98% at 82 epoch
[2022-11-06 09:31:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][0/1251]	eta 0:43:32 lr 0.000825	time 2.0880 (2.0880)	loss 3.2196 (3.2196)	grad_norm 1.1301 (1.1301)	mem 14852MB
[2022-11-06 09:31:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][50/1251]	eta 0:10:08 lr 0.000824	time 0.4787 (0.5065)	loss 4.3635 (3.6235)	grad_norm 1.2488 (1.3198)	mem 14852MB
[2022-11-06 09:32:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][100/1251]	eta 0:09:21 lr 0.000824	time 0.4694 (0.4878)	loss 3.3607 (3.6743)	grad_norm 1.3292 (1.2997)	mem 14852MB
[2022-11-06 09:32:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][150/1251]	eta 0:08:49 lr 0.000824	time 0.4716 (0.4814)	loss 3.7147 (3.6454)	grad_norm 1.1323 (1.3044)	mem 14852MB
[2022-11-06 09:32:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][200/1251]	eta 0:08:22 lr 0.000824	time 0.4712 (0.4779)	loss 3.2790 (3.6203)	grad_norm 1.3265 (1.3024)	mem 14852MB
[2022-11-06 09:33:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][250/1251]	eta 0:07:55 lr 0.000824	time 0.4715 (0.4755)	loss 2.8935 (3.6232)	grad_norm 1.2384 (1.2978)	mem 14852MB
[2022-11-06 09:33:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][300/1251]	eta 0:07:31 lr 0.000824	time 0.4670 (0.4743)	loss 4.1685 (3.6116)	grad_norm 1.2958 (1.2966)	mem 14852MB
[2022-11-06 09:34:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][350/1251]	eta 0:07:06 lr 0.000823	time 0.4639 (0.4730)	loss 3.6094 (3.5982)	grad_norm 1.3053 (1.2966)	mem 14852MB
[2022-11-06 09:34:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][400/1251]	eta 0:06:42 lr 0.000823	time 0.4547 (0.4726)	loss 3.3704 (3.5894)	grad_norm 1.5184 (1.2949)	mem 14852MB
[2022-11-06 09:34:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][450/1251]	eta 0:06:17 lr 0.000823	time 0.4679 (0.4718)	loss 4.1838 (3.5849)	grad_norm 1.2095 (1.2960)	mem 14852MB
[2022-11-06 09:35:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][500/1251]	eta 0:05:53 lr 0.000823	time 0.4646 (0.4713)	loss 3.2387 (3.5780)	grad_norm 1.3372 (1.2955)	mem 14852MB
[2022-11-06 09:35:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][550/1251]	eta 0:05:30 lr 0.000823	time 0.4668 (0.4710)	loss 3.6389 (3.5776)	grad_norm 1.4911 (1.2950)	mem 14852MB
[2022-11-06 09:35:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][600/1251]	eta 0:05:06 lr 0.000823	time 0.4717 (0.4707)	loss 2.6994 (3.5786)	grad_norm 1.3208 (1.2939)	mem 14852MB
[2022-11-06 09:36:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][650/1251]	eta 0:04:42 lr 0.000822	time 0.4733 (0.4704)	loss 4.4518 (3.5875)	grad_norm 1.3556 (1.2937)	mem 14852MB
[2022-11-06 09:36:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][700/1251]	eta 0:04:19 lr 0.000822	time 0.4577 (0.4703)	loss 2.8238 (3.5837)	grad_norm 1.1880 (1.2946)	mem 14852MB
[2022-11-06 09:37:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][750/1251]	eta 0:03:55 lr 0.000822	time 0.4601 (0.4701)	loss 3.8705 (3.5849)	grad_norm 1.3077 (1.2957)	mem 14852MB
[2022-11-06 09:37:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][800/1251]	eta 0:03:32 lr 0.000822	time 0.4613 (0.4701)	loss 3.1208 (3.5806)	grad_norm 1.2556 (1.2953)	mem 14852MB
[2022-11-06 09:37:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][850/1251]	eta 0:03:08 lr 0.000822	time 0.4758 (0.4699)	loss 3.6249 (3.5763)	grad_norm 1.3877 (1.2947)	mem 14852MB
[2022-11-06 09:38:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][900/1251]	eta 0:02:44 lr 0.000822	time 0.4648 (0.4697)	loss 2.8856 (3.5805)	grad_norm 1.2249 (1.2932)	mem 14852MB
[2022-11-06 09:38:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][950/1251]	eta 0:02:21 lr 0.000821	time 0.4538 (0.4696)	loss 3.6419 (3.5815)	grad_norm 1.3113 (1.2939)	mem 14852MB
[2022-11-06 09:39:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][1000/1251]	eta 0:01:57 lr 0.000821	time 0.4542 (0.4695)	loss 3.9876 (3.5842)	grad_norm 1.2548 (1.2945)	mem 14852MB
[2022-11-06 09:39:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][1050/1251]	eta 0:01:34 lr 0.000821	time 0.4549 (0.4695)	loss 4.0050 (3.5760)	grad_norm 1.2432 (1.2937)	mem 14852MB
[2022-11-06 09:39:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][1100/1251]	eta 0:01:10 lr 0.000821	time 0.4697 (0.4693)	loss 3.1840 (3.5765)	grad_norm 1.2718 (1.2936)	mem 14852MB
[2022-11-06 09:40:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][1150/1251]	eta 0:00:47 lr 0.000821	time 0.4627 (0.4693)	loss 2.8612 (3.5770)	grad_norm 1.2916 (inf)	mem 14852MB
[2022-11-06 09:40:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][1200/1251]	eta 0:00:23 lr 0.000821	time 0.4751 (0.4692)	loss 2.9157 (3.5788)	grad_norm 1.4723 (inf)	mem 14852MB
[2022-11-06 09:41:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [83/300][1250/1251]	eta 0:00:00 lr 0.000821	time 0.4570 (0.4690)	loss 3.7568 (3.5781)	grad_norm 1.2768 (inf)	mem 14852MB
[2022-11-06 09:41:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 83 training takes 0:09:46
[2022-11-06 09:41:01 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_83.pth saving......
[2022-11-06 09:41:02 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_83.pth saved !!!
[2022-11-06 09:41:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.508 (1.508)	Loss 1.1136 (1.1136)	Acc@1 74.023 (74.023)	Acc@5 92.383 (92.383)	Mem 14852MB
[2022-11-06 09:41:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.166 Acc@5 92.874
[2022-11-06 09:41:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.2%
[2022-11-06 09:41:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.620 (1.620)	Loss 0.8511 (0.8511)	Acc@1 79.785 (79.785)	Acc@5 95.215 (95.215)	Mem 14852MB
[2022-11-06 09:41:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.040 Acc@5 94.192
[2022-11-06 09:41:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.0%
[2022-11-06 09:41:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.04% at 83 epoch
[2022-11-06 09:41:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][0/1251]	eta 0:39:46 lr 0.000821	time 1.9074 (1.9074)	loss 3.6260 (3.6260)	grad_norm 1.2908 (1.2908)	mem 14852MB
[2022-11-06 09:41:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][50/1251]	eta 0:10:03 lr 0.000820	time 0.4703 (0.5023)	loss 3.3647 (3.5630)	grad_norm 1.1540 (1.3179)	mem 14852MB
[2022-11-06 09:42:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][100/1251]	eta 0:09:18 lr 0.000820	time 0.4541 (0.4855)	loss 2.9995 (3.5795)	grad_norm 1.3024 (1.3104)	mem 14852MB
[2022-11-06 09:42:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][150/1251]	eta 0:08:48 lr 0.000820	time 0.4550 (0.4796)	loss 2.9023 (3.5924)	grad_norm 1.1831 (1.3057)	mem 14852MB
[2022-11-06 09:42:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][200/1251]	eta 0:08:21 lr 0.000820	time 0.4723 (0.4768)	loss 4.0664 (3.5819)	grad_norm 1.1025 (1.3009)	mem 14852MB
[2022-11-06 09:43:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][250/1251]	eta 0:07:55 lr 0.000820	time 0.4750 (0.4753)	loss 3.0614 (3.5503)	grad_norm 1.1555 (1.2982)	mem 14852MB
[2022-11-06 09:43:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][300/1251]	eta 0:07:30 lr 0.000820	time 0.4707 (0.4739)	loss 2.3727 (3.5566)	grad_norm 1.3658 (1.3005)	mem 14852MB
[2022-11-06 09:44:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][350/1251]	eta 0:07:06 lr 0.000819	time 0.4543 (0.4729)	loss 2.6918 (3.5672)	grad_norm 1.3032 (1.3034)	mem 14852MB
[2022-11-06 09:44:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][400/1251]	eta 0:06:41 lr 0.000819	time 0.4712 (0.4720)	loss 2.8593 (3.5769)	grad_norm 1.3136 (1.3017)	mem 14852MB
[2022-11-06 09:44:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][450/1251]	eta 0:06:17 lr 0.000819	time 0.4683 (0.4717)	loss 3.7208 (3.5810)	grad_norm 1.1381 (1.3014)	mem 14852MB
[2022-11-06 09:45:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][500/1251]	eta 0:05:53 lr 0.000819	time 0.4619 (0.4712)	loss 3.7692 (3.5884)	grad_norm 1.1503 (1.3045)	mem 14852MB
[2022-11-06 09:45:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][550/1251]	eta 0:05:30 lr 0.000819	time 0.4777 (0.4709)	loss 3.6051 (3.5856)	grad_norm 1.3033 (1.3037)	mem 14852MB
[2022-11-06 09:46:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][600/1251]	eta 0:05:06 lr 0.000819	time 0.4728 (0.4706)	loss 3.8541 (3.5775)	grad_norm 1.2957 (1.3039)	mem 14852MB
[2022-11-06 09:46:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][650/1251]	eta 0:04:42 lr 0.000818	time 0.4562 (0.4703)	loss 3.8798 (3.5779)	grad_norm 1.1556 (1.3029)	mem 14852MB
[2022-11-06 09:46:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][700/1251]	eta 0:04:19 lr 0.000818	time 0.4666 (0.4701)	loss 3.0312 (3.5841)	grad_norm 1.3144 (1.3022)	mem 14852MB
[2022-11-06 09:47:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][750/1251]	eta 0:03:55 lr 0.000818	time 0.4663 (0.4702)	loss 3.8731 (3.5870)	grad_norm 1.3729 (1.3019)	mem 14852MB
[2022-11-06 09:47:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][800/1251]	eta 0:03:31 lr 0.000818	time 0.4644 (0.4700)	loss 3.6812 (3.5867)	grad_norm 1.3550 (1.3016)	mem 14852MB
[2022-11-06 09:47:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][850/1251]	eta 0:03:08 lr 0.000818	time 0.4618 (0.4699)	loss 3.9138 (3.5946)	grad_norm 1.2752 (1.3013)	mem 14852MB
[2022-11-06 09:48:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][900/1251]	eta 0:02:44 lr 0.000818	time 0.5470 (0.4698)	loss 3.7340 (3.5948)	grad_norm 1.4905 (1.3029)	mem 14852MB
[2022-11-06 09:48:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][950/1251]	eta 0:02:21 lr 0.000817	time 0.5464 (0.4696)	loss 3.9505 (3.5947)	grad_norm 1.3759 (1.3033)	mem 14852MB
[2022-11-06 09:49:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][1000/1251]	eta 0:01:57 lr 0.000817	time 0.4707 (0.4695)	loss 2.5162 (3.5940)	grad_norm 1.3504 (1.3031)	mem 14852MB
[2022-11-06 09:49:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][1050/1251]	eta 0:01:34 lr 0.000817	time 0.4701 (0.4695)	loss 3.5267 (3.5938)	grad_norm 1.3565 (1.3046)	mem 14852MB
[2022-11-06 09:49:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][1100/1251]	eta 0:01:10 lr 0.000817	time 0.4636 (0.4693)	loss 2.4741 (3.5912)	grad_norm 1.1859 (1.3026)	mem 14852MB
[2022-11-06 09:50:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][1150/1251]	eta 0:00:47 lr 0.000817	time 0.4746 (0.4692)	loss 3.6395 (3.5908)	grad_norm 1.2319 (1.3007)	mem 14852MB
[2022-11-06 09:50:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][1200/1251]	eta 0:00:23 lr 0.000817	time 0.4569 (0.4692)	loss 4.2655 (3.5931)	grad_norm 1.5637 (1.2995)	mem 14852MB
[2022-11-06 09:51:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [84/300][1250/1251]	eta 0:00:00 lr 0.000817	time 0.4571 (0.4690)	loss 3.8287 (3.5927)	grad_norm 1.2664 (1.2992)	mem 14852MB
[2022-11-06 09:51:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 84 training takes 0:09:46
[2022-11-06 09:51:06 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_84.pth saving......
[2022-11-06 09:51:07 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_84.pth saved !!!
[2022-11-06 09:51:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.838 (1.838)	Loss 1.0743 (1.0743)	Acc@1 77.051 (77.051)	Acc@5 92.969 (92.969)	Mem 14852MB
[2022-11-06 09:51:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.166 Acc@5 92.874
[2022-11-06 09:51:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.2%
[2022-11-06 09:51:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.573 (1.573)	Loss 0.9244 (0.9244)	Acc@1 77.637 (77.637)	Acc@5 94.238 (94.238)	Mem 14852MB
[2022-11-06 09:51:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.092 Acc@5 94.198
[2022-11-06 09:51:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.1%
[2022-11-06 09:51:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.09% at 84 epoch
[2022-11-06 09:51:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][0/1251]	eta 0:40:24 lr 0.000817	time 1.9379 (1.9379)	loss 2.6081 (2.6081)	grad_norm 1.2888 (1.2888)	mem 14852MB
[2022-11-06 09:51:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][50/1251]	eta 0:10:03 lr 0.000816	time 0.4659 (0.5026)	loss 4.4090 (3.6141)	grad_norm 1.1859 (nan)	mem 14852MB
[2022-11-06 09:52:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][100/1251]	eta 0:09:18 lr 0.000816	time 0.4797 (0.4848)	loss 2.2098 (3.5712)	grad_norm 1.3863 (nan)	mem 14852MB
[2022-11-06 09:52:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][150/1251]	eta 0:08:47 lr 0.000816	time 0.4617 (0.4788)	loss 3.4427 (3.5491)	grad_norm 1.2863 (nan)	mem 14852MB
[2022-11-06 09:53:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][200/1251]	eta 0:08:20 lr 0.000816	time 0.4714 (0.4761)	loss 4.0186 (3.5674)	grad_norm 1.4896 (nan)	mem 14852MB
[2022-11-06 09:53:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][250/1251]	eta 0:07:55 lr 0.000816	time 0.4703 (0.4745)	loss 3.8612 (3.5593)	grad_norm 1.2106 (nan)	mem 14852MB
[2022-11-06 09:53:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][300/1251]	eta 0:07:29 lr 0.000816	time 0.4648 (0.4730)	loss 3.9711 (3.5757)	grad_norm 1.4248 (nan)	mem 14852MB
[2022-11-06 09:54:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][350/1251]	eta 0:07:05 lr 0.000815	time 0.4627 (0.4719)	loss 3.9537 (3.5804)	grad_norm 1.3601 (nan)	mem 14852MB
[2022-11-06 09:54:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][400/1251]	eta 0:06:40 lr 0.000815	time 0.4608 (0.4711)	loss 3.5898 (3.5772)	grad_norm 1.2632 (nan)	mem 14852MB
[2022-11-06 09:54:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][450/1251]	eta 0:06:17 lr 0.000815	time 0.4803 (0.4708)	loss 3.7798 (3.5792)	grad_norm 1.4156 (nan)	mem 14852MB
[2022-11-06 09:55:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][500/1251]	eta 0:05:53 lr 0.000815	time 0.4628 (0.4704)	loss 3.3131 (3.5696)	grad_norm 1.2341 (nan)	mem 14852MB
[2022-11-06 09:55:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][550/1251]	eta 0:05:29 lr 0.000815	time 0.4714 (0.4705)	loss 3.6450 (3.5646)	grad_norm 1.4015 (nan)	mem 14852MB
[2022-11-06 09:56:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][600/1251]	eta 0:05:06 lr 0.000815	time 0.4595 (0.4702)	loss 2.8625 (3.5708)	grad_norm 1.1906 (nan)	mem 14852MB
[2022-11-06 09:56:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][650/1251]	eta 0:04:42 lr 0.000814	time 0.4670 (0.4698)	loss 3.2881 (3.5763)	grad_norm 1.6074 (nan)	mem 14852MB
[2022-11-06 09:56:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][700/1251]	eta 0:04:18 lr 0.000814	time 0.4611 (0.4696)	loss 4.2066 (3.5768)	grad_norm 1.5490 (nan)	mem 14852MB
[2022-11-06 09:57:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][750/1251]	eta 0:03:55 lr 0.000814	time 0.4633 (0.4694)	loss 3.8888 (3.5768)	grad_norm 1.2893 (nan)	mem 14852MB
[2022-11-06 09:57:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][800/1251]	eta 0:03:31 lr 0.000814	time 0.4736 (0.4695)	loss 3.8025 (3.5715)	grad_norm 1.2790 (nan)	mem 14852MB
[2022-11-06 09:58:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][850/1251]	eta 0:03:08 lr 0.000814	time 0.4610 (0.4693)	loss 4.0293 (3.5661)	grad_norm 1.2357 (nan)	mem 14852MB
[2022-11-06 09:58:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][900/1251]	eta 0:02:44 lr 0.000814	time 0.4575 (0.4692)	loss 4.1224 (3.5680)	grad_norm 1.2892 (nan)	mem 14852MB
[2022-11-06 09:58:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][950/1251]	eta 0:02:21 lr 0.000813	time 0.4727 (0.4690)	loss 3.7508 (3.5702)	grad_norm 1.3933 (nan)	mem 14852MB
[2022-11-06 09:59:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][1000/1251]	eta 0:01:57 lr 0.000813	time 0.4617 (0.4689)	loss 3.2860 (3.5715)	grad_norm 1.2118 (nan)	mem 14852MB
[2022-11-06 09:59:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][1050/1251]	eta 0:01:34 lr 0.000813	time 0.4597 (0.4689)	loss 4.2036 (3.5713)	grad_norm 1.3639 (nan)	mem 14852MB
[2022-11-06 10:00:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][1100/1251]	eta 0:01:10 lr 0.000813	time 0.4631 (0.4688)	loss 4.0005 (3.5712)	grad_norm 1.1860 (nan)	mem 14852MB
[2022-11-06 10:00:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][1150/1251]	eta 0:00:47 lr 0.000813	time 0.4662 (0.4687)	loss 4.3550 (3.5780)	grad_norm 1.2939 (nan)	mem 14852MB
[2022-11-06 10:00:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][1200/1251]	eta 0:00:23 lr 0.000813	time 0.4741 (0.4687)	loss 4.2497 (3.5761)	grad_norm 1.2453 (nan)	mem 14852MB
[2022-11-06 10:01:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [85/300][1250/1251]	eta 0:00:00 lr 0.000812	time 0.4578 (0.4685)	loss 3.7630 (3.5793)	grad_norm 1.1139 (nan)	mem 14852MB
[2022-11-06 10:01:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 85 training takes 0:09:46
[2022-11-06 10:01:10 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_85.pth saving......
[2022-11-06 10:01:11 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_85.pth saved !!!
[2022-11-06 10:01:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.589 (1.589)	Loss 1.0246 (1.0246)	Acc@1 77.637 (77.637)	Acc@5 94.043 (94.043)	Mem 14852MB
[2022-11-06 10:01:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.370 Acc@5 93.156
[2022-11-06 10:01:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.4%
[2022-11-06 10:01:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.559 (1.559)	Loss 0.8966 (0.8966)	Acc@1 77.930 (77.930)	Acc@5 94.824 (94.824)	Mem 14852MB
[2022-11-06 10:01:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.132 Acc@5 94.184
[2022-11-06 10:01:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.1%
[2022-11-06 10:01:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.13% at 85 epoch
[2022-11-06 10:01:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][0/1251]	eta 0:41:34 lr 0.000812	time 1.9940 (1.9940)	loss 3.8519 (3.8519)	grad_norm 1.5504 (1.5504)	mem 14852MB
[2022-11-06 10:01:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][50/1251]	eta 0:10:04 lr 0.000812	time 0.4607 (0.5034)	loss 2.6724 (3.6960)	grad_norm 1.4297 (1.3344)	mem 14852MB
[2022-11-06 10:02:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][100/1251]	eta 0:09:21 lr 0.000812	time 0.4756 (0.4877)	loss 4.3082 (3.6396)	grad_norm 1.3625 (1.2998)	mem 14852MB
[2022-11-06 10:02:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][150/1251]	eta 0:08:49 lr 0.000812	time 0.4701 (0.4806)	loss 2.8577 (3.6036)	grad_norm 1.3263 (1.3098)	mem 14852MB
[2022-11-06 10:03:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][200/1251]	eta 0:08:21 lr 0.000812	time 0.4557 (0.4770)	loss 4.0407 (3.6059)	grad_norm 1.3218 (1.3053)	mem 14852MB
[2022-11-06 10:03:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][250/1251]	eta 0:07:55 lr 0.000812	time 0.4575 (0.4749)	loss 3.8603 (3.6320)	grad_norm 1.1675 (1.3047)	mem 14852MB
[2022-11-06 10:03:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][300/1251]	eta 0:07:30 lr 0.000811	time 0.4626 (0.4735)	loss 3.6386 (3.6514)	grad_norm 1.3890 (1.3046)	mem 14852MB
[2022-11-06 10:04:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][350/1251]	eta 0:07:05 lr 0.000811	time 0.4857 (0.4727)	loss 3.6860 (3.6416)	grad_norm 1.4548 (1.3100)	mem 14852MB
[2022-11-06 10:04:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][400/1251]	eta 0:06:41 lr 0.000811	time 0.4735 (0.4721)	loss 4.0058 (3.6358)	grad_norm 1.2128 (1.3081)	mem 14852MB
[2022-11-06 10:05:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][450/1251]	eta 0:06:17 lr 0.000811	time 0.4598 (0.4715)	loss 4.2474 (3.6280)	grad_norm 1.4157 (1.3114)	mem 14852MB
[2022-11-06 10:05:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][500/1251]	eta 0:05:53 lr 0.000811	time 0.4695 (0.4709)	loss 3.7466 (3.6176)	grad_norm 1.4048 (1.3083)	mem 14852MB
[2022-11-06 10:05:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][550/1251]	eta 0:05:30 lr 0.000811	time 0.4656 (0.4711)	loss 2.9230 (3.6168)	grad_norm 1.2468 (1.3083)	mem 14852MB
[2022-11-06 10:06:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][600/1251]	eta 0:05:06 lr 0.000811	time 0.4655 (0.4706)	loss 4.1262 (3.6126)	grad_norm 1.2642 (1.3089)	mem 14852MB
[2022-11-06 10:06:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][650/1251]	eta 0:04:42 lr 0.000810	time 0.4711 (0.4702)	loss 4.1762 (3.6216)	grad_norm 1.2803 (1.3085)	mem 14852MB
[2022-11-06 10:06:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][700/1251]	eta 0:04:19 lr 0.000810	time 0.4651 (0.4701)	loss 2.7072 (3.6142)	grad_norm 1.1781 (1.3087)	mem 14852MB
[2022-11-06 10:07:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][750/1251]	eta 0:03:55 lr 0.000810	time 0.4612 (0.4699)	loss 2.3572 (3.6113)	grad_norm 1.2408 (1.3073)	mem 14852MB
[2022-11-06 10:07:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][800/1251]	eta 0:03:31 lr 0.000810	time 0.4622 (0.4700)	loss 4.1912 (3.6092)	grad_norm 1.3127 (1.3082)	mem 14852MB
[2022-11-06 10:08:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][850/1251]	eta 0:03:08 lr 0.000810	time 0.4650 (0.4698)	loss 3.7581 (3.6114)	grad_norm 1.3262 (1.3080)	mem 14852MB
[2022-11-06 10:08:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][900/1251]	eta 0:02:44 lr 0.000810	time 0.4599 (0.4696)	loss 4.6480 (3.6081)	grad_norm 1.5723 (1.3082)	mem 14852MB
[2022-11-06 10:08:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][950/1251]	eta 0:02:21 lr 0.000809	time 0.4756 (0.4693)	loss 4.1704 (3.6073)	grad_norm 1.3181 (1.3078)	mem 14852MB
[2022-11-06 10:09:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][1000/1251]	eta 0:01:57 lr 0.000809	time 0.4656 (0.4693)	loss 4.0602 (3.6001)	grad_norm 1.2407 (1.3085)	mem 14852MB
[2022-11-06 10:09:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][1050/1251]	eta 0:01:34 lr 0.000809	time 0.4650 (0.4695)	loss 2.7438 (3.5950)	grad_norm 1.3091 (1.3085)	mem 14852MB
[2022-11-06 10:10:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][1100/1251]	eta 0:01:10 lr 0.000809	time 0.4761 (0.4693)	loss 3.4484 (3.5961)	grad_norm 1.3545 (1.3089)	mem 14852MB
[2022-11-06 10:10:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][1150/1251]	eta 0:00:47 lr 0.000809	time 0.4584 (0.4691)	loss 3.9780 (3.6012)	grad_norm 1.2505 (1.3084)	mem 14852MB
[2022-11-06 10:10:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][1200/1251]	eta 0:00:23 lr 0.000809	time 0.4741 (0.4690)	loss 3.9972 (3.5939)	grad_norm 1.2444 (1.3076)	mem 14852MB
[2022-11-06 10:11:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [86/300][1250/1251]	eta 0:00:00 lr 0.000808	time 0.4595 (0.4688)	loss 4.0533 (3.5948)	grad_norm 1.1937 (1.3058)	mem 14852MB
[2022-11-06 10:11:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 86 training takes 0:09:46
[2022-11-06 10:11:15 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_86.pth saving......
[2022-11-06 10:11:16 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_86.pth saved !!!
[2022-11-06 10:11:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.760 (1.760)	Loss 1.0663 (1.0663)	Acc@1 75.391 (75.391)	Acc@5 92.969 (92.969)	Mem 14852MB
[2022-11-06 10:11:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.300 Acc@5 92.978
[2022-11-06 10:11:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.3%
[2022-11-06 10:11:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.674 (1.674)	Loss 0.8583 (0.8583)	Acc@1 81.055 (81.055)	Acc@5 95.215 (95.215)	Mem 14852MB
[2022-11-06 10:11:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.144 Acc@5 94.220
[2022-11-06 10:11:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.1%
[2022-11-06 10:11:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.14% at 86 epoch
[2022-11-06 10:11:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][0/1251]	eta 0:41:06 lr 0.000808	time 1.9716 (1.9716)	loss 3.7903 (3.7903)	grad_norm 1.3249 (1.3249)	mem 14852MB
[2022-11-06 10:11:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][50/1251]	eta 0:10:02 lr 0.000808	time 0.4602 (0.5018)	loss 3.4463 (3.5238)	grad_norm 1.2639 (1.2847)	mem 14852MB
[2022-11-06 10:12:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][100/1251]	eta 0:09:18 lr 0.000808	time 0.4622 (0.4854)	loss 4.1443 (3.5826)	grad_norm 1.3167 (1.2930)	mem 14852MB
[2022-11-06 10:12:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][150/1251]	eta 0:08:47 lr 0.000808	time 0.4572 (0.4787)	loss 3.5464 (3.5013)	grad_norm 1.2928 (1.2979)	mem 14852MB
[2022-11-06 10:13:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][200/1251]	eta 0:08:20 lr 0.000808	time 0.4643 (0.4766)	loss 2.9795 (3.4913)	grad_norm 1.3061 (1.3127)	mem 14852MB
[2022-11-06 10:13:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][250/1251]	eta 0:07:54 lr 0.000808	time 0.4676 (0.4743)	loss 3.1540 (3.5097)	grad_norm 1.2033 (1.3134)	mem 14852MB
[2022-11-06 10:13:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][300/1251]	eta 0:07:30 lr 0.000807	time 0.4557 (0.4735)	loss 3.4589 (3.5203)	grad_norm 1.4005 (1.3138)	mem 14852MB
[2022-11-06 10:14:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][350/1251]	eta 0:07:05 lr 0.000807	time 0.4586 (0.4724)	loss 3.0998 (3.5204)	grad_norm 1.4478 (1.3110)	mem 14852MB
[2022-11-06 10:14:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][400/1251]	eta 0:06:41 lr 0.000807	time 0.4643 (0.4718)	loss 2.4231 (3.5255)	grad_norm 1.3948 (1.3136)	mem 14852MB
[2022-11-06 10:15:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][450/1251]	eta 0:06:17 lr 0.000807	time 0.4629 (0.4713)	loss 3.7153 (3.5232)	grad_norm 1.4797 (1.3166)	mem 14852MB
[2022-11-06 10:15:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][500/1251]	eta 0:05:53 lr 0.000807	time 0.4803 (0.4707)	loss 3.7426 (3.5352)	grad_norm 1.2840 (1.3156)	mem 14852MB
[2022-11-06 10:15:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][550/1251]	eta 0:05:29 lr 0.000807	time 0.4667 (0.4707)	loss 2.9847 (3.5303)	grad_norm 1.2117 (1.3144)	mem 14852MB
[2022-11-06 10:16:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][600/1251]	eta 0:05:06 lr 0.000806	time 0.4653 (0.4703)	loss 3.4736 (3.5404)	grad_norm 1.4795 (1.3136)	mem 14852MB
[2022-11-06 10:16:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][650/1251]	eta 0:04:42 lr 0.000806	time 0.4653 (0.4702)	loss 4.0270 (3.5453)	grad_norm 1.2611 (1.3155)	mem 14852MB
[2022-11-06 10:17:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][700/1251]	eta 0:04:19 lr 0.000806	time 0.4777 (0.4701)	loss 2.6037 (3.5424)	grad_norm 1.2718 (1.3158)	mem 14852MB
[2022-11-06 10:17:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][750/1251]	eta 0:03:55 lr 0.000806	time 0.4669 (0.4698)	loss 2.8009 (3.5469)	grad_norm 1.3212 (1.3139)	mem 14852MB
[2022-11-06 10:17:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][800/1251]	eta 0:03:31 lr 0.000806	time 0.4508 (0.4698)	loss 3.6033 (3.5482)	grad_norm 1.1290 (1.3143)	mem 14852MB
[2022-11-06 10:18:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][850/1251]	eta 0:03:08 lr 0.000806	time 0.4744 (0.4697)	loss 2.5109 (3.5389)	grad_norm 1.3342 (1.3143)	mem 14852MB
[2022-11-06 10:18:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][900/1251]	eta 0:02:44 lr 0.000805	time 0.4692 (0.4697)	loss 3.4934 (3.5384)	grad_norm 1.2897 (1.3135)	mem 14852MB
[2022-11-06 10:19:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][950/1251]	eta 0:02:21 lr 0.000805	time 0.4658 (0.4696)	loss 3.7898 (3.5408)	grad_norm 1.2184 (1.3114)	mem 14852MB
[2022-11-06 10:19:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][1000/1251]	eta 0:01:57 lr 0.000805	time 0.4639 (0.4695)	loss 2.4023 (3.5417)	grad_norm 1.2232 (1.3093)	mem 14852MB
[2022-11-06 10:19:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][1050/1251]	eta 0:01:34 lr 0.000805	time 0.4643 (0.4695)	loss 3.5764 (3.5477)	grad_norm 1.3802 (1.3097)	mem 14852MB
[2022-11-06 10:20:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][1100/1251]	eta 0:01:10 lr 0.000805	time 0.4637 (0.4693)	loss 4.0344 (3.5486)	grad_norm 1.1787 (1.3092)	mem 14852MB
[2022-11-06 10:20:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][1150/1251]	eta 0:00:47 lr 0.000805	time 0.4588 (0.4692)	loss 3.3977 (3.5504)	grad_norm 1.1586 (1.3094)	mem 14852MB
[2022-11-06 10:20:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][1200/1251]	eta 0:00:23 lr 0.000804	time 0.4554 (0.4692)	loss 3.9675 (3.5518)	grad_norm 1.2330 (1.3097)	mem 14852MB
[2022-11-06 10:21:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [87/300][1250/1251]	eta 0:00:00 lr 0.000804	time 0.4584 (0.4690)	loss 2.7544 (3.5518)	grad_norm 1.2943 (1.3089)	mem 14852MB
[2022-11-06 10:21:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 87 training takes 0:09:46
[2022-11-06 10:21:20 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_87.pth saving......
[2022-11-06 10:21:21 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_87.pth saved !!!
[2022-11-06 10:21:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.578 (1.578)	Loss 1.0464 (1.0464)	Acc@1 74.609 (74.609)	Acc@5 93.848 (93.848)	Mem 14852MB
[2022-11-06 10:21:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.556 Acc@5 93.090
[2022-11-06 10:21:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.6%
[2022-11-06 10:21:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.698 (1.698)	Loss 0.8619 (0.8619)	Acc@1 79.199 (79.199)	Acc@5 95.215 (95.215)	Mem 14852MB
[2022-11-06 10:21:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.208 Acc@5 94.232
[2022-11-06 10:21:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.2%
[2022-11-06 10:21:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.21% at 87 epoch
[2022-11-06 10:21:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][0/1251]	eta 0:40:47 lr 0.000804	time 1.9563 (1.9563)	loss 4.0510 (4.0510)	grad_norm 1.1891 (1.1891)	mem 14852MB
[2022-11-06 10:22:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][50/1251]	eta 0:10:01 lr 0.000804	time 0.4747 (0.5011)	loss 2.6337 (3.5328)	grad_norm 1.1835 (1.3206)	mem 14852MB
[2022-11-06 10:22:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][100/1251]	eta 0:09:17 lr 0.000804	time 0.4631 (0.4841)	loss 3.7514 (3.5121)	grad_norm 1.2891 (1.3047)	mem 14852MB
[2022-11-06 10:22:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][150/1251]	eta 0:08:47 lr 0.000804	time 0.4687 (0.4791)	loss 4.3069 (3.5375)	grad_norm 1.4429 (1.3039)	mem 14852MB
[2022-11-06 10:23:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][200/1251]	eta 0:08:20 lr 0.000804	time 0.4649 (0.4763)	loss 4.3048 (3.5265)	grad_norm 1.3054 (1.3114)	mem 14852MB
[2022-11-06 10:23:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][250/1251]	eta 0:07:54 lr 0.000803	time 0.4539 (0.4741)	loss 3.9611 (3.5326)	grad_norm 1.3183 (1.3095)	mem 14852MB
[2022-11-06 10:24:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][300/1251]	eta 0:07:29 lr 0.000803	time 0.4771 (0.4730)	loss 3.7650 (3.5333)	grad_norm 1.3005 (1.3060)	mem 14852MB
[2022-11-06 10:24:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][350/1251]	eta 0:07:05 lr 0.000803	time 0.4651 (0.4719)	loss 2.5028 (3.5206)	grad_norm 1.1944 (1.3088)	mem 14852MB
[2022-11-06 10:24:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][400/1251]	eta 0:06:40 lr 0.000803	time 0.4688 (0.4711)	loss 3.5988 (3.5243)	grad_norm 1.2914 (nan)	mem 14852MB
[2022-11-06 10:25:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][450/1251]	eta 0:06:16 lr 0.000803	time 0.4682 (0.4705)	loss 3.9220 (3.5422)	grad_norm 1.5887 (nan)	mem 14852MB
[2022-11-06 10:25:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][500/1251]	eta 0:05:53 lr 0.000803	time 0.4644 (0.4703)	loss 2.7152 (3.5424)	grad_norm 1.2162 (nan)	mem 14852MB
[2022-11-06 10:25:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][550/1251]	eta 0:05:29 lr 0.000802	time 0.4639 (0.4701)	loss 3.2105 (3.5400)	grad_norm 1.2283 (nan)	mem 14852MB
[2022-11-06 10:26:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][600/1251]	eta 0:05:05 lr 0.000802	time 0.4564 (0.4697)	loss 3.9922 (3.5470)	grad_norm 1.4827 (nan)	mem 14852MB
[2022-11-06 10:26:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][650/1251]	eta 0:04:42 lr 0.000802	time 0.4679 (0.4695)	loss 3.9002 (3.5389)	grad_norm 1.3584 (nan)	mem 14852MB
[2022-11-06 10:27:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][700/1251]	eta 0:04:18 lr 0.000802	time 0.4609 (0.4695)	loss 4.0470 (3.5436)	grad_norm 1.3694 (nan)	mem 14852MB
[2022-11-06 10:27:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][750/1251]	eta 0:03:55 lr 0.000802	time 0.4792 (0.4694)	loss 2.4166 (3.5376)	grad_norm 1.2865 (nan)	mem 14852MB
[2022-11-06 10:27:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][800/1251]	eta 0:03:31 lr 0.000802	time 0.4750 (0.4694)	loss 2.6696 (3.5423)	grad_norm 1.2664 (nan)	mem 14852MB
[2022-11-06 10:28:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][850/1251]	eta 0:03:08 lr 0.000801	time 0.4668 (0.4693)	loss 3.5659 (3.5472)	grad_norm 1.2198 (nan)	mem 14852MB
[2022-11-06 10:28:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][900/1251]	eta 0:02:44 lr 0.000801	time 0.5324 (0.4691)	loss 2.8346 (3.5527)	grad_norm 1.4825 (nan)	mem 14852MB
[2022-11-06 10:29:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][950/1251]	eta 0:02:21 lr 0.000801	time 0.4849 (0.4689)	loss 3.7450 (3.5588)	grad_norm 1.4160 (nan)	mem 14852MB
[2022-11-06 10:29:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][1000/1251]	eta 0:01:57 lr 0.000801	time 0.4792 (0.4690)	loss 3.8681 (3.5671)	grad_norm 1.2806 (nan)	mem 14852MB
[2022-11-06 10:29:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][1050/1251]	eta 0:01:34 lr 0.000801	time 0.4670 (0.4690)	loss 3.6348 (3.5687)	grad_norm 1.3743 (nan)	mem 14852MB
[2022-11-06 10:30:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][1100/1251]	eta 0:01:10 lr 0.000801	time 0.4598 (0.4689)	loss 2.9747 (3.5738)	grad_norm 1.3754 (nan)	mem 14852MB
[2022-11-06 10:30:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][1150/1251]	eta 0:00:47 lr 0.000800	time 0.4676 (0.4688)	loss 3.8342 (3.5742)	grad_norm 1.3049 (nan)	mem 14852MB
[2022-11-06 10:31:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][1200/1251]	eta 0:00:23 lr 0.000800	time 0.4634 (0.4687)	loss 3.7564 (3.5724)	grad_norm 1.3299 (nan)	mem 14852MB
[2022-11-06 10:31:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [88/300][1250/1251]	eta 0:00:00 lr 0.000800	time 0.4572 (0.4684)	loss 3.9675 (3.5697)	grad_norm 1.2152 (nan)	mem 14852MB
[2022-11-06 10:31:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 88 training takes 0:09:46
[2022-11-06 10:31:25 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_88.pth saving......
[2022-11-06 10:31:25 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_88.pth saved !!!
[2022-11-06 10:31:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.563 (1.563)	Loss 1.0663 (1.0663)	Acc@1 75.977 (75.977)	Acc@5 93.066 (93.066)	Mem 14852MB
[2022-11-06 10:31:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.460 Acc@5 93.102
[2022-11-06 10:31:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.5%
[2022-11-06 10:31:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.866 (1.866)	Loss 0.9040 (0.9040)	Acc@1 78.809 (78.809)	Acc@5 94.824 (94.824)	Mem 14852MB
[2022-11-06 10:31:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.252 Acc@5 94.264
[2022-11-06 10:31:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.3%
[2022-11-06 10:31:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.25% at 88 epoch
[2022-11-06 10:31:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][0/1251]	eta 0:42:00 lr 0.000800	time 2.0146 (2.0146)	loss 3.5948 (3.5948)	grad_norm 1.2518 (1.2518)	mem 14852MB
[2022-11-06 10:32:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][50/1251]	eta 0:10:03 lr 0.000800	time 0.4603 (0.5023)	loss 3.9980 (3.5154)	grad_norm 1.3169 (1.3006)	mem 14852MB
[2022-11-06 10:32:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][100/1251]	eta 0:09:19 lr 0.000800	time 0.4595 (0.4863)	loss 3.2918 (3.4813)	grad_norm 1.3173 (1.2920)	mem 14852MB
[2022-11-06 10:32:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][150/1251]	eta 0:08:48 lr 0.000800	time 0.4760 (0.4798)	loss 3.7869 (3.5158)	grad_norm 1.1929 (1.2960)	mem 14852MB
[2022-11-06 10:33:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][200/1251]	eta 0:08:21 lr 0.000799	time 0.4602 (0.4770)	loss 4.0498 (3.5341)	grad_norm 1.1462 (1.2965)	mem 14852MB
[2022-11-06 10:33:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][250/1251]	eta 0:07:55 lr 0.000799	time 0.4694 (0.4749)	loss 3.4858 (3.5588)	grad_norm 1.2998 (1.2990)	mem 14852MB
[2022-11-06 10:34:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][300/1251]	eta 0:07:30 lr 0.000799	time 0.4740 (0.4733)	loss 4.2754 (3.5680)	grad_norm 1.3960 (1.2993)	mem 14852MB
[2022-11-06 10:34:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][350/1251]	eta 0:07:05 lr 0.000799	time 0.4688 (0.4721)	loss 3.5896 (3.5681)	grad_norm 1.2039 (1.3008)	mem 14852MB
[2022-11-06 10:34:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][400/1251]	eta 0:06:41 lr 0.000799	time 0.4582 (0.4712)	loss 3.9525 (3.5570)	grad_norm 1.2624 (1.3014)	mem 14852MB
[2022-11-06 10:35:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][450/1251]	eta 0:06:16 lr 0.000799	time 0.4620 (0.4706)	loss 4.1666 (3.5576)	grad_norm 1.1913 (1.3048)	mem 14852MB
[2022-11-06 10:35:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][500/1251]	eta 0:05:53 lr 0.000798	time 0.4673 (0.4702)	loss 4.0421 (3.5560)	grad_norm 1.2114 (1.3035)	mem 14852MB
[2022-11-06 10:36:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][550/1251]	eta 0:05:29 lr 0.000798	time 0.4530 (0.4703)	loss 3.5976 (3.5496)	grad_norm 1.3652 (1.3011)	mem 14852MB
[2022-11-06 10:36:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][600/1251]	eta 0:05:06 lr 0.000798	time 0.4627 (0.4701)	loss 4.1685 (3.5511)	grad_norm 1.1488 (1.3033)	mem 14852MB
[2022-11-06 10:36:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][650/1251]	eta 0:04:42 lr 0.000798	time 0.4665 (0.4698)	loss 2.8663 (3.5466)	grad_norm 1.2495 (1.3022)	mem 14852MB
[2022-11-06 10:37:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][700/1251]	eta 0:04:18 lr 0.000798	time 0.4723 (0.4695)	loss 3.8313 (3.5598)	grad_norm 1.2868 (1.3030)	mem 14852MB
[2022-11-06 10:37:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][750/1251]	eta 0:03:55 lr 0.000798	time 0.4646 (0.4694)	loss 2.4520 (3.5580)	grad_norm 1.3320 (1.3023)	mem 14852MB
[2022-11-06 10:37:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][800/1251]	eta 0:03:31 lr 0.000797	time 0.4686 (0.4694)	loss 3.4389 (3.5651)	grad_norm 1.3941 (1.3035)	mem 14852MB
[2022-11-06 10:38:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][850/1251]	eta 0:03:08 lr 0.000797	time 0.4664 (0.4693)	loss 3.6864 (3.5699)	grad_norm 1.3467 (1.3014)	mem 14852MB
[2022-11-06 10:38:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][900/1251]	eta 0:02:44 lr 0.000797	time 0.5612 (0.4692)	loss 3.3975 (3.5704)	grad_norm 1.2597 (1.3009)	mem 14852MB
[2022-11-06 10:39:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][950/1251]	eta 0:02:21 lr 0.000797	time 0.4631 (0.4689)	loss 2.4486 (3.5770)	grad_norm 1.1085 (1.3016)	mem 14852MB
[2022-11-06 10:39:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][1000/1251]	eta 0:01:57 lr 0.000797	time 0.4807 (0.4687)	loss 3.0216 (3.5722)	grad_norm 1.1787 (1.3017)	mem 14852MB
[2022-11-06 10:39:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][1050/1251]	eta 0:01:34 lr 0.000797	time 0.4781 (0.4688)	loss 3.6132 (3.5728)	grad_norm 1.1434 (1.3000)	mem 14852MB
[2022-11-06 10:40:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][1100/1251]	eta 0:01:10 lr 0.000796	time 0.4650 (0.4688)	loss 4.0544 (3.5752)	grad_norm 1.2388 (1.2997)	mem 14852MB
[2022-11-06 10:40:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][1150/1251]	eta 0:00:47 lr 0.000796	time 0.4711 (0.4687)	loss 3.5198 (3.5698)	grad_norm 1.2605 (1.2999)	mem 14852MB
[2022-11-06 10:41:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][1200/1251]	eta 0:00:23 lr 0.000796	time 0.4668 (0.4685)	loss 3.7324 (3.5676)	grad_norm 1.2999 (nan)	mem 14852MB
[2022-11-06 10:41:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [89/300][1250/1251]	eta 0:00:00 lr 0.000796	time 0.5313 (0.4684)	loss 3.6222 (3.5646)	grad_norm 1.3403 (nan)	mem 14852MB
[2022-11-06 10:41:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 89 training takes 0:09:46
[2022-11-06 10:41:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_89.pth saving......
[2022-11-06 10:41:30 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_89.pth saved !!!
[2022-11-06 10:41:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.618 (1.618)	Loss 1.1268 (1.1268)	Acc@1 74.316 (74.316)	Acc@5 92.578 (92.578)	Mem 14852MB
[2022-11-06 10:41:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.274 Acc@5 92.880
[2022-11-06 10:41:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.3%
[2022-11-06 10:41:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.609 (1.609)	Loss 0.9372 (0.9372)	Acc@1 77.637 (77.637)	Acc@5 94.141 (94.141)	Mem 14852MB
[2022-11-06 10:41:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.344 Acc@5 94.268
[2022-11-06 10:41:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.3%
[2022-11-06 10:41:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.34% at 89 epoch
[2022-11-06 10:41:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][0/1251]	eta 0:39:39 lr 0.000796	time 1.9019 (1.9019)	loss 4.3493 (4.3493)	grad_norm 1.4226 (1.4226)	mem 14852MB
[2022-11-06 10:42:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][50/1251]	eta 0:09:58 lr 0.000796	time 0.4740 (0.4986)	loss 3.1731 (3.6290)	grad_norm 1.5458 (1.3317)	mem 14852MB
[2022-11-06 10:42:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][100/1251]	eta 0:09:20 lr 0.000796	time 0.4693 (0.4866)	loss 4.3899 (3.6012)	grad_norm 1.2214 (1.3258)	mem 14852MB
[2022-11-06 10:42:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][150/1251]	eta 0:08:48 lr 0.000795	time 0.4562 (0.4797)	loss 3.2034 (3.5390)	grad_norm 1.3862 (1.3208)	mem 14852MB
[2022-11-06 10:43:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][200/1251]	eta 0:08:20 lr 0.000795	time 0.4627 (0.4765)	loss 3.8086 (3.5268)	grad_norm 1.3110 (1.3260)	mem 14852MB
[2022-11-06 10:43:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][250/1251]	eta 0:07:54 lr 0.000795	time 0.4710 (0.4743)	loss 4.1094 (3.5316)	grad_norm 1.4205 (1.3184)	mem 14852MB
[2022-11-06 10:44:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][300/1251]	eta 0:07:29 lr 0.000795	time 0.4544 (0.4731)	loss 3.9425 (3.5402)	grad_norm 1.3318 (1.3172)	mem 14852MB
[2022-11-06 10:44:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][350/1251]	eta 0:07:05 lr 0.000795	time 0.4673 (0.4724)	loss 3.6838 (3.5430)	grad_norm 1.3062 (1.3167)	mem 14852MB
[2022-11-06 10:44:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][400/1251]	eta 0:06:41 lr 0.000795	time 0.4708 (0.4714)	loss 2.8369 (3.5430)	grad_norm 1.2645 (1.3160)	mem 14852MB
[2022-11-06 10:45:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][450/1251]	eta 0:06:17 lr 0.000794	time 0.4662 (0.4708)	loss 3.4760 (3.5562)	grad_norm 1.3216 (1.3172)	mem 14852MB
[2022-11-06 10:45:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][500/1251]	eta 0:05:53 lr 0.000794	time 0.4615 (0.4702)	loss 3.5234 (3.5493)	grad_norm 1.2940 (1.3150)	mem 14852MB
[2022-11-06 10:46:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][550/1251]	eta 0:05:29 lr 0.000794	time 0.4679 (0.4703)	loss 4.2967 (3.5442)	grad_norm 1.2097 (1.3127)	mem 14852MB
[2022-11-06 10:46:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][600/1251]	eta 0:05:05 lr 0.000794	time 0.4604 (0.4700)	loss 3.6049 (3.5423)	grad_norm 1.2264 (1.3131)	mem 14852MB
[2022-11-06 10:46:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][650/1251]	eta 0:04:42 lr 0.000794	time 0.4682 (0.4698)	loss 3.8879 (3.5476)	grad_norm 1.3948 (1.3133)	mem 14852MB
[2022-11-06 10:47:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][700/1251]	eta 0:04:18 lr 0.000794	time 0.4652 (0.4696)	loss 3.7197 (3.5494)	grad_norm 1.1544 (1.3142)	mem 14852MB
[2022-11-06 10:47:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][750/1251]	eta 0:03:55 lr 0.000793	time 0.4588 (0.4692)	loss 3.8269 (3.5500)	grad_norm 1.2770 (1.3150)	mem 14852MB
[2022-11-06 10:48:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][800/1251]	eta 0:03:31 lr 0.000793	time 0.4688 (0.4694)	loss 4.0708 (3.5465)	grad_norm 1.3460 (1.3141)	mem 14852MB
[2022-11-06 10:48:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][850/1251]	eta 0:03:08 lr 0.000793	time 0.4737 (0.4694)	loss 3.2571 (3.5496)	grad_norm 1.4201 (1.3154)	mem 14852MB
[2022-11-06 10:48:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][900/1251]	eta 0:02:44 lr 0.000793	time 0.4557 (0.4693)	loss 3.3955 (3.5495)	grad_norm 1.1766 (1.3133)	mem 14852MB
[2022-11-06 10:49:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][950/1251]	eta 0:02:21 lr 0.000793	time 0.4661 (0.4691)	loss 3.1742 (3.5506)	grad_norm 1.2565 (1.3127)	mem 14852MB
[2022-11-06 10:49:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][1000/1251]	eta 0:01:57 lr 0.000793	time 0.4637 (0.4689)	loss 3.3214 (3.5494)	grad_norm 1.2987 (1.3106)	mem 14852MB
[2022-11-06 10:50:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][1050/1251]	eta 0:01:34 lr 0.000792	time 0.4619 (0.4690)	loss 3.8452 (3.5484)	grad_norm 1.1765 (1.3116)	mem 14852MB
[2022-11-06 10:50:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][1100/1251]	eta 0:01:10 lr 0.000792	time 0.4621 (0.4690)	loss 3.5534 (3.5504)	grad_norm 1.2914 (1.3111)	mem 14852MB
[2022-11-06 10:50:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][1150/1251]	eta 0:00:47 lr 0.000792	time 0.4617 (0.4688)	loss 2.6404 (3.5462)	grad_norm 1.1286 (1.3125)	mem 14852MB
[2022-11-06 10:51:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][1200/1251]	eta 0:00:23 lr 0.000792	time 0.4681 (0.4688)	loss 3.0415 (3.5460)	grad_norm 1.3462 (1.3120)	mem 14852MB
[2022-11-06 10:51:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [90/300][1250/1251]	eta 0:00:00 lr 0.000792	time 0.4561 (0.4685)	loss 3.3389 (3.5494)	grad_norm 1.3130 (1.3130)	mem 14852MB
[2022-11-06 10:51:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 90 training takes 0:09:46
[2022-11-06 10:51:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_90.pth saving......
[2022-11-06 10:51:34 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_90.pth saved !!!
[2022-11-06 10:51:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.640 (1.640)	Loss 0.9804 (0.9804)	Acc@1 76.953 (76.953)	Acc@5 93.652 (93.652)	Mem 14852MB
[2022-11-06 10:51:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.342 Acc@5 92.998
[2022-11-06 10:51:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.3%
[2022-11-06 10:51:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.600 (1.600)	Loss 0.9774 (0.9774)	Acc@1 76.855 (76.855)	Acc@5 94.336 (94.336)	Mem 14852MB
[2022-11-06 10:51:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.386 Acc@5 94.304
[2022-11-06 10:51:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.4%
[2022-11-06 10:51:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.39% at 90 epoch
[2022-11-06 10:51:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][0/1251]	eta 0:40:36 lr 0.000792	time 1.9480 (1.9480)	loss 3.2928 (3.2928)	grad_norm 1.1860 (1.1860)	mem 14852MB
[2022-11-06 10:52:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][50/1251]	eta 0:10:00 lr 0.000792	time 0.4545 (0.4998)	loss 4.0320 (3.5482)	grad_norm 1.3166 (1.3159)	mem 14852MB
[2022-11-06 10:52:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][100/1251]	eta 0:09:17 lr 0.000791	time 0.4689 (0.4845)	loss 3.8946 (3.5637)	grad_norm 1.2248 (1.3182)	mem 14852MB
[2022-11-06 10:53:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][150/1251]	eta 0:08:47 lr 0.000791	time 0.4711 (0.4794)	loss 3.4839 (3.5535)	grad_norm 1.3175 (1.3210)	mem 14852MB
[2022-11-06 10:53:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][200/1251]	eta 0:08:20 lr 0.000791	time 0.4671 (0.4760)	loss 4.0577 (3.5781)	grad_norm 1.3502 (1.3189)	mem 14852MB
[2022-11-06 10:53:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][250/1251]	eta 0:07:54 lr 0.000791	time 0.4739 (0.4741)	loss 2.9260 (3.5669)	grad_norm 1.1650 (1.3116)	mem 14852MB
[2022-11-06 10:54:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][300/1251]	eta 0:07:29 lr 0.000791	time 0.4727 (0.4725)	loss 4.1033 (3.5650)	grad_norm 1.3376 (1.3180)	mem 14852MB
[2022-11-06 10:54:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][350/1251]	eta 0:07:04 lr 0.000791	time 0.4569 (0.4713)	loss 4.0359 (3.5737)	grad_norm 1.3612 (nan)	mem 14852MB
[2022-11-06 10:55:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][400/1251]	eta 0:06:40 lr 0.000790	time 0.4665 (0.4709)	loss 2.7882 (3.5852)	grad_norm 1.3219 (nan)	mem 14852MB
[2022-11-06 10:55:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][450/1251]	eta 0:06:16 lr 0.000790	time 0.4701 (0.4702)	loss 2.9894 (3.5670)	grad_norm 1.2580 (nan)	mem 14852MB
[2022-11-06 10:55:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][500/1251]	eta 0:05:52 lr 0.000790	time 0.4666 (0.4700)	loss 3.1775 (3.5784)	grad_norm 1.2619 (nan)	mem 14852MB
[2022-11-06 10:56:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][550/1251]	eta 0:05:29 lr 0.000790	time 0.4560 (0.4700)	loss 3.8822 (3.5855)	grad_norm 1.3441 (nan)	mem 14852MB
[2022-11-06 10:56:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][600/1251]	eta 0:05:05 lr 0.000790	time 0.4736 (0.4696)	loss 3.6616 (3.5837)	grad_norm 1.4030 (nan)	mem 14852MB
[2022-11-06 10:56:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][650/1251]	eta 0:04:42 lr 0.000790	time 0.4760 (0.4693)	loss 3.8179 (3.5859)	grad_norm 1.2608 (nan)	mem 14852MB
[2022-11-06 10:57:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][700/1251]	eta 0:04:18 lr 0.000789	time 0.4735 (0.4690)	loss 4.2878 (3.5911)	grad_norm 1.4313 (nan)	mem 14852MB
[2022-11-06 10:57:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][750/1251]	eta 0:03:54 lr 0.000789	time 0.4667 (0.4689)	loss 2.8370 (3.5884)	grad_norm 1.2246 (nan)	mem 14852MB
[2022-11-06 10:58:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][800/1251]	eta 0:03:31 lr 0.000789	time 0.4590 (0.4690)	loss 3.7854 (3.5935)	grad_norm 1.2207 (nan)	mem 14852MB
[2022-11-06 10:58:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][850/1251]	eta 0:03:08 lr 0.000789	time 0.4777 (0.4689)	loss 3.4094 (3.5974)	grad_norm 1.3334 (nan)	mem 14852MB
[2022-11-06 10:58:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][900/1251]	eta 0:02:44 lr 0.000789	time 0.4679 (0.4687)	loss 3.8060 (3.5960)	grad_norm 1.1758 (nan)	mem 14852MB
[2022-11-06 10:59:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][950/1251]	eta 0:02:21 lr 0.000789	time 0.4659 (0.4685)	loss 2.7003 (3.5938)	grad_norm 1.3254 (nan)	mem 14852MB
[2022-11-06 10:59:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][1000/1251]	eta 0:01:57 lr 0.000788	time 0.4661 (0.4684)	loss 3.8273 (3.5965)	grad_norm 1.4474 (nan)	mem 14852MB
[2022-11-06 11:00:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][1050/1251]	eta 0:01:34 lr 0.000788	time 0.4639 (0.4686)	loss 4.3217 (3.5946)	grad_norm 1.5463 (nan)	mem 14852MB
[2022-11-06 11:00:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][1100/1251]	eta 0:01:10 lr 0.000788	time 0.4638 (0.4685)	loss 3.3454 (3.5932)	grad_norm 1.3114 (nan)	mem 14852MB
[2022-11-06 11:00:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][1150/1251]	eta 0:00:47 lr 0.000788	time 0.4511 (0.4683)	loss 3.5549 (3.5912)	grad_norm 1.2346 (nan)	mem 14852MB
[2022-11-06 11:01:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][1200/1251]	eta 0:00:23 lr 0.000788	time 0.4721 (0.4682)	loss 2.5403 (3.5845)	grad_norm 1.2148 (nan)	mem 14852MB
[2022-11-06 11:01:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [91/300][1250/1251]	eta 0:00:00 lr 0.000788	time 0.4585 (0.4680)	loss 3.9571 (3.5830)	grad_norm 1.3825 (nan)	mem 14852MB
[2022-11-06 11:01:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 91 training takes 0:09:45
[2022-11-06 11:01:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_91.pth saving......
[2022-11-06 11:01:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_91.pth saved !!!
[2022-11-06 11:01:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.619 (1.619)	Loss 1.0793 (1.0793)	Acc@1 76.758 (76.758)	Acc@5 93.262 (93.262)	Mem 14852MB
[2022-11-06 11:01:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.526 Acc@5 93.100
[2022-11-06 11:01:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.5%
[2022-11-06 11:01:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.642 (1.642)	Loss 0.8561 (0.8561)	Acc@1 81.250 (81.250)	Acc@5 93.848 (93.848)	Mem 14852MB
[2022-11-06 11:01:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.424 Acc@5 94.298
[2022-11-06 11:01:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.4%
[2022-11-06 11:01:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.42% at 91 epoch
[2022-11-06 11:01:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][0/1251]	eta 0:41:19 lr 0.000788	time 1.9822 (1.9822)	loss 3.6480 (3.6480)	grad_norm 1.2727 (1.2727)	mem 14852MB
[2022-11-06 11:02:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][50/1251]	eta 0:10:06 lr 0.000787	time 0.4681 (0.5049)	loss 3.9861 (3.4570)	grad_norm 1.5012 (1.3146)	mem 14852MB
[2022-11-06 11:02:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][100/1251]	eta 0:09:21 lr 0.000787	time 0.5468 (0.4878)	loss 3.6936 (3.4570)	grad_norm 1.3920 (1.3272)	mem 14852MB
[2022-11-06 11:03:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][150/1251]	eta 0:08:48 lr 0.000787	time 0.4744 (0.4804)	loss 3.3704 (3.5154)	grad_norm 1.2682 (1.3177)	mem 14852MB
[2022-11-06 11:03:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][200/1251]	eta 0:08:20 lr 0.000787	time 0.4678 (0.4766)	loss 2.9046 (3.5353)	grad_norm 1.2891 (1.3200)	mem 14852MB
[2022-11-06 11:03:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][250/1251]	eta 0:07:55 lr 0.000787	time 0.4572 (0.4746)	loss 2.9707 (3.5473)	grad_norm 1.3464 (1.3185)	mem 14852MB
[2022-11-06 11:04:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][300/1251]	eta 0:07:29 lr 0.000786	time 0.4638 (0.4730)	loss 3.3766 (3.5453)	grad_norm 1.2989 (1.3249)	mem 14852MB
[2022-11-06 11:04:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][350/1251]	eta 0:07:04 lr 0.000786	time 0.4649 (0.4717)	loss 3.5908 (3.5539)	grad_norm 1.2445 (1.3236)	mem 14852MB
[2022-11-06 11:05:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][400/1251]	eta 0:06:40 lr 0.000786	time 0.4682 (0.4711)	loss 3.3666 (3.5561)	grad_norm 1.3150 (1.3214)	mem 14852MB
[2022-11-06 11:05:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][450/1251]	eta 0:06:16 lr 0.000786	time 0.4603 (0.4703)	loss 3.0695 (3.5578)	grad_norm 1.2478 (1.3239)	mem 14852MB
[2022-11-06 11:05:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][500/1251]	eta 0:05:52 lr 0.000786	time 0.4680 (0.4699)	loss 2.5868 (3.5696)	grad_norm 1.3048 (1.3232)	mem 14852MB
[2022-11-06 11:06:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][550/1251]	eta 0:05:29 lr 0.000786	time 0.4658 (0.4701)	loss 3.9123 (3.5703)	grad_norm 1.2450 (1.3218)	mem 14852MB
[2022-11-06 11:06:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][600/1251]	eta 0:05:05 lr 0.000785	time 0.4674 (0.4696)	loss 4.0057 (3.5705)	grad_norm 1.2963 (1.3227)	mem 14852MB
[2022-11-06 11:07:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][650/1251]	eta 0:04:42 lr 0.000785	time 0.4528 (0.4693)	loss 3.9076 (3.5697)	grad_norm 1.4281 (1.3191)	mem 14852MB
[2022-11-06 11:07:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][700/1251]	eta 0:04:18 lr 0.000785	time 0.4639 (0.4690)	loss 3.3133 (3.5671)	grad_norm 1.2933 (1.3156)	mem 14852MB
[2022-11-06 11:07:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][750/1251]	eta 0:03:54 lr 0.000785	time 0.4646 (0.4689)	loss 4.6371 (3.5746)	grad_norm 1.3065 (1.3176)	mem 14852MB
[2022-11-06 11:08:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][800/1251]	eta 0:03:31 lr 0.000785	time 0.4612 (0.4691)	loss 3.8725 (3.5721)	grad_norm 1.3770 (1.3186)	mem 14852MB
[2022-11-06 11:08:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][850/1251]	eta 0:03:08 lr 0.000785	time 0.4646 (0.4689)	loss 3.9470 (3.5703)	grad_norm 1.4840 (1.3189)	mem 14852MB
[2022-11-06 11:08:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][900/1251]	eta 0:02:44 lr 0.000784	time 0.4534 (0.4688)	loss 4.2224 (3.5729)	grad_norm 1.6068 (1.3200)	mem 14852MB
[2022-11-06 11:09:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][950/1251]	eta 0:02:21 lr 0.000784	time 0.4611 (0.4686)	loss 2.5777 (3.5707)	grad_norm 1.2806 (1.3217)	mem 14852MB
[2022-11-06 11:09:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][1000/1251]	eta 0:01:57 lr 0.000784	time 0.4590 (0.4685)	loss 4.1480 (3.5688)	grad_norm 1.5049 (1.3212)	mem 14852MB
[2022-11-06 11:10:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][1050/1251]	eta 0:01:34 lr 0.000784	time 0.4622 (0.4686)	loss 3.4426 (3.5682)	grad_norm 1.2589 (1.3208)	mem 14852MB
[2022-11-06 11:10:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][1100/1251]	eta 0:01:10 lr 0.000784	time 0.4559 (0.4685)	loss 3.7171 (3.5638)	grad_norm 1.2144 (1.3209)	mem 14852MB
[2022-11-06 11:10:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][1150/1251]	eta 0:00:47 lr 0.000784	time 0.4695 (0.4686)	loss 3.9173 (3.5632)	grad_norm 1.2843 (1.3213)	mem 14852MB
[2022-11-06 11:11:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][1200/1251]	eta 0:00:23 lr 0.000783	time 0.4641 (0.4684)	loss 3.1592 (3.5622)	grad_norm 1.3455 (1.3198)	mem 14852MB
[2022-11-06 11:11:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [92/300][1250/1251]	eta 0:00:00 lr 0.000783	time 0.4574 (0.4683)	loss 3.3848 (3.5643)	grad_norm 1.2995 (1.3198)	mem 14852MB
[2022-11-06 11:11:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 92 training takes 0:09:45
[2022-11-06 11:11:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_92.pth saving......
[2022-11-06 11:11:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_92.pth saved !!!
[2022-11-06 11:11:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.560 (1.560)	Loss 1.0350 (1.0350)	Acc@1 76.074 (76.074)	Acc@5 93.164 (93.164)	Mem 14852MB
[2022-11-06 11:11:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.828 Acc@5 93.274
[2022-11-06 11:11:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.8%
[2022-11-06 11:11:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.629 (1.629)	Loss 1.0116 (1.0116)	Acc@1 76.270 (76.270)	Acc@5 92.480 (92.480)	Mem 14852MB
[2022-11-06 11:11:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.416 Acc@5 94.350
[2022-11-06 11:11:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.4%
[2022-11-06 11:11:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.42% at 91 epoch
[2022-11-06 11:12:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][0/1251]	eta 0:41:07 lr 0.000783	time 1.9725 (1.9725)	loss 4.0959 (4.0959)	grad_norm 1.4130 (1.4130)	mem 14852MB
[2022-11-06 11:12:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][50/1251]	eta 0:10:07 lr 0.000783	time 0.5706 (0.5055)	loss 3.1008 (3.5749)	grad_norm 1.2540 (1.3514)	mem 14852MB
[2022-11-06 11:12:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][100/1251]	eta 0:09:21 lr 0.000783	time 0.4792 (0.4877)	loss 3.7417 (3.6379)	grad_norm 1.5656 (1.3511)	mem 14852MB
[2022-11-06 11:13:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][150/1251]	eta 0:08:48 lr 0.000783	time 0.4595 (0.4802)	loss 3.7860 (3.6221)	grad_norm 1.3022 (1.3421)	mem 14852MB
[2022-11-06 11:13:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][200/1251]	eta 0:08:20 lr 0.000783	time 0.4724 (0.4763)	loss 3.6008 (3.6105)	grad_norm 1.3460 (1.3406)	mem 14852MB
[2022-11-06 11:13:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][250/1251]	eta 0:07:54 lr 0.000782	time 0.4677 (0.4742)	loss 4.2221 (3.5907)	grad_norm 1.5936 (1.3371)	mem 14852MB
[2022-11-06 11:14:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][300/1251]	eta 0:07:29 lr 0.000782	time 0.4706 (0.4731)	loss 2.1308 (3.6112)	grad_norm 1.5044 (1.3384)	mem 14852MB
[2022-11-06 11:14:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][350/1251]	eta 0:07:05 lr 0.000782	time 0.4650 (0.4721)	loss 4.0007 (3.6041)	grad_norm 1.3580 (1.3364)	mem 14852MB
[2022-11-06 11:15:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][400/1251]	eta 0:06:41 lr 0.000782	time 0.4727 (0.4714)	loss 2.9385 (3.5939)	grad_norm 1.3430 (1.3308)	mem 14852MB
[2022-11-06 11:15:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][450/1251]	eta 0:06:17 lr 0.000782	time 0.4621 (0.4708)	loss 3.0622 (3.5920)	grad_norm 1.3899 (1.3293)	mem 14852MB
[2022-11-06 11:15:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][500/1251]	eta 0:05:53 lr 0.000782	time 0.4655 (0.4705)	loss 3.9163 (3.5948)	grad_norm 1.2645 (1.3301)	mem 14852MB
[2022-11-06 11:16:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][550/1251]	eta 0:05:29 lr 0.000781	time 0.4661 (0.4707)	loss 3.5548 (3.5771)	grad_norm 1.2730 (nan)	mem 14852MB
[2022-11-06 11:16:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][600/1251]	eta 0:05:06 lr 0.000781	time 0.4640 (0.4702)	loss 3.3642 (3.5809)	grad_norm 1.5276 (nan)	mem 14852MB
[2022-11-06 11:17:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][650/1251]	eta 0:04:42 lr 0.000781	time 0.4553 (0.4699)	loss 3.4153 (3.5802)	grad_norm 1.3311 (nan)	mem 14852MB
[2022-11-06 11:17:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][700/1251]	eta 0:04:18 lr 0.000781	time 0.4619 (0.4696)	loss 4.3024 (3.5770)	grad_norm 1.3132 (nan)	mem 14852MB
[2022-11-06 11:17:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][750/1251]	eta 0:03:55 lr 0.000781	time 0.4727 (0.4695)	loss 4.3923 (3.5818)	grad_norm 1.2587 (nan)	mem 14852MB
[2022-11-06 11:18:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][800/1251]	eta 0:03:31 lr 0.000780	time 0.4610 (0.4694)	loss 2.9875 (3.5818)	grad_norm 1.3239 (nan)	mem 14852MB
[2022-11-06 11:18:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][850/1251]	eta 0:03:08 lr 0.000780	time 0.4543 (0.4691)	loss 3.8775 (3.5764)	grad_norm 1.2966 (nan)	mem 14852MB
[2022-11-06 11:19:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][900/1251]	eta 0:02:44 lr 0.000780	time 0.4702 (0.4688)	loss 2.3883 (3.5772)	grad_norm 1.3227 (nan)	mem 14852MB
[2022-11-06 11:19:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][950/1251]	eta 0:02:21 lr 0.000780	time 0.4688 (0.4687)	loss 3.4792 (3.5758)	grad_norm 1.2328 (nan)	mem 14852MB
[2022-11-06 11:19:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][1000/1251]	eta 0:01:57 lr 0.000780	time 0.4566 (0.4687)	loss 4.1276 (3.5735)	grad_norm 1.1511 (nan)	mem 14852MB
[2022-11-06 11:20:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][1050/1251]	eta 0:01:34 lr 0.000780	time 0.4688 (0.4689)	loss 3.2188 (3.5731)	grad_norm 1.2566 (nan)	mem 14852MB
[2022-11-06 11:20:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][1100/1251]	eta 0:01:10 lr 0.000779	time 0.4718 (0.4688)	loss 3.4222 (3.5749)	grad_norm 1.2442 (nan)	mem 14852MB
[2022-11-06 11:20:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][1150/1251]	eta 0:00:47 lr 0.000779	time 0.4639 (0.4686)	loss 3.0831 (3.5695)	grad_norm 1.2348 (nan)	mem 14852MB
[2022-11-06 11:21:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][1200/1251]	eta 0:00:23 lr 0.000779	time 0.4717 (0.4685)	loss 3.3074 (3.5716)	grad_norm 1.4076 (nan)	mem 14852MB
[2022-11-06 11:21:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [93/300][1250/1251]	eta 0:00:00 lr 0.000779	time 0.4586 (0.4682)	loss 3.8791 (3.5744)	grad_norm 1.3021 (nan)	mem 14852MB
[2022-11-06 11:21:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 93 training takes 0:09:45
[2022-11-06 11:21:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_93.pth saving......
[2022-11-06 11:21:46 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_93.pth saved !!!
[2022-11-06 11:21:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.613 (1.613)	Loss 1.0701 (1.0701)	Acc@1 75.488 (75.488)	Acc@5 92.773 (92.773)	Mem 14852MB
[2022-11-06 11:21:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.580 Acc@5 93.094
[2022-11-06 11:21:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.6%
[2022-11-06 11:21:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.980 (1.980)	Loss 0.9014 (0.9014)	Acc@1 78.906 (78.906)	Acc@5 94.238 (94.238)	Mem 14852MB
[2022-11-06 11:22:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.442 Acc@5 94.364
[2022-11-06 11:22:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.4%
[2022-11-06 11:22:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.44% at 93 epoch
[2022-11-06 11:22:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][0/1251]	eta 0:40:23 lr 0.000779	time 1.9372 (1.9372)	loss 3.2085 (3.2085)	grad_norm 1.2458 (1.2458)	mem 14852MB
[2022-11-06 11:22:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][50/1251]	eta 0:09:58 lr 0.000779	time 0.4657 (0.4985)	loss 4.0128 (3.5098)	grad_norm 1.2876 (1.3096)	mem 14852MB
[2022-11-06 11:22:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][100/1251]	eta 0:09:15 lr 0.000779	time 0.4641 (0.4829)	loss 4.0977 (3.5045)	grad_norm 1.2513 (1.2939)	mem 14852MB
[2022-11-06 11:23:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][150/1251]	eta 0:08:45 lr 0.000778	time 0.4655 (0.4775)	loss 3.8782 (3.5431)	grad_norm 1.3269 (1.3011)	mem 14852MB
[2022-11-06 11:23:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][200/1251]	eta 0:08:18 lr 0.000778	time 0.4577 (0.4747)	loss 2.6567 (3.5404)	grad_norm 1.2773 (1.3137)	mem 14852MB
[2022-11-06 11:24:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][250/1251]	eta 0:07:53 lr 0.000778	time 0.4649 (0.4732)	loss 3.7089 (3.5330)	grad_norm 1.3350 (1.3111)	mem 14852MB
[2022-11-06 11:24:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][300/1251]	eta 0:07:28 lr 0.000778	time 0.4604 (0.4719)	loss 3.0727 (3.5397)	grad_norm 1.2521 (1.3113)	mem 14852MB
[2022-11-06 11:24:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][350/1251]	eta 0:07:04 lr 0.000778	time 0.4657 (0.4710)	loss 3.7661 (3.5548)	grad_norm 1.2377 (1.3128)	mem 14852MB
[2022-11-06 11:25:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][400/1251]	eta 0:06:40 lr 0.000778	time 0.4713 (0.4704)	loss 3.8362 (3.5681)	grad_norm 1.3207 (1.3152)	mem 14852MB
[2022-11-06 11:25:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][450/1251]	eta 0:06:16 lr 0.000777	time 0.4591 (0.4702)	loss 3.5158 (3.5636)	grad_norm 1.2689 (1.3140)	mem 14852MB
[2022-11-06 11:25:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][500/1251]	eta 0:05:52 lr 0.000777	time 0.4638 (0.4698)	loss 3.7877 (3.5450)	grad_norm 1.4434 (1.3127)	mem 14852MB
[2022-11-06 11:26:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][550/1251]	eta 0:05:29 lr 0.000777	time 0.4672 (0.4699)	loss 3.4129 (3.5477)	grad_norm 1.3067 (1.3143)	mem 14852MB
[2022-11-06 11:26:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][600/1251]	eta 0:05:05 lr 0.000777	time 0.4592 (0.4695)	loss 3.0779 (3.5398)	grad_norm 1.2604 (1.3150)	mem 14852MB
[2022-11-06 11:27:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][650/1251]	eta 0:04:42 lr 0.000777	time 0.5572 (0.4694)	loss 3.6771 (3.5369)	grad_norm 1.2890 (1.3151)	mem 14852MB
[2022-11-06 11:27:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][700/1251]	eta 0:04:18 lr 0.000777	time 0.4776 (0.4691)	loss 3.1442 (3.5506)	grad_norm 1.3372 (1.3169)	mem 14852MB
[2022-11-06 11:27:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][750/1251]	eta 0:03:54 lr 0.000776	time 0.4681 (0.4688)	loss 4.0781 (3.5565)	grad_norm 1.3297 (1.3162)	mem 14852MB
[2022-11-06 11:28:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][800/1251]	eta 0:03:31 lr 0.000776	time 0.4668 (0.4688)	loss 3.4911 (3.5593)	grad_norm 1.2036 (1.3153)	mem 14852MB
[2022-11-06 11:28:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][850/1251]	eta 0:03:07 lr 0.000776	time 0.4593 (0.4687)	loss 3.9970 (3.5515)	grad_norm 1.2779 (1.3150)	mem 14852MB
[2022-11-06 11:29:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][900/1251]	eta 0:02:44 lr 0.000776	time 0.4744 (0.4687)	loss 4.1078 (3.5560)	grad_norm 1.2977 (1.3190)	mem 14852MB
[2022-11-06 11:29:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][950/1251]	eta 0:02:21 lr 0.000776	time 0.4511 (0.4686)	loss 2.7408 (3.5520)	grad_norm 1.2643 (1.3185)	mem 14852MB
[2022-11-06 11:29:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][1000/1251]	eta 0:01:57 lr 0.000775	time 0.4711 (0.4685)	loss 3.8927 (3.5554)	grad_norm 1.2911 (1.3199)	mem 14852MB
[2022-11-06 11:30:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][1050/1251]	eta 0:01:34 lr 0.000775	time 0.4636 (0.4685)	loss 3.9310 (3.5530)	grad_norm 1.3676 (1.3199)	mem 14852MB
[2022-11-06 11:30:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][1100/1251]	eta 0:01:10 lr 0.000775	time 0.4535 (0.4684)	loss 3.6637 (3.5521)	grad_norm 1.2674 (1.3214)	mem 14852MB
[2022-11-06 11:31:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][1150/1251]	eta 0:00:47 lr 0.000775	time 0.4597 (0.4683)	loss 3.2954 (3.5500)	grad_norm 1.2366 (1.3194)	mem 14852MB
[2022-11-06 11:31:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][1200/1251]	eta 0:00:23 lr 0.000775	time 0.4685 (0.4683)	loss 3.0607 (3.5459)	grad_norm 1.3000 (1.3209)	mem 14852MB
[2022-11-06 11:31:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [94/300][1250/1251]	eta 0:00:00 lr 0.000775	time 0.4562 (0.4681)	loss 3.6813 (3.5440)	grad_norm 1.2157 (1.3214)	mem 14852MB
[2022-11-06 11:31:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 94 training takes 0:09:45
[2022-11-06 11:31:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_94.pth saving......
[2022-11-06 11:31:50 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_94.pth saved !!!
[2022-11-06 11:31:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.620 (1.620)	Loss 1.0612 (1.0612)	Acc@1 75.488 (75.488)	Acc@5 93.652 (93.652)	Mem 14852MB
[2022-11-06 11:31:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.632 Acc@5 93.224
[2022-11-06 11:31:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.6%
[2022-11-06 11:32:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.682 (1.682)	Loss 0.8538 (0.8538)	Acc@1 81.055 (81.055)	Acc@5 94.922 (94.922)	Mem 14852MB
[2022-11-06 11:32:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.492 Acc@5 94.376
[2022-11-06 11:32:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.5%
[2022-11-06 11:32:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.49% at 94 epoch
[2022-11-06 11:32:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][0/1251]	eta 0:41:58 lr 0.000775	time 2.0131 (2.0131)	loss 3.8458 (3.8458)	grad_norm 1.2813 (1.2813)	mem 14852MB
[2022-11-06 11:32:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][50/1251]	eta 0:10:03 lr 0.000774	time 0.4681 (0.5029)	loss 3.5682 (3.4707)	grad_norm 1.2342 (1.2899)	mem 14852MB
[2022-11-06 11:32:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][100/1251]	eta 0:09:18 lr 0.000774	time 0.4697 (0.4851)	loss 2.8817 (3.5549)	grad_norm 1.2974 (1.3148)	mem 14852MB
[2022-11-06 11:33:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][150/1251]	eta 0:08:46 lr 0.000774	time 0.4730 (0.4786)	loss 3.9585 (3.5565)	grad_norm 1.2159 (1.3194)	mem 14852MB
[2022-11-06 11:33:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][200/1251]	eta 0:08:19 lr 0.000774	time 0.4715 (0.4752)	loss 2.8701 (3.5306)	grad_norm 1.2719 (1.3119)	mem 14852MB
[2022-11-06 11:34:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][250/1251]	eta 0:07:53 lr 0.000774	time 0.4744 (0.4731)	loss 4.1003 (3.5496)	grad_norm 1.3141 (1.3236)	mem 14852MB
[2022-11-06 11:34:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][300/1251]	eta 0:07:29 lr 0.000774	time 0.4712 (0.4723)	loss 4.1937 (3.5617)	grad_norm 1.2894 (1.3264)	mem 14852MB
[2022-11-06 11:34:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][350/1251]	eta 0:07:04 lr 0.000773	time 0.4627 (0.4714)	loss 3.1441 (3.5698)	grad_norm 1.1543 (1.3260)	mem 14852MB
[2022-11-06 11:35:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][400/1251]	eta 0:06:40 lr 0.000773	time 0.4645 (0.4705)	loss 3.9560 (3.5615)	grad_norm 1.4070 (1.3254)	mem 14852MB
[2022-11-06 11:35:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][450/1251]	eta 0:06:16 lr 0.000773	time 0.4599 (0.4698)	loss 3.4073 (3.5514)	grad_norm 1.2770 (1.3279)	mem 14852MB
[2022-11-06 11:36:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][500/1251]	eta 0:05:52 lr 0.000773	time 0.4662 (0.4695)	loss 2.5348 (3.5514)	grad_norm 1.3839 (1.3307)	mem 14852MB
[2022-11-06 11:36:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][550/1251]	eta 0:05:29 lr 0.000773	time 0.4560 (0.4697)	loss 3.2965 (3.5537)	grad_norm 1.2572 (1.3289)	mem 14852MB
[2022-11-06 11:36:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][600/1251]	eta 0:05:05 lr 0.000773	time 0.5250 (0.4694)	loss 2.2914 (3.5527)	grad_norm 1.2070 (1.3248)	mem 14852MB
[2022-11-06 11:37:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][650/1251]	eta 0:04:41 lr 0.000772	time 0.4743 (0.4692)	loss 4.0996 (3.5503)	grad_norm 1.3111 (1.3246)	mem 14852MB
[2022-11-06 11:37:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][700/1251]	eta 0:04:18 lr 0.000772	time 0.4584 (0.4689)	loss 3.0481 (3.5441)	grad_norm 1.3579 (1.3240)	mem 14852MB
[2022-11-06 11:38:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][750/1251]	eta 0:03:54 lr 0.000772	time 0.4690 (0.4687)	loss 3.4301 (3.5386)	grad_norm 1.2474 (1.3247)	mem 14852MB
[2022-11-06 11:38:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][800/1251]	eta 0:03:31 lr 0.000772	time 0.4661 (0.4689)	loss 3.2514 (3.5421)	grad_norm 1.6941 (1.3244)	mem 14852MB
[2022-11-06 11:38:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][850/1251]	eta 0:03:08 lr 0.000772	time 0.4637 (0.4688)	loss 3.7651 (3.5443)	grad_norm 1.2457 (1.3252)	mem 14852MB
[2022-11-06 11:39:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][900/1251]	eta 0:02:44 lr 0.000771	time 0.4596 (0.4686)	loss 3.8302 (3.5436)	grad_norm 1.3680 (1.3247)	mem 14852MB
[2022-11-06 11:39:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][950/1251]	eta 0:02:21 lr 0.000771	time 0.4761 (0.4686)	loss 3.3625 (3.5479)	grad_norm 1.3240 (1.3254)	mem 14852MB
[2022-11-06 11:39:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][1000/1251]	eta 0:01:57 lr 0.000771	time 0.4758 (0.4685)	loss 2.8688 (3.5420)	grad_norm 1.3406 (1.3242)	mem 14852MB
[2022-11-06 11:40:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][1050/1251]	eta 0:01:34 lr 0.000771	time 0.4744 (0.4687)	loss 3.1653 (3.5415)	grad_norm 1.3500 (1.3240)	mem 14852MB
[2022-11-06 11:40:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][1100/1251]	eta 0:01:10 lr 0.000771	time 0.5438 (0.4686)	loss 4.3914 (3.5439)	grad_norm 1.3988 (1.3246)	mem 14852MB
[2022-11-06 11:41:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][1150/1251]	eta 0:00:47 lr 0.000771	time 0.4629 (0.4684)	loss 3.9650 (3.5439)	grad_norm 1.2492 (1.3249)	mem 14852MB
[2022-11-06 11:41:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][1200/1251]	eta 0:00:23 lr 0.000770	time 0.4546 (0.4683)	loss 3.9230 (3.5447)	grad_norm 1.2538 (1.3251)	mem 14852MB
[2022-11-06 11:41:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [95/300][1250/1251]	eta 0:00:00 lr 0.000770	time 0.4586 (0.4681)	loss 4.0831 (3.5452)	grad_norm 1.4994 (1.3251)	mem 14852MB
[2022-11-06 11:41:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 95 training takes 0:09:45
[2022-11-06 11:41:53 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_95.pth saving......
[2022-11-06 11:41:54 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_95.pth saved !!!
[2022-11-06 11:41:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.583 (1.583)	Loss 1.0101 (1.0101)	Acc@1 74.707 (74.707)	Acc@5 93.750 (93.750)	Mem 14852MB
[2022-11-06 11:42:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.850 Acc@5 93.228
[2022-11-06 11:42:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.9%
[2022-11-06 11:42:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.671 (1.671)	Loss 0.9140 (0.9140)	Acc@1 78.320 (78.320)	Acc@5 94.434 (94.434)	Mem 14852MB
[2022-11-06 11:42:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.526 Acc@5 94.404
[2022-11-06 11:42:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.5%
[2022-11-06 11:42:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.53% at 95 epoch
[2022-11-06 11:42:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][0/1251]	eta 0:44:18 lr 0.000770	time 2.1251 (2.1251)	loss 4.3008 (4.3008)	grad_norm 1.3804 (1.3804)	mem 14852MB
[2022-11-06 11:42:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][50/1251]	eta 0:10:07 lr 0.000770	time 0.4603 (0.5058)	loss 3.3386 (3.4196)	grad_norm 1.4204 (1.3255)	mem 14852MB
[2022-11-06 11:43:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][100/1251]	eta 0:09:20 lr 0.000770	time 0.4689 (0.4871)	loss 4.2554 (3.4468)	grad_norm 1.4449 (1.3262)	mem 14852MB
[2022-11-06 11:43:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][150/1251]	eta 0:08:48 lr 0.000770	time 0.4855 (0.4800)	loss 4.2881 (3.5053)	grad_norm 1.4411 (1.3186)	mem 14852MB
[2022-11-06 11:43:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][200/1251]	eta 0:08:21 lr 0.000770	time 0.4681 (0.4774)	loss 2.4051 (3.5200)	grad_norm 1.3100 (1.3238)	mem 14852MB
[2022-11-06 11:44:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][250/1251]	eta 0:07:55 lr 0.000769	time 0.4743 (0.4752)	loss 3.9758 (3.5058)	grad_norm 1.3061 (1.3260)	mem 14852MB
[2022-11-06 11:44:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][300/1251]	eta 0:07:30 lr 0.000769	time 0.4547 (0.4735)	loss 2.7603 (3.5156)	grad_norm 1.3344 (1.3325)	mem 14852MB
[2022-11-06 11:44:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][350/1251]	eta 0:07:05 lr 0.000769	time 0.4605 (0.4724)	loss 4.2610 (3.5210)	grad_norm 1.6375 (1.3316)	mem 14852MB
[2022-11-06 11:45:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][400/1251]	eta 0:06:41 lr 0.000769	time 0.4822 (0.4716)	loss 2.6859 (3.5253)	grad_norm 1.2623 (1.3344)	mem 14852MB
[2022-11-06 11:45:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][450/1251]	eta 0:06:17 lr 0.000769	time 0.4638 (0.4708)	loss 4.0087 (3.5288)	grad_norm 1.4049 (1.3356)	mem 14852MB
[2022-11-06 11:46:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][500/1251]	eta 0:05:53 lr 0.000768	time 0.4641 (0.4705)	loss 3.0266 (3.5170)	grad_norm 1.1243 (1.3326)	mem 14852MB
[2022-11-06 11:46:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][550/1251]	eta 0:05:29 lr 0.000768	time 0.4620 (0.4704)	loss 3.5412 (3.5304)	grad_norm 1.1986 (1.3289)	mem 14852MB
[2022-11-06 11:46:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][600/1251]	eta 0:05:05 lr 0.000768	time 0.4555 (0.4700)	loss 3.5322 (3.5429)	grad_norm 1.3755 (1.3290)	mem 14852MB
[2022-11-06 11:47:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][650/1251]	eta 0:04:42 lr 0.000768	time 0.4624 (0.4697)	loss 3.1462 (3.5520)	grad_norm 1.3007 (1.3286)	mem 14852MB
[2022-11-06 11:47:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][700/1251]	eta 0:04:18 lr 0.000768	time 0.4577 (0.4694)	loss 4.2084 (3.5456)	grad_norm 1.2640 (1.3294)	mem 14852MB
[2022-11-06 11:48:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][750/1251]	eta 0:03:55 lr 0.000768	time 0.4718 (0.4691)	loss 3.1632 (3.5481)	grad_norm 1.3195 (1.3300)	mem 14852MB
[2022-11-06 11:48:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][800/1251]	eta 0:03:31 lr 0.000767	time 0.4658 (0.4690)	loss 4.1776 (3.5578)	grad_norm 1.3044 (1.3297)	mem 14852MB
[2022-11-06 11:48:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][850/1251]	eta 0:03:08 lr 0.000767	time 0.4619 (0.4690)	loss 4.0719 (3.5593)	grad_norm 1.3466 (1.3303)	mem 14852MB
[2022-11-06 11:49:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][900/1251]	eta 0:02:44 lr 0.000767	time 0.4661 (0.4688)	loss 4.3177 (3.5615)	grad_norm 1.4686 (1.3298)	mem 14852MB
[2022-11-06 11:49:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][950/1251]	eta 0:02:21 lr 0.000767	time 0.4816 (0.4687)	loss 3.1801 (3.5549)	grad_norm 1.4165 (nan)	mem 14852MB
[2022-11-06 11:50:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][1000/1251]	eta 0:01:57 lr 0.000767	time 0.4641 (0.4686)	loss 3.5708 (3.5563)	grad_norm 1.1961 (nan)	mem 14852MB
[2022-11-06 11:50:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][1050/1251]	eta 0:01:34 lr 0.000767	time 0.4661 (0.4686)	loss 3.6944 (3.5522)	grad_norm 1.2700 (nan)	mem 14852MB
[2022-11-06 11:50:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][1100/1251]	eta 0:01:10 lr 0.000766	time 0.4558 (0.4686)	loss 3.1594 (3.5583)	grad_norm 1.2649 (nan)	mem 14852MB
[2022-11-06 11:51:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][1150/1251]	eta 0:00:47 lr 0.000766	time 0.4538 (0.4685)	loss 3.5820 (3.5607)	grad_norm 1.2516 (nan)	mem 14852MB
[2022-11-06 11:51:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][1200/1251]	eta 0:00:23 lr 0.000766	time 0.4683 (0.4685)	loss 4.0510 (3.5615)	grad_norm 1.3756 (nan)	mem 14852MB
[2022-11-06 11:51:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [96/300][1250/1251]	eta 0:00:00 lr 0.000766	time 0.4591 (0.4683)	loss 4.1837 (3.5606)	grad_norm 1.3134 (nan)	mem 14852MB
[2022-11-06 11:51:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 96 training takes 0:09:46
[2022-11-06 11:51:58 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_96.pth saving......
[2022-11-06 11:51:58 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_96.pth saved !!!
[2022-11-06 11:52:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.545 (1.545)	Loss 1.0496 (1.0496)	Acc@1 76.074 (76.074)	Acc@5 93.359 (93.359)	Mem 14852MB
[2022-11-06 11:52:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.570 Acc@5 93.134
[2022-11-06 11:52:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.6%
[2022-11-06 11:52:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.548 (1.548)	Loss 0.9119 (0.9119)	Acc@1 78.613 (78.613)	Acc@5 94.629 (94.629)	Mem 14852MB
[2022-11-06 11:52:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.562 Acc@5 94.440
[2022-11-06 11:52:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.6%
[2022-11-06 11:52:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.56% at 96 epoch
[2022-11-06 11:52:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][0/1251]	eta 0:41:50 lr 0.000766	time 2.0072 (2.0072)	loss 3.1484 (3.1484)	grad_norm 1.2787 (1.2787)	mem 14852MB
[2022-11-06 11:52:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][50/1251]	eta 0:10:03 lr 0.000766	time 0.4655 (0.5028)	loss 3.7051 (3.5742)	grad_norm 1.2680 (1.3366)	mem 14852MB
[2022-11-06 11:53:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][100/1251]	eta 0:09:22 lr 0.000765	time 0.4681 (0.4883)	loss 3.8343 (3.5087)	grad_norm 1.2445 (1.3104)	mem 14852MB
[2022-11-06 11:53:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][150/1251]	eta 0:08:50 lr 0.000765	time 0.4766 (0.4817)	loss 4.4329 (3.5719)	grad_norm 1.2669 (1.3188)	mem 14852MB
[2022-11-06 11:53:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][200/1251]	eta 0:08:22 lr 0.000765	time 0.4722 (0.4778)	loss 3.4554 (3.5687)	grad_norm 1.1744 (1.3153)	mem 14852MB
[2022-11-06 11:54:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][250/1251]	eta 0:07:56 lr 0.000765	time 0.4623 (0.4756)	loss 3.3946 (3.5743)	grad_norm 1.3044 (1.3151)	mem 14852MB
[2022-11-06 11:54:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][300/1251]	eta 0:07:30 lr 0.000765	time 0.4794 (0.4739)	loss 3.9986 (3.5754)	grad_norm 1.2500 (1.3209)	mem 14852MB
[2022-11-06 11:55:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][350/1251]	eta 0:07:06 lr 0.000765	time 0.4714 (0.4729)	loss 2.7075 (3.5724)	grad_norm 1.2509 (1.3205)	mem 14852MB
[2022-11-06 11:55:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][400/1251]	eta 0:06:41 lr 0.000764	time 0.4778 (0.4720)	loss 3.9808 (3.5772)	grad_norm 1.3490 (1.3191)	mem 14852MB
[2022-11-06 11:55:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][450/1251]	eta 0:06:17 lr 0.000764	time 0.4681 (0.4713)	loss 3.5944 (3.5654)	grad_norm 1.3017 (1.3183)	mem 14852MB
[2022-11-06 11:56:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][500/1251]	eta 0:05:53 lr 0.000764	time 0.4694 (0.4707)	loss 3.7676 (3.5638)	grad_norm 1.4974 (1.3196)	mem 14852MB
[2022-11-06 11:56:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][550/1251]	eta 0:05:30 lr 0.000764	time 0.4608 (0.4709)	loss 3.9066 (3.5564)	grad_norm 1.4052 (1.3229)	mem 14852MB
[2022-11-06 11:56:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][600/1251]	eta 0:05:06 lr 0.000764	time 0.4699 (0.4706)	loss 4.1435 (3.5554)	grad_norm 1.3954 (1.3253)	mem 14852MB
[2022-11-06 11:57:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][650/1251]	eta 0:04:42 lr 0.000764	time 0.4639 (0.4704)	loss 3.3090 (3.5573)	grad_norm 1.0885 (1.3262)	mem 14852MB
[2022-11-06 11:57:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][700/1251]	eta 0:04:18 lr 0.000763	time 0.4703 (0.4700)	loss 3.4504 (3.5563)	grad_norm 1.3817 (1.3248)	mem 14852MB
[2022-11-06 11:58:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][750/1251]	eta 0:03:55 lr 0.000763	time 0.4560 (0.4697)	loss 2.6519 (3.5465)	grad_norm 1.3002 (1.3244)	mem 14852MB
[2022-11-06 11:58:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][800/1251]	eta 0:03:31 lr 0.000763	time 0.4565 (0.4697)	loss 4.2741 (3.5482)	grad_norm 1.3879 (1.3257)	mem 14852MB
[2022-11-06 11:58:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][850/1251]	eta 0:03:08 lr 0.000763	time 0.4581 (0.4695)	loss 3.7610 (3.5502)	grad_norm 1.2450 (1.3266)	mem 14852MB
[2022-11-06 11:59:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][900/1251]	eta 0:02:44 lr 0.000763	time 0.4687 (0.4693)	loss 4.2956 (3.5529)	grad_norm 1.3010 (1.3280)	mem 14852MB
[2022-11-06 11:59:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][950/1251]	eta 0:02:21 lr 0.000762	time 0.4760 (0.4693)	loss 3.5692 (3.5446)	grad_norm 1.4291 (1.3291)	mem 14852MB
[2022-11-06 12:00:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][1000/1251]	eta 0:01:57 lr 0.000762	time 0.4609 (0.4690)	loss 2.4616 (3.5416)	grad_norm 1.2731 (1.3299)	mem 14852MB
[2022-11-06 12:00:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][1050/1251]	eta 0:01:34 lr 0.000762	time 0.4641 (0.4691)	loss 3.8955 (3.5409)	grad_norm 1.2901 (1.3313)	mem 14852MB
[2022-11-06 12:00:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][1100/1251]	eta 0:01:10 lr 0.000762	time 0.4638 (0.4690)	loss 3.6070 (3.5344)	grad_norm 1.4617 (1.3310)	mem 14852MB
[2022-11-06 12:01:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][1150/1251]	eta 0:00:47 lr 0.000762	time 0.4587 (0.4689)	loss 3.3072 (3.5341)	grad_norm 1.4511 (1.3326)	mem 14852MB
[2022-11-06 12:01:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][1200/1251]	eta 0:00:23 lr 0.000762	time 0.4650 (0.4688)	loss 3.1584 (3.5385)	grad_norm 1.4802 (1.3324)	mem 14852MB
[2022-11-06 12:02:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [97/300][1250/1251]	eta 0:00:00 lr 0.000761	time 0.4586 (0.4687)	loss 4.2753 (3.5335)	grad_norm 1.3796 (1.3327)	mem 14852MB
[2022-11-06 12:02:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 97 training takes 0:09:46
[2022-11-06 12:02:02 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_97.pth saving......
[2022-11-06 12:02:03 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_97.pth saved !!!
[2022-11-06 12:02:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.502 (1.502)	Loss 1.1046 (1.1046)	Acc@1 73.535 (73.535)	Acc@5 93.945 (93.945)	Mem 14852MB
[2022-11-06 12:02:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.972 Acc@5 93.198
[2022-11-06 12:02:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.0%
[2022-11-06 12:02:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.664 (1.664)	Loss 0.8897 (0.8897)	Acc@1 79.785 (79.785)	Acc@5 94.043 (94.043)	Mem 14852MB
[2022-11-06 12:02:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.586 Acc@5 94.448
[2022-11-06 12:02:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.6%
[2022-11-06 12:02:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.59% at 97 epoch
[2022-11-06 12:02:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][0/1251]	eta 0:41:01 lr 0.000761	time 1.9673 (1.9673)	loss 4.1282 (4.1282)	grad_norm 1.3034 (1.3034)	mem 14852MB
[2022-11-06 12:02:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][50/1251]	eta 0:10:03 lr 0.000761	time 0.4700 (0.5022)	loss 3.6339 (3.5627)	grad_norm 1.3281 (1.3427)	mem 14852MB
[2022-11-06 12:03:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][100/1251]	eta 0:09:18 lr 0.000761	time 0.4630 (0.4852)	loss 3.9975 (3.5181)	grad_norm 1.2973 (1.3306)	mem 14852MB
[2022-11-06 12:03:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][150/1251]	eta 0:08:47 lr 0.000761	time 0.4719 (0.4789)	loss 3.7730 (3.4954)	grad_norm 1.4627 (1.3284)	mem 14852MB
[2022-11-06 12:03:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][200/1251]	eta 0:08:20 lr 0.000761	time 0.4665 (0.4762)	loss 4.2097 (3.5132)	grad_norm 1.4687 (1.3331)	mem 14852MB
[2022-11-06 12:04:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][250/1251]	eta 0:07:54 lr 0.000761	time 0.4658 (0.4739)	loss 3.2200 (3.5348)	grad_norm 1.2923 (1.3358)	mem 14852MB
[2022-11-06 12:04:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][300/1251]	eta 0:07:29 lr 0.000760	time 0.4605 (0.4725)	loss 3.8562 (3.5303)	grad_norm 1.2423 (1.3404)	mem 14852MB
[2022-11-06 12:05:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][350/1251]	eta 0:07:04 lr 0.000760	time 0.4667 (0.4716)	loss 3.7986 (3.5251)	grad_norm 1.3069 (1.3394)	mem 14852MB
[2022-11-06 12:05:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][400/1251]	eta 0:06:40 lr 0.000760	time 0.4720 (0.4709)	loss 3.1158 (3.5247)	grad_norm 1.3256 (1.3408)	mem 14852MB
[2022-11-06 12:05:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][450/1251]	eta 0:06:16 lr 0.000760	time 0.4519 (0.4704)	loss 3.9155 (3.5411)	grad_norm 1.2865 (nan)	mem 14852MB
[2022-11-06 12:06:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][500/1251]	eta 0:05:53 lr 0.000760	time 0.4616 (0.4701)	loss 3.2813 (3.5242)	grad_norm 1.5319 (nan)	mem 14852MB
[2022-11-06 12:06:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][550/1251]	eta 0:05:29 lr 0.000759	time 0.4652 (0.4703)	loss 2.6105 (3.5159)	grad_norm 1.5179 (nan)	mem 14852MB
[2022-11-06 12:07:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][600/1251]	eta 0:05:05 lr 0.000759	time 0.4615 (0.4700)	loss 3.7501 (3.5166)	grad_norm 1.2229 (nan)	mem 14852MB
[2022-11-06 12:07:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][650/1251]	eta 0:04:42 lr 0.000759	time 0.4641 (0.4697)	loss 3.0084 (3.5050)	grad_norm 1.2302 (nan)	mem 14852MB
[2022-11-06 12:07:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][700/1251]	eta 0:04:18 lr 0.000759	time 0.4652 (0.4695)	loss 2.7848 (3.5128)	grad_norm 1.3037 (nan)	mem 14852MB
[2022-11-06 12:08:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][750/1251]	eta 0:03:55 lr 0.000759	time 0.4625 (0.4692)	loss 2.6797 (3.5101)	grad_norm 1.3025 (nan)	mem 14852MB
[2022-11-06 12:08:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][800/1251]	eta 0:03:31 lr 0.000759	time 0.4699 (0.4694)	loss 3.2279 (3.5123)	grad_norm 1.2577 (nan)	mem 14852MB
[2022-11-06 12:09:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][850/1251]	eta 0:03:08 lr 0.000758	time 0.4575 (0.4694)	loss 4.2075 (3.5032)	grad_norm 1.3872 (nan)	mem 14852MB
[2022-11-06 12:09:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][900/1251]	eta 0:02:44 lr 0.000758	time 0.4639 (0.4692)	loss 3.2636 (3.5095)	grad_norm 1.2373 (nan)	mem 14852MB
[2022-11-06 12:09:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][950/1251]	eta 0:02:21 lr 0.000758	time 0.4635 (0.4690)	loss 4.0295 (3.5192)	grad_norm 1.4909 (nan)	mem 14852MB
[2022-11-06 12:10:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][1000/1251]	eta 0:01:57 lr 0.000758	time 0.4765 (0.4689)	loss 3.9405 (3.5203)	grad_norm 1.2842 (nan)	mem 14852MB
[2022-11-06 12:10:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][1050/1251]	eta 0:01:34 lr 0.000758	time 0.4606 (0.4690)	loss 4.0816 (3.5242)	grad_norm 1.4385 (nan)	mem 14852MB
[2022-11-06 12:10:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][1100/1251]	eta 0:01:10 lr 0.000758	time 0.4743 (0.4690)	loss 3.8098 (3.5247)	grad_norm 1.4112 (nan)	mem 14852MB
[2022-11-06 12:11:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][1150/1251]	eta 0:00:47 lr 0.000757	time 0.4626 (0.4688)	loss 4.2135 (3.5260)	grad_norm 1.3990 (nan)	mem 14852MB
[2022-11-06 12:11:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][1200/1251]	eta 0:00:23 lr 0.000757	time 0.4722 (0.4687)	loss 2.6005 (3.5292)	grad_norm 1.4284 (nan)	mem 14852MB
[2022-11-06 12:12:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [98/300][1250/1251]	eta 0:00:00 lr 0.000757	time 0.4628 (0.4685)	loss 3.8565 (3.5281)	grad_norm 1.2774 (nan)	mem 14852MB
[2022-11-06 12:12:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 98 training takes 0:09:46
[2022-11-06 12:12:07 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_98.pth saving......
[2022-11-06 12:12:07 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_98.pth saved !!!
[2022-11-06 12:12:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.550 (1.550)	Loss 1.0295 (1.0295)	Acc@1 76.758 (76.758)	Acc@5 92.871 (92.871)	Mem 14852MB
[2022-11-06 12:12:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.124 Acc@5 93.314
[2022-11-06 12:12:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.1%
[2022-11-06 12:12:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.677 (1.677)	Loss 0.8363 (0.8363)	Acc@1 80.176 (80.176)	Acc@5 95.410 (95.410)	Mem 14852MB
[2022-11-06 12:12:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.658 Acc@5 94.470
[2022-11-06 12:12:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.7%
[2022-11-06 12:12:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.66% at 98 epoch
[2022-11-06 12:12:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][0/1251]	eta 0:40:51 lr 0.000757	time 1.9599 (1.9599)	loss 2.6688 (2.6688)	grad_norm 1.1897 (1.1897)	mem 14852MB
[2022-11-06 12:12:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][50/1251]	eta 0:10:05 lr 0.000757	time 0.4680 (0.5039)	loss 3.5609 (3.5857)	grad_norm 1.4009 (1.3353)	mem 14852MB
[2022-11-06 12:13:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][100/1251]	eta 0:09:21 lr 0.000757	time 0.4863 (0.4875)	loss 2.3394 (3.4593)	grad_norm 1.2024 (1.3480)	mem 14852MB
[2022-11-06 12:13:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][150/1251]	eta 0:08:48 lr 0.000756	time 0.4693 (0.4805)	loss 3.7898 (3.4408)	grad_norm 1.3740 (1.3571)	mem 14852MB
[2022-11-06 12:14:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][200/1251]	eta 0:08:21 lr 0.000756	time 0.4662 (0.4770)	loss 2.8715 (3.4632)	grad_norm 1.3739 (1.3543)	mem 14852MB
[2022-11-06 12:14:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][250/1251]	eta 0:07:55 lr 0.000756	time 0.4666 (0.4749)	loss 3.4842 (3.4796)	grad_norm 1.4346 (1.3472)	mem 14852MB
[2022-11-06 12:14:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][300/1251]	eta 0:07:30 lr 0.000756	time 0.4705 (0.4740)	loss 2.6801 (3.4728)	grad_norm 1.3270 (1.3456)	mem 14852MB
[2022-11-06 12:15:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][350/1251]	eta 0:07:06 lr 0.000756	time 0.4736 (0.4729)	loss 2.9326 (3.4968)	grad_norm 1.2800 (1.3471)	mem 14852MB
[2022-11-06 12:15:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][400/1251]	eta 0:06:41 lr 0.000756	time 0.4641 (0.4720)	loss 2.9853 (3.5138)	grad_norm 1.3060 (1.3531)	mem 14852MB
[2022-11-06 12:15:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][450/1251]	eta 0:06:17 lr 0.000755	time 0.4681 (0.4712)	loss 2.8411 (3.5206)	grad_norm 1.2748 (1.3529)	mem 14852MB
[2022-11-06 12:16:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][500/1251]	eta 0:05:53 lr 0.000755	time 0.4605 (0.4706)	loss 2.9211 (3.5209)	grad_norm 1.2573 (1.3503)	mem 14852MB
[2022-11-06 12:16:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][550/1251]	eta 0:05:29 lr 0.000755	time 0.4636 (0.4706)	loss 2.4411 (3.5092)	grad_norm 1.3405 (1.3506)	mem 14852MB
[2022-11-06 12:17:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][600/1251]	eta 0:05:06 lr 0.000755	time 0.4749 (0.4704)	loss 4.2468 (3.5117)	grad_norm 1.6194 (1.3510)	mem 14852MB
[2022-11-06 12:17:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][650/1251]	eta 0:04:42 lr 0.000755	time 0.4539 (0.4700)	loss 3.9766 (3.5128)	grad_norm 1.3765 (1.3509)	mem 14852MB
[2022-11-06 12:17:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][700/1251]	eta 0:04:18 lr 0.000754	time 0.5514 (0.4699)	loss 3.3848 (3.5091)	grad_norm 1.4487 (1.3509)	mem 14852MB
[2022-11-06 12:18:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][750/1251]	eta 0:03:55 lr 0.000754	time 0.4694 (0.4697)	loss 3.8628 (3.5115)	grad_norm 1.2683 (1.3504)	mem 14852MB
[2022-11-06 12:18:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][800/1251]	eta 0:03:31 lr 0.000754	time 0.4565 (0.4698)	loss 3.7932 (3.5137)	grad_norm 1.4996 (1.3509)	mem 14852MB
[2022-11-06 12:19:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][850/1251]	eta 0:03:08 lr 0.000754	time 0.4629 (0.4697)	loss 3.0793 (3.5216)	grad_norm 1.5715 (1.3514)	mem 14852MB
[2022-11-06 12:19:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][900/1251]	eta 0:02:44 lr 0.000754	time 0.4741 (0.4696)	loss 2.7613 (3.5255)	grad_norm 1.2556 (1.3503)	mem 14852MB
[2022-11-06 12:19:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][950/1251]	eta 0:02:21 lr 0.000754	time 0.4620 (0.4694)	loss 4.0221 (3.5291)	grad_norm 1.3918 (1.3509)	mem 14852MB
[2022-11-06 12:20:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][1000/1251]	eta 0:01:57 lr 0.000753	time 0.4691 (0.4693)	loss 2.7637 (3.5327)	grad_norm 1.3256 (1.3496)	mem 14852MB
[2022-11-06 12:20:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][1050/1251]	eta 0:01:34 lr 0.000753	time 0.4728 (0.4694)	loss 2.4696 (3.5320)	grad_norm 1.3050 (1.3501)	mem 14852MB
[2022-11-06 12:21:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][1100/1251]	eta 0:01:10 lr 0.000753	time 0.4648 (0.4693)	loss 2.8460 (3.5336)	grad_norm 1.4067 (1.3493)	mem 14852MB
[2022-11-06 12:21:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][1150/1251]	eta 0:00:47 lr 0.000753	time 0.4620 (0.4691)	loss 4.1051 (3.5344)	grad_norm 1.3787 (1.3487)	mem 14852MB
[2022-11-06 12:21:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][1200/1251]	eta 0:00:23 lr 0.000753	time 0.4785 (0.4690)	loss 4.1124 (3.5377)	grad_norm 1.4668 (1.3476)	mem 14852MB
[2022-11-06 12:22:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [99/300][1250/1251]	eta 0:00:00 lr 0.000753	time 0.4579 (0.4689)	loss 4.1714 (3.5426)	grad_norm 1.1619 (1.3474)	mem 14852MB
[2022-11-06 12:22:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 99 training takes 0:09:46
[2022-11-06 12:22:12 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_99.pth saving......
[2022-11-06 12:22:12 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_99.pth saved !!!
[2022-11-06 12:22:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.507 (1.507)	Loss 1.0061 (1.0061)	Acc@1 75.781 (75.781)	Acc@5 93.359 (93.359)	Mem 14852MB
[2022-11-06 12:22:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.324 Acc@5 93.458
[2022-11-06 12:22:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.3%
[2022-11-06 12:22:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.586 (1.586)	Loss 0.8751 (0.8751)	Acc@1 78.906 (78.906)	Acc@5 94.043 (94.043)	Mem 14852MB
[2022-11-06 12:22:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.668 Acc@5 94.484
[2022-11-06 12:22:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.7%
[2022-11-06 12:22:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.67% at 99 epoch
[2022-11-06 12:22:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][0/1251]	eta 0:40:22 lr 0.000753	time 1.9368 (1.9368)	loss 2.6383 (2.6383)	grad_norm 1.3356 (1.3356)	mem 14852MB
[2022-11-06 12:22:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][50/1251]	eta 0:10:00 lr 0.000752	time 0.4590 (0.5003)	loss 4.1569 (3.5869)	grad_norm 1.3744 (1.3541)	mem 14852MB
[2022-11-06 12:23:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][100/1251]	eta 0:09:19 lr 0.000752	time 0.4757 (0.4858)	loss 2.9545 (3.5270)	grad_norm 1.2253 (nan)	mem 14852MB
[2022-11-06 12:23:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][150/1251]	eta 0:08:48 lr 0.000752	time 0.4619 (0.4796)	loss 3.9860 (3.4656)	grad_norm 1.3669 (nan)	mem 14852MB
[2022-11-06 12:24:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][200/1251]	eta 0:08:20 lr 0.000752	time 0.4630 (0.4765)	loss 3.0233 (3.4629)	grad_norm 1.3733 (nan)	mem 14852MB
[2022-11-06 12:24:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][250/1251]	eta 0:07:54 lr 0.000752	time 0.4637 (0.4744)	loss 3.5316 (3.4787)	grad_norm 1.4471 (nan)	mem 14852MB
[2022-11-06 12:24:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][300/1251]	eta 0:07:29 lr 0.000751	time 0.4706 (0.4729)	loss 3.1434 (3.4730)	grad_norm 1.3041 (nan)	mem 14852MB
[2022-11-06 12:25:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][350/1251]	eta 0:07:05 lr 0.000751	time 0.4744 (0.4719)	loss 3.4429 (3.4868)	grad_norm 1.4723 (nan)	mem 14852MB
[2022-11-06 12:25:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][400/1251]	eta 0:06:41 lr 0.000751	time 0.4697 (0.4713)	loss 3.4281 (3.4862)	grad_norm 1.3048 (nan)	mem 14852MB
[2022-11-06 12:26:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][450/1251]	eta 0:06:17 lr 0.000751	time 0.4664 (0.4708)	loss 2.9242 (3.4890)	grad_norm 1.2416 (nan)	mem 14852MB
[2022-11-06 12:26:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][500/1251]	eta 0:05:53 lr 0.000751	time 0.4812 (0.4702)	loss 3.7153 (3.5056)	grad_norm 1.4296 (nan)	mem 14852MB
[2022-11-06 12:26:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][550/1251]	eta 0:05:29 lr 0.000751	time 0.4737 (0.4702)	loss 4.2639 (3.5131)	grad_norm 1.3095 (nan)	mem 14852MB
[2022-11-06 12:27:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][600/1251]	eta 0:05:05 lr 0.000750	time 0.4613 (0.4699)	loss 3.1781 (3.5210)	grad_norm 1.3726 (nan)	mem 14852MB
[2022-11-06 12:27:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][650/1251]	eta 0:04:42 lr 0.000750	time 0.5410 (0.4697)	loss 2.7114 (3.5157)	grad_norm 1.3296 (nan)	mem 14852MB
[2022-11-06 12:27:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][700/1251]	eta 0:04:18 lr 0.000750	time 0.4663 (0.4695)	loss 3.8367 (3.5268)	grad_norm 1.4278 (nan)	mem 14852MB
[2022-11-06 12:28:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][750/1251]	eta 0:03:55 lr 0.000750	time 0.4647 (0.4692)	loss 4.0242 (3.5208)	grad_norm 1.2971 (nan)	mem 14852MB
[2022-11-06 12:28:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][800/1251]	eta 0:03:31 lr 0.000750	time 0.4634 (0.4692)	loss 3.5651 (3.5178)	grad_norm 1.3514 (nan)	mem 14852MB
[2022-11-06 12:29:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][850/1251]	eta 0:03:08 lr 0.000749	time 0.4800 (0.4690)	loss 3.2812 (3.5170)	grad_norm 1.4766 (nan)	mem 14852MB
[2022-11-06 12:29:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][900/1251]	eta 0:02:44 lr 0.000749	time 0.4690 (0.4688)	loss 3.6471 (3.5195)	grad_norm 1.5102 (nan)	mem 14852MB
[2022-11-06 12:29:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][950/1251]	eta 0:02:21 lr 0.000749	time 0.4605 (0.4687)	loss 3.2559 (3.5201)	grad_norm 1.4416 (nan)	mem 14852MB
[2022-11-06 12:30:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][1000/1251]	eta 0:01:57 lr 0.000749	time 0.4616 (0.4686)	loss 3.5363 (3.5167)	grad_norm 1.3847 (nan)	mem 14852MB
[2022-11-06 12:30:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][1050/1251]	eta 0:01:34 lr 0.000749	time 0.4632 (0.4687)	loss 3.4894 (3.5135)	grad_norm 1.2272 (nan)	mem 14852MB
[2022-11-06 12:31:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][1100/1251]	eta 0:01:10 lr 0.000749	time 0.4674 (0.4686)	loss 2.9931 (3.5228)	grad_norm 1.4100 (nan)	mem 14852MB
[2022-11-06 12:31:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][1150/1251]	eta 0:00:47 lr 0.000748	time 0.5425 (0.4686)	loss 2.9401 (3.5218)	grad_norm 1.6415 (nan)	mem 14852MB
[2022-11-06 12:31:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][1200/1251]	eta 0:00:23 lr 0.000748	time 0.4676 (0.4684)	loss 2.9352 (3.5203)	grad_norm 1.9082 (nan)	mem 14852MB
[2022-11-06 12:32:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [100/300][1250/1251]	eta 0:00:00 lr 0.000748	time 0.4582 (0.4681)	loss 3.3987 (3.5211)	grad_norm 1.3561 (nan)	mem 14852MB
[2022-11-06 12:32:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 100 training takes 0:09:45
[2022-11-06 12:32:15 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_100.pth saving......
[2022-11-06 12:32:16 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_100.pth saved !!!
[2022-11-06 12:32:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.661 (1.661)	Loss 1.1040 (1.1040)	Acc@1 73.926 (73.926)	Acc@5 92.773 (92.773)	Mem 14852MB
[2022-11-06 12:32:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.000 Acc@5 93.330
[2022-11-06 12:32:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.0%
[2022-11-06 12:32:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.619 (1.619)	Loss 0.9271 (0.9271)	Acc@1 78.613 (78.613)	Acc@5 93.750 (93.750)	Mem 14852MB
[2022-11-06 12:32:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.664 Acc@5 94.540
[2022-11-06 12:32:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.7%
[2022-11-06 12:32:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.67% at 99 epoch
[2022-11-06 12:32:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][0/1251]	eta 0:40:40 lr 0.000748	time 1.9506 (1.9506)	loss 3.0797 (3.0797)	grad_norm 1.2286 (1.2286)	mem 14852MB
[2022-11-06 12:32:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][50/1251]	eta 0:10:00 lr 0.000748	time 0.4675 (0.4999)	loss 3.8023 (3.4470)	grad_norm 1.2235 (1.3586)	mem 14852MB
[2022-11-06 12:33:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][100/1251]	eta 0:09:18 lr 0.000748	time 0.4608 (0.4849)	loss 3.8675 (3.5300)	grad_norm 1.4704 (1.3393)	mem 14852MB
[2022-11-06 12:33:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][150/1251]	eta 0:08:47 lr 0.000747	time 0.4618 (0.4792)	loss 3.9454 (3.5468)	grad_norm 1.3375 (1.3366)	mem 14852MB
[2022-11-06 12:34:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][200/1251]	eta 0:08:19 lr 0.000747	time 0.4700 (0.4756)	loss 3.6497 (3.5427)	grad_norm 1.2717 (1.3359)	mem 14852MB
[2022-11-06 12:34:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][250/1251]	eta 0:07:54 lr 0.000747	time 0.4748 (0.4738)	loss 2.8502 (3.5127)	grad_norm 1.3850 (1.3393)	mem 14852MB
[2022-11-06 12:34:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][300/1251]	eta 0:07:29 lr 0.000747	time 0.4714 (0.4723)	loss 3.6240 (3.5028)	grad_norm 1.4253 (1.3392)	mem 14852MB
[2022-11-06 12:35:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][350/1251]	eta 0:07:04 lr 0.000747	time 0.4606 (0.4713)	loss 3.3763 (3.5060)	grad_norm 1.2684 (1.3424)	mem 14852MB
[2022-11-06 12:35:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][400/1251]	eta 0:06:40 lr 0.000747	time 0.4617 (0.4705)	loss 2.8750 (3.5004)	grad_norm 1.2938 (1.3411)	mem 14852MB
[2022-11-06 12:36:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][450/1251]	eta 0:06:16 lr 0.000746	time 0.4643 (0.4700)	loss 2.6414 (3.5018)	grad_norm 1.4299 (1.3403)	mem 14852MB
[2022-11-06 12:36:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][500/1251]	eta 0:05:52 lr 0.000746	time 0.4660 (0.4695)	loss 2.8993 (3.5039)	grad_norm 1.3298 (1.3418)	mem 14852MB
[2022-11-06 12:36:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][550/1251]	eta 0:05:29 lr 0.000746	time 0.5489 (0.4697)	loss 3.2927 (3.5075)	grad_norm 1.2532 (1.3437)	mem 14852MB
[2022-11-06 12:37:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][600/1251]	eta 0:05:05 lr 0.000746	time 0.4681 (0.4693)	loss 3.1312 (3.5110)	grad_norm 1.1821 (1.3451)	mem 14852MB
[2022-11-06 12:37:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][650/1251]	eta 0:04:41 lr 0.000746	time 0.4602 (0.4690)	loss 4.3824 (3.5097)	grad_norm 1.4283 (1.3462)	mem 14852MB
[2022-11-06 12:38:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][700/1251]	eta 0:04:18 lr 0.000745	time 0.4657 (0.4688)	loss 3.8182 (3.5095)	grad_norm 1.2939 (1.3458)	mem 14852MB
[2022-11-06 12:38:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][750/1251]	eta 0:03:54 lr 0.000745	time 0.4602 (0.4687)	loss 3.5846 (3.5137)	grad_norm 1.4429 (1.3473)	mem 14852MB
[2022-11-06 12:38:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][800/1251]	eta 0:03:31 lr 0.000745	time 0.4684 (0.4688)	loss 3.7784 (3.5213)	grad_norm 1.4360 (1.3467)	mem 14852MB
[2022-11-06 12:39:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][850/1251]	eta 0:03:07 lr 0.000745	time 0.4593 (0.4687)	loss 4.0151 (3.5207)	grad_norm 1.3759 (1.3457)	mem 14852MB
[2022-11-06 12:39:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][900/1251]	eta 0:02:44 lr 0.000745	time 0.4609 (0.4686)	loss 3.4814 (3.5184)	grad_norm 1.2690 (1.3458)	mem 14852MB
[2022-11-06 12:39:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][950/1251]	eta 0:02:20 lr 0.000745	time 0.4608 (0.4684)	loss 2.5500 (3.5183)	grad_norm 1.4079 (1.3468)	mem 14852MB
[2022-11-06 12:40:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][1000/1251]	eta 0:01:57 lr 0.000744	time 0.4610 (0.4682)	loss 3.1570 (3.5178)	grad_norm 1.2610 (1.3445)	mem 14852MB
[2022-11-06 12:40:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][1050/1251]	eta 0:01:34 lr 0.000744	time 0.4710 (0.4683)	loss 4.2225 (3.5170)	grad_norm 1.3031 (1.3441)	mem 14852MB
[2022-11-06 12:41:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][1100/1251]	eta 0:01:10 lr 0.000744	time 0.4715 (0.4682)	loss 3.6770 (3.5170)	grad_norm 1.3814 (1.3445)	mem 14852MB
[2022-11-06 12:41:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][1150/1251]	eta 0:00:47 lr 0.000744	time 0.4625 (0.4682)	loss 4.1272 (3.5171)	grad_norm 1.5475 (1.3455)	mem 14852MB
[2022-11-06 12:41:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][1200/1251]	eta 0:00:23 lr 0.000744	time 0.4631 (0.4681)	loss 3.7470 (3.5189)	grad_norm 1.4121 (1.3464)	mem 14852MB
[2022-11-06 12:42:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [101/300][1250/1251]	eta 0:00:00 lr 0.000743	time 0.4598 (0.4679)	loss 4.2830 (3.5217)	grad_norm 1.3484 (1.3458)	mem 14852MB
[2022-11-06 12:42:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 101 training takes 0:09:45
[2022-11-06 12:42:19 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_101.pth saving......
[2022-11-06 12:42:20 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_101.pth saved !!!
[2022-11-06 12:42:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.618 (1.618)	Loss 1.0039 (1.0039)	Acc@1 76.660 (76.660)	Acc@5 93.164 (93.164)	Mem 14852MB
[2022-11-06 12:42:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.256 Acc@5 93.412
[2022-11-06 12:42:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.3%
[2022-11-06 12:42:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.548 (1.548)	Loss 0.9027 (0.9027)	Acc@1 78.516 (78.516)	Acc@5 94.336 (94.336)	Mem 14852MB
[2022-11-06 12:42:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.680 Acc@5 94.544
[2022-11-06 12:42:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.7%
[2022-11-06 12:42:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.68% at 101 epoch
[2022-11-06 12:42:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][0/1251]	eta 0:43:00 lr 0.000743	time 2.0625 (2.0625)	loss 3.6673 (3.6673)	grad_norm 1.3028 (1.3028)	mem 14852MB
[2022-11-06 12:43:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][50/1251]	eta 0:10:07 lr 0.000743	time 0.4608 (0.5056)	loss 4.4310 (3.4982)	grad_norm 1.6022 (1.3489)	mem 14852MB
[2022-11-06 12:43:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][100/1251]	eta 0:09:22 lr 0.000743	time 0.4679 (0.4883)	loss 4.3336 (3.4484)	grad_norm 1.3084 (1.3499)	mem 14852MB
[2022-11-06 12:43:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][150/1251]	eta 0:08:49 lr 0.000743	time 0.4589 (0.4809)	loss 2.5744 (3.4359)	grad_norm 1.5716 (1.3532)	mem 14852MB
[2022-11-06 12:44:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][200/1251]	eta 0:08:21 lr 0.000743	time 0.4620 (0.4769)	loss 3.9443 (3.4358)	grad_norm 1.5447 (1.3539)	mem 14852MB
[2022-11-06 12:44:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][250/1251]	eta 0:07:55 lr 0.000743	time 0.4687 (0.4749)	loss 3.8297 (3.4576)	grad_norm 1.4574 (1.3563)	mem 14852MB
[2022-11-06 12:44:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][300/1251]	eta 0:07:30 lr 0.000742	time 0.4598 (0.4733)	loss 3.5145 (3.4926)	grad_norm 1.4320 (1.3578)	mem 14852MB
[2022-11-06 12:45:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][350/1251]	eta 0:07:05 lr 0.000742	time 0.4599 (0.4723)	loss 3.0444 (3.5104)	grad_norm 1.2669 (1.3616)	mem 14852MB
[2022-11-06 12:45:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][400/1251]	eta 0:06:41 lr 0.000742	time 0.4585 (0.4716)	loss 3.8709 (3.5133)	grad_norm 1.3889 (1.3609)	mem 14852MB
[2022-11-06 12:46:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][450/1251]	eta 0:06:17 lr 0.000742	time 0.4653 (0.4709)	loss 3.0881 (3.5098)	grad_norm 1.5561 (1.3594)	mem 14852MB
[2022-11-06 12:46:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][500/1251]	eta 0:05:53 lr 0.000742	time 0.4606 (0.4704)	loss 3.4009 (3.5081)	grad_norm 1.3079 (1.3612)	mem 14852MB
[2022-11-06 12:46:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][550/1251]	eta 0:05:29 lr 0.000741	time 0.4685 (0.4703)	loss 3.6543 (3.5011)	grad_norm 1.3875 (1.3600)	mem 14852MB
[2022-11-06 12:47:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][600/1251]	eta 0:05:06 lr 0.000741	time 0.4655 (0.4702)	loss 3.1422 (3.5038)	grad_norm 1.3488 (1.3618)	mem 14852MB
[2022-11-06 12:47:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][650/1251]	eta 0:04:42 lr 0.000741	time 0.4648 (0.4698)	loss 2.7419 (3.5048)	grad_norm 1.2062 (1.3601)	mem 14852MB
[2022-11-06 12:48:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][700/1251]	eta 0:04:18 lr 0.000741	time 0.4682 (0.4697)	loss 3.9539 (3.5047)	grad_norm 1.3641 (1.3603)	mem 14852MB
[2022-11-06 12:48:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][750/1251]	eta 0:03:55 lr 0.000741	time 0.4645 (0.4693)	loss 3.3700 (3.5024)	grad_norm 1.3456 (1.3588)	mem 14852MB
[2022-11-06 12:48:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][800/1251]	eta 0:03:31 lr 0.000741	time 0.4748 (0.4694)	loss 3.7770 (3.5120)	grad_norm 1.2929 (1.3577)	mem 14852MB
[2022-11-06 12:49:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][850/1251]	eta 0:03:08 lr 0.000740	time 0.4612 (0.4692)	loss 2.5854 (3.5146)	grad_norm 1.2801 (1.3563)	mem 14852MB
[2022-11-06 12:49:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][900/1251]	eta 0:02:44 lr 0.000740	time 0.4685 (0.4690)	loss 4.2866 (3.5166)	grad_norm 1.3838 (1.3567)	mem 14852MB
[2022-11-06 12:50:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][950/1251]	eta 0:02:21 lr 0.000740	time 0.4582 (0.4688)	loss 3.7273 (3.5241)	grad_norm 1.3543 (1.3562)	mem 14852MB
[2022-11-06 12:50:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][1000/1251]	eta 0:01:57 lr 0.000740	time 0.4736 (0.4688)	loss 3.9466 (3.5210)	grad_norm 1.5283 (1.3564)	mem 14852MB
[2022-11-06 12:50:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][1050/1251]	eta 0:01:34 lr 0.000740	time 0.4749 (0.4688)	loss 4.2156 (3.5208)	grad_norm 1.4400 (1.3585)	mem 14852MB
[2022-11-06 12:51:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][1100/1251]	eta 0:01:10 lr 0.000739	time 0.5418 (0.4688)	loss 4.1901 (3.5201)	grad_norm 1.4409 (1.3571)	mem 14852MB
[2022-11-06 12:51:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][1150/1251]	eta 0:00:47 lr 0.000739	time 0.4690 (0.4686)	loss 3.3300 (3.5134)	grad_norm 1.3358 (1.3560)	mem 14852MB
[2022-11-06 12:52:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][1200/1251]	eta 0:00:23 lr 0.000739	time 0.4620 (0.4685)	loss 2.9687 (3.5141)	grad_norm 1.3301 (1.3565)	mem 14852MB
[2022-11-06 12:52:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [102/300][1250/1251]	eta 0:00:00 lr 0.000739	time 0.4568 (0.4684)	loss 3.9849 (3.5133)	grad_norm 1.2876 (1.3558)	mem 14852MB
[2022-11-06 12:52:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 102 training takes 0:09:46
[2022-11-06 12:52:23 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_102.pth saving......
[2022-11-06 12:52:24 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_102.pth saved !!!
[2022-11-06 12:52:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.509 (1.509)	Loss 1.0892 (1.0892)	Acc@1 75.000 (75.000)	Acc@5 92.285 (92.285)	Mem 14852MB
[2022-11-06 12:52:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.160 Acc@5 93.244
[2022-11-06 12:52:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.2%
[2022-11-06 12:52:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.582 (1.582)	Loss 0.8391 (0.8391)	Acc@1 81.055 (81.055)	Acc@5 95.312 (95.312)	Mem 14852MB
[2022-11-06 12:52:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.714 Acc@5 94.536
[2022-11-06 12:52:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.7%
[2022-11-06 12:52:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.71% at 102 epoch
[2022-11-06 12:52:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][0/1251]	eta 0:41:57 lr 0.000739	time 2.0123 (2.0123)	loss 3.5217 (3.5217)	grad_norm 1.4040 (1.4040)	mem 14852MB
[2022-11-06 12:53:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][50/1251]	eta 0:10:00 lr 0.000739	time 0.4591 (0.5003)	loss 4.4081 (3.3947)	grad_norm 1.2512 (1.3530)	mem 14852MB
[2022-11-06 12:53:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][100/1251]	eta 0:09:19 lr 0.000739	time 0.4590 (0.4858)	loss 3.9135 (3.4730)	grad_norm 1.5151 (1.3590)	mem 14852MB
[2022-11-06 12:53:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][150/1251]	eta 0:08:47 lr 0.000738	time 0.4661 (0.4793)	loss 4.2774 (3.4553)	grad_norm 1.2879 (1.3505)	mem 14852MB
[2022-11-06 12:54:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][200/1251]	eta 0:08:20 lr 0.000738	time 0.4688 (0.4767)	loss 3.8423 (3.4725)	grad_norm 1.3346 (1.3475)	mem 14852MB
[2022-11-06 12:54:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][250/1251]	eta 0:07:54 lr 0.000738	time 0.4637 (0.4743)	loss 2.8523 (3.4883)	grad_norm 1.4241 (1.3445)	mem 14852MB
[2022-11-06 12:55:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][300/1251]	eta 0:07:29 lr 0.000738	time 0.4651 (0.4729)	loss 3.0028 (3.4748)	grad_norm 1.2933 (1.3518)	mem 14852MB
[2022-11-06 12:55:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][350/1251]	eta 0:07:05 lr 0.000738	time 0.4650 (0.4719)	loss 2.9042 (3.4668)	grad_norm 1.4628 (1.3550)	mem 14852MB
[2022-11-06 12:55:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][400/1251]	eta 0:06:40 lr 0.000737	time 0.4618 (0.4712)	loss 4.1624 (3.4781)	grad_norm 1.3420 (1.3552)	mem 14852MB
[2022-11-06 12:56:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][450/1251]	eta 0:06:17 lr 0.000737	time 0.4645 (0.4708)	loss 3.6067 (3.4867)	grad_norm 1.3495 (nan)	mem 14852MB
[2022-11-06 12:56:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][500/1251]	eta 0:05:53 lr 0.000737	time 0.4755 (0.4705)	loss 3.7594 (3.4826)	grad_norm 1.4344 (nan)	mem 14852MB
[2022-11-06 12:57:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][550/1251]	eta 0:05:29 lr 0.000737	time 0.4615 (0.4706)	loss 3.5941 (3.4797)	grad_norm 1.3164 (nan)	mem 14852MB
[2022-11-06 12:57:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][600/1251]	eta 0:05:06 lr 0.000737	time 0.4712 (0.4702)	loss 2.3340 (3.4853)	grad_norm 1.2526 (nan)	mem 14852MB
[2022-11-06 12:57:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][650/1251]	eta 0:04:42 lr 0.000737	time 0.4646 (0.4699)	loss 3.0448 (3.4970)	grad_norm 1.2870 (nan)	mem 14852MB
[2022-11-06 12:58:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][700/1251]	eta 0:04:18 lr 0.000736	time 0.4625 (0.4697)	loss 2.4579 (3.4962)	grad_norm 1.2894 (nan)	mem 14852MB
[2022-11-06 12:58:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][750/1251]	eta 0:03:55 lr 0.000736	time 0.4805 (0.4695)	loss 3.7443 (3.5008)	grad_norm 1.4865 (nan)	mem 14852MB
[2022-11-06 12:58:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][800/1251]	eta 0:03:31 lr 0.000736	time 0.4744 (0.4697)	loss 4.3524 (3.4911)	grad_norm 1.3025 (nan)	mem 14852MB
[2022-11-06 12:59:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][850/1251]	eta 0:03:08 lr 0.000736	time 0.4708 (0.4694)	loss 3.6615 (3.4945)	grad_norm 1.3234 (nan)	mem 14852MB
[2022-11-06 12:59:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][900/1251]	eta 0:02:44 lr 0.000736	time 0.5401 (0.4692)	loss 3.6432 (3.4922)	grad_norm 1.1811 (nan)	mem 14852MB
[2022-11-06 13:00:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][950/1251]	eta 0:02:21 lr 0.000735	time 0.4693 (0.4692)	loss 3.7195 (3.5008)	grad_norm 1.3566 (nan)	mem 14852MB
[2022-11-06 13:00:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][1000/1251]	eta 0:01:57 lr 0.000735	time 0.4667 (0.4690)	loss 3.6063 (3.5077)	grad_norm 1.3993 (nan)	mem 14852MB
[2022-11-06 13:00:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][1050/1251]	eta 0:01:34 lr 0.000735	time 0.4653 (0.4691)	loss 4.2158 (3.5072)	grad_norm 1.3377 (nan)	mem 14852MB
[2022-11-06 13:01:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][1100/1251]	eta 0:01:10 lr 0.000735	time 0.4526 (0.4690)	loss 3.9932 (3.5070)	grad_norm 1.4298 (nan)	mem 14852MB
[2022-11-06 13:01:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][1150/1251]	eta 0:00:47 lr 0.000735	time 0.4682 (0.4689)	loss 3.5061 (3.5119)	grad_norm 1.2440 (nan)	mem 14852MB
[2022-11-06 13:02:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][1200/1251]	eta 0:00:23 lr 0.000735	time 0.4662 (0.4688)	loss 3.9381 (3.5125)	grad_norm 1.3603 (nan)	mem 14852MB
[2022-11-06 13:02:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [103/300][1250/1251]	eta 0:00:00 lr 0.000734	time 0.4590 (0.4686)	loss 3.4471 (3.5135)	grad_norm 1.2917 (nan)	mem 14852MB
[2022-11-06 13:02:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 103 training takes 0:09:46
[2022-11-06 13:02:27 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_103.pth saving......
[2022-11-06 13:02:28 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_103.pth saved !!!
[2022-11-06 13:02:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.584 (1.584)	Loss 1.0314 (1.0314)	Acc@1 77.051 (77.051)	Acc@5 92.871 (92.871)	Mem 14852MB
[2022-11-06 13:02:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.978 Acc@5 93.410
[2022-11-06 13:02:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.0%
[2022-11-06 13:02:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.641 (1.641)	Loss 0.8874 (0.8874)	Acc@1 79.102 (79.102)	Acc@5 95.020 (95.020)	Mem 14852MB
[2022-11-06 13:02:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.748 Acc@5 94.566
[2022-11-06 13:02:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.7%
[2022-11-06 13:02:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.75% at 103 epoch
[2022-11-06 13:02:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][0/1251]	eta 0:41:21 lr 0.000734	time 1.9838 (1.9838)	loss 3.6084 (3.6084)	grad_norm 1.3387 (1.3387)	mem 14852MB
[2022-11-06 13:03:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][50/1251]	eta 0:10:05 lr 0.000734	time 0.4574 (0.5039)	loss 3.7487 (3.3680)	grad_norm 1.7458 (1.3710)	mem 14852MB
[2022-11-06 13:03:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][100/1251]	eta 0:09:18 lr 0.000734	time 0.4635 (0.4856)	loss 3.8730 (3.4840)	grad_norm 1.3279 (1.3539)	mem 14852MB
[2022-11-06 13:03:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][150/1251]	eta 0:08:47 lr 0.000734	time 0.4685 (0.4789)	loss 2.9726 (3.5021)	grad_norm 1.2809 (1.3647)	mem 14852MB
[2022-11-06 13:04:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][200/1251]	eta 0:08:20 lr 0.000734	time 0.4594 (0.4760)	loss 3.2425 (3.5236)	grad_norm 1.6377 (1.3714)	mem 14852MB
[2022-11-06 13:04:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][250/1251]	eta 0:07:54 lr 0.000733	time 0.4739 (0.4740)	loss 3.3929 (3.5073)	grad_norm 1.2344 (1.3652)	mem 14852MB
[2022-11-06 13:05:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][300/1251]	eta 0:07:30 lr 0.000733	time 0.4728 (0.4733)	loss 4.0594 (3.5163)	grad_norm 1.2249 (1.3591)	mem 14852MB
[2022-11-06 13:05:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][350/1251]	eta 0:07:05 lr 0.000733	time 0.4680 (0.4721)	loss 3.1079 (3.5028)	grad_norm 1.3306 (1.3593)	mem 14852MB
[2022-11-06 13:05:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][400/1251]	eta 0:06:40 lr 0.000733	time 0.4633 (0.4712)	loss 3.4425 (3.5159)	grad_norm 1.3778 (1.3643)	mem 14852MB
[2022-11-06 13:06:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][450/1251]	eta 0:06:17 lr 0.000733	time 0.4621 (0.4707)	loss 3.7008 (3.4997)	grad_norm 1.3755 (1.3609)	mem 14852MB
[2022-11-06 13:06:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][500/1251]	eta 0:05:53 lr 0.000732	time 0.4609 (0.4702)	loss 3.7984 (3.5032)	grad_norm 1.3330 (1.3653)	mem 14852MB
[2022-11-06 13:07:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][550/1251]	eta 0:05:29 lr 0.000732	time 0.4630 (0.4701)	loss 3.7194 (3.5105)	grad_norm 1.2940 (1.3683)	mem 14852MB
[2022-11-06 13:07:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][600/1251]	eta 0:05:05 lr 0.000732	time 0.4661 (0.4696)	loss 3.7223 (3.5020)	grad_norm 1.3832 (1.3694)	mem 14852MB
[2022-11-06 13:07:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][650/1251]	eta 0:04:42 lr 0.000732	time 0.4660 (0.4695)	loss 2.3494 (3.5013)	grad_norm 1.2172 (1.3703)	mem 14852MB
[2022-11-06 13:08:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][700/1251]	eta 0:04:18 lr 0.000732	time 0.4759 (0.4693)	loss 3.5766 (3.5009)	grad_norm 1.3013 (1.3686)	mem 14852MB
[2022-11-06 13:08:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][750/1251]	eta 0:03:55 lr 0.000732	time 0.4677 (0.4692)	loss 3.4377 (3.5035)	grad_norm 1.2145 (1.3675)	mem 14852MB
[2022-11-06 13:09:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][800/1251]	eta 0:03:31 lr 0.000731	time 0.4712 (0.4693)	loss 4.0091 (3.5068)	grad_norm 1.3546 (1.3671)	mem 14852MB
[2022-11-06 13:09:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][850/1251]	eta 0:03:08 lr 0.000731	time 0.4709 (0.4690)	loss 4.0248 (3.5085)	grad_norm 1.3149 (1.3658)	mem 14852MB
[2022-11-06 13:09:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][900/1251]	eta 0:02:44 lr 0.000731	time 0.4653 (0.4688)	loss 3.7842 (3.5094)	grad_norm 1.3775 (1.3651)	mem 14852MB
[2022-11-06 13:10:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][950/1251]	eta 0:02:21 lr 0.000731	time 0.4530 (0.4688)	loss 2.7304 (3.5055)	grad_norm 1.3167 (1.3658)	mem 14852MB
[2022-11-06 13:10:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][1000/1251]	eta 0:01:57 lr 0.000731	time 0.4636 (0.4687)	loss 3.6338 (3.5048)	grad_norm 1.4265 (1.3666)	mem 14852MB
[2022-11-06 13:10:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][1050/1251]	eta 0:01:34 lr 0.000730	time 0.4679 (0.4688)	loss 3.7371 (3.5084)	grad_norm 1.2487 (1.3672)	mem 14852MB
[2022-11-06 13:11:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][1100/1251]	eta 0:01:10 lr 0.000730	time 0.4742 (0.4687)	loss 4.3055 (3.5071)	grad_norm 1.4032 (1.3673)	mem 14852MB
[2022-11-06 13:11:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][1150/1251]	eta 0:00:47 lr 0.000730	time 0.4772 (0.4686)	loss 3.6422 (3.5096)	grad_norm 1.3646 (1.3668)	mem 14852MB
[2022-11-06 13:12:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][1200/1251]	eta 0:00:23 lr 0.000730	time 0.4580 (0.4686)	loss 2.4335 (3.5097)	grad_norm 1.5149 (1.3678)	mem 14852MB
[2022-11-06 13:12:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [104/300][1250/1251]	eta 0:00:00 lr 0.000730	time 0.4594 (0.4684)	loss 3.8375 (3.5097)	grad_norm 1.4857 (1.3667)	mem 14852MB
[2022-11-06 13:12:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 104 training takes 0:09:46
[2022-11-06 13:12:32 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_104.pth saving......
[2022-11-06 13:12:32 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_104.pth saved !!!
[2022-11-06 13:12:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.649 (1.649)	Loss 1.0063 (1.0063)	Acc@1 76.074 (76.074)	Acc@5 94.434 (94.434)	Mem 14852MB
[2022-11-06 13:12:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.196 Acc@5 93.402
[2022-11-06 13:12:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.2%
[2022-11-06 13:12:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.601 (1.601)	Loss 0.8506 (0.8506)	Acc@1 79.102 (79.102)	Acc@5 95.312 (95.312)	Mem 14852MB
[2022-11-06 13:12:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.770 Acc@5 94.618
[2022-11-06 13:12:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.8%
[2022-11-06 13:12:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.77% at 104 epoch
[2022-11-06 13:12:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][0/1251]	eta 0:40:01 lr 0.000730	time 1.9198 (1.9198)	loss 3.6200 (3.6200)	grad_norm 1.6241 (1.6241)	mem 14852MB
[2022-11-06 13:13:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][50/1251]	eta 0:10:03 lr 0.000730	time 0.4618 (0.5023)	loss 3.7319 (3.5368)	grad_norm 1.2696 (1.3541)	mem 14852MB
[2022-11-06 13:13:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][100/1251]	eta 0:09:18 lr 0.000729	time 0.4689 (0.4848)	loss 4.1721 (3.4311)	grad_norm 1.3644 (1.3518)	mem 14852MB
[2022-11-06 13:14:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][150/1251]	eta 0:08:47 lr 0.000729	time 0.4727 (0.4795)	loss 3.8231 (3.4406)	grad_norm 1.4961 (1.3652)	mem 14852MB
[2022-11-06 13:14:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][200/1251]	eta 0:08:20 lr 0.000729	time 0.4690 (0.4765)	loss 4.1839 (3.4357)	grad_norm 1.3844 (1.3630)	mem 14852MB
[2022-11-06 13:14:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][250/1251]	eta 0:07:54 lr 0.000729	time 0.4637 (0.4742)	loss 3.5378 (3.4504)	grad_norm 1.3242 (1.3686)	mem 14852MB
[2022-11-06 13:15:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][300/1251]	eta 0:07:29 lr 0.000729	time 0.4595 (0.4731)	loss 3.6774 (3.4685)	grad_norm 1.3275 (1.3682)	mem 14852MB
[2022-11-06 13:15:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][350/1251]	eta 0:07:05 lr 0.000728	time 0.4603 (0.4723)	loss 3.4976 (3.4625)	grad_norm 1.3547 (1.3673)	mem 14852MB
[2022-11-06 13:15:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][400/1251]	eta 0:06:41 lr 0.000728	time 0.4616 (0.4714)	loss 3.0279 (3.4629)	grad_norm 1.2499 (1.3683)	mem 14852MB
[2022-11-06 13:16:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][450/1251]	eta 0:06:17 lr 0.000728	time 0.4585 (0.4709)	loss 3.5675 (3.4674)	grad_norm 1.5206 (1.3662)	mem 14852MB
[2022-11-06 13:16:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][500/1251]	eta 0:05:53 lr 0.000728	time 0.4656 (0.4705)	loss 2.9831 (3.4729)	grad_norm 1.4076 (1.3663)	mem 14852MB
[2022-11-06 13:17:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][550/1251]	eta 0:05:29 lr 0.000728	time 0.4796 (0.4704)	loss 3.8615 (3.4721)	grad_norm 1.4317 (1.3649)	mem 14852MB
[2022-11-06 13:17:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][600/1251]	eta 0:05:05 lr 0.000728	time 0.4725 (0.4700)	loss 3.2759 (3.4678)	grad_norm 1.3980 (1.3622)	mem 14852MB
[2022-11-06 13:17:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][650/1251]	eta 0:04:42 lr 0.000727	time 0.4674 (0.4698)	loss 4.1025 (3.4690)	grad_norm 1.3601 (1.3626)	mem 14852MB
[2022-11-06 13:18:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][700/1251]	eta 0:04:18 lr 0.000727	time 0.4653 (0.4694)	loss 4.0343 (3.4739)	grad_norm 1.3757 (1.3628)	mem 14852MB
[2022-11-06 13:18:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][750/1251]	eta 0:03:55 lr 0.000727	time 0.4699 (0.4694)	loss 4.0242 (3.4850)	grad_norm 1.2422 (1.3610)	mem 14852MB
[2022-11-06 13:19:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][800/1251]	eta 0:03:31 lr 0.000727	time 0.4678 (0.4694)	loss 3.8024 (3.4825)	grad_norm 1.4094 (1.3626)	mem 14852MB
[2022-11-06 13:19:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][850/1251]	eta 0:03:08 lr 0.000727	time 0.4705 (0.4692)	loss 2.8691 (3.4875)	grad_norm 1.3887 (1.3618)	mem 14852MB
[2022-11-06 13:19:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][900/1251]	eta 0:02:44 lr 0.000726	time 0.4717 (0.4691)	loss 3.3304 (3.4881)	grad_norm 1.2909 (1.3645)	mem 14852MB
[2022-11-06 13:20:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][950/1251]	eta 0:02:21 lr 0.000726	time 0.4618 (0.4690)	loss 3.8710 (3.4927)	grad_norm 1.5481 (1.3647)	mem 14852MB
[2022-11-06 13:20:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][1000/1251]	eta 0:01:57 lr 0.000726	time 0.4654 (0.4688)	loss 3.3619 (3.4881)	grad_norm 1.1178 (1.3638)	mem 14852MB
[2022-11-06 13:21:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][1050/1251]	eta 0:01:34 lr 0.000726	time 0.4591 (0.4689)	loss 4.1477 (3.4923)	grad_norm 1.2389 (1.3640)	mem 14852MB
[2022-11-06 13:21:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][1100/1251]	eta 0:01:10 lr 0.000726	time 0.4667 (0.4688)	loss 2.3474 (3.4942)	grad_norm 1.5166 (1.3638)	mem 14852MB
[2022-11-06 13:21:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][1150/1251]	eta 0:00:47 lr 0.000725	time 0.4623 (0.4687)	loss 3.9295 (3.4992)	grad_norm 1.3985 (1.3635)	mem 14852MB
[2022-11-06 13:22:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][1200/1251]	eta 0:00:23 lr 0.000725	time 0.4619 (0.4686)	loss 3.9960 (3.5020)	grad_norm 1.2979 (1.3632)	mem 14852MB
[2022-11-06 13:22:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [105/300][1250/1251]	eta 0:00:00 lr 0.000725	time 0.4581 (0.4684)	loss 2.7347 (3.5029)	grad_norm 1.3043 (1.3630)	mem 14852MB
[2022-11-06 13:22:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 105 training takes 0:09:46
[2022-11-06 13:22:36 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_105.pth saving......
[2022-11-06 13:22:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_105.pth saved !!!
[2022-11-06 13:22:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.615 (1.615)	Loss 1.0487 (1.0487)	Acc@1 77.051 (77.051)	Acc@5 93.359 (93.359)	Mem 14852MB
[2022-11-06 13:22:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.152 Acc@5 93.398
[2022-11-06 13:22:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.2%
[2022-11-06 13:22:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.785 (1.785)	Loss 0.8030 (0.8030)	Acc@1 81.934 (81.934)	Acc@5 95.410 (95.410)	Mem 14852MB
[2022-11-06 13:22:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.830 Acc@5 94.628
[2022-11-06 13:22:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.8%
[2022-11-06 13:22:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.83% at 105 epoch
[2022-11-06 13:22:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][0/1251]	eta 0:41:09 lr 0.000725	time 1.9740 (1.9740)	loss 3.3116 (3.3116)	grad_norm 1.3935 (1.3935)	mem 14852MB
[2022-11-06 13:23:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][50/1251]	eta 0:09:59 lr 0.000725	time 0.4539 (0.4990)	loss 3.9206 (3.4575)	grad_norm 1.2868 (1.3665)	mem 14852MB
[2022-11-06 13:23:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][100/1251]	eta 0:09:17 lr 0.000725	time 0.4575 (0.4845)	loss 3.1708 (3.4368)	grad_norm 1.4544 (1.3695)	mem 14852MB
[2022-11-06 13:24:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][150/1251]	eta 0:08:47 lr 0.000725	time 0.4612 (0.4787)	loss 3.2462 (3.3882)	grad_norm 1.6288 (1.3697)	mem 14852MB
[2022-11-06 13:24:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][200/1251]	eta 0:08:19 lr 0.000724	time 0.4677 (0.4757)	loss 4.0549 (3.4369)	grad_norm 1.2932 (1.3754)	mem 14852MB
[2022-11-06 13:24:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][250/1251]	eta 0:07:54 lr 0.000724	time 0.4585 (0.4739)	loss 3.2684 (3.4776)	grad_norm 1.5069 (1.3730)	mem 14852MB
[2022-11-06 13:25:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][300/1251]	eta 0:07:29 lr 0.000724	time 0.4663 (0.4729)	loss 3.2302 (3.4872)	grad_norm 1.3526 (1.3723)	mem 14852MB
[2022-11-06 13:25:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][350/1251]	eta 0:07:05 lr 0.000724	time 0.4674 (0.4720)	loss 3.3082 (3.4982)	grad_norm 1.3399 (1.3668)	mem 14852MB
[2022-11-06 13:26:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][400/1251]	eta 0:06:41 lr 0.000724	time 0.4650 (0.4713)	loss 3.3776 (3.5024)	grad_norm 1.3673 (1.3674)	mem 14852MB
[2022-11-06 13:26:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][450/1251]	eta 0:06:16 lr 0.000723	time 0.4728 (0.4706)	loss 3.7381 (3.5056)	grad_norm 1.2786 (nan)	mem 14852MB
[2022-11-06 13:26:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][500/1251]	eta 0:05:53 lr 0.000723	time 0.4709 (0.4702)	loss 3.5335 (3.5140)	grad_norm 1.3320 (nan)	mem 14852MB
[2022-11-06 13:27:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][550/1251]	eta 0:05:29 lr 0.000723	time 0.4575 (0.4703)	loss 2.9176 (3.5128)	grad_norm 1.3141 (nan)	mem 14852MB
[2022-11-06 13:27:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][600/1251]	eta 0:05:05 lr 0.000723	time 0.4640 (0.4699)	loss 3.1899 (3.5043)	grad_norm 1.3313 (nan)	mem 14852MB
[2022-11-06 13:28:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][650/1251]	eta 0:04:42 lr 0.000723	time 0.4707 (0.4695)	loss 3.0878 (3.5136)	grad_norm 1.2401 (nan)	mem 14852MB
[2022-11-06 13:28:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][700/1251]	eta 0:04:18 lr 0.000722	time 0.4655 (0.4693)	loss 2.8853 (3.5155)	grad_norm 1.2126 (nan)	mem 14852MB
[2022-11-06 13:28:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][750/1251]	eta 0:03:55 lr 0.000722	time 0.4669 (0.4691)	loss 3.8958 (3.5193)	grad_norm 1.4041 (nan)	mem 14852MB
[2022-11-06 13:29:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][800/1251]	eta 0:03:31 lr 0.000722	time 0.4639 (0.4690)	loss 4.1612 (3.5199)	grad_norm 1.3523 (nan)	mem 14852MB
[2022-11-06 13:29:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][850/1251]	eta 0:03:08 lr 0.000722	time 0.4528 (0.4689)	loss 3.5469 (3.5194)	grad_norm 1.2263 (nan)	mem 14852MB
[2022-11-06 13:29:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][900/1251]	eta 0:02:44 lr 0.000722	time 0.4594 (0.4688)	loss 3.5675 (3.5226)	grad_norm 1.2734 (nan)	mem 14852MB
[2022-11-06 13:30:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][950/1251]	eta 0:02:21 lr 0.000722	time 0.4684 (0.4688)	loss 4.1523 (3.5188)	grad_norm 1.3721 (nan)	mem 14852MB
[2022-11-06 13:30:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][1000/1251]	eta 0:01:57 lr 0.000721	time 0.4622 (0.4686)	loss 3.7452 (3.5222)	grad_norm 1.4325 (nan)	mem 14852MB
[2022-11-06 13:31:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][1050/1251]	eta 0:01:34 lr 0.000721	time 0.4714 (0.4685)	loss 2.9353 (3.5194)	grad_norm 1.2194 (nan)	mem 14852MB
[2022-11-06 13:31:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][1100/1251]	eta 0:01:10 lr 0.000721	time 0.4625 (0.4686)	loss 4.1077 (3.5209)	grad_norm 1.2730 (nan)	mem 14852MB
[2022-11-06 13:31:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][1150/1251]	eta 0:00:47 lr 0.000721	time 0.4689 (0.4685)	loss 3.5595 (3.5180)	grad_norm 1.3769 (nan)	mem 14852MB
[2022-11-06 13:32:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][1200/1251]	eta 0:00:23 lr 0.000721	time 0.4527 (0.4685)	loss 2.6418 (3.5142)	grad_norm 1.4677 (nan)	mem 14852MB
[2022-11-06 13:32:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [106/300][1250/1251]	eta 0:00:00 lr 0.000720	time 0.4576 (0.4683)	loss 3.6855 (3.5165)	grad_norm 1.1761 (nan)	mem 14852MB
[2022-11-06 13:32:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 106 training takes 0:09:45
[2022-11-06 13:32:40 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_106.pth saving......
[2022-11-06 13:32:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_106.pth saved !!!
[2022-11-06 13:32:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.630 (1.630)	Loss 1.0337 (1.0337)	Acc@1 76.953 (76.953)	Acc@5 93.262 (93.262)	Mem 14852MB
[2022-11-06 13:32:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.100 Acc@5 93.364
[2022-11-06 13:32:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.1%
[2022-11-06 13:32:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.752 (1.752)	Loss 0.8415 (0.8415)	Acc@1 80.273 (80.273)	Acc@5 95.410 (95.410)	Mem 14852MB
[2022-11-06 13:32:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.898 Acc@5 94.626
[2022-11-06 13:32:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.9%
[2022-11-06 13:32:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.90% at 106 epoch
[2022-11-06 13:33:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][0/1251]	eta 0:40:28 lr 0.000720	time 1.9409 (1.9409)	loss 2.5892 (2.5892)	grad_norm 1.3988 (1.3988)	mem 14852MB
[2022-11-06 13:33:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][50/1251]	eta 0:10:01 lr 0.000720	time 0.4709 (0.5007)	loss 3.5405 (3.4565)	grad_norm 1.5516 (1.3661)	mem 14852MB
[2022-11-06 13:33:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][100/1251]	eta 0:09:20 lr 0.000720	time 0.4576 (0.4867)	loss 3.7323 (3.4558)	grad_norm 1.3479 (1.3660)	mem 14852MB
[2022-11-06 13:34:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][150/1251]	eta 0:08:48 lr 0.000720	time 0.4644 (0.4803)	loss 3.6513 (3.4469)	grad_norm 1.1959 (1.3716)	mem 14852MB
[2022-11-06 13:34:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][200/1251]	eta 0:08:20 lr 0.000720	time 0.4597 (0.4763)	loss 2.7420 (3.4435)	grad_norm 1.2689 (1.3737)	mem 14852MB
[2022-11-06 13:34:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][250/1251]	eta 0:07:54 lr 0.000720	time 0.4707 (0.4744)	loss 3.4885 (3.4601)	grad_norm 1.5268 (1.3684)	mem 14852MB
[2022-11-06 13:35:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][300/1251]	eta 0:07:29 lr 0.000719	time 0.4538 (0.4729)	loss 4.0028 (3.4694)	grad_norm 1.2595 (1.3696)	mem 14852MB
[2022-11-06 13:35:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][350/1251]	eta 0:07:05 lr 0.000719	time 0.4622 (0.4719)	loss 3.9675 (3.4885)	grad_norm 1.5465 (1.3717)	mem 14852MB
[2022-11-06 13:36:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][400/1251]	eta 0:06:41 lr 0.000719	time 0.4667 (0.4713)	loss 4.0915 (3.4896)	grad_norm 1.2389 (1.3711)	mem 14852MB
[2022-11-06 13:36:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][450/1251]	eta 0:06:17 lr 0.000719	time 0.4742 (0.4707)	loss 3.3654 (3.4885)	grad_norm 1.3077 (1.3704)	mem 14852MB
[2022-11-06 13:36:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][500/1251]	eta 0:05:53 lr 0.000719	time 0.5382 (0.4702)	loss 2.5423 (3.4874)	grad_norm 1.5720 (1.3686)	mem 14852MB
[2022-11-06 13:37:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][550/1251]	eta 0:05:29 lr 0.000718	time 0.4619 (0.4701)	loss 3.9206 (3.4884)	grad_norm 1.2698 (1.3690)	mem 14852MB
[2022-11-06 13:37:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][600/1251]	eta 0:05:05 lr 0.000718	time 0.4652 (0.4699)	loss 2.7529 (3.4849)	grad_norm 1.7313 (1.3700)	mem 14852MB
[2022-11-06 13:38:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][650/1251]	eta 0:04:42 lr 0.000718	time 0.4619 (0.4697)	loss 2.7799 (3.4907)	grad_norm 1.4069 (1.3719)	mem 14852MB
[2022-11-06 13:38:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][700/1251]	eta 0:04:18 lr 0.000718	time 0.4738 (0.4693)	loss 3.7088 (3.4866)	grad_norm 1.3086 (1.3718)	mem 14852MB
[2022-11-06 13:38:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][750/1251]	eta 0:03:54 lr 0.000718	time 0.4768 (0.4690)	loss 2.9796 (3.4787)	grad_norm 1.2577 (1.3733)	mem 14852MB
[2022-11-06 13:39:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][800/1251]	eta 0:03:31 lr 0.000717	time 0.4680 (0.4691)	loss 3.8757 (3.4816)	grad_norm 1.2587 (1.3693)	mem 14852MB
[2022-11-06 13:39:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][850/1251]	eta 0:03:08 lr 0.000717	time 0.4588 (0.4690)	loss 3.8422 (3.4879)	grad_norm 1.2867 (1.3702)	mem 14852MB
[2022-11-06 13:40:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][900/1251]	eta 0:02:44 lr 0.000717	time 0.4644 (0.4689)	loss 3.0928 (3.4883)	grad_norm 1.2832 (1.3697)	mem 14852MB
[2022-11-06 13:40:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][950/1251]	eta 0:02:21 lr 0.000717	time 0.4641 (0.4688)	loss 2.7947 (3.4958)	grad_norm 1.3976 (1.3715)	mem 14852MB
[2022-11-06 13:40:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][1000/1251]	eta 0:01:57 lr 0.000717	time 0.4639 (0.4687)	loss 3.8464 (3.4955)	grad_norm 1.2676 (1.3736)	mem 14852MB
[2022-11-06 13:41:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][1050/1251]	eta 0:01:34 lr 0.000717	time 0.4601 (0.4688)	loss 3.2194 (3.4981)	grad_norm 1.5233 (1.3749)	mem 14852MB
[2022-11-06 13:41:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][1100/1251]	eta 0:01:10 lr 0.000716	time 0.4651 (0.4687)	loss 3.9307 (3.5004)	grad_norm 1.5400 (1.3759)	mem 14852MB
[2022-11-06 13:41:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][1150/1251]	eta 0:00:47 lr 0.000716	time 0.4730 (0.4686)	loss 2.9493 (3.4978)	grad_norm 1.2377 (1.3763)	mem 14852MB
[2022-11-06 13:42:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][1200/1251]	eta 0:00:23 lr 0.000716	time 0.4889 (0.4685)	loss 3.8513 (3.4989)	grad_norm 1.4743 (1.3749)	mem 14852MB
[2022-11-06 13:42:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [107/300][1250/1251]	eta 0:00:00 lr 0.000716	time 0.4575 (0.4683)	loss 2.5168 (3.4932)	grad_norm 1.3821 (1.3752)	mem 14852MB
[2022-11-06 13:42:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 107 training takes 0:09:46
[2022-11-06 13:42:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_107.pth saving......
[2022-11-06 13:42:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_107.pth saved !!!
[2022-11-06 13:42:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.570 (1.570)	Loss 1.0612 (1.0612)	Acc@1 74.512 (74.512)	Acc@5 93.262 (93.262)	Mem 14852MB
[2022-11-06 13:42:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.246 Acc@5 93.388
[2022-11-06 13:42:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.2%
[2022-11-06 13:42:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.643 (1.643)	Loss 0.8819 (0.8819)	Acc@1 78.516 (78.516)	Acc@5 94.824 (94.824)	Mem 14852MB
[2022-11-06 13:43:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.912 Acc@5 94.666
[2022-11-06 13:43:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.9%
[2022-11-06 13:43:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.91% at 107 epoch
[2022-11-06 13:43:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][0/1251]	eta 0:41:09 lr 0.000716	time 1.9743 (1.9743)	loss 3.6093 (3.6093)	grad_norm 1.2525 (1.2525)	mem 14852MB
[2022-11-06 13:43:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][50/1251]	eta 0:10:01 lr 0.000716	time 0.4824 (0.5010)	loss 2.7454 (3.5717)	grad_norm 1.3506 (1.3689)	mem 14852MB
[2022-11-06 13:43:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][100/1251]	eta 0:09:18 lr 0.000715	time 0.4600 (0.4853)	loss 4.0173 (3.5145)	grad_norm 1.3123 (1.3820)	mem 14852MB
[2022-11-06 13:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][150/1251]	eta 0:08:47 lr 0.000715	time 0.4538 (0.4789)	loss 2.4575 (3.5200)	grad_norm 1.8577 (1.4017)	mem 14852MB
[2022-11-06 13:44:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][200/1251]	eta 0:08:20 lr 0.000715	time 0.4681 (0.4761)	loss 3.9188 (3.5295)	grad_norm 1.3814 (1.3905)	mem 14852MB
[2022-11-06 13:45:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][250/1251]	eta 0:07:54 lr 0.000715	time 0.4707 (0.4737)	loss 2.9809 (3.5313)	grad_norm 1.2849 (1.3830)	mem 14852MB
[2022-11-06 13:45:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][300/1251]	eta 0:07:29 lr 0.000715	time 0.4620 (0.4727)	loss 3.7317 (3.5360)	grad_norm 1.4920 (1.3753)	mem 14852MB
[2022-11-06 13:45:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][350/1251]	eta 0:07:04 lr 0.000714	time 0.4520 (0.4717)	loss 3.5413 (3.5436)	grad_norm 1.3300 (1.3761)	mem 14852MB
[2022-11-06 13:46:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][400/1251]	eta 0:06:41 lr 0.000714	time 0.4625 (0.4712)	loss 3.8322 (3.5120)	grad_norm 1.2427 (1.3733)	mem 14852MB
[2022-11-06 13:46:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][450/1251]	eta 0:06:16 lr 0.000714	time 0.4639 (0.4706)	loss 3.8967 (3.5267)	grad_norm 1.2474 (1.3714)	mem 14852MB
[2022-11-06 13:46:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][500/1251]	eta 0:05:53 lr 0.000714	time 0.4673 (0.4703)	loss 2.6152 (3.5230)	grad_norm 1.3503 (1.3719)	mem 14852MB
[2022-11-06 13:47:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][550/1251]	eta 0:05:29 lr 0.000714	time 0.4550 (0.4702)	loss 3.7244 (3.5274)	grad_norm 1.3058 (1.3772)	mem 14852MB
[2022-11-06 13:47:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][600/1251]	eta 0:05:05 lr 0.000714	time 0.4654 (0.4700)	loss 2.6837 (3.5299)	grad_norm 1.3111 (1.3770)	mem 14852MB
[2022-11-06 13:48:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][650/1251]	eta 0:04:42 lr 0.000713	time 0.4635 (0.4696)	loss 4.0123 (3.5350)	grad_norm 1.2315 (1.3759)	mem 14852MB
[2022-11-06 13:48:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][700/1251]	eta 0:04:18 lr 0.000713	time 0.4642 (0.4694)	loss 2.7215 (3.5277)	grad_norm 1.3000 (1.3771)	mem 14852MB
[2022-11-06 13:48:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][750/1251]	eta 0:03:55 lr 0.000713	time 0.4633 (0.4691)	loss 4.4133 (3.5293)	grad_norm 1.3823 (1.3770)	mem 14852MB
[2022-11-06 13:49:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][800/1251]	eta 0:03:31 lr 0.000713	time 0.4649 (0.4693)	loss 3.7204 (3.5289)	grad_norm 1.3992 (1.3762)	mem 14852MB
[2022-11-06 13:49:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][850/1251]	eta 0:03:08 lr 0.000713	time 0.4717 (0.4692)	loss 4.3321 (3.5352)	grad_norm 1.3171 (1.3778)	mem 14852MB
[2022-11-06 13:50:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][900/1251]	eta 0:02:44 lr 0.000712	time 0.4775 (0.4691)	loss 2.9162 (3.5317)	grad_norm 1.2422 (1.3773)	mem 14852MB
[2022-11-06 13:50:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][950/1251]	eta 0:02:21 lr 0.000712	time 0.4523 (0.4689)	loss 3.7616 (3.5309)	grad_norm 1.2891 (1.3761)	mem 14852MB
[2022-11-06 13:50:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][1000/1251]	eta 0:01:57 lr 0.000712	time 0.4662 (0.4687)	loss 3.9952 (3.5352)	grad_norm 1.3103 (1.3755)	mem 14852MB
[2022-11-06 13:51:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][1050/1251]	eta 0:01:34 lr 0.000712	time 0.4642 (0.4688)	loss 2.9968 (3.5321)	grad_norm 1.3237 (1.3755)	mem 14852MB
[2022-11-06 13:51:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][1100/1251]	eta 0:01:10 lr 0.000712	time 0.4660 (0.4689)	loss 3.0975 (3.5320)	grad_norm 1.3355 (1.3766)	mem 14852MB
[2022-11-06 13:52:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][1150/1251]	eta 0:00:47 lr 0.000711	time 0.4595 (0.4687)	loss 3.5782 (3.5304)	grad_norm 1.3194 (1.3775)	mem 14852MB
[2022-11-06 13:52:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][1200/1251]	eta 0:00:23 lr 0.000711	time 0.4585 (0.4686)	loss 3.0559 (3.5362)	grad_norm 1.2391 (1.3767)	mem 14852MB
[2022-11-06 13:52:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [108/300][1250/1251]	eta 0:00:00 lr 0.000711	time 0.4571 (0.4684)	loss 3.9022 (3.5372)	grad_norm 1.2855 (1.3756)	mem 14852MB
[2022-11-06 13:52:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 108 training takes 0:09:46
[2022-11-06 13:52:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_108.pth saving......
[2022-11-06 13:52:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_108.pth saved !!!
[2022-11-06 13:52:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.478 (1.478)	Loss 1.0638 (1.0638)	Acc@1 76.367 (76.367)	Acc@5 91.504 (91.504)	Mem 14852MB
[2022-11-06 13:52:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.490 Acc@5 93.410
[2022-11-06 13:52:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.5%
[2022-11-06 13:53:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.608 (1.608)	Loss 0.8177 (0.8177)	Acc@1 80.566 (80.566)	Acc@5 95.703 (95.703)	Mem 14852MB
[2022-11-06 13:53:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.946 Acc@5 94.662
[2022-11-06 13:53:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.9%
[2022-11-06 13:53:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.95% at 108 epoch
[2022-11-06 13:53:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][0/1251]	eta 0:40:32 lr 0.000711	time 1.9448 (1.9448)	loss 3.3508 (3.3508)	grad_norm 1.3570 (1.3570)	mem 14852MB
[2022-11-06 13:53:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][50/1251]	eta 0:10:02 lr 0.000711	time 0.5409 (0.5014)	loss 3.4408 (3.4442)	grad_norm 1.6456 (1.3884)	mem 14852MB
[2022-11-06 13:53:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][100/1251]	eta 0:09:17 lr 0.000711	time 0.4597 (0.4844)	loss 3.1030 (3.4311)	grad_norm 1.4472 (1.3885)	mem 14852MB
[2022-11-06 13:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][150/1251]	eta 0:08:46 lr 0.000710	time 0.4661 (0.4780)	loss 3.6254 (3.4108)	grad_norm 1.5990 (1.3949)	mem 14852MB
[2022-11-06 13:54:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][200/1251]	eta 0:08:19 lr 0.000710	time 0.4588 (0.4752)	loss 3.2227 (3.4145)	grad_norm 1.3528 (1.3938)	mem 14852MB
[2022-11-06 13:55:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][250/1251]	eta 0:07:53 lr 0.000710	time 0.4556 (0.4734)	loss 2.4866 (3.4579)	grad_norm 1.3628 (1.3968)	mem 14852MB
[2022-11-06 13:55:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][300/1251]	eta 0:07:28 lr 0.000710	time 0.4756 (0.4719)	loss 3.7505 (3.4471)	grad_norm 1.3856 (1.3948)	mem 14852MB
[2022-11-06 13:55:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][350/1251]	eta 0:07:04 lr 0.000710	time 0.4614 (0.4712)	loss 3.8576 (3.4396)	grad_norm 1.4203 (1.3950)	mem 14852MB
[2022-11-06 13:56:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][400/1251]	eta 0:06:40 lr 0.000710	time 0.4573 (0.4704)	loss 2.5648 (3.4528)	grad_norm 1.4270 (1.3931)	mem 14852MB
[2022-11-06 13:56:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][450/1251]	eta 0:06:16 lr 0.000709	time 0.4766 (0.4697)	loss 3.5971 (3.4552)	grad_norm 1.3443 (1.3897)	mem 14852MB
[2022-11-06 13:57:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][500/1251]	eta 0:05:52 lr 0.000709	time 0.4703 (0.4695)	loss 3.9913 (3.4594)	grad_norm 1.4365 (1.3855)	mem 14852MB
[2022-11-06 13:57:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][550/1251]	eta 0:05:28 lr 0.000709	time 0.4692 (0.4692)	loss 4.0399 (3.4641)	grad_norm 1.3556 (1.3880)	mem 14852MB
[2022-11-06 13:57:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][600/1251]	eta 0:05:05 lr 0.000709	time 0.4735 (0.4691)	loss 3.3117 (3.4662)	grad_norm 1.2330 (1.3865)	mem 14852MB
[2022-11-06 13:58:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][650/1251]	eta 0:04:41 lr 0.000709	time 0.4537 (0.4688)	loss 3.7828 (3.4740)	grad_norm 1.2522 (1.3884)	mem 14852MB
[2022-11-06 13:58:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][700/1251]	eta 0:04:18 lr 0.000708	time 0.4645 (0.4686)	loss 3.8422 (3.4817)	grad_norm 1.4474 (nan)	mem 14852MB
[2022-11-06 13:58:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][750/1251]	eta 0:03:54 lr 0.000708	time 0.4553 (0.4686)	loss 3.6954 (3.4847)	grad_norm 1.2882 (nan)	mem 14852MB
[2022-11-06 13:59:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][800/1251]	eta 0:03:31 lr 0.000708	time 0.4541 (0.4687)	loss 4.2076 (3.4834)	grad_norm 1.5190 (nan)	mem 14852MB
[2022-11-06 13:59:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][850/1251]	eta 0:03:07 lr 0.000708	time 0.4603 (0.4684)	loss 3.4692 (3.4804)	grad_norm 1.3059 (nan)	mem 14852MB
[2022-11-06 14:00:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][900/1251]	eta 0:02:44 lr 0.000708	time 0.4646 (0.4682)	loss 3.1071 (3.4801)	grad_norm 1.5491 (nan)	mem 14852MB
[2022-11-06 14:00:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][950/1251]	eta 0:02:20 lr 0.000707	time 0.4561 (0.4682)	loss 3.3867 (3.4851)	grad_norm 1.2600 (nan)	mem 14852MB
[2022-11-06 14:00:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][1000/1251]	eta 0:01:57 lr 0.000707	time 0.4662 (0.4681)	loss 2.9250 (3.4847)	grad_norm 1.1977 (nan)	mem 14852MB
[2022-11-06 14:01:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][1050/1251]	eta 0:01:34 lr 0.000707	time 0.4593 (0.4681)	loss 3.1097 (3.4789)	grad_norm 1.3475 (nan)	mem 14852MB
[2022-11-06 14:01:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][1100/1251]	eta 0:01:10 lr 0.000707	time 0.4577 (0.4680)	loss 3.5344 (3.4825)	grad_norm 1.3683 (nan)	mem 14852MB
[2022-11-06 14:02:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][1150/1251]	eta 0:00:47 lr 0.000707	time 0.4619 (0.4679)	loss 3.9616 (3.4844)	grad_norm 1.3450 (nan)	mem 14852MB
[2022-11-06 14:02:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][1200/1251]	eta 0:00:23 lr 0.000707	time 0.4674 (0.4679)	loss 2.5104 (3.4821)	grad_norm 1.4215 (nan)	mem 14852MB
[2022-11-06 14:02:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [109/300][1250/1251]	eta 0:00:00 lr 0.000706	time 0.4579 (0.4678)	loss 3.1148 (3.4823)	grad_norm 1.3725 (nan)	mem 14852MB
[2022-11-06 14:02:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 109 training takes 0:09:45
[2022-11-06 14:02:52 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_109.pth saving......
[2022-11-06 14:02:53 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_109.pth saved !!!
[2022-11-06 14:02:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.619 (1.619)	Loss 1.1138 (1.1138)	Acc@1 73.340 (73.340)	Acc@5 91.992 (91.992)	Mem 14852MB
[2022-11-06 14:03:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.500 Acc@5 93.508
[2022-11-06 14:03:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.5%
[2022-11-06 14:03:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.589 (1.589)	Loss 0.8409 (0.8409)	Acc@1 81.250 (81.250)	Acc@5 95.117 (95.117)	Mem 14852MB
[2022-11-06 14:03:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.010 Acc@5 94.656
[2022-11-06 14:03:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.0%
[2022-11-06 14:03:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.01% at 109 epoch
[2022-11-06 14:03:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][0/1251]	eta 0:40:43 lr 0.000706	time 1.9535 (1.9535)	loss 3.9147 (3.9147)	grad_norm 1.4164 (1.4164)	mem 14852MB
[2022-11-06 14:03:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][50/1251]	eta 0:10:03 lr 0.000706	time 0.4684 (0.5024)	loss 3.1937 (3.5097)	grad_norm 1.2929 (1.3563)	mem 14852MB
[2022-11-06 14:03:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][100/1251]	eta 0:09:19 lr 0.000706	time 0.4641 (0.4861)	loss 3.5759 (3.4814)	grad_norm 1.3288 (1.3736)	mem 14852MB
[2022-11-06 14:04:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][150/1251]	eta 0:08:48 lr 0.000706	time 0.4572 (0.4802)	loss 3.6118 (3.4895)	grad_norm 1.4264 (1.3730)	mem 14852MB
[2022-11-06 14:04:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][200/1251]	eta 0:08:21 lr 0.000706	time 0.4697 (0.4769)	loss 3.5729 (3.4914)	grad_norm 1.2141 (1.3704)	mem 14852MB
[2022-11-06 14:05:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][250/1251]	eta 0:07:55 lr 0.000705	time 0.4551 (0.4745)	loss 2.4046 (3.4867)	grad_norm 1.5276 (1.3688)	mem 14852MB
[2022-11-06 14:05:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][300/1251]	eta 0:07:29 lr 0.000705	time 0.4671 (0.4731)	loss 2.5218 (3.4833)	grad_norm 1.2999 (1.3650)	mem 14852MB
[2022-11-06 14:05:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][350/1251]	eta 0:07:05 lr 0.000705	time 0.4712 (0.4719)	loss 3.7234 (3.4770)	grad_norm 1.4074 (1.3669)	mem 14852MB
[2022-11-06 14:06:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][400/1251]	eta 0:06:40 lr 0.000705	time 0.4560 (0.4710)	loss 3.1908 (3.4871)	grad_norm 1.2410 (1.3685)	mem 14852MB
[2022-11-06 14:06:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][450/1251]	eta 0:06:16 lr 0.000705	time 0.4639 (0.4704)	loss 2.1674 (3.4858)	grad_norm 1.3616 (1.3660)	mem 14852MB
[2022-11-06 14:07:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][500/1251]	eta 0:05:53 lr 0.000704	time 0.4729 (0.4702)	loss 3.8017 (3.4920)	grad_norm 1.4443 (1.3643)	mem 14852MB
[2022-11-06 14:07:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][550/1251]	eta 0:05:29 lr 0.000704	time 0.4604 (0.4700)	loss 3.3177 (3.5005)	grad_norm 1.3318 (1.3620)	mem 14852MB
[2022-11-06 14:07:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][600/1251]	eta 0:05:05 lr 0.000704	time 0.4637 (0.4698)	loss 4.0891 (3.4977)	grad_norm 1.5052 (1.3651)	mem 14852MB
[2022-11-06 14:08:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][650/1251]	eta 0:04:42 lr 0.000704	time 0.4810 (0.4695)	loss 3.8854 (3.4990)	grad_norm 1.2540 (1.3666)	mem 14852MB
[2022-11-06 14:08:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][700/1251]	eta 0:04:18 lr 0.000704	time 0.4645 (0.4691)	loss 3.5014 (3.4887)	grad_norm 1.4041 (1.3691)	mem 14852MB
[2022-11-06 14:09:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][750/1251]	eta 0:03:54 lr 0.000703	time 0.4539 (0.4689)	loss 3.8944 (3.4852)	grad_norm 1.2677 (1.3710)	mem 14852MB
[2022-11-06 14:09:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][800/1251]	eta 0:03:31 lr 0.000703	time 0.5401 (0.4691)	loss 2.8773 (3.4818)	grad_norm 1.5349 (1.3736)	mem 14852MB
[2022-11-06 14:09:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][850/1251]	eta 0:03:08 lr 0.000703	time 0.4701 (0.4689)	loss 3.6918 (3.4838)	grad_norm 1.4749 (1.3724)	mem 14852MB
[2022-11-06 14:10:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][900/1251]	eta 0:02:44 lr 0.000703	time 0.4640 (0.4687)	loss 3.9835 (3.4783)	grad_norm 1.6196 (1.3726)	mem 14852MB
[2022-11-06 14:10:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][950/1251]	eta 0:02:21 lr 0.000703	time 0.4635 (0.4685)	loss 2.6783 (3.4768)	grad_norm 1.3826 (1.3723)	mem 14852MB
[2022-11-06 14:10:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][1000/1251]	eta 0:01:57 lr 0.000703	time 0.4640 (0.4684)	loss 3.9707 (3.4731)	grad_norm 1.3466 (1.3712)	mem 14852MB
[2022-11-06 14:11:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][1050/1251]	eta 0:01:34 lr 0.000702	time 0.4582 (0.4685)	loss 4.2799 (3.4704)	grad_norm 1.4652 (1.3721)	mem 14852MB
[2022-11-06 14:11:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][1100/1251]	eta 0:01:10 lr 0.000702	time 0.4709 (0.4685)	loss 4.1557 (3.4689)	grad_norm 1.5379 (1.3725)	mem 14852MB
[2022-11-06 14:12:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][1150/1251]	eta 0:00:47 lr 0.000702	time 0.4632 (0.4684)	loss 2.9752 (3.4724)	grad_norm 1.3537 (1.3717)	mem 14852MB
[2022-11-06 14:12:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][1200/1251]	eta 0:00:23 lr 0.000702	time 0.4748 (0.4683)	loss 3.7535 (3.4694)	grad_norm 1.3104 (1.3716)	mem 14852MB
[2022-11-06 14:12:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [110/300][1250/1251]	eta 0:00:00 lr 0.000702	time 0.4571 (0.4681)	loss 4.1243 (3.4717)	grad_norm 1.3212 (1.3715)	mem 14852MB
[2022-11-06 14:12:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 110 training takes 0:09:45
[2022-11-06 14:12:56 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_110.pth saving......
[2022-11-06 14:12:57 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_110.pth saved !!!
[2022-11-06 14:12:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.629 (1.629)	Loss 1.0105 (1.0105)	Acc@1 74.316 (74.316)	Acc@5 93.848 (93.848)	Mem 14852MB
[2022-11-06 14:13:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.452 Acc@5 93.586
[2022-11-06 14:13:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.5%
[2022-11-06 14:13:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.720 (1.720)	Loss 0.8338 (0.8338)	Acc@1 81.152 (81.152)	Acc@5 93.945 (93.945)	Mem 14852MB
[2022-11-06 14:13:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.032 Acc@5 94.664
[2022-11-06 14:13:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.0%
[2022-11-06 14:13:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.03% at 110 epoch
[2022-11-06 14:13:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][0/1251]	eta 0:41:20 lr 0.000702	time 1.9824 (1.9824)	loss 3.2619 (3.2619)	grad_norm 1.3663 (1.3663)	mem 14852MB
[2022-11-06 14:13:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][50/1251]	eta 0:10:09 lr 0.000701	time 0.4618 (0.5078)	loss 4.2416 (3.4314)	grad_norm 1.4121 (1.4038)	mem 14852MB
[2022-11-06 14:14:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][100/1251]	eta 0:09:20 lr 0.000701	time 0.4626 (0.4866)	loss 3.5556 (3.4668)	grad_norm 1.4193 (1.3862)	mem 14852MB
[2022-11-06 14:14:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][150/1251]	eta 0:08:48 lr 0.000701	time 0.4633 (0.4799)	loss 3.2352 (3.4377)	grad_norm 1.5114 (1.3842)	mem 14852MB
[2022-11-06 14:14:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][200/1251]	eta 0:08:20 lr 0.000701	time 0.4771 (0.4762)	loss 3.8783 (3.4709)	grad_norm 1.3366 (1.3914)	mem 14852MB
[2022-11-06 14:15:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][250/1251]	eta 0:07:54 lr 0.000701	time 0.4659 (0.4739)	loss 4.0593 (3.4866)	grad_norm 1.2607 (1.3858)	mem 14852MB
[2022-11-06 14:15:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][300/1251]	eta 0:07:29 lr 0.000700	time 0.4676 (0.4732)	loss 2.4665 (3.4789)	grad_norm 1.4284 (1.3863)	mem 14852MB
[2022-11-06 14:16:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][350/1251]	eta 0:07:05 lr 0.000700	time 0.4714 (0.4721)	loss 3.9142 (3.4740)	grad_norm 1.3441 (1.3865)	mem 14852MB
[2022-11-06 14:16:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][400/1251]	eta 0:06:40 lr 0.000700	time 0.4653 (0.4712)	loss 2.6470 (3.4692)	grad_norm 1.1791 (1.3898)	mem 14852MB
[2022-11-06 14:16:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][450/1251]	eta 0:06:16 lr 0.000700	time 0.4634 (0.4705)	loss 2.3264 (3.4778)	grad_norm 1.5413 (1.3858)	mem 14852MB
[2022-11-06 14:17:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][500/1251]	eta 0:05:53 lr 0.000700	time 0.4629 (0.4701)	loss 3.4741 (3.4849)	grad_norm 1.3833 (1.3871)	mem 14852MB
[2022-11-06 14:17:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][550/1251]	eta 0:05:29 lr 0.000699	time 0.4642 (0.4705)	loss 3.5077 (3.4866)	grad_norm 1.3797 (1.3877)	mem 14852MB
[2022-11-06 14:17:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][600/1251]	eta 0:05:06 lr 0.000699	time 0.4545 (0.4702)	loss 3.8280 (3.4882)	grad_norm 1.2824 (1.3873)	mem 14852MB
[2022-11-06 14:18:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][650/1251]	eta 0:04:42 lr 0.000699	time 0.4592 (0.4698)	loss 3.8359 (3.4896)	grad_norm 1.4381 (1.3870)	mem 14852MB
[2022-11-06 14:18:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][700/1251]	eta 0:04:18 lr 0.000699	time 0.4634 (0.4695)	loss 3.8349 (3.4947)	grad_norm 1.3148 (1.3859)	mem 14852MB
[2022-11-06 14:19:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][750/1251]	eta 0:03:55 lr 0.000699	time 0.4736 (0.4694)	loss 4.2127 (3.4901)	grad_norm 1.2748 (1.3847)	mem 14852MB
[2022-11-06 14:19:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][800/1251]	eta 0:03:31 lr 0.000699	time 0.4764 (0.4694)	loss 2.6063 (3.4885)	grad_norm 1.4592 (1.3846)	mem 14852MB
[2022-11-06 14:19:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][850/1251]	eta 0:03:08 lr 0.000698	time 0.4647 (0.4692)	loss 3.4510 (3.4899)	grad_norm 1.3259 (1.3850)	mem 14852MB
[2022-11-06 14:20:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][900/1251]	eta 0:02:44 lr 0.000698	time 0.4550 (0.4691)	loss 4.0752 (3.4867)	grad_norm 1.3875 (1.3869)	mem 14852MB
[2022-11-06 14:20:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][950/1251]	eta 0:02:21 lr 0.000698	time 0.4643 (0.4689)	loss 2.9605 (3.4902)	grad_norm 1.3366 (1.3859)	mem 14852MB
[2022-11-06 14:21:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][1000/1251]	eta 0:01:57 lr 0.000698	time 0.4665 (0.4688)	loss 3.0584 (3.4966)	grad_norm 1.2335 (1.3835)	mem 14852MB
[2022-11-06 14:21:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][1050/1251]	eta 0:01:34 lr 0.000698	time 0.4705 (0.4691)	loss 2.9717 (3.5007)	grad_norm 1.3546 (1.3831)	mem 14852MB
[2022-11-06 14:21:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][1100/1251]	eta 0:01:10 lr 0.000697	time 0.4546 (0.4688)	loss 2.5693 (3.4985)	grad_norm 1.3251 (1.3838)	mem 14852MB
[2022-11-06 14:22:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][1150/1251]	eta 0:00:47 lr 0.000697	time 0.4709 (0.4687)	loss 3.9687 (3.4936)	grad_norm 1.4658 (1.3839)	mem 14852MB
[2022-11-06 14:22:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][1200/1251]	eta 0:00:23 lr 0.000697	time 0.4630 (0.4685)	loss 4.3839 (3.4987)	grad_norm 1.3337 (1.3841)	mem 14852MB
[2022-11-06 14:23:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [111/300][1250/1251]	eta 0:00:00 lr 0.000697	time 0.4575 (0.4684)	loss 2.8353 (3.4969)	grad_norm 1.3346 (1.3842)	mem 14852MB
[2022-11-06 14:23:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 111 training takes 0:09:46
[2022-11-06 14:23:00 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_111.pth saving......
[2022-11-06 14:23:01 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_111.pth saved !!!
[2022-11-06 14:23:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.524 (1.524)	Loss 1.0223 (1.0223)	Acc@1 75.391 (75.391)	Acc@5 93.457 (93.457)	Mem 14852MB
[2022-11-06 14:23:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.476 Acc@5 93.604
[2022-11-06 14:23:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.5%
[2022-11-06 14:23:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.548 (1.548)	Loss 0.8434 (0.8434)	Acc@1 80.078 (80.078)	Acc@5 95.410 (95.410)	Mem 14852MB
[2022-11-06 14:23:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.074 Acc@5 94.706
[2022-11-06 14:23:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.1%
[2022-11-06 14:23:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.07% at 111 epoch
[2022-11-06 14:23:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][0/1251]	eta 0:40:41 lr 0.000697	time 1.9515 (1.9515)	loss 3.9077 (3.9077)	grad_norm 1.5616 (1.5616)	mem 14852MB
[2022-11-06 14:23:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][50/1251]	eta 0:10:02 lr 0.000697	time 0.4616 (0.5017)	loss 3.4915 (3.4517)	grad_norm 1.6096 (1.3933)	mem 14852MB
[2022-11-06 14:24:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][100/1251]	eta 0:09:19 lr 0.000696	time 0.4643 (0.4858)	loss 3.8063 (3.5018)	grad_norm 1.6771 (1.3893)	mem 14852MB
[2022-11-06 14:24:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][150/1251]	eta 0:08:48 lr 0.000696	time 0.4569 (0.4798)	loss 3.5632 (3.4820)	grad_norm 1.4005 (1.3879)	mem 14852MB
[2022-11-06 14:24:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][200/1251]	eta 0:08:20 lr 0.000696	time 0.4699 (0.4763)	loss 3.9834 (3.4719)	grad_norm 1.3294 (1.3831)	mem 14852MB
[2022-11-06 14:25:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][250/1251]	eta 0:07:54 lr 0.000696	time 0.4747 (0.4740)	loss 2.9249 (3.4486)	grad_norm 1.3431 (1.3866)	mem 14852MB
[2022-11-06 14:25:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][300/1251]	eta 0:07:29 lr 0.000696	time 0.4636 (0.4725)	loss 3.7934 (3.4588)	grad_norm 1.7121 (1.3864)	mem 14852MB
[2022-11-06 14:26:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][350/1251]	eta 0:07:05 lr 0.000695	time 0.4626 (0.4718)	loss 4.1814 (3.4627)	grad_norm 1.3100 (1.3853)	mem 14852MB
[2022-11-06 14:26:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][400/1251]	eta 0:06:41 lr 0.000695	time 0.4629 (0.4712)	loss 3.6365 (3.4811)	grad_norm 1.6362 (1.3844)	mem 14852MB
[2022-11-06 14:26:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][450/1251]	eta 0:06:16 lr 0.000695	time 0.4562 (0.4705)	loss 3.8009 (3.4822)	grad_norm 1.3381 (1.3860)	mem 14852MB
[2022-11-06 14:27:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][500/1251]	eta 0:05:53 lr 0.000695	time 0.4703 (0.4702)	loss 3.5310 (3.4795)	grad_norm 1.3939 (1.3867)	mem 14852MB
[2022-11-06 14:27:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][550/1251]	eta 0:05:29 lr 0.000695	time 0.4531 (0.4703)	loss 3.8628 (3.4892)	grad_norm 1.3169 (1.3888)	mem 14852MB
[2022-11-06 14:28:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][600/1251]	eta 0:05:05 lr 0.000695	time 0.4603 (0.4698)	loss 4.0283 (3.4831)	grad_norm 1.3007 (1.3894)	mem 14852MB
[2022-11-06 14:28:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][650/1251]	eta 0:04:42 lr 0.000694	time 0.4668 (0.4697)	loss 3.6471 (3.4797)	grad_norm 1.3505 (1.3890)	mem 14852MB
[2022-11-06 14:28:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][700/1251]	eta 0:04:18 lr 0.000694	time 0.4752 (0.4694)	loss 3.1945 (3.4802)	grad_norm 1.3419 (1.3873)	mem 14852MB
[2022-11-06 14:29:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][750/1251]	eta 0:03:55 lr 0.000694	time 0.4567 (0.4693)	loss 4.1279 (3.4780)	grad_norm 1.6380 (1.3881)	mem 14852MB
[2022-11-06 14:29:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][800/1251]	eta 0:03:31 lr 0.000694	time 0.4686 (0.4694)	loss 3.1521 (3.4785)	grad_norm 1.5615 (1.3884)	mem 14852MB
[2022-11-06 14:29:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][850/1251]	eta 0:03:08 lr 0.000694	time 0.4705 (0.4693)	loss 3.6841 (3.4820)	grad_norm 1.1893 (1.3872)	mem 14852MB
[2022-11-06 14:30:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][900/1251]	eta 0:02:44 lr 0.000693	time 0.4799 (0.4692)	loss 2.7340 (3.4809)	grad_norm 1.3716 (1.3889)	mem 14852MB
[2022-11-06 14:30:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][950/1251]	eta 0:02:21 lr 0.000693	time 0.4753 (0.4690)	loss 2.2494 (3.4786)	grad_norm 1.4299 (1.3900)	mem 14852MB
[2022-11-06 14:31:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][1000/1251]	eta 0:01:57 lr 0.000693	time 0.4638 (0.4688)	loss 4.0383 (3.4833)	grad_norm 1.3029 (1.3893)	mem 14852MB
[2022-11-06 14:31:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][1050/1251]	eta 0:01:34 lr 0.000693	time 0.4627 (0.4689)	loss 2.9338 (3.4790)	grad_norm 1.4593 (1.3897)	mem 14852MB
[2022-11-06 14:31:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][1100/1251]	eta 0:01:10 lr 0.000693	time 0.4606 (0.4688)	loss 3.2923 (3.4790)	grad_norm 1.4783 (1.3908)	mem 14852MB
[2022-11-06 14:32:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][1150/1251]	eta 0:00:47 lr 0.000692	time 0.4627 (0.4687)	loss 3.0462 (3.4709)	grad_norm 1.4842 (1.3915)	mem 14852MB
[2022-11-06 14:32:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][1200/1251]	eta 0:00:23 lr 0.000692	time 0.4580 (0.4685)	loss 3.6617 (3.4704)	grad_norm 1.2527 (1.3909)	mem 14852MB
[2022-11-06 14:33:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [112/300][1250/1251]	eta 0:00:00 lr 0.000692	time 0.4572 (0.4683)	loss 3.7198 (3.4686)	grad_norm 1.6628 (1.3911)	mem 14852MB
[2022-11-06 14:33:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 112 training takes 0:09:45
[2022-11-06 14:33:04 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_112.pth saving......
[2022-11-06 14:33:05 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_112.pth saved !!!
[2022-11-06 14:33:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.667 (1.667)	Loss 1.0783 (1.0783)	Acc@1 74.707 (74.707)	Acc@5 93.652 (93.652)	Mem 14852MB
[2022-11-06 14:33:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.386 Acc@5 93.620
[2022-11-06 14:33:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.4%
[2022-11-06 14:33:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.608 (1.608)	Loss 0.8557 (0.8557)	Acc@1 79.785 (79.785)	Acc@5 94.629 (94.629)	Mem 14852MB
[2022-11-06 14:33:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.072 Acc@5 94.728
[2022-11-06 14:33:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.1%
[2022-11-06 14:33:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.07% at 111 epoch
[2022-11-06 14:33:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][0/1251]	eta 0:39:43 lr 0.000692	time 1.9050 (1.9050)	loss 2.8680 (2.8680)	grad_norm 1.4093 (1.4093)	mem 14852MB
[2022-11-06 14:33:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][50/1251]	eta 0:10:04 lr 0.000692	time 0.4580 (0.5031)	loss 3.5246 (3.3886)	grad_norm 1.3858 (1.3599)	mem 14852MB
[2022-11-06 14:34:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][100/1251]	eta 0:09:19 lr 0.000692	time 0.4714 (0.4861)	loss 3.5968 (3.4503)	grad_norm 1.3151 (1.3695)	mem 14852MB
[2022-11-06 14:34:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][150/1251]	eta 0:08:47 lr 0.000691	time 0.4669 (0.4793)	loss 2.5805 (3.4741)	grad_norm 1.5203 (1.3852)	mem 14852MB
[2022-11-06 14:34:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][200/1251]	eta 0:08:20 lr 0.000691	time 0.4749 (0.4758)	loss 4.2137 (3.4663)	grad_norm 1.4571 (1.3886)	mem 14852MB
[2022-11-06 14:35:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][250/1251]	eta 0:07:54 lr 0.000691	time 0.4656 (0.4741)	loss 2.9979 (3.4623)	grad_norm 1.4769 (1.3887)	mem 14852MB
[2022-11-06 14:35:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][300/1251]	eta 0:07:29 lr 0.000691	time 0.4784 (0.4729)	loss 3.6852 (3.4640)	grad_norm 1.4169 (1.3883)	mem 14852MB
[2022-11-06 14:36:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][350/1251]	eta 0:07:05 lr 0.000691	time 0.4626 (0.4721)	loss 2.6096 (3.4742)	grad_norm 1.3723 (nan)	mem 14852MB
[2022-11-06 14:36:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][400/1251]	eta 0:06:41 lr 0.000690	time 0.4662 (0.4713)	loss 3.9845 (3.4923)	grad_norm 1.3651 (nan)	mem 14852MB
[2022-11-06 14:36:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][450/1251]	eta 0:06:16 lr 0.000690	time 0.4848 (0.4706)	loss 4.1127 (3.4879)	grad_norm 1.4432 (nan)	mem 14852MB
[2022-11-06 14:37:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][500/1251]	eta 0:05:52 lr 0.000690	time 0.4693 (0.4699)	loss 3.8839 (3.4924)	grad_norm 1.4387 (nan)	mem 14852MB
[2022-11-06 14:37:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][550/1251]	eta 0:05:29 lr 0.000690	time 0.4685 (0.4699)	loss 3.3061 (3.4877)	grad_norm 1.2999 (nan)	mem 14852MB
[2022-11-06 14:38:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][600/1251]	eta 0:05:05 lr 0.000690	time 0.4685 (0.4698)	loss 3.7425 (3.4812)	grad_norm 1.2847 (nan)	mem 14852MB
[2022-11-06 14:38:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][650/1251]	eta 0:04:42 lr 0.000690	time 0.4608 (0.4695)	loss 3.7542 (3.4759)	grad_norm 1.3993 (nan)	mem 14852MB
[2022-11-06 14:38:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][700/1251]	eta 0:04:18 lr 0.000689	time 0.4596 (0.4693)	loss 4.0215 (3.4734)	grad_norm 1.2933 (nan)	mem 14852MB
[2022-11-06 14:39:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][750/1251]	eta 0:03:54 lr 0.000689	time 0.4690 (0.4690)	loss 4.1116 (3.4739)	grad_norm 1.3719 (nan)	mem 14852MB
[2022-11-06 14:39:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][800/1251]	eta 0:03:31 lr 0.000689	time 0.4731 (0.4691)	loss 3.2740 (3.4775)	grad_norm 1.8704 (nan)	mem 14852MB
[2022-11-06 14:40:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][850/1251]	eta 0:03:08 lr 0.000689	time 0.4617 (0.4689)	loss 2.7712 (3.4714)	grad_norm 1.2712 (nan)	mem 14852MB
[2022-11-06 14:40:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][900/1251]	eta 0:02:44 lr 0.000689	time 0.4653 (0.4689)	loss 3.7375 (3.4713)	grad_norm 1.3629 (nan)	mem 14852MB
[2022-11-06 14:40:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][950/1251]	eta 0:02:21 lr 0.000688	time 0.4625 (0.4687)	loss 3.1830 (3.4732)	grad_norm 1.3727 (nan)	mem 14852MB
[2022-11-06 14:41:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][1000/1251]	eta 0:01:57 lr 0.000688	time 0.4634 (0.4685)	loss 3.4929 (3.4686)	grad_norm 1.5679 (nan)	mem 14852MB
[2022-11-06 14:41:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][1050/1251]	eta 0:01:34 lr 0.000688	time 0.4585 (0.4686)	loss 3.0774 (3.4670)	grad_norm 1.4177 (nan)	mem 14852MB
[2022-11-06 14:41:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][1100/1251]	eta 0:01:10 lr 0.000688	time 0.4649 (0.4686)	loss 3.8443 (3.4675)	grad_norm 1.4384 (nan)	mem 14852MB
[2022-11-06 14:42:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][1150/1251]	eta 0:00:47 lr 0.000688	time 0.4743 (0.4685)	loss 2.7875 (3.4655)	grad_norm 1.4169 (nan)	mem 14852MB
[2022-11-06 14:42:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][1200/1251]	eta 0:00:23 lr 0.000687	time 0.5322 (0.4684)	loss 3.5239 (3.4692)	grad_norm 1.2546 (nan)	mem 14852MB
[2022-11-06 14:43:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [113/300][1250/1251]	eta 0:00:00 lr 0.000687	time 0.4570 (0.4681)	loss 3.7588 (3.4678)	grad_norm 1.5447 (nan)	mem 14852MB
[2022-11-06 14:43:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 113 training takes 0:09:45
[2022-11-06 14:43:08 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_113.pth saving......
[2022-11-06 14:43:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_113.pth saved !!!
[2022-11-06 14:43:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.513 (1.513)	Loss 1.0486 (1.0486)	Acc@1 74.707 (74.707)	Acc@5 94.531 (94.531)	Mem 14852MB
[2022-11-06 14:43:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.580 Acc@5 93.658
[2022-11-06 14:43:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.6%
[2022-11-06 14:43:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.752 (1.752)	Loss 0.7858 (0.7858)	Acc@1 81.250 (81.250)	Acc@5 96.484 (96.484)	Mem 14852MB
[2022-11-06 14:43:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.082 Acc@5 94.754
[2022-11-06 14:43:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.1%
[2022-11-06 14:43:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.08% at 113 epoch
[2022-11-06 14:43:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][0/1251]	eta 0:40:58 lr 0.000687	time 1.9654 (1.9654)	loss 4.2365 (4.2365)	grad_norm 1.3216 (1.3216)	mem 14852MB
[2022-11-06 14:43:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][50/1251]	eta 0:10:01 lr 0.000687	time 0.4651 (0.5011)	loss 3.8878 (3.4106)	grad_norm 1.4774 (1.3806)	mem 14852MB
[2022-11-06 14:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][100/1251]	eta 0:09:18 lr 0.000687	time 0.4719 (0.4849)	loss 4.2980 (3.4260)	grad_norm 1.4582 (1.3893)	mem 14852MB
[2022-11-06 14:44:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][150/1251]	eta 0:08:47 lr 0.000687	time 0.4658 (0.4792)	loss 3.0647 (3.4659)	grad_norm 1.3704 (1.3938)	mem 14852MB
[2022-11-06 14:45:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][200/1251]	eta 0:08:20 lr 0.000686	time 0.4725 (0.4759)	loss 3.8461 (3.4690)	grad_norm 1.4274 (1.4013)	mem 14852MB
[2022-11-06 14:45:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][250/1251]	eta 0:07:54 lr 0.000686	time 0.4620 (0.4741)	loss 3.3694 (3.4562)	grad_norm 1.2465 (1.3976)	mem 14852MB
[2022-11-06 14:45:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][300/1251]	eta 0:07:29 lr 0.000686	time 0.4670 (0.4726)	loss 3.4787 (3.4664)	grad_norm 1.3180 (1.3960)	mem 14852MB
[2022-11-06 14:46:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][350/1251]	eta 0:07:05 lr 0.000686	time 0.4654 (0.4717)	loss 4.2602 (3.4619)	grad_norm 1.2983 (1.3938)	mem 14852MB
[2022-11-06 14:46:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][400/1251]	eta 0:06:40 lr 0.000686	time 0.4737 (0.4711)	loss 4.0306 (3.4660)	grad_norm 1.3985 (1.3962)	mem 14852MB
[2022-11-06 14:46:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][450/1251]	eta 0:06:16 lr 0.000685	time 0.4653 (0.4706)	loss 3.0105 (3.4579)	grad_norm 1.3262 (1.3949)	mem 14852MB
[2022-11-06 14:47:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][500/1251]	eta 0:05:53 lr 0.000685	time 0.4634 (0.4701)	loss 2.7467 (3.4536)	grad_norm 1.2877 (1.3908)	mem 14852MB
[2022-11-06 14:47:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][550/1251]	eta 0:05:29 lr 0.000685	time 0.4572 (0.4701)	loss 2.9852 (3.4463)	grad_norm 1.3291 (1.3916)	mem 14852MB
[2022-11-06 14:48:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][600/1251]	eta 0:05:05 lr 0.000685	time 0.4624 (0.4697)	loss 3.8825 (3.4402)	grad_norm 1.4546 (1.3901)	mem 14852MB
[2022-11-06 14:48:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][650/1251]	eta 0:04:42 lr 0.000685	time 0.4573 (0.4694)	loss 2.3729 (3.4544)	grad_norm 1.4057 (1.3956)	mem 14852MB
[2022-11-06 14:48:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][700/1251]	eta 0:04:18 lr 0.000685	time 0.4697 (0.4694)	loss 2.5624 (3.4575)	grad_norm 1.3927 (1.3966)	mem 14852MB
[2022-11-06 14:49:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][750/1251]	eta 0:03:55 lr 0.000684	time 0.4732 (0.4692)	loss 3.1437 (3.4594)	grad_norm 1.2754 (1.3946)	mem 14852MB
[2022-11-06 14:49:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][800/1251]	eta 0:03:31 lr 0.000684	time 0.4710 (0.4692)	loss 4.1481 (3.4584)	grad_norm 1.4954 (1.3957)	mem 14852MB
[2022-11-06 14:50:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][850/1251]	eta 0:03:08 lr 0.000684	time 0.4610 (0.4690)	loss 3.8952 (3.4595)	grad_norm 1.3908 (1.3937)	mem 14852MB
[2022-11-06 14:50:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][900/1251]	eta 0:02:44 lr 0.000684	time 0.4641 (0.4688)	loss 3.6727 (3.4641)	grad_norm 1.4067 (1.3955)	mem 14852MB
[2022-11-06 14:50:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][950/1251]	eta 0:02:21 lr 0.000684	time 0.4631 (0.4686)	loss 3.5936 (3.4663)	grad_norm 1.6480 (1.3965)	mem 14852MB
[2022-11-06 14:51:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][1000/1251]	eta 0:01:57 lr 0.000683	time 0.4778 (0.4686)	loss 3.2294 (3.4610)	grad_norm 1.4597 (1.3970)	mem 14852MB
[2022-11-06 14:51:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][1050/1251]	eta 0:01:34 lr 0.000683	time 0.4542 (0.4686)	loss 3.5033 (3.4564)	grad_norm 1.3778 (1.3987)	mem 14852MB
[2022-11-06 14:52:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][1100/1251]	eta 0:01:10 lr 0.000683	time 0.4704 (0.4685)	loss 3.5693 (3.4546)	grad_norm 1.2962 (1.3978)	mem 14852MB
[2022-11-06 14:52:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][1150/1251]	eta 0:00:47 lr 0.000683	time 0.4693 (0.4684)	loss 3.4674 (3.4598)	grad_norm 1.4780 (1.3993)	mem 14852MB
[2022-11-06 14:52:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][1200/1251]	eta 0:00:23 lr 0.000683	time 0.5397 (0.4684)	loss 3.7866 (3.4573)	grad_norm 1.2332 (1.3995)	mem 14852MB
[2022-11-06 14:53:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [114/300][1250/1251]	eta 0:00:00 lr 0.000682	time 0.4589 (0.4682)	loss 4.1347 (3.4518)	grad_norm 1.3322 (1.3975)	mem 14852MB
[2022-11-06 14:53:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 114 training takes 0:09:45
[2022-11-06 14:53:12 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_114.pth saving......
[2022-11-06 14:53:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_114.pth saved !!!
[2022-11-06 14:53:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.631 (1.631)	Loss 0.9653 (0.9653)	Acc@1 76.367 (76.367)	Acc@5 94.434 (94.434)	Mem 14852MB
[2022-11-06 14:53:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.454 Acc@5 93.666
[2022-11-06 14:53:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.5%
[2022-11-06 14:53:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.659 (1.659)	Loss 0.9473 (0.9473)	Acc@1 77.734 (77.734)	Acc@5 94.043 (94.043)	Mem 14852MB
[2022-11-06 14:53:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.124 Acc@5 94.764
[2022-11-06 14:53:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.1%
[2022-11-06 14:53:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.12% at 114 epoch
[2022-11-06 14:53:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][0/1251]	eta 0:40:51 lr 0.000682	time 1.9593 (1.9593)	loss 4.2360 (4.2360)	grad_norm 1.3767 (1.3767)	mem 14852MB
[2022-11-06 14:53:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][50/1251]	eta 0:10:03 lr 0.000682	time 0.4648 (0.5027)	loss 2.7264 (3.4710)	grad_norm 1.2617 (nan)	mem 14852MB
[2022-11-06 14:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][100/1251]	eta 0:09:18 lr 0.000682	time 0.4639 (0.4854)	loss 2.1410 (3.4459)	grad_norm 1.4853 (nan)	mem 14852MB
[2022-11-06 14:54:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][150/1251]	eta 0:08:47 lr 0.000682	time 0.4733 (0.4790)	loss 3.8569 (3.4442)	grad_norm 1.3257 (nan)	mem 14852MB
[2022-11-06 14:55:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][200/1251]	eta 0:08:20 lr 0.000682	time 0.4681 (0.4763)	loss 3.9021 (3.4428)	grad_norm 1.2671 (nan)	mem 14852MB
[2022-11-06 14:55:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][250/1251]	eta 0:07:55 lr 0.000681	time 0.4611 (0.4748)	loss 3.9721 (3.4779)	grad_norm 1.3155 (nan)	mem 14852MB
[2022-11-06 14:55:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][300/1251]	eta 0:07:30 lr 0.000681	time 0.4754 (0.4733)	loss 3.8973 (3.4693)	grad_norm 1.5644 (nan)	mem 14852MB
[2022-11-06 14:56:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][350/1251]	eta 0:07:05 lr 0.000681	time 0.4685 (0.4722)	loss 3.2704 (3.4731)	grad_norm 1.4512 (nan)	mem 14852MB
[2022-11-06 14:56:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][400/1251]	eta 0:06:41 lr 0.000681	time 0.4622 (0.4716)	loss 2.6552 (3.4610)	grad_norm 1.4211 (nan)	mem 14852MB
[2022-11-06 14:57:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][450/1251]	eta 0:06:17 lr 0.000681	time 0.4664 (0.4710)	loss 3.9989 (3.4574)	grad_norm 1.4052 (nan)	mem 14852MB
[2022-11-06 14:57:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][500/1251]	eta 0:05:53 lr 0.000680	time 0.4734 (0.4703)	loss 2.3331 (3.4513)	grad_norm 1.2992 (nan)	mem 14852MB
[2022-11-06 14:57:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][550/1251]	eta 0:05:29 lr 0.000680	time 0.4597 (0.4704)	loss 3.4727 (3.4576)	grad_norm 1.3464 (nan)	mem 14852MB
[2022-11-06 14:58:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][600/1251]	eta 0:05:05 lr 0.000680	time 0.4511 (0.4700)	loss 3.7041 (3.4537)	grad_norm 1.3728 (nan)	mem 14852MB
[2022-11-06 14:58:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][650/1251]	eta 0:04:42 lr 0.000680	time 0.4666 (0.4697)	loss 3.8790 (3.4465)	grad_norm 1.2918 (nan)	mem 14852MB
[2022-11-06 14:58:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][700/1251]	eta 0:04:18 lr 0.000680	time 0.4605 (0.4697)	loss 3.4006 (3.4588)	grad_norm 1.3667 (nan)	mem 14852MB
[2022-11-06 14:59:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][750/1251]	eta 0:03:55 lr 0.000679	time 0.4770 (0.4694)	loss 3.2989 (3.4622)	grad_norm 1.2845 (nan)	mem 14852MB
[2022-11-06 14:59:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][800/1251]	eta 0:03:31 lr 0.000679	time 0.4610 (0.4694)	loss 4.0662 (3.4667)	grad_norm 1.3630 (nan)	mem 14852MB
[2022-11-06 15:00:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][850/1251]	eta 0:03:08 lr 0.000679	time 0.4622 (0.4694)	loss 3.1705 (3.4592)	grad_norm 1.4244 (nan)	mem 14852MB
[2022-11-06 15:00:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][900/1251]	eta 0:02:44 lr 0.000679	time 0.4613 (0.4692)	loss 4.0520 (3.4546)	grad_norm 1.4460 (nan)	mem 14852MB
[2022-11-06 15:00:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][950/1251]	eta 0:02:21 lr 0.000679	time 0.4697 (0.4691)	loss 3.5818 (3.4541)	grad_norm 1.3660 (nan)	mem 14852MB
[2022-11-06 15:01:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][1000/1251]	eta 0:01:57 lr 0.000679	time 0.4698 (0.4689)	loss 3.3400 (3.4533)	grad_norm 1.6734 (nan)	mem 14852MB
[2022-11-06 15:01:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][1050/1251]	eta 0:01:34 lr 0.000678	time 0.4619 (0.4690)	loss 3.5397 (3.4553)	grad_norm 1.3934 (nan)	mem 14852MB
[2022-11-06 15:02:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][1100/1251]	eta 0:01:10 lr 0.000678	time 0.4770 (0.4689)	loss 2.9801 (3.4526)	grad_norm 1.4554 (nan)	mem 14852MB
[2022-11-06 15:02:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][1150/1251]	eta 0:00:47 lr 0.000678	time 0.4566 (0.4688)	loss 3.2246 (3.4544)	grad_norm 1.4387 (nan)	mem 14852MB
[2022-11-06 15:02:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][1200/1251]	eta 0:00:23 lr 0.000678	time 0.4731 (0.4688)	loss 3.1852 (3.4512)	grad_norm 1.4770 (nan)	mem 14852MB
[2022-11-06 15:03:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [115/300][1250/1251]	eta 0:00:00 lr 0.000678	time 0.4572 (0.4686)	loss 3.4033 (3.4519)	grad_norm 1.4065 (nan)	mem 14852MB
[2022-11-06 15:03:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 115 training takes 0:09:46
[2022-11-06 15:03:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_115.pth saving......
[2022-11-06 15:03:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_115.pth saved !!!
[2022-11-06 15:03:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.472 (1.472)	Loss 1.0774 (1.0774)	Acc@1 75.488 (75.488)	Acc@5 92.578 (92.578)	Mem 14852MB
[2022-11-06 15:03:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.610 Acc@5 93.584
[2022-11-06 15:03:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.6%
[2022-11-06 15:03:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.825 (1.825)	Loss 0.9042 (0.9042)	Acc@1 79.004 (79.004)	Acc@5 93.945 (93.945)	Mem 14852MB
[2022-11-06 15:03:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.154 Acc@5 94.778
[2022-11-06 15:03:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.2%
[2022-11-06 15:03:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.15% at 115 epoch
[2022-11-06 15:03:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][0/1251]	eta 0:39:15 lr 0.000678	time 1.8829 (1.8829)	loss 3.4791 (3.4791)	grad_norm 1.4325 (1.4325)	mem 14852MB
[2022-11-06 15:04:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][50/1251]	eta 0:09:59 lr 0.000677	time 0.4618 (0.4988)	loss 3.3582 (3.4924)	grad_norm 1.3255 (1.4088)	mem 14852MB
[2022-11-06 15:04:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][100/1251]	eta 0:09:17 lr 0.000677	time 0.4657 (0.4843)	loss 4.1089 (3.4852)	grad_norm 1.4539 (1.4157)	mem 14852MB
[2022-11-06 15:04:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][150/1251]	eta 0:08:46 lr 0.000677	time 0.4683 (0.4786)	loss 4.0240 (3.4525)	grad_norm 1.4694 (1.4090)	mem 14852MB
[2022-11-06 15:05:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][200/1251]	eta 0:08:20 lr 0.000677	time 0.4620 (0.4760)	loss 3.2318 (3.4591)	grad_norm 1.4186 (1.4053)	mem 14852MB
[2022-11-06 15:05:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][250/1251]	eta 0:07:54 lr 0.000677	time 0.4511 (0.4740)	loss 2.9063 (3.4501)	grad_norm 1.3430 (1.4053)	mem 14852MB
[2022-11-06 15:05:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][300/1251]	eta 0:07:29 lr 0.000676	time 0.4726 (0.4725)	loss 3.6597 (3.4496)	grad_norm 1.3096 (1.4127)	mem 14852MB
[2022-11-06 15:06:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][350/1251]	eta 0:07:05 lr 0.000676	time 0.4711 (0.4717)	loss 2.4667 (3.4635)	grad_norm 1.6073 (1.4157)	mem 14852MB
[2022-11-06 15:06:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][400/1251]	eta 0:06:40 lr 0.000676	time 0.4545 (0.4711)	loss 2.6620 (3.4584)	grad_norm 1.3593 (1.4139)	mem 14852MB
[2022-11-06 15:07:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][450/1251]	eta 0:06:17 lr 0.000676	time 0.4715 (0.4707)	loss 3.3288 (3.4574)	grad_norm 1.4147 (1.4163)	mem 14852MB
[2022-11-06 15:07:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][500/1251]	eta 0:05:53 lr 0.000676	time 0.4643 (0.4703)	loss 3.6698 (3.4689)	grad_norm 1.4046 (1.4120)	mem 14852MB
[2022-11-06 15:07:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][550/1251]	eta 0:05:29 lr 0.000675	time 0.4687 (0.4703)	loss 2.4535 (3.4578)	grad_norm 1.4634 (1.4123)	mem 14852MB
[2022-11-06 15:08:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][600/1251]	eta 0:05:05 lr 0.000675	time 0.4623 (0.4699)	loss 3.7890 (3.4727)	grad_norm 1.3098 (1.4112)	mem 14852MB
[2022-11-06 15:08:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][650/1251]	eta 0:04:42 lr 0.000675	time 0.4563 (0.4698)	loss 3.6813 (3.4705)	grad_norm 1.2381 (1.4067)	mem 14852MB
[2022-11-06 15:09:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][700/1251]	eta 0:04:18 lr 0.000675	time 0.4712 (0.4697)	loss 3.1638 (3.4690)	grad_norm 1.3496 (1.4066)	mem 14852MB
[2022-11-06 15:09:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][750/1251]	eta 0:03:55 lr 0.000675	time 0.4721 (0.4695)	loss 4.2767 (3.4731)	grad_norm 1.3526 (1.4056)	mem 14852MB
[2022-11-06 15:09:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][800/1251]	eta 0:03:31 lr 0.000674	time 0.4728 (0.4695)	loss 3.7273 (3.4683)	grad_norm 1.4467 (nan)	mem 14852MB
[2022-11-06 15:10:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][850/1251]	eta 0:03:08 lr 0.000674	time 0.4625 (0.4693)	loss 3.1732 (3.4670)	grad_norm 1.3216 (nan)	mem 14852MB
[2022-11-06 15:10:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][900/1251]	eta 0:02:44 lr 0.000674	time 0.4650 (0.4692)	loss 3.7477 (3.4726)	grad_norm 1.4021 (nan)	mem 14852MB
[2022-11-06 15:11:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][950/1251]	eta 0:02:21 lr 0.000674	time 0.4624 (0.4690)	loss 2.8669 (3.4741)	grad_norm 1.5839 (nan)	mem 14852MB
[2022-11-06 15:11:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][1000/1251]	eta 0:01:57 lr 0.000674	time 0.4621 (0.4689)	loss 3.3818 (3.4764)	grad_norm 1.2591 (nan)	mem 14852MB
[2022-11-06 15:11:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][1050/1251]	eta 0:01:34 lr 0.000673	time 0.4611 (0.4690)	loss 3.2608 (3.4755)	grad_norm 1.3837 (nan)	mem 14852MB
[2022-11-06 15:12:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][1100/1251]	eta 0:01:10 lr 0.000673	time 0.4643 (0.4688)	loss 3.7151 (3.4800)	grad_norm 1.4494 (nan)	mem 14852MB
[2022-11-06 15:12:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][1150/1251]	eta 0:00:47 lr 0.000673	time 0.4672 (0.4687)	loss 3.4415 (3.4782)	grad_norm 1.4227 (nan)	mem 14852MB
[2022-11-06 15:12:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][1200/1251]	eta 0:00:23 lr 0.000673	time 0.4598 (0.4687)	loss 3.5329 (3.4747)	grad_norm 1.3303 (nan)	mem 14852MB
[2022-11-06 15:13:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [116/300][1250/1251]	eta 0:00:00 lr 0.000673	time 0.4582 (0.4685)	loss 2.5737 (3.4746)	grad_norm 1.3983 (nan)	mem 14852MB
[2022-11-06 15:13:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 116 training takes 0:09:46
[2022-11-06 15:13:21 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_116.pth saving......
[2022-11-06 15:13:22 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_116.pth saved !!!
[2022-11-06 15:13:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.492 (1.492)	Loss 1.0413 (1.0413)	Acc@1 75.391 (75.391)	Acc@5 93.750 (93.750)	Mem 14852MB
[2022-11-06 15:13:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.722 Acc@5 93.686
[2022-11-06 15:13:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.7%
[2022-11-06 15:13:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.665 (1.665)	Loss 0.7973 (0.7973)	Acc@1 80.273 (80.273)	Acc@5 95.605 (95.605)	Mem 14852MB
[2022-11-06 15:13:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.198 Acc@5 94.780
[2022-11-06 15:13:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.2%
[2022-11-06 15:13:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.20% at 116 epoch
[2022-11-06 15:13:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][0/1251]	eta 0:39:06 lr 0.000673	time 1.8760 (1.8760)	loss 3.8291 (3.8291)	grad_norm 1.3486 (1.3486)	mem 14852MB
[2022-11-06 15:14:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][50/1251]	eta 0:09:59 lr 0.000672	time 0.4653 (0.4994)	loss 3.4238 (3.5881)	grad_norm 1.3742 (1.3866)	mem 14852MB
[2022-11-06 15:14:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][100/1251]	eta 0:09:16 lr 0.000672	time 0.4555 (0.4834)	loss 4.1099 (3.5323)	grad_norm 1.4252 (1.4184)	mem 14852MB
[2022-11-06 15:14:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][150/1251]	eta 0:08:47 lr 0.000672	time 0.4740 (0.4787)	loss 3.8412 (3.5096)	grad_norm 1.2372 (1.4174)	mem 14852MB
[2022-11-06 15:15:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][200/1251]	eta 0:08:20 lr 0.000672	time 0.4677 (0.4760)	loss 3.1876 (3.5084)	grad_norm 1.5702 (1.4154)	mem 14852MB
[2022-11-06 15:15:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][250/1251]	eta 0:07:54 lr 0.000672	time 0.4583 (0.4742)	loss 2.6200 (3.4869)	grad_norm 1.3142 (1.4208)	mem 14852MB
[2022-11-06 15:16:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][300/1251]	eta 0:07:29 lr 0.000672	time 0.4686 (0.4727)	loss 2.5005 (3.4642)	grad_norm 1.5076 (1.4140)	mem 14852MB
[2022-11-06 15:16:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][350/1251]	eta 0:07:05 lr 0.000671	time 0.4541 (0.4718)	loss 3.8781 (3.4698)	grad_norm 1.5623 (1.4186)	mem 14852MB
[2022-11-06 15:16:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][400/1251]	eta 0:06:40 lr 0.000671	time 0.4643 (0.4712)	loss 2.5049 (3.4648)	grad_norm 1.3576 (1.4219)	mem 14852MB
[2022-11-06 15:17:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][450/1251]	eta 0:06:17 lr 0.000671	time 0.4633 (0.4709)	loss 4.2958 (3.4729)	grad_norm 1.3928 (1.4234)	mem 14852MB
[2022-11-06 15:17:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][500/1251]	eta 0:05:53 lr 0.000671	time 0.4575 (0.4704)	loss 3.3955 (3.4811)	grad_norm 1.2872 (1.4187)	mem 14852MB
[2022-11-06 15:17:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][550/1251]	eta 0:05:29 lr 0.000671	time 0.4549 (0.4701)	loss 3.1118 (3.4778)	grad_norm 1.5652 (1.4190)	mem 14852MB
[2022-11-06 15:18:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][600/1251]	eta 0:05:05 lr 0.000670	time 0.4553 (0.4698)	loss 3.4129 (3.4773)	grad_norm 1.3678 (1.4196)	mem 14852MB
[2022-11-06 15:18:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][650/1251]	eta 0:04:42 lr 0.000670	time 0.4662 (0.4696)	loss 3.6269 (3.4814)	grad_norm 1.3779 (1.4162)	mem 14852MB
[2022-11-06 15:19:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][700/1251]	eta 0:04:18 lr 0.000670	time 0.4784 (0.4695)	loss 3.4793 (3.4710)	grad_norm 1.3479 (1.4167)	mem 14852MB
[2022-11-06 15:19:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][750/1251]	eta 0:03:55 lr 0.000670	time 0.4605 (0.4693)	loss 3.5635 (3.4679)	grad_norm 1.4760 (1.4160)	mem 14852MB
[2022-11-06 15:19:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][800/1251]	eta 0:03:31 lr 0.000670	time 0.4779 (0.4690)	loss 3.8301 (3.4730)	grad_norm 1.4132 (1.4174)	mem 14852MB
[2022-11-06 15:20:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][850/1251]	eta 0:03:08 lr 0.000669	time 0.4655 (0.4691)	loss 4.0179 (3.4707)	grad_norm 1.2983 (1.4164)	mem 14852MB
[2022-11-06 15:20:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][900/1251]	eta 0:02:44 lr 0.000669	time 0.4631 (0.4688)	loss 3.9376 (3.4684)	grad_norm 1.3185 (1.4158)	mem 14852MB
[2022-11-06 15:21:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][950/1251]	eta 0:02:21 lr 0.000669	time 0.4674 (0.4687)	loss 3.9868 (3.4683)	grad_norm 1.5469 (1.4168)	mem 14852MB
[2022-11-06 15:21:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][1000/1251]	eta 0:01:57 lr 0.000669	time 0.4765 (0.4685)	loss 3.0133 (3.4729)	grad_norm 1.3704 (1.4162)	mem 14852MB
[2022-11-06 15:21:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][1050/1251]	eta 0:01:34 lr 0.000669	time 0.4604 (0.4685)	loss 3.5734 (3.4704)	grad_norm 1.3532 (1.4177)	mem 14852MB
[2022-11-06 15:22:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][1100/1251]	eta 0:01:10 lr 0.000668	time 0.4597 (0.4685)	loss 4.1141 (3.4708)	grad_norm 1.4732 (1.4163)	mem 14852MB
[2022-11-06 15:22:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][1150/1251]	eta 0:00:47 lr 0.000668	time 0.4692 (0.4684)	loss 3.6720 (3.4729)	grad_norm 1.3559 (1.4162)	mem 14852MB
[2022-11-06 15:23:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][1200/1251]	eta 0:00:23 lr 0.000668	time 0.4708 (0.4684)	loss 3.5252 (3.4734)	grad_norm 1.3489 (1.4151)	mem 14852MB
[2022-11-06 15:23:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [117/300][1250/1251]	eta 0:00:00 lr 0.000668	time 0.4582 (0.4682)	loss 3.6589 (3.4813)	grad_norm 1.4488 (1.4149)	mem 14852MB
[2022-11-06 15:23:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 117 training takes 0:09:45
[2022-11-06 15:23:25 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_117.pth saving......
[2022-11-06 15:23:26 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_117.pth saved !!!
[2022-11-06 15:23:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.551 (1.551)	Loss 1.0389 (1.0389)	Acc@1 75.293 (75.293)	Acc@5 93.945 (93.945)	Mem 14852MB
[2022-11-06 15:23:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.588 Acc@5 93.666
[2022-11-06 15:23:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.6%
[2022-11-06 15:23:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.754 (1.754)	Loss 0.8699 (0.8699)	Acc@1 79.492 (79.492)	Acc@5 95.020 (95.020)	Mem 14852MB
[2022-11-06 15:23:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.172 Acc@5 94.804
[2022-11-06 15:23:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.2%
[2022-11-06 15:23:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.20% at 116 epoch
[2022-11-06 15:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][0/1251]	eta 0:40:43 lr 0.000668	time 1.9530 (1.9530)	loss 2.5013 (2.5013)	grad_norm 1.4882 (1.4882)	mem 14852MB
[2022-11-06 15:24:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][50/1251]	eta 0:09:56 lr 0.000668	time 0.4635 (0.4963)	loss 3.8891 (3.5246)	grad_norm 1.3039 (1.4175)	mem 14852MB
[2022-11-06 15:24:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][100/1251]	eta 0:09:16 lr 0.000667	time 0.4644 (0.4838)	loss 3.4434 (3.4408)	grad_norm 1.3028 (1.4129)	mem 14852MB
[2022-11-06 15:24:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][150/1251]	eta 0:08:47 lr 0.000667	time 0.4743 (0.4791)	loss 3.6784 (3.4522)	grad_norm 1.6430 (1.4211)	mem 14852MB
[2022-11-06 15:25:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][200/1251]	eta 0:08:19 lr 0.000667	time 0.4646 (0.4755)	loss 2.7609 (3.4782)	grad_norm 1.4838 (1.4137)	mem 14852MB
[2022-11-06 15:25:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][250/1251]	eta 0:07:54 lr 0.000667	time 0.4668 (0.4740)	loss 2.9123 (3.4529)	grad_norm 1.5111 (1.4097)	mem 14852MB
[2022-11-06 15:26:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][300/1251]	eta 0:07:29 lr 0.000667	time 0.5395 (0.4731)	loss 3.5280 (3.4361)	grad_norm 1.3788 (1.4075)	mem 14852MB
[2022-11-06 15:26:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][350/1251]	eta 0:07:05 lr 0.000666	time 0.4671 (0.4721)	loss 3.7673 (3.4318)	grad_norm 1.3122 (1.4059)	mem 14852MB
[2022-11-06 15:26:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][400/1251]	eta 0:06:41 lr 0.000666	time 0.4627 (0.4714)	loss 3.2147 (3.4441)	grad_norm 1.3102 (1.4072)	mem 14852MB
[2022-11-06 15:27:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][450/1251]	eta 0:06:17 lr 0.000666	time 0.4651 (0.4707)	loss 2.5755 (3.4445)	grad_norm 1.3116 (1.4037)	mem 14852MB
[2022-11-06 15:27:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][500/1251]	eta 0:05:53 lr 0.000666	time 0.4598 (0.4703)	loss 3.1571 (3.4483)	grad_norm 1.3663 (1.4042)	mem 14852MB
[2022-11-06 15:28:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][550/1251]	eta 0:05:29 lr 0.000666	time 0.5757 (0.4702)	loss 3.5544 (3.4467)	grad_norm 1.4570 (1.4069)	mem 14852MB
[2022-11-06 15:28:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][600/1251]	eta 0:05:05 lr 0.000665	time 0.4653 (0.4698)	loss 4.1089 (3.4617)	grad_norm 1.4512 (1.4077)	mem 14852MB
[2022-11-06 15:28:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][650/1251]	eta 0:04:42 lr 0.000665	time 0.4589 (0.4697)	loss 3.7261 (3.4618)	grad_norm 1.4874 (1.4077)	mem 14852MB
[2022-11-06 15:29:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][700/1251]	eta 0:04:18 lr 0.000665	time 0.4691 (0.4694)	loss 3.4257 (3.4600)	grad_norm 1.4763 (1.4075)	mem 14852MB
[2022-11-06 15:29:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][750/1251]	eta 0:03:55 lr 0.000665	time 0.4608 (0.4692)	loss 3.9570 (3.4568)	grad_norm 1.4989 (1.4084)	mem 14852MB
[2022-11-06 15:29:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][800/1251]	eta 0:03:31 lr 0.000665	time 0.5451 (0.4690)	loss 3.5514 (3.4661)	grad_norm 1.2775 (nan)	mem 14852MB
[2022-11-06 15:30:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][850/1251]	eta 0:03:08 lr 0.000664	time 0.4518 (0.4689)	loss 3.0846 (3.4639)	grad_norm 1.4325 (nan)	mem 14852MB
[2022-11-06 15:30:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][900/1251]	eta 0:02:44 lr 0.000664	time 0.4737 (0.4689)	loss 3.5997 (3.4596)	grad_norm 1.3700 (nan)	mem 14852MB
[2022-11-06 15:31:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][950/1251]	eta 0:02:21 lr 0.000664	time 0.4655 (0.4687)	loss 3.5947 (3.4594)	grad_norm 1.1682 (nan)	mem 14852MB
[2022-11-06 15:31:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][1000/1251]	eta 0:01:57 lr 0.000664	time 0.5355 (0.4686)	loss 3.8730 (3.4582)	grad_norm 1.3167 (nan)	mem 14852MB
[2022-11-06 15:31:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][1050/1251]	eta 0:01:34 lr 0.000664	time 0.4527 (0.4686)	loss 3.7236 (3.4618)	grad_norm 1.4715 (nan)	mem 14852MB
[2022-11-06 15:32:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][1100/1251]	eta 0:01:10 lr 0.000663	time 0.4581 (0.4685)	loss 3.0674 (3.4577)	grad_norm 1.3102 (nan)	mem 14852MB
[2022-11-06 15:32:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][1150/1251]	eta 0:00:47 lr 0.000663	time 0.4637 (0.4685)	loss 4.3048 (3.4638)	grad_norm 1.8437 (nan)	mem 14852MB
[2022-11-06 15:33:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][1200/1251]	eta 0:00:23 lr 0.000663	time 0.4719 (0.4684)	loss 3.8458 (3.4616)	grad_norm 1.2861 (nan)	mem 14852MB
[2022-11-06 15:33:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [118/300][1250/1251]	eta 0:00:00 lr 0.000663	time 0.4576 (0.4684)	loss 3.4081 (3.4594)	grad_norm 1.4919 (nan)	mem 14852MB
[2022-11-06 15:33:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 118 training takes 0:09:46
[2022-11-06 15:33:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_118.pth saving......
[2022-11-06 15:33:30 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_118.pth saved !!!
[2022-11-06 15:33:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.633 (1.633)	Loss 1.0368 (1.0368)	Acc@1 75.195 (75.195)	Acc@5 94.043 (94.043)	Mem 14852MB
[2022-11-06 15:33:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.646 Acc@5 93.694
[2022-11-06 15:33:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.6%
[2022-11-06 15:33:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.647 (1.647)	Loss 0.7980 (0.7980)	Acc@1 80.371 (80.371)	Acc@5 95.020 (95.020)	Mem 14852MB
[2022-11-06 15:33:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.146 Acc@5 94.816
[2022-11-06 15:33:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.1%
[2022-11-06 15:33:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.20% at 116 epoch
[2022-11-06 15:33:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][0/1251]	eta 0:40:00 lr 0.000663	time 1.9191 (1.9191)	loss 3.9786 (3.9786)	grad_norm 1.3719 (1.3719)	mem 14852MB
[2022-11-06 15:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][50/1251]	eta 0:09:57 lr 0.000663	time 0.4618 (0.4976)	loss 3.8513 (3.4273)	grad_norm 1.2466 (1.4364)	mem 14852MB
[2022-11-06 15:34:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][100/1251]	eta 0:09:17 lr 0.000662	time 0.4647 (0.4848)	loss 3.4307 (3.3664)	grad_norm 1.4404 (1.4242)	mem 14852MB
[2022-11-06 15:34:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][150/1251]	eta 0:08:46 lr 0.000662	time 0.4614 (0.4786)	loss 3.2115 (3.3809)	grad_norm 1.3448 (1.4295)	mem 14852MB
[2022-11-06 15:35:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][200/1251]	eta 0:08:19 lr 0.000662	time 0.4796 (0.4757)	loss 4.0062 (3.3898)	grad_norm 1.2728 (1.4268)	mem 14852MB
[2022-11-06 15:35:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][250/1251]	eta 0:07:54 lr 0.000662	time 0.4587 (0.4739)	loss 3.3847 (3.4116)	grad_norm 1.4506 (1.4226)	mem 14852MB
[2022-11-06 15:36:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][300/1251]	eta 0:07:29 lr 0.000662	time 0.5430 (0.4727)	loss 3.2028 (3.3937)	grad_norm 1.3978 (1.4202)	mem 14852MB
[2022-11-06 15:36:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][350/1251]	eta 0:07:04 lr 0.000662	time 0.4537 (0.4716)	loss 3.6648 (3.4009)	grad_norm 1.4580 (1.4197)	mem 14852MB
[2022-11-06 15:36:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][400/1251]	eta 0:06:41 lr 0.000661	time 0.4584 (0.4714)	loss 2.8444 (3.4096)	grad_norm 1.1884 (1.4194)	mem 14852MB
[2022-11-06 15:37:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][450/1251]	eta 0:06:16 lr 0.000661	time 0.4754 (0.4706)	loss 3.9399 (3.4057)	grad_norm 1.3700 (1.4209)	mem 14852MB
[2022-11-06 15:37:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][500/1251]	eta 0:05:53 lr 0.000661	time 0.4716 (0.4701)	loss 3.3160 (3.4192)	grad_norm 1.2892 (1.4195)	mem 14852MB
[2022-11-06 15:38:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][550/1251]	eta 0:05:29 lr 0.000661	time 0.4584 (0.4699)	loss 3.2275 (3.4250)	grad_norm 1.5123 (1.4172)	mem 14852MB
[2022-11-06 15:38:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][600/1251]	eta 0:05:05 lr 0.000661	time 0.4784 (0.4696)	loss 3.1102 (3.4330)	grad_norm 1.5701 (1.4194)	mem 14852MB
[2022-11-06 15:38:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][650/1251]	eta 0:04:42 lr 0.000660	time 0.4657 (0.4693)	loss 3.8542 (3.4411)	grad_norm 1.3099 (1.4179)	mem 14852MB
[2022-11-06 15:39:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][700/1251]	eta 0:04:18 lr 0.000660	time 0.4637 (0.4693)	loss 2.2804 (3.4446)	grad_norm 1.3675 (1.4207)	mem 14852MB
[2022-11-06 15:39:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][750/1251]	eta 0:03:54 lr 0.000660	time 0.4614 (0.4690)	loss 2.8383 (3.4357)	grad_norm 1.3512 (1.4195)	mem 14852MB
[2022-11-06 15:40:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][800/1251]	eta 0:03:31 lr 0.000660	time 0.4613 (0.4688)	loss 3.6932 (3.4384)	grad_norm 1.4938 (1.4193)	mem 14852MB
[2022-11-06 15:40:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][850/1251]	eta 0:03:07 lr 0.000660	time 0.4621 (0.4687)	loss 3.7859 (3.4398)	grad_norm 1.4535 (1.4174)	mem 14852MB
[2022-11-06 15:40:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][900/1251]	eta 0:02:44 lr 0.000659	time 0.4702 (0.4688)	loss 2.3927 (3.4449)	grad_norm 1.5345 (1.4176)	mem 14852MB
[2022-11-06 15:41:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][950/1251]	eta 0:02:21 lr 0.000659	time 0.4601 (0.4686)	loss 2.9935 (3.4428)	grad_norm 1.4903 (1.4166)	mem 14852MB
[2022-11-06 15:41:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][1000/1251]	eta 0:01:57 lr 0.000659	time 0.4661 (0.4685)	loss 3.5300 (3.4459)	grad_norm 1.4066 (1.4172)	mem 14852MB
[2022-11-06 15:41:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][1050/1251]	eta 0:01:34 lr 0.000659	time 0.4572 (0.4685)	loss 3.9950 (3.4519)	grad_norm 1.6696 (1.4175)	mem 14852MB
[2022-11-06 15:42:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][1100/1251]	eta 0:01:10 lr 0.000659	time 0.4786 (0.4684)	loss 3.8636 (3.4557)	grad_norm 1.4362 (1.4177)	mem 14852MB
[2022-11-06 15:42:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][1150/1251]	eta 0:00:47 lr 0.000658	time 0.4758 (0.4685)	loss 2.2383 (3.4596)	grad_norm 1.3653 (1.4179)	mem 14852MB
[2022-11-06 15:43:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][1200/1251]	eta 0:00:23 lr 0.000658	time 0.4725 (0.4683)	loss 4.1721 (3.4613)	grad_norm 1.3492 (1.4191)	mem 14852MB
[2022-11-06 15:43:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [119/300][1250/1251]	eta 0:00:00 lr 0.000658	time 0.4570 (0.4681)	loss 3.3899 (3.4617)	grad_norm 1.4209 (1.4188)	mem 14852MB
[2022-11-06 15:43:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 119 training takes 0:09:45
[2022-11-06 15:43:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_119.pth saving......
[2022-11-06 15:43:34 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_119.pth saved !!!
[2022-11-06 15:43:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.628 (1.628)	Loss 0.9562 (0.9562)	Acc@1 77.441 (77.441)	Acc@5 94.141 (94.141)	Mem 14852MB
[2022-11-06 15:43:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.780 Acc@5 93.738
[2022-11-06 15:43:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.8%
[2022-11-06 15:43:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.580 (1.580)	Loss 0.8253 (0.8253)	Acc@1 80.566 (80.566)	Acc@5 95.508 (95.508)	Mem 14852MB
[2022-11-06 15:43:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.188 Acc@5 94.834
[2022-11-06 15:43:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.2%
[2022-11-06 15:43:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.20% at 116 epoch
[2022-11-06 15:43:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][0/1251]	eta 0:42:21 lr 0.000658	time 2.0314 (2.0314)	loss 3.1689 (3.1689)	grad_norm 1.3673 (1.3673)	mem 14852MB
[2022-11-06 15:44:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][50/1251]	eta 0:10:03 lr 0.000658	time 0.4588 (0.5028)	loss 2.5728 (3.4653)	grad_norm 1.3612 (1.3896)	mem 14852MB
[2022-11-06 15:44:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][100/1251]	eta 0:09:18 lr 0.000658	time 0.4733 (0.4855)	loss 3.8743 (3.4094)	grad_norm 1.3363 (1.4001)	mem 14852MB
[2022-11-06 15:45:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][150/1251]	eta 0:08:48 lr 0.000657	time 0.4647 (0.4800)	loss 3.4047 (3.4019)	grad_norm 1.4177 (1.4135)	mem 14852MB
[2022-11-06 15:45:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][200/1251]	eta 0:08:20 lr 0.000657	time 0.4747 (0.4762)	loss 3.6642 (3.4270)	grad_norm 1.3801 (1.4113)	mem 14852MB
[2022-11-06 15:45:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][250/1251]	eta 0:07:55 lr 0.000657	time 0.4681 (0.4747)	loss 3.1071 (3.4362)	grad_norm 1.3720 (1.4110)	mem 14852MB
[2022-11-06 15:46:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][300/1251]	eta 0:07:30 lr 0.000657	time 0.4711 (0.4732)	loss 2.7241 (3.4557)	grad_norm 1.5429 (1.4164)	mem 14852MB
[2022-11-06 15:46:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][350/1251]	eta 0:07:05 lr 0.000657	time 0.4694 (0.4718)	loss 4.4131 (3.4675)	grad_norm 1.4730 (nan)	mem 14852MB
[2022-11-06 15:47:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][400/1251]	eta 0:06:40 lr 0.000656	time 0.4682 (0.4711)	loss 2.3170 (3.4693)	grad_norm 1.3624 (nan)	mem 14852MB
[2022-11-06 15:47:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][450/1251]	eta 0:06:16 lr 0.000656	time 0.4677 (0.4705)	loss 3.0143 (3.4713)	grad_norm 1.6703 (nan)	mem 14852MB
[2022-11-06 15:47:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][500/1251]	eta 0:05:53 lr 0.000656	time 0.4535 (0.4702)	loss 3.3812 (3.4703)	grad_norm 1.2518 (nan)	mem 14852MB
[2022-11-06 15:48:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][550/1251]	eta 0:05:29 lr 0.000656	time 0.4738 (0.4700)	loss 3.5566 (3.4728)	grad_norm 1.3159 (nan)	mem 14852MB
[2022-11-06 15:48:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][600/1251]	eta 0:05:05 lr 0.000656	time 0.4619 (0.4699)	loss 3.4803 (3.4732)	grad_norm 1.3541 (nan)	mem 14852MB
[2022-11-06 15:48:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][650/1251]	eta 0:04:42 lr 0.000655	time 0.4693 (0.4698)	loss 3.8390 (3.4677)	grad_norm 1.3900 (nan)	mem 14852MB
[2022-11-06 15:49:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][700/1251]	eta 0:04:18 lr 0.000655	time 0.4675 (0.4694)	loss 2.4617 (3.4684)	grad_norm 1.3469 (nan)	mem 14852MB
[2022-11-06 15:49:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][750/1251]	eta 0:03:55 lr 0.000655	time 0.4712 (0.4694)	loss 4.3139 (3.4683)	grad_norm 1.4566 (nan)	mem 14852MB
[2022-11-06 15:50:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][800/1251]	eta 0:03:31 lr 0.000655	time 0.4562 (0.4692)	loss 3.1213 (3.4680)	grad_norm 1.7210 (nan)	mem 14852MB
[2022-11-06 15:50:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][850/1251]	eta 0:03:08 lr 0.000655	time 0.4619 (0.4691)	loss 2.8290 (3.4701)	grad_norm 1.5195 (nan)	mem 14852MB
[2022-11-06 15:50:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][900/1251]	eta 0:02:44 lr 0.000654	time 0.4669 (0.4691)	loss 3.5090 (3.4692)	grad_norm 1.3100 (nan)	mem 14852MB
[2022-11-06 15:51:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][950/1251]	eta 0:02:21 lr 0.000654	time 0.4593 (0.4689)	loss 2.9186 (3.4677)	grad_norm 1.4711 (nan)	mem 14852MB
[2022-11-06 15:51:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][1000/1251]	eta 0:01:57 lr 0.000654	time 0.4671 (0.4689)	loss 3.0937 (3.4717)	grad_norm 1.2520 (nan)	mem 14852MB
[2022-11-06 15:52:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][1050/1251]	eta 0:01:34 lr 0.000654	time 0.4591 (0.4687)	loss 3.3797 (3.4711)	grad_norm 1.4267 (nan)	mem 14852MB
[2022-11-06 15:52:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][1100/1251]	eta 0:01:10 lr 0.000654	time 0.4595 (0.4687)	loss 3.9206 (3.4707)	grad_norm 1.4127 (nan)	mem 14852MB
[2022-11-06 15:52:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][1150/1251]	eta 0:00:47 lr 0.000653	time 0.4563 (0.4686)	loss 3.7156 (3.4697)	grad_norm 1.3535 (nan)	mem 14852MB
[2022-11-06 15:53:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][1200/1251]	eta 0:00:23 lr 0.000653	time 0.4557 (0.4685)	loss 4.3658 (3.4685)	grad_norm 1.4297 (nan)	mem 14852MB
[2022-11-06 15:53:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [120/300][1250/1251]	eta 0:00:00 lr 0.000653	time 0.4563 (0.4684)	loss 2.4605 (3.4708)	grad_norm 1.6411 (nan)	mem 14852MB
[2022-11-06 15:53:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 120 training takes 0:09:46
[2022-11-06 15:53:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_120.pth saving......
[2022-11-06 15:53:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_120.pth saved !!!
[2022-11-06 15:53:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.576 (1.576)	Loss 0.9671 (0.9671)	Acc@1 77.637 (77.637)	Acc@5 95.117 (95.117)	Mem 14852MB
[2022-11-06 15:53:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.808 Acc@5 93.712
[2022-11-06 15:53:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.8%
[2022-11-06 15:53:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.592 (1.592)	Loss 0.8577 (0.8577)	Acc@1 78.711 (78.711)	Acc@5 94.727 (94.727)	Mem 14852MB
[2022-11-06 15:53:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.222 Acc@5 94.858
[2022-11-06 15:53:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.2%
[2022-11-06 15:53:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.22% at 120 epoch
[2022-11-06 15:53:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][0/1251]	eta 0:43:01 lr 0.000653	time 2.0633 (2.0633)	loss 2.5959 (2.5959)	grad_norm 1.3420 (1.3420)	mem 14852MB
[2022-11-06 15:54:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][50/1251]	eta 0:10:04 lr 0.000653	time 0.4637 (0.5031)	loss 2.6334 (3.4567)	grad_norm 1.2517 (nan)	mem 14852MB
[2022-11-06 15:54:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][100/1251]	eta 0:09:19 lr 0.000653	time 0.4575 (0.4864)	loss 3.0623 (3.4020)	grad_norm 1.3154 (nan)	mem 14852MB
[2022-11-06 15:55:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][150/1251]	eta 0:08:49 lr 0.000652	time 0.4591 (0.4806)	loss 4.0145 (3.3838)	grad_norm 1.4222 (nan)	mem 14852MB
[2022-11-06 15:55:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][200/1251]	eta 0:08:21 lr 0.000652	time 0.4763 (0.4769)	loss 3.1918 (3.3783)	grad_norm 1.3620 (nan)	mem 14852MB
[2022-11-06 15:55:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][250/1251]	eta 0:07:55 lr 0.000652	time 0.4591 (0.4746)	loss 4.0156 (3.3959)	grad_norm 1.4136 (nan)	mem 14852MB
[2022-11-06 15:56:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][300/1251]	eta 0:07:30 lr 0.000652	time 0.4628 (0.4733)	loss 3.0855 (3.3987)	grad_norm 1.3108 (nan)	mem 14852MB
[2022-11-06 15:56:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][350/1251]	eta 0:07:05 lr 0.000652	time 0.4721 (0.4724)	loss 2.9327 (3.4058)	grad_norm 1.4675 (nan)	mem 14852MB
[2022-11-06 15:57:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][400/1251]	eta 0:06:41 lr 0.000651	time 0.4694 (0.4717)	loss 3.3373 (3.4139)	grad_norm 1.3059 (nan)	mem 14852MB
[2022-11-06 15:57:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][450/1251]	eta 0:06:17 lr 0.000651	time 0.4642 (0.4709)	loss 4.1404 (3.4285)	grad_norm 1.4674 (nan)	mem 14852MB
[2022-11-06 15:57:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][500/1251]	eta 0:05:53 lr 0.000651	time 0.4619 (0.4705)	loss 3.6937 (3.4394)	grad_norm 1.6127 (nan)	mem 14852MB
[2022-11-06 15:58:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][550/1251]	eta 0:05:29 lr 0.000651	time 0.4677 (0.4703)	loss 2.3267 (3.4410)	grad_norm 1.3885 (nan)	mem 14852MB
[2022-11-06 15:58:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][600/1251]	eta 0:05:05 lr 0.000651	time 0.4548 (0.4700)	loss 3.0514 (3.4359)	grad_norm 1.2921 (nan)	mem 14852MB
[2022-11-06 15:59:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][650/1251]	eta 0:04:42 lr 0.000650	time 0.4710 (0.4699)	loss 3.7024 (3.4384)	grad_norm 1.4179 (nan)	mem 14852MB
[2022-11-06 15:59:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][700/1251]	eta 0:04:18 lr 0.000650	time 0.4627 (0.4698)	loss 3.6675 (3.4373)	grad_norm 1.4449 (nan)	mem 14852MB
[2022-11-06 15:59:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][750/1251]	eta 0:03:55 lr 0.000650	time 0.4639 (0.4695)	loss 3.7006 (3.4481)	grad_norm 1.4256 (nan)	mem 14852MB
[2022-11-06 16:00:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][800/1251]	eta 0:03:31 lr 0.000650	time 0.4604 (0.4696)	loss 3.1087 (3.4499)	grad_norm 1.2663 (nan)	mem 14852MB
[2022-11-06 16:00:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][850/1251]	eta 0:03:08 lr 0.000650	time 0.4598 (0.4696)	loss 3.2318 (3.4472)	grad_norm 1.6377 (nan)	mem 14852MB
[2022-11-06 16:00:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][900/1251]	eta 0:02:44 lr 0.000649	time 0.4620 (0.4694)	loss 3.9899 (3.4492)	grad_norm 1.3531 (nan)	mem 14852MB
[2022-11-06 16:01:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][950/1251]	eta 0:02:21 lr 0.000649	time 0.4602 (0.4692)	loss 3.3315 (3.4474)	grad_norm 1.3234 (nan)	mem 14852MB
[2022-11-06 16:01:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][1000/1251]	eta 0:01:57 lr 0.000649	time 0.4554 (0.4692)	loss 3.1790 (3.4496)	grad_norm 1.6393 (nan)	mem 14852MB
[2022-11-06 16:02:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][1050/1251]	eta 0:01:34 lr 0.000649	time 0.4612 (0.4693)	loss 3.9915 (3.4534)	grad_norm 1.4624 (nan)	mem 14852MB
[2022-11-06 16:02:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][1100/1251]	eta 0:01:10 lr 0.000649	time 0.4750 (0.4691)	loss 3.1355 (3.4499)	grad_norm 1.5508 (nan)	mem 14852MB
[2022-11-06 16:02:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][1150/1251]	eta 0:00:47 lr 0.000648	time 0.4592 (0.4690)	loss 3.7215 (3.4441)	grad_norm 1.2642 (nan)	mem 14852MB
[2022-11-06 16:03:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][1200/1251]	eta 0:00:23 lr 0.000648	time 0.4635 (0.4689)	loss 2.5416 (3.4445)	grad_norm 1.3223 (nan)	mem 14852MB
[2022-11-06 16:03:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [121/300][1250/1251]	eta 0:00:00 lr 0.000648	time 0.4574 (0.4686)	loss 2.6543 (3.4484)	grad_norm 1.3041 (nan)	mem 14852MB
[2022-11-06 16:03:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 121 training takes 0:09:46
[2022-11-06 16:03:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_121.pth saving......
[2022-11-06 16:03:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_121.pth saved !!!
[2022-11-06 16:03:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.769 (1.769)	Loss 0.9535 (0.9535)	Acc@1 77.539 (77.539)	Acc@5 95.508 (95.508)	Mem 14852MB
[2022-11-06 16:03:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.902 Acc@5 93.886
[2022-11-06 16:03:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.9%
[2022-11-06 16:03:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.639 (1.639)	Loss 0.8280 (0.8280)	Acc@1 79.883 (79.883)	Acc@5 95.605 (95.605)	Mem 14852MB
[2022-11-06 16:04:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.278 Acc@5 94.898
[2022-11-06 16:04:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.3%
[2022-11-06 16:04:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.28% at 121 epoch
[2022-11-06 16:04:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][0/1251]	eta 0:40:22 lr 0.000648	time 1.9361 (1.9361)	loss 3.8629 (3.8629)	grad_norm 1.4248 (1.4248)	mem 14852MB
[2022-11-06 16:04:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][50/1251]	eta 0:09:57 lr 0.000648	time 0.4702 (0.4979)	loss 2.5379 (3.4370)	grad_norm 1.2764 (1.4429)	mem 14852MB
[2022-11-06 16:04:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][100/1251]	eta 0:09:17 lr 0.000648	time 0.4646 (0.4840)	loss 2.5145 (3.3322)	grad_norm 1.6722 (1.4455)	mem 14852MB
[2022-11-06 16:05:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][150/1251]	eta 0:08:46 lr 0.000647	time 0.4617 (0.4784)	loss 3.1236 (3.3502)	grad_norm 1.5131 (1.4353)	mem 14852MB
[2022-11-06 16:05:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][200/1251]	eta 0:08:20 lr 0.000647	time 0.4710 (0.4759)	loss 3.1795 (3.3596)	grad_norm 1.4782 (1.4407)	mem 14852MB
[2022-11-06 16:05:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][250/1251]	eta 0:07:54 lr 0.000647	time 0.4626 (0.4736)	loss 3.7891 (3.3740)	grad_norm 1.2937 (1.4349)	mem 14852MB
[2022-11-06 16:06:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][300/1251]	eta 0:07:28 lr 0.000647	time 0.4630 (0.4721)	loss 3.2082 (3.3820)	grad_norm 1.3954 (1.4349)	mem 14852MB
[2022-11-06 16:06:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][350/1251]	eta 0:07:04 lr 0.000647	time 0.4543 (0.4713)	loss 3.0906 (3.3824)	grad_norm 1.2928 (1.4336)	mem 14852MB
[2022-11-06 16:07:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][400/1251]	eta 0:06:40 lr 0.000646	time 0.4620 (0.4706)	loss 3.7874 (3.3829)	grad_norm 1.3864 (1.4344)	mem 14852MB
[2022-11-06 16:07:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][450/1251]	eta 0:06:16 lr 0.000646	time 0.4609 (0.4700)	loss 2.7029 (3.3934)	grad_norm 1.5852 (1.4367)	mem 14852MB
[2022-11-06 16:07:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][500/1251]	eta 0:05:52 lr 0.000646	time 0.4628 (0.4698)	loss 3.9243 (3.3896)	grad_norm 1.4125 (1.4363)	mem 14852MB
[2022-11-06 16:08:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][550/1251]	eta 0:05:29 lr 0.000646	time 0.4609 (0.4695)	loss 2.9878 (3.3904)	grad_norm 1.2419 (1.4336)	mem 14852MB
[2022-11-06 16:08:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][600/1251]	eta 0:05:05 lr 0.000646	time 0.4571 (0.4694)	loss 2.5797 (3.3855)	grad_norm 1.4261 (1.4324)	mem 14852MB
[2022-11-06 16:09:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][650/1251]	eta 0:04:41 lr 0.000645	time 0.4646 (0.4692)	loss 4.0943 (3.3999)	grad_norm 1.5732 (1.4327)	mem 14852MB
[2022-11-06 16:09:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][700/1251]	eta 0:04:18 lr 0.000645	time 0.4649 (0.4690)	loss 3.6334 (3.4089)	grad_norm 1.3395 (1.4331)	mem 14852MB
[2022-11-06 16:09:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][750/1251]	eta 0:03:54 lr 0.000645	time 0.4622 (0.4688)	loss 3.4225 (3.4135)	grad_norm 1.3594 (1.4329)	mem 14852MB
[2022-11-06 16:10:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][800/1251]	eta 0:03:31 lr 0.000645	time 0.4559 (0.4685)	loss 3.6616 (3.4211)	grad_norm 1.3425 (1.4343)	mem 14852MB
[2022-11-06 16:10:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][850/1251]	eta 0:03:07 lr 0.000645	time 0.4653 (0.4685)	loss 3.5068 (3.4277)	grad_norm 1.3710 (1.4348)	mem 14852MB
[2022-11-06 16:11:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][900/1251]	eta 0:02:44 lr 0.000644	time 0.5458 (0.4684)	loss 3.8922 (3.4298)	grad_norm 1.3175 (1.4332)	mem 14852MB
[2022-11-06 16:11:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][950/1251]	eta 0:02:20 lr 0.000644	time 0.4616 (0.4682)	loss 3.2919 (3.4282)	grad_norm 1.3171 (1.4342)	mem 14852MB
[2022-11-06 16:11:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][1000/1251]	eta 0:01:57 lr 0.000644	time 0.4661 (0.4681)	loss 3.5324 (3.4296)	grad_norm 1.3225 (1.4341)	mem 14852MB
[2022-11-06 16:12:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][1050/1251]	eta 0:01:34 lr 0.000644	time 0.4721 (0.4681)	loss 3.3905 (3.4315)	grad_norm 1.2688 (1.4353)	mem 14852MB
[2022-11-06 16:12:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][1100/1251]	eta 0:01:10 lr 0.000644	time 0.4700 (0.4681)	loss 3.4455 (3.4301)	grad_norm 1.3706 (1.4355)	mem 14852MB
[2022-11-06 16:12:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][1150/1251]	eta 0:00:47 lr 0.000644	time 0.4693 (0.4682)	loss 3.9149 (3.4334)	grad_norm 1.3440 (1.4351)	mem 14852MB
[2022-11-06 16:13:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][1200/1251]	eta 0:00:23 lr 0.000643	time 0.4657 (0.4681)	loss 3.9029 (3.4345)	grad_norm 1.5220 (1.4343)	mem 14852MB
[2022-11-06 16:13:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [122/300][1250/1251]	eta 0:00:00 lr 0.000643	time 0.4578 (0.4679)	loss 3.6081 (3.4362)	grad_norm 1.3354 (1.4350)	mem 14852MB
[2022-11-06 16:13:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 122 training takes 0:09:45
[2022-11-06 16:13:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_122.pth saving......
[2022-11-06 16:13:46 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_122.pth saved !!!
[2022-11-06 16:13:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.528 (1.528)	Loss 0.9766 (0.9766)	Acc@1 76.172 (76.172)	Acc@5 94.434 (94.434)	Mem 14852MB
[2022-11-06 16:13:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.604 Acc@5 93.652
[2022-11-06 16:13:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.6%
[2022-11-06 16:13:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.564 (1.564)	Loss 0.9508 (0.9508)	Acc@1 77.148 (77.148)	Acc@5 94.531 (94.531)	Mem 14852MB
[2022-11-06 16:14:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.338 Acc@5 94.904
[2022-11-06 16:14:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.3%
[2022-11-06 16:14:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.34% at 122 epoch
[2022-11-06 16:14:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][0/1251]	eta 0:40:57 lr 0.000643	time 1.9647 (1.9647)	loss 3.5764 (3.5764)	grad_norm 1.4303 (1.4303)	mem 14852MB
[2022-11-06 16:14:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][50/1251]	eta 0:10:02 lr 0.000643	time 0.4564 (0.5020)	loss 4.2012 (3.3779)	grad_norm 1.4528 (1.4420)	mem 14852MB
[2022-11-06 16:14:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][100/1251]	eta 0:09:20 lr 0.000643	time 0.4622 (0.4868)	loss 3.3166 (3.3697)	grad_norm 1.4405 (1.4193)	mem 14852MB
[2022-11-06 16:15:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][150/1251]	eta 0:08:48 lr 0.000643	time 0.4690 (0.4797)	loss 3.2302 (3.3966)	grad_norm 1.2484 (1.4237)	mem 14852MB
[2022-11-06 16:15:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][200/1251]	eta 0:08:20 lr 0.000642	time 0.4609 (0.4760)	loss 3.6142 (3.4436)	grad_norm 1.3630 (1.4243)	mem 14852MB
[2022-11-06 16:16:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][250/1251]	eta 0:07:54 lr 0.000642	time 0.4670 (0.4741)	loss 3.5130 (3.4295)	grad_norm 1.3646 (1.4148)	mem 14852MB
[2022-11-06 16:16:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][300/1251]	eta 0:07:29 lr 0.000642	time 0.4593 (0.4730)	loss 3.8353 (3.4080)	grad_norm 1.2989 (1.4121)	mem 14852MB
[2022-11-06 16:16:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][350/1251]	eta 0:07:05 lr 0.000642	time 0.4664 (0.4721)	loss 3.9248 (3.4335)	grad_norm 1.4373 (1.4163)	mem 14852MB
[2022-11-06 16:17:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][400/1251]	eta 0:06:41 lr 0.000642	time 0.4606 (0.4713)	loss 3.7697 (3.4355)	grad_norm 1.4050 (1.4200)	mem 14852MB
[2022-11-06 16:17:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][450/1251]	eta 0:06:17 lr 0.000641	time 0.4670 (0.4707)	loss 3.5308 (3.4374)	grad_norm 1.3397 (1.4211)	mem 14852MB
[2022-11-06 16:17:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][500/1251]	eta 0:05:53 lr 0.000641	time 0.4545 (0.4702)	loss 3.5580 (3.4329)	grad_norm 1.4429 (1.4241)	mem 14852MB
[2022-11-06 16:18:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][550/1251]	eta 0:05:29 lr 0.000641	time 0.4620 (0.4701)	loss 3.1464 (3.4425)	grad_norm 1.4303 (1.4272)	mem 14852MB
[2022-11-06 16:18:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][600/1251]	eta 0:05:05 lr 0.000641	time 0.4561 (0.4700)	loss 4.0362 (3.4494)	grad_norm 1.4880 (1.4301)	mem 14852MB
[2022-11-06 16:19:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][650/1251]	eta 0:04:42 lr 0.000641	time 0.4693 (0.4697)	loss 2.5269 (3.4543)	grad_norm 1.4700 (1.4295)	mem 14852MB
[2022-11-06 16:19:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][700/1251]	eta 0:04:18 lr 0.000640	time 0.4632 (0.4694)	loss 3.1544 (3.4510)	grad_norm 1.9312 (1.4292)	mem 14852MB
[2022-11-06 16:19:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][750/1251]	eta 0:03:55 lr 0.000640	time 0.4684 (0.4693)	loss 4.1977 (3.4507)	grad_norm 1.5438 (1.4307)	mem 14852MB
[2022-11-06 16:20:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][800/1251]	eta 0:03:31 lr 0.000640	time 0.4608 (0.4691)	loss 3.7694 (3.4529)	grad_norm 1.6513 (1.4315)	mem 14852MB
[2022-11-06 16:20:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][850/1251]	eta 0:03:08 lr 0.000640	time 0.4550 (0.4689)	loss 3.4839 (3.4534)	grad_norm 1.4014 (1.4316)	mem 14852MB
[2022-11-06 16:21:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][900/1251]	eta 0:02:44 lr 0.000640	time 0.4663 (0.4688)	loss 3.9674 (3.4509)	grad_norm 1.5152 (1.4331)	mem 14852MB
[2022-11-06 16:21:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][950/1251]	eta 0:02:21 lr 0.000639	time 0.4684 (0.4687)	loss 3.4679 (3.4517)	grad_norm 1.2865 (1.4330)	mem 14852MB
[2022-11-06 16:21:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][1000/1251]	eta 0:01:57 lr 0.000639	time 0.4714 (0.4686)	loss 3.4839 (3.4544)	grad_norm 1.4044 (1.4337)	mem 14852MB
[2022-11-06 16:22:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][1050/1251]	eta 0:01:34 lr 0.000639	time 0.4631 (0.4685)	loss 3.6715 (3.4542)	grad_norm 1.3490 (1.4325)	mem 14852MB
[2022-11-06 16:22:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][1100/1251]	eta 0:01:10 lr 0.000639	time 0.4685 (0.4685)	loss 3.7171 (3.4566)	grad_norm 1.3141 (1.4316)	mem 14852MB
[2022-11-06 16:23:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][1150/1251]	eta 0:00:47 lr 0.000639	time 0.4645 (0.4684)	loss 3.8157 (3.4556)	grad_norm 1.4115 (1.4306)	mem 14852MB
[2022-11-06 16:23:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][1200/1251]	eta 0:00:23 lr 0.000638	time 0.4749 (0.4682)	loss 3.6686 (3.4565)	grad_norm 1.3791 (1.4301)	mem 14852MB
[2022-11-06 16:23:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [123/300][1250/1251]	eta 0:00:00 lr 0.000638	time 0.4576 (0.4681)	loss 3.6317 (3.4520)	grad_norm 1.4348 (1.4294)	mem 14852MB
[2022-11-06 16:23:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 123 training takes 0:09:45
[2022-11-06 16:23:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_123.pth saving......
[2022-11-06 16:23:50 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_123.pth saved !!!
[2022-11-06 16:23:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.524 (1.524)	Loss 0.9741 (0.9741)	Acc@1 76.074 (76.074)	Acc@5 94.141 (94.141)	Mem 14852MB
[2022-11-06 16:23:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.142 Acc@5 93.726
[2022-11-06 16:23:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.1%
[2022-11-06 16:24:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.641 (1.641)	Loss 0.8316 (0.8316)	Acc@1 81.152 (81.152)	Acc@5 94.629 (94.629)	Mem 14852MB
[2022-11-06 16:24:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.352 Acc@5 94.942
[2022-11-06 16:24:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.4%
[2022-11-06 16:24:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.35% at 123 epoch
[2022-11-06 16:24:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][0/1251]	eta 0:39:43 lr 0.000638	time 1.9055 (1.9055)	loss 3.4680 (3.4680)	grad_norm 1.3212 (1.3212)	mem 14852MB
[2022-11-06 16:24:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][50/1251]	eta 0:09:57 lr 0.000638	time 0.4731 (0.4974)	loss 3.6755 (3.4258)	grad_norm 1.2828 (1.4750)	mem 14852MB
[2022-11-06 16:24:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][100/1251]	eta 0:09:15 lr 0.000638	time 0.4622 (0.4827)	loss 4.1287 (3.3925)	grad_norm 1.3885 (1.4645)	mem 14852MB
[2022-11-06 16:25:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][150/1251]	eta 0:08:46 lr 0.000638	time 0.4740 (0.4783)	loss 3.6547 (3.4031)	grad_norm 1.5065 (1.4520)	mem 14852MB
[2022-11-06 16:25:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][200/1251]	eta 0:08:19 lr 0.000637	time 0.4747 (0.4753)	loss 4.0105 (3.4103)	grad_norm 1.4763 (1.4557)	mem 14852MB
[2022-11-06 16:26:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][250/1251]	eta 0:07:53 lr 0.000637	time 0.4699 (0.4734)	loss 3.6808 (3.4389)	grad_norm 1.3951 (1.4547)	mem 14852MB
[2022-11-06 16:26:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][300/1251]	eta 0:07:29 lr 0.000637	time 0.4557 (0.4723)	loss 3.1386 (3.4361)	grad_norm 1.5735 (1.4536)	mem 14852MB
[2022-11-06 16:26:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][350/1251]	eta 0:07:04 lr 0.000637	time 0.4668 (0.4712)	loss 2.6415 (3.4481)	grad_norm 1.4168 (1.4537)	mem 14852MB
[2022-11-06 16:27:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][400/1251]	eta 0:06:40 lr 0.000637	time 0.4701 (0.4706)	loss 3.4352 (3.4485)	grad_norm 1.5009 (1.4509)	mem 14852MB
[2022-11-06 16:27:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][450/1251]	eta 0:06:16 lr 0.000636	time 0.4705 (0.4703)	loss 2.4434 (3.4472)	grad_norm 1.6792 (inf)	mem 14852MB
[2022-11-06 16:28:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][500/1251]	eta 0:05:52 lr 0.000636	time 0.4743 (0.4697)	loss 3.3305 (3.4531)	grad_norm 1.2444 (inf)	mem 14852MB
[2022-11-06 16:28:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][550/1251]	eta 0:05:29 lr 0.000636	time 0.4643 (0.4698)	loss 3.3816 (3.4497)	grad_norm 1.5180 (inf)	mem 14852MB
[2022-11-06 16:28:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][600/1251]	eta 0:05:05 lr 0.000636	time 0.4601 (0.4695)	loss 3.8059 (3.4520)	grad_norm 1.5515 (inf)	mem 14852MB
[2022-11-06 16:29:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][650/1251]	eta 0:04:42 lr 0.000636	time 0.4674 (0.4693)	loss 2.9372 (3.4534)	grad_norm 1.4137 (inf)	mem 14852MB
[2022-11-06 16:29:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][700/1251]	eta 0:04:18 lr 0.000635	time 0.4668 (0.4692)	loss 3.8469 (3.4510)	grad_norm 1.4419 (inf)	mem 14852MB
[2022-11-06 16:30:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][750/1251]	eta 0:03:55 lr 0.000635	time 0.4598 (0.4692)	loss 2.9607 (3.4567)	grad_norm 1.3615 (inf)	mem 14852MB
[2022-11-06 16:30:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][800/1251]	eta 0:03:31 lr 0.000635	time 0.4657 (0.4692)	loss 4.6675 (3.4531)	grad_norm 1.4630 (inf)	mem 14852MB
[2022-11-06 16:30:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][850/1251]	eta 0:03:08 lr 0.000635	time 0.4706 (0.4690)	loss 2.1714 (3.4508)	grad_norm 1.4782 (inf)	mem 14852MB
[2022-11-06 16:31:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][900/1251]	eta 0:02:44 lr 0.000635	time 0.5424 (0.4690)	loss 3.7750 (3.4510)	grad_norm 1.3439 (inf)	mem 14852MB
[2022-11-06 16:31:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][950/1251]	eta 0:02:21 lr 0.000634	time 0.4563 (0.4688)	loss 3.6805 (3.4527)	grad_norm 1.3923 (inf)	mem 14852MB
[2022-11-06 16:31:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][1000/1251]	eta 0:01:57 lr 0.000634	time 0.4663 (0.4687)	loss 3.3363 (3.4533)	grad_norm 1.4850 (inf)	mem 14852MB
[2022-11-06 16:32:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][1050/1251]	eta 0:01:34 lr 0.000634	time 0.4764 (0.4690)	loss 3.8689 (3.4505)	grad_norm 1.3730 (inf)	mem 14852MB
[2022-11-06 16:32:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][1100/1251]	eta 0:01:10 lr 0.000634	time 0.4666 (0.4688)	loss 3.5444 (3.4568)	grad_norm 1.4462 (inf)	mem 14852MB
[2022-11-06 16:33:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][1150/1251]	eta 0:00:47 lr 0.000634	time 0.4693 (0.4688)	loss 3.1431 (3.4574)	grad_norm 1.2407 (inf)	mem 14852MB
[2022-11-06 16:33:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][1200/1251]	eta 0:00:23 lr 0.000633	time 0.4662 (0.4686)	loss 2.8198 (3.4610)	grad_norm 1.4815 (inf)	mem 14852MB
[2022-11-06 16:33:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [124/300][1250/1251]	eta 0:00:00 lr 0.000633	time 0.4579 (0.4684)	loss 2.8677 (3.4595)	grad_norm 1.3734 (inf)	mem 14852MB
[2022-11-06 16:33:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 124 training takes 0:09:46
[2022-11-06 16:33:54 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_124.pth saving......
[2022-11-06 16:33:54 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_124.pth saved !!!
[2022-11-06 16:33:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.559 (1.559)	Loss 0.9858 (0.9858)	Acc@1 77.637 (77.637)	Acc@5 92.969 (92.969)	Mem 14852MB
[2022-11-06 16:34:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.990 Acc@5 93.762
[2022-11-06 16:34:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.0%
[2022-11-06 16:34:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.668 (1.668)	Loss 0.8317 (0.8317)	Acc@1 80.371 (80.371)	Acc@5 95.215 (95.215)	Mem 14852MB
[2022-11-06 16:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.360 Acc@5 94.940
[2022-11-06 16:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.4%
[2022-11-06 16:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.36% at 124 epoch
[2022-11-06 16:34:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][0/1251]	eta 0:39:49 lr 0.000633	time 1.9099 (1.9099)	loss 3.2767 (3.2767)	grad_norm 1.5520 (1.5520)	mem 14852MB
[2022-11-06 16:34:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][50/1251]	eta 0:10:02 lr 0.000633	time 0.4561 (0.5020)	loss 3.6801 (3.3303)	grad_norm 1.5573 (1.4666)	mem 14852MB
[2022-11-06 16:35:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][100/1251]	eta 0:09:18 lr 0.000633	time 0.4671 (0.4856)	loss 3.4871 (3.3962)	grad_norm 1.3833 (1.4506)	mem 14852MB
[2022-11-06 16:35:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][150/1251]	eta 0:08:47 lr 0.000633	time 0.4678 (0.4787)	loss 3.4532 (3.4542)	grad_norm 1.6303 (1.4683)	mem 14852MB
[2022-11-06 16:35:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][200/1251]	eta 0:08:20 lr 0.000632	time 0.4633 (0.4759)	loss 3.8299 (3.4277)	grad_norm 1.3816 (1.4573)	mem 14852MB
[2022-11-06 16:36:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][250/1251]	eta 0:07:54 lr 0.000632	time 0.4673 (0.4737)	loss 3.2448 (3.4372)	grad_norm 1.3537 (1.4538)	mem 14852MB
[2022-11-06 16:36:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][300/1251]	eta 0:07:29 lr 0.000632	time 0.4640 (0.4726)	loss 3.3038 (3.4627)	grad_norm 1.3905 (1.4563)	mem 14852MB
[2022-11-06 16:36:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][350/1251]	eta 0:07:05 lr 0.000632	time 0.4691 (0.4717)	loss 3.0912 (3.4591)	grad_norm 1.2260 (1.4527)	mem 14852MB
[2022-11-06 16:37:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][400/1251]	eta 0:06:40 lr 0.000632	time 0.4666 (0.4708)	loss 3.9487 (3.4471)	grad_norm 1.4750 (1.4499)	mem 14852MB
[2022-11-06 16:37:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][450/1251]	eta 0:06:16 lr 0.000631	time 0.4655 (0.4701)	loss 2.9832 (3.4428)	grad_norm 1.1834 (1.4491)	mem 14852MB
[2022-11-06 16:38:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][500/1251]	eta 0:05:52 lr 0.000631	time 0.4679 (0.4697)	loss 4.1284 (3.4265)	grad_norm 1.4682 (1.4481)	mem 14852MB
[2022-11-06 16:38:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][550/1251]	eta 0:05:29 lr 0.000631	time 0.4634 (0.4695)	loss 3.1404 (3.4303)	grad_norm 1.4181 (1.4509)	mem 14852MB
[2022-11-06 16:38:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][600/1251]	eta 0:05:05 lr 0.000631	time 0.4582 (0.4695)	loss 2.8447 (3.4313)	grad_norm 1.5455 (1.4520)	mem 14852MB
[2022-11-06 16:39:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][650/1251]	eta 0:04:41 lr 0.000631	time 0.4617 (0.4692)	loss 2.3148 (3.4230)	grad_norm 1.4622 (1.4504)	mem 14852MB
[2022-11-06 16:39:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][700/1251]	eta 0:04:18 lr 0.000630	time 0.4830 (0.4690)	loss 3.6181 (3.4209)	grad_norm 1.4443 (1.4497)	mem 14852MB
[2022-11-06 16:40:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][750/1251]	eta 0:03:54 lr 0.000630	time 0.4543 (0.4688)	loss 2.6227 (3.4220)	grad_norm 1.3381 (1.4490)	mem 14852MB
[2022-11-06 16:40:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][800/1251]	eta 0:03:31 lr 0.000630	time 0.4631 (0.4688)	loss 3.2793 (3.4220)	grad_norm 1.2772 (1.4456)	mem 14852MB
[2022-11-06 16:40:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][850/1251]	eta 0:03:07 lr 0.000630	time 0.4698 (0.4688)	loss 3.8858 (3.4193)	grad_norm 1.7584 (1.4456)	mem 14852MB
[2022-11-06 16:41:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][900/1251]	eta 0:02:44 lr 0.000630	time 0.5528 (0.4688)	loss 3.7515 (3.4261)	grad_norm 1.4681 (1.4468)	mem 14852MB
[2022-11-06 16:41:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][950/1251]	eta 0:02:21 lr 0.000629	time 0.4701 (0.4686)	loss 3.2020 (3.4242)	grad_norm 1.3906 (1.4471)	mem 14852MB
[2022-11-06 16:42:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][1000/1251]	eta 0:01:57 lr 0.000629	time 0.4616 (0.4685)	loss 3.5759 (3.4271)	grad_norm 1.5379 (1.4462)	mem 14852MB
[2022-11-06 16:42:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][1050/1251]	eta 0:01:34 lr 0.000629	time 0.4744 (0.4685)	loss 3.7307 (3.4265)	grad_norm 1.3396 (1.4474)	mem 14852MB
[2022-11-06 16:42:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][1100/1251]	eta 0:01:10 lr 0.000629	time 0.4716 (0.4686)	loss 3.7001 (3.4249)	grad_norm 1.4878 (1.4468)	mem 14852MB
[2022-11-06 16:43:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][1150/1251]	eta 0:00:47 lr 0.000629	time 0.4735 (0.4685)	loss 3.6451 (3.4286)	grad_norm 1.3302 (1.4469)	mem 14852MB
[2022-11-06 16:43:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][1200/1251]	eta 0:00:23 lr 0.000628	time 0.4658 (0.4685)	loss 3.4174 (3.4331)	grad_norm 1.4192 (nan)	mem 14852MB
[2022-11-06 16:43:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [125/300][1250/1251]	eta 0:00:00 lr 0.000628	time 0.4571 (0.4683)	loss 3.4157 (3.4363)	grad_norm 1.3505 (nan)	mem 14852MB
[2022-11-06 16:43:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 125 training takes 0:09:45
[2022-11-06 16:43:58 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_125.pth saving......
[2022-11-06 16:43:58 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_125.pth saved !!!
[2022-11-06 16:44:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.477 (1.477)	Loss 0.9337 (0.9337)	Acc@1 77.051 (77.051)	Acc@5 93.750 (93.750)	Mem 14852MB
[2022-11-06 16:44:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.980 Acc@5 93.878
[2022-11-06 16:44:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.0%
[2022-11-06 16:44:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.540 (1.540)	Loss 0.8769 (0.8769)	Acc@1 78.516 (78.516)	Acc@5 94.336 (94.336)	Mem 14852MB
[2022-11-06 16:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.372 Acc@5 94.924
[2022-11-06 16:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.4%
[2022-11-06 16:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.37% at 125 epoch
[2022-11-06 16:44:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][0/1251]	eta 0:41:29 lr 0.000628	time 1.9896 (1.9896)	loss 3.5318 (3.5318)	grad_norm 1.3201 (1.3201)	mem 14852MB
[2022-11-06 16:44:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][50/1251]	eta 0:10:02 lr 0.000628	time 0.4625 (0.5014)	loss 3.2993 (3.4557)	grad_norm 1.4686 (1.4522)	mem 14852MB
[2022-11-06 16:45:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][100/1251]	eta 0:09:20 lr 0.000628	time 0.4520 (0.4869)	loss 3.7534 (3.4374)	grad_norm 1.4799 (1.4534)	mem 14852MB
[2022-11-06 16:45:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][150/1251]	eta 0:08:48 lr 0.000627	time 0.4625 (0.4800)	loss 2.7372 (3.4296)	grad_norm 1.4352 (1.4545)	mem 14852MB
[2022-11-06 16:45:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][200/1251]	eta 0:08:20 lr 0.000627	time 0.4570 (0.4764)	loss 4.1167 (3.4496)	grad_norm 1.4361 (1.4506)	mem 14852MB
[2022-11-06 16:46:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][250/1251]	eta 0:07:55 lr 0.000627	time 0.4574 (0.4748)	loss 3.7948 (3.4466)	grad_norm 1.5214 (1.4501)	mem 14852MB
[2022-11-06 16:46:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][300/1251]	eta 0:07:30 lr 0.000627	time 0.4736 (0.4733)	loss 2.4864 (3.4685)	grad_norm 1.5995 (1.4603)	mem 14852MB
[2022-11-06 16:47:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][350/1251]	eta 0:07:05 lr 0.000627	time 0.4643 (0.4722)	loss 3.7792 (3.4646)	grad_norm 1.6910 (1.4574)	mem 14852MB
[2022-11-06 16:47:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][400/1251]	eta 0:06:41 lr 0.000626	time 0.4650 (0.4717)	loss 3.2205 (3.4635)	grad_norm 1.3080 (1.4597)	mem 14852MB
[2022-11-06 16:47:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][450/1251]	eta 0:06:17 lr 0.000626	time 0.4696 (0.4711)	loss 3.7696 (3.4578)	grad_norm 1.5391 (1.4552)	mem 14852MB
[2022-11-06 16:48:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][500/1251]	eta 0:05:53 lr 0.000626	time 0.4621 (0.4708)	loss 3.1504 (3.4524)	grad_norm 1.3584 (1.4559)	mem 14852MB
[2022-11-06 16:48:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][550/1251]	eta 0:05:29 lr 0.000626	time 0.4633 (0.4706)	loss 3.8470 (3.4419)	grad_norm 1.4694 (1.4537)	mem 14852MB
[2022-11-06 16:48:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][600/1251]	eta 0:05:06 lr 0.000626	time 0.4701 (0.4704)	loss 2.9715 (3.4438)	grad_norm 1.3992 (1.4565)	mem 14852MB
[2022-11-06 16:49:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][650/1251]	eta 0:04:42 lr 0.000625	time 0.4667 (0.4701)	loss 3.4047 (3.4470)	grad_norm 1.5603 (1.4564)	mem 14852MB
[2022-11-06 16:49:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][700/1251]	eta 0:04:18 lr 0.000625	time 0.4675 (0.4699)	loss 3.2178 (3.4494)	grad_norm 1.4183 (1.4531)	mem 14852MB
[2022-11-06 16:50:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][750/1251]	eta 0:03:55 lr 0.000625	time 0.5300 (0.4699)	loss 3.0248 (3.4412)	grad_norm 1.3281 (1.4489)	mem 14852MB
[2022-11-06 16:50:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][800/1251]	eta 0:03:31 lr 0.000625	time 0.4717 (0.4698)	loss 3.9882 (3.4387)	grad_norm 1.3441 (1.4476)	mem 14852MB
[2022-11-06 16:50:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][850/1251]	eta 0:03:08 lr 0.000625	time 0.4759 (0.4697)	loss 3.2785 (3.4464)	grad_norm 1.4400 (1.4460)	mem 14852MB
[2022-11-06 16:51:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][900/1251]	eta 0:02:44 lr 0.000624	time 0.4775 (0.4696)	loss 3.7797 (3.4431)	grad_norm 1.4143 (1.4459)	mem 14852MB
[2022-11-06 16:51:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][950/1251]	eta 0:02:21 lr 0.000624	time 0.4622 (0.4695)	loss 2.5110 (3.4391)	grad_norm 1.4945 (1.4450)	mem 14852MB
[2022-11-06 16:52:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][1000/1251]	eta 0:01:57 lr 0.000624	time 0.5324 (0.4696)	loss 3.4285 (3.4386)	grad_norm 1.4333 (1.4459)	mem 14852MB
[2022-11-06 16:52:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][1050/1251]	eta 0:01:34 lr 0.000624	time 0.4682 (0.4696)	loss 3.1918 (3.4369)	grad_norm 1.3034 (1.4464)	mem 14852MB
[2022-11-06 16:52:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][1100/1251]	eta 0:01:10 lr 0.000624	time 0.4626 (0.4696)	loss 2.9403 (3.4341)	grad_norm 1.5067 (1.4443)	mem 14852MB
[2022-11-06 16:53:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][1150/1251]	eta 0:00:47 lr 0.000623	time 0.4630 (0.4695)	loss 3.2631 (3.4349)	grad_norm 1.6117 (1.4441)	mem 14852MB
[2022-11-06 16:53:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][1200/1251]	eta 0:00:23 lr 0.000623	time 0.4566 (0.4694)	loss 3.8022 (3.4310)	grad_norm 1.3410 (1.4446)	mem 14852MB
[2022-11-06 16:54:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [126/300][1250/1251]	eta 0:00:00 lr 0.000623	time 0.4575 (0.4692)	loss 3.8138 (3.4384)	grad_norm 1.4080 (1.4445)	mem 14852MB
[2022-11-06 16:54:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 126 training takes 0:09:47
[2022-11-06 16:54:03 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_126.pth saving......
[2022-11-06 16:54:03 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_126.pth saved !!!
[2022-11-06 16:54:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.719 (1.719)	Loss 1.0076 (1.0076)	Acc@1 75.488 (75.488)	Acc@5 93.750 (93.750)	Mem 14852MB
[2022-11-06 16:54:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.034 Acc@5 93.818
[2022-11-06 16:54:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.0%
[2022-11-06 16:54:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.638 (1.638)	Loss 0.9040 (0.9040)	Acc@1 77.246 (77.246)	Acc@5 95.215 (95.215)	Mem 14852MB
[2022-11-06 16:54:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.414 Acc@5 94.952
[2022-11-06 16:54:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.4%
[2022-11-06 16:54:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.41% at 126 epoch
[2022-11-06 16:54:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][0/1251]	eta 0:40:21 lr 0.000623	time 1.9358 (1.9358)	loss 4.1733 (4.1733)	grad_norm 1.7424 (1.7424)	mem 14852MB
[2022-11-06 16:54:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][50/1251]	eta 0:10:03 lr 0.000623	time 0.4551 (0.5022)	loss 2.1108 (3.3794)	grad_norm 1.3449 (1.4579)	mem 14852MB
[2022-11-06 16:55:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][100/1251]	eta 0:09:18 lr 0.000623	time 0.4652 (0.4851)	loss 3.4874 (3.3622)	grad_norm 1.4369 (1.4733)	mem 14852MB
[2022-11-06 16:55:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][150/1251]	eta 0:08:46 lr 0.000622	time 0.4706 (0.4785)	loss 3.1001 (3.3857)	grad_norm 1.3212 (1.4740)	mem 14852MB
[2022-11-06 16:55:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][200/1251]	eta 0:08:19 lr 0.000622	time 0.4601 (0.4756)	loss 3.5249 (3.4268)	grad_norm 1.5302 (1.4649)	mem 14852MB
[2022-11-06 16:56:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][250/1251]	eta 0:07:54 lr 0.000622	time 0.4544 (0.4740)	loss 4.1120 (3.4395)	grad_norm 1.5420 (1.4736)	mem 14852MB
[2022-11-06 16:56:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][300/1251]	eta 0:07:29 lr 0.000622	time 0.4667 (0.4724)	loss 3.8240 (3.4246)	grad_norm 1.3523 (1.4646)	mem 14852MB
[2022-11-06 16:57:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][350/1251]	eta 0:07:04 lr 0.000622	time 0.4615 (0.4713)	loss 3.6131 (3.4250)	grad_norm 1.4896 (1.4573)	mem 14852MB
[2022-11-06 16:57:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][400/1251]	eta 0:06:40 lr 0.000621	time 0.4710 (0.4707)	loss 3.4750 (3.4320)	grad_norm 1.4174 (1.4576)	mem 14852MB
[2022-11-06 16:57:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][450/1251]	eta 0:06:16 lr 0.000621	time 0.4613 (0.4703)	loss 3.0718 (3.4257)	grad_norm 1.3761 (1.4562)	mem 14852MB
[2022-11-06 16:58:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][500/1251]	eta 0:05:53 lr 0.000621	time 0.4737 (0.4701)	loss 3.3395 (3.4213)	grad_norm 1.4020 (1.4545)	mem 14852MB
[2022-11-06 16:58:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][550/1251]	eta 0:05:29 lr 0.000621	time 0.4570 (0.4700)	loss 4.0984 (3.4141)	grad_norm 1.6957 (1.4528)	mem 14852MB
[2022-11-06 16:59:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][600/1251]	eta 0:05:05 lr 0.000621	time 0.4609 (0.4696)	loss 3.8172 (3.4171)	grad_norm 1.5154 (1.4541)	mem 14852MB
[2022-11-06 16:59:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][650/1251]	eta 0:04:42 lr 0.000620	time 0.4630 (0.4694)	loss 3.7211 (3.4217)	grad_norm 1.4770 (1.4535)	mem 14852MB
[2022-11-06 16:59:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][700/1251]	eta 0:04:18 lr 0.000620	time 0.4544 (0.4691)	loss 3.8055 (3.4253)	grad_norm 1.3321 (1.4527)	mem 14852MB
[2022-11-06 17:00:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][750/1251]	eta 0:03:54 lr 0.000620	time 0.4663 (0.4690)	loss 3.4007 (3.4295)	grad_norm 1.2837 (nan)	mem 14852MB
[2022-11-06 17:00:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][800/1251]	eta 0:03:31 lr 0.000620	time 0.4626 (0.4689)	loss 2.8459 (3.4214)	grad_norm 1.3338 (nan)	mem 14852MB
[2022-11-06 17:01:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][850/1251]	eta 0:03:08 lr 0.000620	time 0.5441 (0.4689)	loss 3.0705 (3.4229)	grad_norm 1.3846 (nan)	mem 14852MB
[2022-11-06 17:01:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][900/1251]	eta 0:02:44 lr 0.000619	time 0.5253 (0.4687)	loss 3.7498 (3.4146)	grad_norm 1.4665 (nan)	mem 14852MB
[2022-11-06 17:01:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][950/1251]	eta 0:02:21 lr 0.000619	time 0.4788 (0.4686)	loss 4.0927 (3.4187)	grad_norm 1.5430 (nan)	mem 14852MB
[2022-11-06 17:02:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][1000/1251]	eta 0:01:57 lr 0.000619	time 0.4681 (0.4685)	loss 3.3481 (3.4208)	grad_norm 1.1704 (nan)	mem 14852MB
[2022-11-06 17:02:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][1050/1251]	eta 0:01:34 lr 0.000619	time 0.4695 (0.4685)	loss 2.6647 (3.4181)	grad_norm 1.4695 (nan)	mem 14852MB
[2022-11-06 17:02:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][1100/1251]	eta 0:01:10 lr 0.000619	time 0.4645 (0.4684)	loss 3.8939 (3.4168)	grad_norm 1.5622 (nan)	mem 14852MB
[2022-11-06 17:03:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][1150/1251]	eta 0:00:47 lr 0.000618	time 0.4606 (0.4684)	loss 3.9228 (3.4144)	grad_norm 1.2328 (nan)	mem 14852MB
[2022-11-06 17:03:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][1200/1251]	eta 0:00:23 lr 0.000618	time 0.4782 (0.4682)	loss 2.6983 (3.4149)	grad_norm 1.8228 (nan)	mem 14852MB
[2022-11-06 17:04:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [127/300][1250/1251]	eta 0:00:00 lr 0.000618	time 0.4561 (0.4681)	loss 4.0898 (3.4144)	grad_norm 1.4751 (nan)	mem 14852MB
[2022-11-06 17:04:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 127 training takes 0:09:45
[2022-11-06 17:04:07 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_127.pth saving......
[2022-11-06 17:04:07 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_127.pth saved !!!
[2022-11-06 17:04:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.489 (1.489)	Loss 0.9669 (0.9669)	Acc@1 79.102 (79.102)	Acc@5 94.434 (94.434)	Mem 14852MB
[2022-11-06 17:04:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.210 Acc@5 93.926
[2022-11-06 17:04:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.2%
[2022-11-06 17:04:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.743 (1.743)	Loss 0.8514 (0.8514)	Acc@1 79.102 (79.102)	Acc@5 94.727 (94.727)	Mem 14852MB
[2022-11-06 17:04:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.420 Acc@5 94.974
[2022-11-06 17:04:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.4%
[2022-11-06 17:04:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.42% at 127 epoch
[2022-11-06 17:04:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][0/1251]	eta 0:42:05 lr 0.000618	time 2.0191 (2.0191)	loss 3.3818 (3.3818)	grad_norm 1.5513 (1.5513)	mem 14852MB
[2022-11-06 17:04:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][50/1251]	eta 0:10:05 lr 0.000618	time 0.4507 (0.5044)	loss 4.0863 (3.3770)	grad_norm 1.4249 (1.4666)	mem 14852MB
[2022-11-06 17:05:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][100/1251]	eta 0:09:20 lr 0.000618	time 0.4654 (0.4868)	loss 4.1545 (3.4641)	grad_norm 1.3627 (1.4699)	mem 14852MB
[2022-11-06 17:05:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][150/1251]	eta 0:08:48 lr 0.000617	time 0.4540 (0.4799)	loss 3.6643 (3.4516)	grad_norm 1.7236 (1.4636)	mem 14852MB
[2022-11-06 17:06:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][200/1251]	eta 0:08:20 lr 0.000617	time 0.4622 (0.4762)	loss 4.0695 (3.4663)	grad_norm 1.6762 (1.4711)	mem 14852MB
[2022-11-06 17:06:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][250/1251]	eta 0:07:54 lr 0.000617	time 0.4638 (0.4744)	loss 3.9357 (3.4459)	grad_norm 1.5988 (1.4627)	mem 14852MB
[2022-11-06 17:06:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][300/1251]	eta 0:07:30 lr 0.000617	time 0.4573 (0.4735)	loss 3.2748 (3.4243)	grad_norm 1.5237 (1.4663)	mem 14852MB
[2022-11-06 17:07:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][350/1251]	eta 0:07:06 lr 0.000617	time 0.4702 (0.4731)	loss 3.4108 (3.4190)	grad_norm 1.4711 (1.4610)	mem 14852MB
[2022-11-06 17:07:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][400/1251]	eta 0:06:42 lr 0.000616	time 0.4644 (0.4725)	loss 3.7713 (3.4338)	grad_norm 1.3988 (1.4553)	mem 14852MB
[2022-11-06 17:07:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][450/1251]	eta 0:06:17 lr 0.000616	time 0.4639 (0.4718)	loss 2.5043 (3.4237)	grad_norm 1.4758 (1.4540)	mem 14852MB
[2022-11-06 17:08:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][500/1251]	eta 0:05:53 lr 0.000616	time 0.4587 (0.4713)	loss 3.7172 (3.4308)	grad_norm 1.4590 (1.4514)	mem 14852MB
[2022-11-06 17:08:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][550/1251]	eta 0:05:30 lr 0.000616	time 0.4596 (0.4710)	loss 3.4503 (3.4336)	grad_norm 1.3334 (1.4512)	mem 14852MB
[2022-11-06 17:09:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][600/1251]	eta 0:05:06 lr 0.000616	time 0.4648 (0.4705)	loss 2.8397 (3.4251)	grad_norm 1.3041 (1.4508)	mem 14852MB
[2022-11-06 17:09:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][650/1251]	eta 0:04:42 lr 0.000615	time 0.4663 (0.4703)	loss 3.8022 (3.4199)	grad_norm 1.5643 (1.4531)	mem 14852MB
[2022-11-06 17:09:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][700/1251]	eta 0:04:19 lr 0.000615	time 0.5341 (0.4701)	loss 3.6215 (3.4218)	grad_norm 1.4699 (1.4583)	mem 14852MB
[2022-11-06 17:10:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][750/1251]	eta 0:03:55 lr 0.000615	time 0.4652 (0.4699)	loss 3.1641 (3.4214)	grad_norm 1.2939 (1.4588)	mem 14852MB
[2022-11-06 17:10:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][800/1251]	eta 0:03:31 lr 0.000615	time 0.4620 (0.4699)	loss 3.1420 (3.4167)	grad_norm 1.4141 (1.4566)	mem 14852MB
[2022-11-06 17:11:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][850/1251]	eta 0:03:08 lr 0.000615	time 0.4652 (0.4698)	loss 3.5611 (3.4201)	grad_norm 1.5217 (1.4574)	mem 14852MB
[2022-11-06 17:11:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][900/1251]	eta 0:02:44 lr 0.000614	time 0.4605 (0.4695)	loss 3.7324 (3.4172)	grad_norm 1.3791 (1.4569)	mem 14852MB
[2022-11-06 17:11:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][950/1251]	eta 0:02:21 lr 0.000614	time 0.4499 (0.4695)	loss 3.4959 (3.4165)	grad_norm 1.3354 (1.4566)	mem 14852MB
[2022-11-06 17:12:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][1000/1251]	eta 0:01:57 lr 0.000614	time 0.4678 (0.4694)	loss 3.7156 (3.4199)	grad_norm 1.6699 (1.4567)	mem 14852MB
[2022-11-06 17:12:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][1050/1251]	eta 0:01:34 lr 0.000614	time 0.4566 (0.4694)	loss 2.9497 (3.4216)	grad_norm 1.4547 (1.4557)	mem 14852MB
[2022-11-06 17:13:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][1100/1251]	eta 0:01:10 lr 0.000614	time 0.4702 (0.4693)	loss 3.8953 (3.4214)	grad_norm 1.3274 (1.4546)	mem 14852MB
[2022-11-06 17:13:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][1150/1251]	eta 0:00:47 lr 0.000613	time 0.4654 (0.4691)	loss 3.4450 (3.4214)	grad_norm 1.6262 (1.4569)	mem 14852MB
[2022-11-06 17:13:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][1200/1251]	eta 0:00:23 lr 0.000613	time 0.5316 (0.4690)	loss 3.6206 (3.4211)	grad_norm 1.4932 (1.4566)	mem 14852MB
[2022-11-06 17:14:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [128/300][1250/1251]	eta 0:00:00 lr 0.000613	time 0.4509 (0.4688)	loss 2.8902 (3.4206)	grad_norm 1.3537 (1.4558)	mem 14852MB
[2022-11-06 17:14:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 128 training takes 0:09:46
[2022-11-06 17:14:12 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_128.pth saving......
[2022-11-06 17:14:12 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_128.pth saved !!!
[2022-11-06 17:14:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.432 (1.432)	Loss 0.9187 (0.9187)	Acc@1 78.320 (78.320)	Acc@5 94.629 (94.629)	Mem 14852MB
[2022-11-06 17:14:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.324 Acc@5 93.980
[2022-11-06 17:14:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.3%
[2022-11-06 17:14:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.605 (1.605)	Loss 0.8317 (0.8317)	Acc@1 80.566 (80.566)	Acc@5 95.996 (95.996)	Mem 14852MB
[2022-11-06 17:14:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.476 Acc@5 94.980
[2022-11-06 17:14:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.5%
[2022-11-06 17:14:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.48% at 128 epoch
[2022-11-06 17:14:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][0/1251]	eta 0:40:32 lr 0.000613	time 1.9442 (1.9442)	loss 4.3846 (4.3846)	grad_norm 1.5057 (1.5057)	mem 14852MB
[2022-11-06 17:14:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][50/1251]	eta 0:10:00 lr 0.000613	time 0.4675 (0.4998)	loss 3.2700 (3.4423)	grad_norm 1.5413 (1.4559)	mem 14852MB
[2022-11-06 17:15:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][100/1251]	eta 0:09:15 lr 0.000613	time 0.4668 (0.4829)	loss 3.6052 (3.3958)	grad_norm 1.5749 (1.4584)	mem 14852MB
[2022-11-06 17:15:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][150/1251]	eta 0:08:45 lr 0.000612	time 0.4621 (0.4777)	loss 3.7265 (3.4054)	grad_norm 1.4978 (1.4615)	mem 14852MB
[2022-11-06 17:16:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][200/1251]	eta 0:08:20 lr 0.000612	time 0.4668 (0.4759)	loss 4.0389 (3.4176)	grad_norm 1.4708 (1.4577)	mem 14852MB
[2022-11-06 17:16:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][250/1251]	eta 0:07:54 lr 0.000612	time 0.4607 (0.4743)	loss 3.6453 (3.4331)	grad_norm 1.4045 (1.4593)	mem 14852MB
[2022-11-06 17:16:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][300/1251]	eta 0:07:29 lr 0.000612	time 0.4713 (0.4728)	loss 3.5438 (3.4311)	grad_norm 1.3696 (1.4619)	mem 14852MB
[2022-11-06 17:17:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][350/1251]	eta 0:07:04 lr 0.000612	time 0.4653 (0.4715)	loss 3.4509 (3.4411)	grad_norm 1.2566 (nan)	mem 14852MB
[2022-11-06 17:17:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][400/1251]	eta 0:06:40 lr 0.000611	time 0.4769 (0.4708)	loss 3.1072 (3.4517)	grad_norm 1.5904 (nan)	mem 14852MB
[2022-11-06 17:18:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][450/1251]	eta 0:06:16 lr 0.000611	time 0.4610 (0.4703)	loss 2.4413 (3.4422)	grad_norm 1.5967 (nan)	mem 14852MB
[2022-11-06 17:18:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][500/1251]	eta 0:05:53 lr 0.000611	time 0.4637 (0.4703)	loss 3.7911 (3.4460)	grad_norm 1.7180 (nan)	mem 14852MB
[2022-11-06 17:18:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][550/1251]	eta 0:05:29 lr 0.000611	time 0.4571 (0.4699)	loss 3.6042 (3.4357)	grad_norm 1.5150 (nan)	mem 14852MB
[2022-11-06 17:19:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][600/1251]	eta 0:05:05 lr 0.000611	time 0.4607 (0.4695)	loss 3.1353 (3.4440)	grad_norm 1.4793 (nan)	mem 14852MB
[2022-11-06 17:19:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][650/1251]	eta 0:04:42 lr 0.000610	time 0.4658 (0.4692)	loss 3.7272 (3.4430)	grad_norm 1.4639 (nan)	mem 14852MB
[2022-11-06 17:19:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][700/1251]	eta 0:04:18 lr 0.000610	time 0.4612 (0.4691)	loss 3.6433 (3.4372)	grad_norm 1.4890 (nan)	mem 14852MB
[2022-11-06 17:20:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][750/1251]	eta 0:03:55 lr 0.000610	time 0.4632 (0.4692)	loss 3.4991 (3.4367)	grad_norm 1.5677 (nan)	mem 14852MB
[2022-11-06 17:20:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][800/1251]	eta 0:03:31 lr 0.000610	time 0.4575 (0.4691)	loss 4.2364 (3.4424)	grad_norm 1.3995 (nan)	mem 14852MB
[2022-11-06 17:21:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][850/1251]	eta 0:03:08 lr 0.000610	time 0.4601 (0.4688)	loss 3.8235 (3.4433)	grad_norm 1.4936 (nan)	mem 14852MB
[2022-11-06 17:21:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][900/1251]	eta 0:02:44 lr 0.000609	time 0.4615 (0.4686)	loss 3.3160 (3.4475)	grad_norm 1.3881 (nan)	mem 14852MB
[2022-11-06 17:21:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][950/1251]	eta 0:02:21 lr 0.000609	time 0.4674 (0.4686)	loss 2.2773 (3.4502)	grad_norm 1.4261 (nan)	mem 14852MB
[2022-11-06 17:22:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][1000/1251]	eta 0:01:57 lr 0.000609	time 0.4623 (0.4687)	loss 3.5548 (3.4514)	grad_norm 1.3966 (nan)	mem 14852MB
[2022-11-06 17:22:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][1050/1251]	eta 0:01:34 lr 0.000609	time 0.4527 (0.4688)	loss 3.3977 (3.4563)	grad_norm 1.3782 (nan)	mem 14852MB
[2022-11-06 17:23:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][1100/1251]	eta 0:01:10 lr 0.000609	time 0.4676 (0.4686)	loss 3.9198 (3.4566)	grad_norm 1.5625 (nan)	mem 14852MB
[2022-11-06 17:23:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][1150/1251]	eta 0:00:47 lr 0.000608	time 0.4548 (0.4684)	loss 2.2103 (3.4488)	grad_norm 1.5673 (nan)	mem 14852MB
[2022-11-06 17:23:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][1200/1251]	eta 0:00:23 lr 0.000608	time 0.4596 (0.4684)	loss 3.6057 (3.4524)	grad_norm 1.2776 (nan)	mem 14852MB
[2022-11-06 17:24:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [129/300][1250/1251]	eta 0:00:00 lr 0.000608	time 0.4578 (0.4683)	loss 3.5964 (3.4511)	grad_norm 1.2715 (nan)	mem 14852MB
[2022-11-06 17:24:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 129 training takes 0:09:45
[2022-11-06 17:24:16 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_129.pth saving......
[2022-11-06 17:24:16 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_129.pth saved !!!
[2022-11-06 17:24:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.565 (1.565)	Loss 1.0292 (1.0292)	Acc@1 76.758 (76.758)	Acc@5 93.555 (93.555)	Mem 14852MB
[2022-11-06 17:24:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.114 Acc@5 93.868
[2022-11-06 17:24:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.1%
[2022-11-06 17:24:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.638 (1.638)	Loss 0.8258 (0.8258)	Acc@1 79.199 (79.199)	Acc@5 95.312 (95.312)	Mem 14852MB
[2022-11-06 17:24:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.450 Acc@5 95.026
[2022-11-06 17:24:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.5%
[2022-11-06 17:24:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.48% at 128 epoch
[2022-11-06 17:24:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][0/1251]	eta 0:46:35 lr 0.000608	time 2.2344 (2.2344)	loss 3.6024 (3.6024)	grad_norm 1.3901 (1.3901)	mem 14852MB
[2022-11-06 17:24:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][50/1251]	eta 0:10:06 lr 0.000608	time 0.4655 (0.5047)	loss 2.3471 (3.5373)	grad_norm 1.4364 (1.4413)	mem 14852MB
[2022-11-06 17:25:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][100/1251]	eta 0:09:21 lr 0.000608	time 0.4559 (0.4881)	loss 2.7914 (3.4370)	grad_norm 1.3060 (1.4548)	mem 14852MB
[2022-11-06 17:25:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][150/1251]	eta 0:08:50 lr 0.000607	time 0.4642 (0.4818)	loss 3.8956 (3.4626)	grad_norm 1.5336 (1.4618)	mem 14852MB
[2022-11-06 17:26:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][200/1251]	eta 0:08:22 lr 0.000607	time 0.4653 (0.4777)	loss 3.9651 (3.4716)	grad_norm 1.3685 (1.4632)	mem 14852MB
[2022-11-06 17:26:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][250/1251]	eta 0:07:55 lr 0.000607	time 0.4640 (0.4754)	loss 2.1046 (3.4485)	grad_norm 1.5775 (1.4652)	mem 14852MB
[2022-11-06 17:26:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][300/1251]	eta 0:07:30 lr 0.000607	time 0.4613 (0.4741)	loss 3.8993 (3.4694)	grad_norm 1.3944 (1.4638)	mem 14852MB
[2022-11-06 17:27:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][350/1251]	eta 0:07:06 lr 0.000606	time 0.4628 (0.4734)	loss 2.9292 (3.4484)	grad_norm 1.4356 (1.4663)	mem 14852MB
[2022-11-06 17:27:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][400/1251]	eta 0:06:41 lr 0.000606	time 0.4623 (0.4723)	loss 3.4636 (3.4512)	grad_norm 1.2679 (1.4655)	mem 14852MB
[2022-11-06 17:28:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][450/1251]	eta 0:06:17 lr 0.000606	time 0.4551 (0.4717)	loss 3.3157 (3.4540)	grad_norm 1.3590 (1.4643)	mem 14852MB
[2022-11-06 17:28:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][500/1251]	eta 0:05:53 lr 0.000606	time 0.4639 (0.4709)	loss 4.1174 (3.4549)	grad_norm 1.4339 (1.4654)	mem 14852MB
[2022-11-06 17:28:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][550/1251]	eta 0:05:30 lr 0.000606	time 0.4635 (0.4710)	loss 3.8777 (3.4552)	grad_norm 1.5076 (1.4633)	mem 14852MB
[2022-11-06 17:29:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][600/1251]	eta 0:05:06 lr 0.000605	time 0.4611 (0.4704)	loss 3.2126 (3.4546)	grad_norm 1.3430 (1.4683)	mem 14852MB
[2022-11-06 17:29:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][650/1251]	eta 0:04:42 lr 0.000605	time 0.4586 (0.4701)	loss 3.1906 (3.4492)	grad_norm 1.5054 (1.4651)	mem 14852MB
[2022-11-06 17:30:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][700/1251]	eta 0:04:18 lr 0.000605	time 0.4618 (0.4697)	loss 3.5861 (3.4441)	grad_norm 1.3243 (1.4668)	mem 14852MB
[2022-11-06 17:30:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][750/1251]	eta 0:03:55 lr 0.000605	time 0.4599 (0.4695)	loss 3.8885 (3.4488)	grad_norm 1.7230 (1.4693)	mem 14852MB
[2022-11-06 17:30:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][800/1251]	eta 0:03:31 lr 0.000605	time 0.4658 (0.4695)	loss 3.9106 (3.4463)	grad_norm 1.4489 (1.4702)	mem 14852MB
[2022-11-06 17:31:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][850/1251]	eta 0:03:08 lr 0.000604	time 0.5304 (0.4694)	loss 2.6412 (3.4407)	grad_norm 1.2890 (1.4694)	mem 14852MB
[2022-11-06 17:31:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][900/1251]	eta 0:02:44 lr 0.000604	time 0.4648 (0.4694)	loss 3.2961 (3.4472)	grad_norm 1.4681 (1.4710)	mem 14852MB
[2022-11-06 17:32:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][950/1251]	eta 0:02:21 lr 0.000604	time 0.4641 (0.4691)	loss 4.0770 (3.4516)	grad_norm 1.5778 (1.4685)	mem 14852MB
[2022-11-06 17:32:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][1000/1251]	eta 0:01:57 lr 0.000604	time 0.4744 (0.4689)	loss 3.4514 (3.4545)	grad_norm 1.4582 (1.4685)	mem 14852MB
[2022-11-06 17:32:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][1050/1251]	eta 0:01:34 lr 0.000604	time 0.4544 (0.4689)	loss 3.5922 (3.4545)	grad_norm 1.4485 (1.4694)	mem 14852MB
[2022-11-06 17:33:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][1100/1251]	eta 0:01:10 lr 0.000603	time 0.4573 (0.4688)	loss 3.6059 (3.4507)	grad_norm 1.4125 (1.4698)	mem 14852MB
[2022-11-06 17:33:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][1150/1251]	eta 0:00:47 lr 0.000603	time 0.4581 (0.4687)	loss 3.8798 (3.4521)	grad_norm 1.3351 (nan)	mem 14852MB
[2022-11-06 17:33:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][1200/1251]	eta 0:00:23 lr 0.000603	time 0.4547 (0.4686)	loss 4.1057 (3.4560)	grad_norm 1.4776 (nan)	mem 14852MB
[2022-11-06 17:34:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [130/300][1250/1251]	eta 0:00:00 lr 0.000603	time 0.4567 (0.4684)	loss 3.2385 (3.4587)	grad_norm 1.5730 (nan)	mem 14852MB
[2022-11-06 17:34:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 130 training takes 0:09:46
[2022-11-06 17:34:20 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_130.pth saving......
[2022-11-06 17:34:21 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_130.pth saved !!!
[2022-11-06 17:34:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.525 (1.525)	Loss 0.9512 (0.9512)	Acc@1 78.125 (78.125)	Acc@5 93.555 (93.555)	Mem 14852MB
[2022-11-06 17:34:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.334 Acc@5 93.990
[2022-11-06 17:34:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.3%
[2022-11-06 17:34:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.595 (1.595)	Loss 0.9197 (0.9197)	Acc@1 78.223 (78.223)	Acc@5 94.141 (94.141)	Mem 14852MB
[2022-11-06 17:34:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.508 Acc@5 95.020
[2022-11-06 17:34:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.5%
[2022-11-06 17:34:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.51% at 130 epoch
[2022-11-06 17:34:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][0/1251]	eta 0:40:47 lr 0.000603	time 1.9565 (1.9565)	loss 3.4303 (3.4303)	grad_norm 1.4159 (1.4159)	mem 14852MB
[2022-11-06 17:35:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][50/1251]	eta 0:10:01 lr 0.000603	time 0.4669 (0.5011)	loss 4.0278 (3.4313)	grad_norm 1.4606 (1.4478)	mem 14852MB
[2022-11-06 17:35:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][100/1251]	eta 0:09:17 lr 0.000602	time 0.4683 (0.4848)	loss 3.5383 (3.3897)	grad_norm 1.4405 (1.4776)	mem 14852MB
[2022-11-06 17:35:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][150/1251]	eta 0:08:47 lr 0.000602	time 0.4691 (0.4790)	loss 3.4470 (3.3819)	grad_norm 1.5490 (1.4911)	mem 14852MB
[2022-11-06 17:36:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][200/1251]	eta 0:08:19 lr 0.000602	time 0.4665 (0.4755)	loss 2.6331 (3.3779)	grad_norm 1.3659 (1.4863)	mem 14852MB
[2022-11-06 17:36:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][250/1251]	eta 0:07:54 lr 0.000602	time 0.4695 (0.4741)	loss 3.5733 (3.3708)	grad_norm 1.6327 (1.4851)	mem 14852MB
[2022-11-06 17:37:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][300/1251]	eta 0:07:29 lr 0.000602	time 0.4661 (0.4731)	loss 3.1972 (3.3971)	grad_norm 1.5522 (1.4796)	mem 14852MB
[2022-11-06 17:37:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][350/1251]	eta 0:07:05 lr 0.000601	time 0.4560 (0.4721)	loss 4.0947 (3.3925)	grad_norm 1.3000 (1.4792)	mem 14852MB
[2022-11-06 17:37:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][400/1251]	eta 0:06:40 lr 0.000601	time 0.4625 (0.4712)	loss 3.4233 (3.3901)	grad_norm 1.3631 (1.4808)	mem 14852MB
[2022-11-06 17:38:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][450/1251]	eta 0:06:16 lr 0.000601	time 0.4670 (0.4706)	loss 3.3419 (3.3864)	grad_norm 1.8718 (1.4795)	mem 14852MB
[2022-11-06 17:38:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][500/1251]	eta 0:05:53 lr 0.000601	time 0.4645 (0.4702)	loss 4.0610 (3.3736)	grad_norm 1.4757 (1.4767)	mem 14852MB
[2022-11-06 17:38:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][550/1251]	eta 0:05:29 lr 0.000601	time 0.4633 (0.4698)	loss 3.9728 (3.3825)	grad_norm 1.5522 (1.4732)	mem 14852MB
[2022-11-06 17:39:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][600/1251]	eta 0:05:05 lr 0.000600	time 0.4715 (0.4695)	loss 3.9912 (3.3866)	grad_norm 1.7600 (1.4744)	mem 14852MB
[2022-11-06 17:39:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][650/1251]	eta 0:04:42 lr 0.000600	time 0.4689 (0.4693)	loss 3.0292 (3.3836)	grad_norm 1.3866 (1.4732)	mem 14852MB
[2022-11-06 17:40:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][700/1251]	eta 0:04:18 lr 0.000600	time 0.4623 (0.4690)	loss 3.5320 (3.3868)	grad_norm 1.3808 (1.4727)	mem 14852MB
[2022-11-06 17:40:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][750/1251]	eta 0:03:54 lr 0.000600	time 0.4675 (0.4689)	loss 2.5228 (3.3931)	grad_norm 1.3683 (1.4736)	mem 14852MB
[2022-11-06 17:40:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][800/1251]	eta 0:03:31 lr 0.000600	time 0.4711 (0.4689)	loss 3.5689 (3.3937)	grad_norm 1.5145 (1.4715)	mem 14852MB
[2022-11-06 17:41:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][850/1251]	eta 0:03:07 lr 0.000599	time 0.4638 (0.4687)	loss 3.7251 (3.4035)	grad_norm 1.3952 (1.4697)	mem 14852MB
[2022-11-06 17:41:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][900/1251]	eta 0:02:44 lr 0.000599	time 0.4682 (0.4685)	loss 3.9116 (3.4020)	grad_norm 1.3589 (1.4718)	mem 14852MB
[2022-11-06 17:42:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][950/1251]	eta 0:02:20 lr 0.000599	time 0.5646 (0.4683)	loss 3.8792 (3.4016)	grad_norm 1.4228 (1.4712)	mem 14852MB
[2022-11-06 17:42:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][1000/1251]	eta 0:01:57 lr 0.000599	time 0.5534 (0.4683)	loss 3.2883 (3.4012)	grad_norm 1.4733 (1.4707)	mem 14852MB
[2022-11-06 17:42:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][1050/1251]	eta 0:01:34 lr 0.000599	time 0.4621 (0.4683)	loss 3.5146 (3.4081)	grad_norm 1.4876 (1.4708)	mem 14852MB
[2022-11-06 17:43:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][1100/1251]	eta 0:01:10 lr 0.000598	time 0.4716 (0.4682)	loss 3.8659 (3.4131)	grad_norm 1.6446 (1.4697)	mem 14852MB
[2022-11-06 17:43:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][1150/1251]	eta 0:00:47 lr 0.000598	time 0.4609 (0.4681)	loss 3.5543 (3.4119)	grad_norm 1.4146 (1.4711)	mem 14852MB
[2022-11-06 17:44:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][1200/1251]	eta 0:00:23 lr 0.000598	time 0.4664 (0.4679)	loss 2.4494 (3.4159)	grad_norm 1.4170 (1.4706)	mem 14852MB
[2022-11-06 17:44:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [131/300][1250/1251]	eta 0:00:00 lr 0.000598	time 0.4569 (0.4679)	loss 2.5113 (3.4172)	grad_norm 1.4292 (1.4699)	mem 14852MB
[2022-11-06 17:44:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 131 training takes 0:09:45
[2022-11-06 17:44:23 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_131.pth saving......
[2022-11-06 17:44:24 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_131.pth saved !!!
[2022-11-06 17:44:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.584 (1.584)	Loss 0.9485 (0.9485)	Acc@1 77.246 (77.246)	Acc@5 94.434 (94.434)	Mem 14852MB
[2022-11-06 17:44:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.464 Acc@5 94.088
[2022-11-06 17:44:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.5%
[2022-11-06 17:44:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.726 (1.726)	Loss 0.7802 (0.7802)	Acc@1 81.055 (81.055)	Acc@5 94.824 (94.824)	Mem 14852MB
[2022-11-06 17:44:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.558 Acc@5 95.058
[2022-11-06 17:44:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.6%
[2022-11-06 17:44:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.56% at 131 epoch
[2022-11-06 17:44:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][0/1251]	eta 0:42:13 lr 0.000598	time 2.0256 (2.0256)	loss 3.9302 (3.9302)	grad_norm 1.4967 (1.4967)	mem 14852MB
[2022-11-06 17:45:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][50/1251]	eta 0:10:01 lr 0.000598	time 0.4707 (0.5004)	loss 4.0356 (3.3060)	grad_norm 1.7067 (1.4916)	mem 14852MB
[2022-11-06 17:45:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][100/1251]	eta 0:09:17 lr 0.000597	time 0.4696 (0.4847)	loss 2.5146 (3.3155)	grad_norm 1.4418 (1.4828)	mem 14852MB
[2022-11-06 17:45:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][150/1251]	eta 0:08:47 lr 0.000597	time 0.4663 (0.4791)	loss 2.2686 (3.3275)	grad_norm 1.6306 (1.4783)	mem 14852MB
[2022-11-06 17:46:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][200/1251]	eta 0:08:21 lr 0.000597	time 0.4787 (0.4769)	loss 2.8510 (3.3707)	grad_norm 1.5423 (1.4829)	mem 14852MB
[2022-11-06 17:46:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][250/1251]	eta 0:07:55 lr 0.000597	time 0.4627 (0.4746)	loss 3.6417 (3.3752)	grad_norm 1.2888 (1.4783)	mem 14852MB
[2022-11-06 17:47:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][300/1251]	eta 0:07:29 lr 0.000597	time 0.4676 (0.4730)	loss 2.7753 (3.3766)	grad_norm 1.5035 (1.4825)	mem 14852MB
[2022-11-06 17:47:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][350/1251]	eta 0:07:05 lr 0.000596	time 0.4695 (0.4722)	loss 4.0704 (3.3958)	grad_norm 1.3813 (1.4806)	mem 14852MB
[2022-11-06 17:47:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][400/1251]	eta 0:06:41 lr 0.000596	time 0.4670 (0.4715)	loss 4.0907 (3.4024)	grad_norm 1.5828 (1.4737)	mem 14852MB
[2022-11-06 17:48:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][450/1251]	eta 0:06:17 lr 0.000596	time 0.4618 (0.4708)	loss 3.6186 (3.4004)	grad_norm 1.4356 (1.4815)	mem 14852MB
[2022-11-06 17:48:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][500/1251]	eta 0:05:53 lr 0.000596	time 0.4782 (0.4707)	loss 3.6090 (3.4034)	grad_norm 1.5114 (1.4781)	mem 14852MB
[2022-11-06 17:49:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][550/1251]	eta 0:05:29 lr 0.000596	time 0.4595 (0.4704)	loss 3.5176 (3.4147)	grad_norm 1.3160 (1.4810)	mem 14852MB
[2022-11-06 17:49:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][600/1251]	eta 0:05:05 lr 0.000595	time 0.4651 (0.4700)	loss 3.7428 (3.4184)	grad_norm 1.4354 (1.4802)	mem 14852MB
[2022-11-06 17:49:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][650/1251]	eta 0:04:42 lr 0.000595	time 0.4715 (0.4698)	loss 3.8735 (3.4170)	grad_norm 1.6445 (nan)	mem 14852MB
[2022-11-06 17:50:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][700/1251]	eta 0:04:18 lr 0.000595	time 0.4638 (0.4696)	loss 3.7798 (3.4068)	grad_norm 1.6149 (nan)	mem 14852MB
[2022-11-06 17:50:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][750/1251]	eta 0:03:55 lr 0.000595	time 0.4664 (0.4693)	loss 3.0949 (3.4042)	grad_norm 1.4333 (nan)	mem 14852MB
[2022-11-06 17:50:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][800/1251]	eta 0:03:31 lr 0.000594	time 0.4557 (0.4694)	loss 2.4270 (3.4038)	grad_norm 1.3969 (nan)	mem 14852MB
[2022-11-06 17:51:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][850/1251]	eta 0:03:08 lr 0.000594	time 0.4566 (0.4691)	loss 3.9867 (3.4062)	grad_norm 1.4840 (nan)	mem 14852MB
[2022-11-06 17:51:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][900/1251]	eta 0:02:44 lr 0.000594	time 0.4632 (0.4691)	loss 3.0917 (3.4056)	grad_norm 1.3301 (nan)	mem 14852MB
[2022-11-06 17:52:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][950/1251]	eta 0:02:21 lr 0.000594	time 0.4656 (0.4690)	loss 3.1673 (3.4110)	grad_norm 1.5405 (nan)	mem 14852MB
[2022-11-06 17:52:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][1000/1251]	eta 0:01:57 lr 0.000594	time 0.4714 (0.4690)	loss 3.0627 (3.4112)	grad_norm 1.5678 (nan)	mem 14852MB
[2022-11-06 17:52:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][1050/1251]	eta 0:01:34 lr 0.000593	time 0.4630 (0.4690)	loss 4.0877 (3.4106)	grad_norm 1.3331 (nan)	mem 14852MB
[2022-11-06 17:53:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][1100/1251]	eta 0:01:10 lr 0.000593	time 0.4606 (0.4689)	loss 2.5805 (3.4110)	grad_norm 1.3559 (nan)	mem 14852MB
[2022-11-06 17:53:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][1150/1251]	eta 0:00:47 lr 0.000593	time 0.4673 (0.4688)	loss 3.6761 (3.4133)	grad_norm 1.4917 (nan)	mem 14852MB
[2022-11-06 17:54:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][1200/1251]	eta 0:00:23 lr 0.000593	time 0.4565 (0.4688)	loss 3.1247 (3.4174)	grad_norm 1.3164 (nan)	mem 14852MB
[2022-11-06 17:54:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [132/300][1250/1251]	eta 0:00:00 lr 0.000593	time 0.4571 (0.4686)	loss 3.7261 (3.4171)	grad_norm 1.3688 (nan)	mem 14852MB
[2022-11-06 17:54:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 132 training takes 0:09:46
[2022-11-06 17:54:28 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_132.pth saving......
[2022-11-06 17:54:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_132.pth saved !!!
[2022-11-06 17:54:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.643 (1.643)	Loss 1.0294 (1.0294)	Acc@1 75.781 (75.781)	Acc@5 92.969 (92.969)	Mem 14852MB
[2022-11-06 17:54:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.042 Acc@5 94.012
[2022-11-06 17:54:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.0%
[2022-11-06 17:54:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.624 (1.624)	Loss 0.7679 (0.7679)	Acc@1 81.348 (81.348)	Acc@5 95.801 (95.801)	Mem 14852MB
[2022-11-06 17:54:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.614 Acc@5 95.118
[2022-11-06 17:54:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.6%
[2022-11-06 17:54:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.61% at 132 epoch
[2022-11-06 17:54:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][0/1251]	eta 0:45:17 lr 0.000593	time 2.1724 (2.1724)	loss 3.9175 (3.9175)	grad_norm 1.5668 (1.5668)	mem 14852MB
[2022-11-06 17:55:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][50/1251]	eta 0:10:09 lr 0.000592	time 0.4702 (0.5077)	loss 3.7797 (3.3281)	grad_norm 1.4237 (1.4451)	mem 14852MB
[2022-11-06 17:55:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][100/1251]	eta 0:09:22 lr 0.000592	time 0.4700 (0.4887)	loss 3.9840 (3.3348)	grad_norm 1.4793 (1.4759)	mem 14852MB
[2022-11-06 17:55:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][150/1251]	eta 0:08:49 lr 0.000592	time 0.4680 (0.4810)	loss 3.9448 (3.4018)	grad_norm 1.4678 (1.4722)	mem 14852MB
[2022-11-06 17:56:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][200/1251]	eta 0:08:22 lr 0.000592	time 0.4626 (0.4777)	loss 3.6054 (3.4190)	grad_norm 1.3859 (1.4748)	mem 14852MB
[2022-11-06 17:56:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][250/1251]	eta 0:07:55 lr 0.000592	time 0.4576 (0.4751)	loss 3.3545 (3.4194)	grad_norm 1.4405 (1.4735)	mem 14852MB
[2022-11-06 17:57:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][300/1251]	eta 0:07:30 lr 0.000591	time 0.4608 (0.4738)	loss 2.3637 (3.3943)	grad_norm 1.4050 (1.4782)	mem 14852MB
[2022-11-06 17:57:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][350/1251]	eta 0:07:05 lr 0.000591	time 0.4706 (0.4727)	loss 2.9376 (3.4096)	grad_norm 1.4866 (1.4798)	mem 14852MB
[2022-11-06 17:57:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][400/1251]	eta 0:06:41 lr 0.000591	time 0.4698 (0.4718)	loss 2.4413 (3.4044)	grad_norm 1.6153 (1.4791)	mem 14852MB
[2022-11-06 17:58:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][450/1251]	eta 0:06:17 lr 0.000591	time 0.4638 (0.4710)	loss 2.6454 (3.4033)	grad_norm 1.4838 (1.4754)	mem 14852MB
[2022-11-06 17:58:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][500/1251]	eta 0:05:53 lr 0.000591	time 0.4768 (0.4706)	loss 2.9969 (3.4103)	grad_norm 1.4691 (1.4778)	mem 14852MB
[2022-11-06 17:59:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][550/1251]	eta 0:05:29 lr 0.000590	time 0.4509 (0.4706)	loss 4.1338 (3.4159)	grad_norm 1.3277 (1.4790)	mem 14852MB
[2022-11-06 17:59:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][600/1251]	eta 0:05:06 lr 0.000590	time 0.4640 (0.4704)	loss 3.8844 (3.4171)	grad_norm 1.4980 (1.4776)	mem 14852MB
[2022-11-06 17:59:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][650/1251]	eta 0:04:42 lr 0.000590	time 0.4651 (0.4700)	loss 2.6427 (3.4093)	grad_norm 1.5535 (1.4803)	mem 14852MB
[2022-11-06 18:00:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][700/1251]	eta 0:04:18 lr 0.000590	time 0.4750 (0.4696)	loss 3.6427 (3.4008)	grad_norm 1.3446 (1.4813)	mem 14852MB
[2022-11-06 18:00:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][750/1251]	eta 0:03:55 lr 0.000590	time 0.4654 (0.4695)	loss 3.5512 (3.4068)	grad_norm 1.4381 (1.4830)	mem 14852MB
[2022-11-06 18:01:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][800/1251]	eta 0:03:31 lr 0.000589	time 0.5511 (0.4695)	loss 3.8855 (3.4055)	grad_norm 1.6006 (1.4822)	mem 14852MB
[2022-11-06 18:01:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][850/1251]	eta 0:03:08 lr 0.000589	time 0.4672 (0.4693)	loss 3.8084 (3.4094)	grad_norm 1.3999 (1.4806)	mem 14852MB
[2022-11-06 18:01:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][900/1251]	eta 0:02:44 lr 0.000589	time 0.4686 (0.4691)	loss 4.2039 (3.4051)	grad_norm 1.5291 (1.4796)	mem 14852MB
[2022-11-06 18:02:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][950/1251]	eta 0:02:21 lr 0.000589	time 0.4617 (0.4690)	loss 3.3035 (3.4031)	grad_norm 1.5929 (1.4805)	mem 14852MB
[2022-11-06 18:02:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][1000/1251]	eta 0:01:57 lr 0.000589	time 0.4666 (0.4688)	loss 3.6241 (3.4040)	grad_norm 1.5770 (1.4813)	mem 14852MB
[2022-11-06 18:02:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][1050/1251]	eta 0:01:34 lr 0.000588	time 0.4704 (0.4690)	loss 2.9473 (3.4040)	grad_norm 1.5557 (1.4819)	mem 14852MB
[2022-11-06 18:03:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][1100/1251]	eta 0:01:10 lr 0.000588	time 0.4573 (0.4689)	loss 3.8854 (3.4044)	grad_norm 1.6055 (1.4811)	mem 14852MB
[2022-11-06 18:03:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][1150/1251]	eta 0:00:47 lr 0.000588	time 0.4693 (0.4688)	loss 3.2566 (3.4053)	grad_norm 1.4246 (1.4798)	mem 14852MB
[2022-11-06 18:04:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][1200/1251]	eta 0:00:23 lr 0.000588	time 0.4766 (0.4687)	loss 3.3133 (3.4069)	grad_norm 1.4177 (1.4799)	mem 14852MB
[2022-11-06 18:04:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [133/300][1250/1251]	eta 0:00:00 lr 0.000588	time 0.4579 (0.4685)	loss 3.0411 (3.4059)	grad_norm 1.4980 (1.4815)	mem 14852MB
[2022-11-06 18:04:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 133 training takes 0:09:46
[2022-11-06 18:04:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_133.pth saving......
[2022-11-06 18:04:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_133.pth saved !!!
[2022-11-06 18:04:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.619 (1.619)	Loss 0.9170 (0.9170)	Acc@1 79.297 (79.297)	Acc@5 95.117 (95.117)	Mem 14852MB
[2022-11-06 18:04:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.490 Acc@5 94.078
[2022-11-06 18:04:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.5%
[2022-11-06 18:04:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.824 (1.824)	Loss 0.9446 (0.9446)	Acc@1 76.758 (76.758)	Acc@5 93.750 (93.750)	Mem 14852MB
[2022-11-06 18:04:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.598 Acc@5 95.128
[2022-11-06 18:04:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.6%
[2022-11-06 18:04:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.61% at 132 epoch
[2022-11-06 18:04:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][0/1251]	eta 0:40:57 lr 0.000588	time 1.9644 (1.9644)	loss 3.5281 (3.5281)	grad_norm 1.4232 (1.4232)	mem 14852MB
[2022-11-06 18:05:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][50/1251]	eta 0:10:02 lr 0.000587	time 0.4749 (0.5014)	loss 3.6708 (3.3596)	grad_norm 1.4441 (1.4819)	mem 14852MB
[2022-11-06 18:05:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][100/1251]	eta 0:09:19 lr 0.000587	time 0.4770 (0.4857)	loss 2.3283 (3.3285)	grad_norm 1.4766 (1.4788)	mem 14852MB
[2022-11-06 18:06:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][150/1251]	eta 0:08:47 lr 0.000587	time 0.4568 (0.4791)	loss 2.8630 (3.3449)	grad_norm 1.3411 (1.4795)	mem 14852MB
[2022-11-06 18:06:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][200/1251]	eta 0:08:20 lr 0.000587	time 0.4622 (0.4760)	loss 3.5942 (3.3355)	grad_norm 1.5244 (1.4879)	mem 14852MB
[2022-11-06 18:06:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][250/1251]	eta 0:07:54 lr 0.000587	time 0.4744 (0.4739)	loss 2.9644 (3.3464)	grad_norm 1.3971 (1.4864)	mem 14852MB
[2022-11-06 18:07:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][300/1251]	eta 0:07:29 lr 0.000586	time 0.4632 (0.4725)	loss 3.7202 (3.3852)	grad_norm 1.5099 (1.4829)	mem 14852MB
[2022-11-06 18:07:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][350/1251]	eta 0:07:04 lr 0.000586	time 0.4620 (0.4717)	loss 2.4632 (3.3701)	grad_norm 1.3156 (1.4801)	mem 14852MB
[2022-11-06 18:08:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][400/1251]	eta 0:06:41 lr 0.000586	time 0.4685 (0.4713)	loss 4.0989 (3.3912)	grad_norm 1.4167 (1.4826)	mem 14852MB
[2022-11-06 18:08:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][450/1251]	eta 0:06:16 lr 0.000586	time 0.4521 (0.4706)	loss 3.7342 (3.3693)	grad_norm 1.5558 (1.4847)	mem 14852MB
[2022-11-06 18:08:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][500/1251]	eta 0:05:53 lr 0.000586	time 0.4698 (0.4701)	loss 3.7914 (3.3721)	grad_norm 1.5556 (1.4879)	mem 14852MB
[2022-11-06 18:09:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][550/1251]	eta 0:05:29 lr 0.000585	time 0.4736 (0.4702)	loss 3.5799 (3.3807)	grad_norm 1.5576 (1.4891)	mem 14852MB
[2022-11-06 18:09:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][600/1251]	eta 0:05:05 lr 0.000585	time 0.4675 (0.4698)	loss 3.0917 (3.3822)	grad_norm 1.5231 (1.4925)	mem 14852MB
[2022-11-06 18:09:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][650/1251]	eta 0:04:42 lr 0.000585	time 0.4727 (0.4695)	loss 3.2247 (3.3971)	grad_norm 1.7288 (1.4947)	mem 14852MB
[2022-11-06 18:10:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][700/1251]	eta 0:04:18 lr 0.000585	time 0.4761 (0.4695)	loss 2.6164 (3.3882)	grad_norm 1.6992 (1.4941)	mem 14852MB
[2022-11-06 18:10:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][750/1251]	eta 0:03:55 lr 0.000584	time 0.4677 (0.4693)	loss 2.4831 (3.3911)	grad_norm 1.5955 (nan)	mem 14852MB
[2022-11-06 18:11:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][800/1251]	eta 0:03:31 lr 0.000584	time 0.4736 (0.4693)	loss 2.6801 (3.3970)	grad_norm 1.3655 (nan)	mem 14852MB
[2022-11-06 18:11:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][850/1251]	eta 0:03:08 lr 0.000584	time 0.4700 (0.4691)	loss 3.3387 (3.4009)	grad_norm 1.4812 (nan)	mem 14852MB
[2022-11-06 18:11:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][900/1251]	eta 0:02:44 lr 0.000584	time 0.5641 (0.4691)	loss 3.9495 (3.4025)	grad_norm 1.4406 (nan)	mem 14852MB
[2022-11-06 18:12:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][950/1251]	eta 0:02:21 lr 0.000584	time 0.4624 (0.4689)	loss 3.4670 (3.3995)	grad_norm 1.6875 (nan)	mem 14852MB
[2022-11-06 18:12:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][1000/1251]	eta 0:01:57 lr 0.000583	time 0.5621 (0.4687)	loss 3.8981 (3.3977)	grad_norm 1.4726 (nan)	mem 14852MB
[2022-11-06 18:13:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][1050/1251]	eta 0:01:34 lr 0.000583	time 0.4615 (0.4688)	loss 2.8502 (3.4002)	grad_norm 1.4588 (nan)	mem 14852MB
[2022-11-06 18:13:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][1100/1251]	eta 0:01:10 lr 0.000583	time 0.4608 (0.4688)	loss 3.5688 (3.4008)	grad_norm 1.9327 (nan)	mem 14852MB
[2022-11-06 18:13:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][1150/1251]	eta 0:00:47 lr 0.000583	time 0.4739 (0.4686)	loss 3.5690 (3.4055)	grad_norm 1.4866 (nan)	mem 14852MB
[2022-11-06 18:14:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][1200/1251]	eta 0:00:23 lr 0.000583	time 0.4607 (0.4685)	loss 3.1633 (3.4039)	grad_norm 1.6236 (nan)	mem 14852MB
[2022-11-06 18:14:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [134/300][1250/1251]	eta 0:00:00 lr 0.000582	time 0.4582 (0.4683)	loss 3.2938 (3.4036)	grad_norm 1.5323 (nan)	mem 14852MB
[2022-11-06 18:14:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 134 training takes 0:09:45
[2022-11-06 18:14:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_134.pth saving......
[2022-11-06 18:14:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_134.pth saved !!!
[2022-11-06 18:14:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.522 (1.522)	Loss 0.9617 (0.9617)	Acc@1 77.344 (77.344)	Acc@5 94.531 (94.531)	Mem 14852MB
[2022-11-06 18:14:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.372 Acc@5 93.952
[2022-11-06 18:14:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.4%
[2022-11-06 18:14:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.673 (1.673)	Loss 0.8155 (0.8155)	Acc@1 80.859 (80.859)	Acc@5 95.508 (95.508)	Mem 14852MB
[2022-11-06 18:14:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.632 Acc@5 95.128
[2022-11-06 18:14:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.6%
[2022-11-06 18:14:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.63% at 134 epoch
[2022-11-06 18:14:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][0/1251]	eta 0:39:43 lr 0.000582	time 1.9052 (1.9052)	loss 3.8432 (3.8432)	grad_norm 1.5246 (1.5246)	mem 14852MB
[2022-11-06 18:15:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][50/1251]	eta 0:10:03 lr 0.000582	time 0.4696 (0.5023)	loss 3.8797 (3.3944)	grad_norm 1.3253 (1.4462)	mem 14852MB
[2022-11-06 18:15:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][100/1251]	eta 0:09:19 lr 0.000582	time 0.4799 (0.4860)	loss 3.7300 (3.4210)	grad_norm 1.2687 (1.4544)	mem 14852MB
[2022-11-06 18:16:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][150/1251]	eta 0:08:47 lr 0.000582	time 0.4558 (0.4793)	loss 4.0662 (3.4476)	grad_norm 1.4947 (1.4729)	mem 14852MB
[2022-11-06 18:16:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][200/1251]	eta 0:08:20 lr 0.000582	time 0.4699 (0.4758)	loss 3.5388 (3.4686)	grad_norm 1.5061 (1.4760)	mem 14852MB
[2022-11-06 18:16:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][250/1251]	eta 0:07:54 lr 0.000581	time 0.4737 (0.4741)	loss 2.8588 (3.4482)	grad_norm 1.4708 (1.4748)	mem 14852MB
[2022-11-06 18:17:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][300/1251]	eta 0:07:29 lr 0.000581	time 0.4589 (0.4730)	loss 3.1303 (3.4303)	grad_norm 1.4230 (1.4697)	mem 14852MB
[2022-11-06 18:17:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][350/1251]	eta 0:07:05 lr 0.000581	time 0.4746 (0.4719)	loss 3.6768 (3.4343)	grad_norm 1.4371 (1.4705)	mem 14852MB
[2022-11-06 18:18:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][400/1251]	eta 0:06:40 lr 0.000581	time 0.4544 (0.4710)	loss 3.4558 (3.4306)	grad_norm 1.4370 (1.4721)	mem 14852MB
[2022-11-06 18:18:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][450/1251]	eta 0:06:16 lr 0.000581	time 0.4585 (0.4704)	loss 2.4224 (3.4356)	grad_norm 1.5253 (1.4723)	mem 14852MB
[2022-11-06 18:18:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][500/1251]	eta 0:05:52 lr 0.000580	time 0.4663 (0.4700)	loss 3.2419 (3.4418)	grad_norm 1.4033 (1.4768)	mem 14852MB
[2022-11-06 18:19:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][550/1251]	eta 0:05:29 lr 0.000580	time 0.4652 (0.4698)	loss 3.8747 (3.4499)	grad_norm 1.6805 (1.4773)	mem 14852MB
[2022-11-06 18:19:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][600/1251]	eta 0:05:05 lr 0.000580	time 0.4575 (0.4699)	loss 3.6902 (3.4465)	grad_norm 1.5490 (1.4795)	mem 14852MB
[2022-11-06 18:20:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][650/1251]	eta 0:04:42 lr 0.000580	time 0.4604 (0.4695)	loss 3.2863 (3.4437)	grad_norm 1.3754 (1.4807)	mem 14852MB
[2022-11-06 18:20:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][700/1251]	eta 0:04:18 lr 0.000580	time 0.5400 (0.4692)	loss 3.8415 (3.4390)	grad_norm 1.5280 (1.4820)	mem 14852MB
[2022-11-06 18:20:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][750/1251]	eta 0:03:54 lr 0.000579	time 0.4751 (0.4690)	loss 4.0807 (3.4413)	grad_norm 1.5945 (1.4833)	mem 14852MB
[2022-11-06 18:21:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][800/1251]	eta 0:03:31 lr 0.000579	time 0.4610 (0.4690)	loss 4.1667 (3.4412)	grad_norm 1.4240 (1.4839)	mem 14852MB
[2022-11-06 18:21:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][850/1251]	eta 0:03:08 lr 0.000579	time 0.4662 (0.4689)	loss 3.6371 (3.4383)	grad_norm 1.3939 (1.4846)	mem 14852MB
[2022-11-06 18:21:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][900/1251]	eta 0:02:44 lr 0.000579	time 0.4685 (0.4687)	loss 2.9035 (3.4365)	grad_norm 1.4473 (1.4855)	mem 14852MB
[2022-11-06 18:22:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][950/1251]	eta 0:02:21 lr 0.000579	time 0.4555 (0.4686)	loss 3.7449 (3.4350)	grad_norm 1.4277 (1.4853)	mem 14852MB
[2022-11-06 18:22:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][1000/1251]	eta 0:01:57 lr 0.000578	time 0.4618 (0.4685)	loss 2.5584 (3.4347)	grad_norm 1.5251 (1.4851)	mem 14852MB
[2022-11-06 18:23:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][1050/1251]	eta 0:01:34 lr 0.000578	time 0.4601 (0.4685)	loss 4.0791 (3.4318)	grad_norm 1.5030 (1.4861)	mem 14852MB
[2022-11-06 18:23:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][1100/1251]	eta 0:01:10 lr 0.000578	time 0.4617 (0.4685)	loss 3.6446 (3.4325)	grad_norm 1.5082 (1.4856)	mem 14852MB
[2022-11-06 18:23:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][1150/1251]	eta 0:00:47 lr 0.000578	time 0.4617 (0.4684)	loss 3.9530 (3.4295)	grad_norm 1.4872 (1.4868)	mem 14852MB
[2022-11-06 18:24:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][1200/1251]	eta 0:00:23 lr 0.000578	time 0.5498 (0.4684)	loss 3.0947 (3.4288)	grad_norm 1.4250 (1.4893)	mem 14852MB
[2022-11-06 18:24:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [135/300][1250/1251]	eta 0:00:00 lr 0.000577	time 0.4575 (0.4681)	loss 2.3730 (3.4276)	grad_norm 1.3691 (1.4892)	mem 14852MB
[2022-11-06 18:24:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 135 training takes 0:09:45
[2022-11-06 18:24:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_135.pth saving......
[2022-11-06 18:24:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_135.pth saved !!!
[2022-11-06 18:24:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.661 (1.661)	Loss 0.9439 (0.9439)	Acc@1 77.344 (77.344)	Acc@5 94.824 (94.824)	Mem 14852MB
[2022-11-06 18:24:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.612 Acc@5 94.092
[2022-11-06 18:24:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.6%
[2022-11-06 18:24:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.641 (1.641)	Loss 0.7990 (0.7990)	Acc@1 81.348 (81.348)	Acc@5 95.410 (95.410)	Mem 14852MB
[2022-11-06 18:24:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.640 Acc@5 95.136
[2022-11-06 18:24:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.6%
[2022-11-06 18:24:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.64% at 135 epoch
[2022-11-06 18:25:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][0/1251]	eta 0:40:59 lr 0.000577	time 1.9660 (1.9660)	loss 3.2685 (3.2685)	grad_norm 1.4681 (1.4681)	mem 14852MB
[2022-11-06 18:25:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][50/1251]	eta 0:10:05 lr 0.000577	time 0.4664 (0.5044)	loss 3.3950 (3.3522)	grad_norm 1.4544 (1.5438)	mem 14852MB
[2022-11-06 18:25:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][100/1251]	eta 0:09:19 lr 0.000577	time 0.4744 (0.4865)	loss 4.0659 (3.3135)	grad_norm 1.4988 (1.5188)	mem 14852MB
[2022-11-06 18:26:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][150/1251]	eta 0:08:47 lr 0.000577	time 0.4721 (0.4795)	loss 3.9319 (3.4054)	grad_norm 1.4862 (1.5226)	mem 14852MB
[2022-11-06 18:26:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][200/1251]	eta 0:08:20 lr 0.000576	time 0.4626 (0.4764)	loss 3.4576 (3.4347)	grad_norm 1.6141 (1.5126)	mem 14852MB
[2022-11-06 18:26:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][250/1251]	eta 0:07:54 lr 0.000576	time 0.4620 (0.4741)	loss 3.8276 (3.4258)	grad_norm 1.4312 (1.5059)	mem 14852MB
[2022-11-06 18:27:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][300/1251]	eta 0:07:29 lr 0.000576	time 0.4580 (0.4725)	loss 3.5793 (3.4218)	grad_norm 1.4123 (1.5036)	mem 14852MB
[2022-11-06 18:27:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][350/1251]	eta 0:07:04 lr 0.000576	time 0.4669 (0.4715)	loss 3.5981 (3.4265)	grad_norm 1.6094 (1.5004)	mem 14852MB
[2022-11-06 18:28:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][400/1251]	eta 0:06:40 lr 0.000576	time 0.4548 (0.4706)	loss 2.2585 (3.4201)	grad_norm 1.4081 (1.5068)	mem 14852MB
[2022-11-06 18:28:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][450/1251]	eta 0:06:16 lr 0.000575	time 0.4617 (0.4701)	loss 2.0989 (3.4198)	grad_norm 1.6248 (1.5074)	mem 14852MB
[2022-11-06 18:28:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][500/1251]	eta 0:05:52 lr 0.000575	time 0.4645 (0.4700)	loss 3.8549 (3.4177)	grad_norm 1.6492 (1.5067)	mem 14852MB
[2022-11-06 18:29:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][550/1251]	eta 0:05:29 lr 0.000575	time 0.4588 (0.4701)	loss 2.3849 (3.4242)	grad_norm 1.4156 (1.5055)	mem 14852MB
[2022-11-06 18:29:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][600/1251]	eta 0:05:05 lr 0.000575	time 0.4678 (0.4700)	loss 3.6995 (3.4248)	grad_norm 1.8887 (1.5081)	mem 14852MB
[2022-11-06 18:30:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][650/1251]	eta 0:04:42 lr 0.000575	time 0.4657 (0.4696)	loss 2.2840 (3.4277)	grad_norm 1.3919 (1.5096)	mem 14852MB
[2022-11-06 18:30:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][700/1251]	eta 0:04:18 lr 0.000574	time 0.4553 (0.4694)	loss 4.2325 (3.4279)	grad_norm 1.6269 (1.5077)	mem 14852MB
[2022-11-06 18:30:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][750/1251]	eta 0:03:55 lr 0.000574	time 0.4741 (0.4694)	loss 3.9233 (3.4284)	grad_norm 1.3968 (1.5059)	mem 14852MB
[2022-11-06 18:31:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][800/1251]	eta 0:03:31 lr 0.000574	time 0.4642 (0.4694)	loss 3.8252 (3.4285)	grad_norm 1.4459 (1.5070)	mem 14852MB
[2022-11-06 18:31:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][850/1251]	eta 0:03:08 lr 0.000574	time 0.4699 (0.4694)	loss 2.5384 (3.4288)	grad_norm 1.3582 (1.5066)	mem 14852MB
[2022-11-06 18:32:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][900/1251]	eta 0:02:44 lr 0.000574	time 0.4630 (0.4692)	loss 3.8610 (3.4252)	grad_norm 1.5529 (1.5065)	mem 14852MB
[2022-11-06 18:32:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][950/1251]	eta 0:02:21 lr 0.000573	time 0.4518 (0.4690)	loss 2.3661 (3.4170)	grad_norm 1.4591 (1.5071)	mem 14852MB
[2022-11-06 18:32:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][1000/1251]	eta 0:01:57 lr 0.000573	time 0.4558 (0.4689)	loss 3.7881 (3.4158)	grad_norm 1.4758 (1.5064)	mem 14852MB
[2022-11-06 18:33:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][1050/1251]	eta 0:01:34 lr 0.000573	time 0.4613 (0.4691)	loss 2.6924 (3.4140)	grad_norm 1.5104 (1.5071)	mem 14852MB
[2022-11-06 18:33:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][1100/1251]	eta 0:01:10 lr 0.000573	time 0.4581 (0.4690)	loss 3.7485 (3.4139)	grad_norm 1.4342 (1.5057)	mem 14852MB
[2022-11-06 18:33:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][1150/1251]	eta 0:00:47 lr 0.000573	time 0.4570 (0.4688)	loss 3.7625 (3.4163)	grad_norm 1.6275 (1.5044)	mem 14852MB
[2022-11-06 18:34:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][1200/1251]	eta 0:00:23 lr 0.000572	time 0.4720 (0.4687)	loss 4.1397 (3.4151)	grad_norm 1.4949 (1.5047)	mem 14852MB
[2022-11-06 18:34:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [136/300][1250/1251]	eta 0:00:00 lr 0.000572	time 0.4593 (0.4685)	loss 3.9744 (3.4173)	grad_norm 1.5708 (1.5055)	mem 14852MB
[2022-11-06 18:34:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 136 training takes 0:09:46
[2022-11-06 18:34:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_136.pth saving......
[2022-11-06 18:34:46 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_136.pth saved !!!
[2022-11-06 18:34:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.473 (1.473)	Loss 0.9498 (0.9498)	Acc@1 80.957 (80.957)	Acc@5 94.531 (94.531)	Mem 14852MB
[2022-11-06 18:34:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.432 Acc@5 94.032
[2022-11-06 18:34:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.4%
[2022-11-06 18:34:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.692 (1.692)	Loss 0.8147 (0.8147)	Acc@1 80.762 (80.762)	Acc@5 95.020 (95.020)	Mem 14852MB
[2022-11-06 18:35:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.662 Acc@5 95.102
[2022-11-06 18:35:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.7%
[2022-11-06 18:35:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.66% at 136 epoch
[2022-11-06 18:35:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][0/1251]	eta 0:44:43 lr 0.000572	time 2.1450 (2.1450)	loss 3.2591 (3.2591)	grad_norm 1.4100 (1.4100)	mem 14852MB
[2022-11-06 18:35:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][50/1251]	eta 0:10:05 lr 0.000572	time 0.4637 (0.5041)	loss 3.4587 (3.3415)	grad_norm 1.4312 (1.5221)	mem 14852MB
[2022-11-06 18:35:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][100/1251]	eta 0:09:22 lr 0.000572	time 0.4735 (0.4884)	loss 2.2613 (3.3890)	grad_norm 1.4706 (1.5251)	mem 14852MB
[2022-11-06 18:36:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][150/1251]	eta 0:08:49 lr 0.000572	time 0.4616 (0.4808)	loss 4.2557 (3.3940)	grad_norm 1.5589 (1.5114)	mem 14852MB
[2022-11-06 18:36:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][200/1251]	eta 0:08:21 lr 0.000571	time 0.4540 (0.4768)	loss 4.0579 (3.4148)	grad_norm 1.3316 (1.5167)	mem 14852MB
[2022-11-06 18:37:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][250/1251]	eta 0:07:55 lr 0.000571	time 0.4553 (0.4749)	loss 3.0883 (3.3918)	grad_norm 1.5393 (1.5206)	mem 14852MB
[2022-11-06 18:37:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][300/1251]	eta 0:07:30 lr 0.000571	time 0.4617 (0.4736)	loss 2.9834 (3.3987)	grad_norm 1.3268 (1.5133)	mem 14852MB
[2022-11-06 18:37:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][350/1251]	eta 0:07:05 lr 0.000571	time 0.4609 (0.4727)	loss 2.4349 (3.3772)	grad_norm 1.5457 (1.5127)	mem 14852MB
[2022-11-06 18:38:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][400/1251]	eta 0:06:41 lr 0.000571	time 0.4649 (0.4718)	loss 3.6047 (3.3808)	grad_norm 1.4709 (1.5131)	mem 14852MB
[2022-11-06 18:38:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][450/1251]	eta 0:06:17 lr 0.000570	time 0.4593 (0.4712)	loss 2.3985 (3.3752)	grad_norm 1.4978 (1.5118)	mem 14852MB
[2022-11-06 18:38:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][500/1251]	eta 0:05:53 lr 0.000570	time 0.4641 (0.4705)	loss 3.3082 (3.3840)	grad_norm 1.4798 (1.5143)	mem 14852MB
[2022-11-06 18:39:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][550/1251]	eta 0:05:29 lr 0.000570	time 0.5401 (0.4706)	loss 3.9882 (3.3723)	grad_norm 1.4733 (1.5118)	mem 14852MB
[2022-11-06 18:39:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][600/1251]	eta 0:05:06 lr 0.000570	time 0.4697 (0.4704)	loss 4.1260 (3.3713)	grad_norm 1.6752 (1.5102)	mem 14852MB
[2022-11-06 18:40:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][650/1251]	eta 0:04:42 lr 0.000570	time 0.4576 (0.4700)	loss 3.2126 (3.3775)	grad_norm 1.6332 (1.5125)	mem 14852MB
[2022-11-06 18:40:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][700/1251]	eta 0:04:18 lr 0.000569	time 0.4698 (0.4697)	loss 2.6361 (3.3779)	grad_norm 1.5468 (1.5146)	mem 14852MB
[2022-11-06 18:40:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][750/1251]	eta 0:03:55 lr 0.000569	time 0.4596 (0.4696)	loss 2.5192 (3.3690)	grad_norm 1.4704 (1.5118)	mem 14852MB
[2022-11-06 18:41:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][800/1251]	eta 0:03:31 lr 0.000569	time 0.4710 (0.4697)	loss 3.3542 (3.3718)	grad_norm 1.5171 (1.5119)	mem 14852MB
[2022-11-06 18:41:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][850/1251]	eta 0:03:08 lr 0.000569	time 0.4698 (0.4696)	loss 3.6992 (3.3712)	grad_norm 1.3421 (1.5096)	mem 14852MB
[2022-11-06 18:42:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][900/1251]	eta 0:02:44 lr 0.000568	time 0.4668 (0.4695)	loss 3.3546 (3.3762)	grad_norm 1.6389 (1.5084)	mem 14852MB
[2022-11-06 18:42:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][950/1251]	eta 0:02:21 lr 0.000568	time 0.4610 (0.4694)	loss 3.0956 (3.3785)	grad_norm 1.4497 (1.5084)	mem 14852MB
[2022-11-06 18:42:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][1000/1251]	eta 0:01:57 lr 0.000568	time 0.4744 (0.4692)	loss 3.2832 (3.3879)	grad_norm 1.4993 (1.5097)	mem 14852MB
[2022-11-06 18:43:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][1050/1251]	eta 0:01:34 lr 0.000568	time 0.5313 (0.4693)	loss 3.6984 (3.3898)	grad_norm 1.9459 (inf)	mem 14852MB
[2022-11-06 18:43:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][1100/1251]	eta 0:01:10 lr 0.000568	time 0.4682 (0.4692)	loss 2.9833 (3.3956)	grad_norm 1.5265 (inf)	mem 14852MB
[2022-11-06 18:44:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][1150/1251]	eta 0:00:47 lr 0.000567	time 0.4583 (0.4690)	loss 4.0023 (3.3965)	grad_norm 1.5521 (inf)	mem 14852MB
[2022-11-06 18:44:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][1200/1251]	eta 0:00:23 lr 0.000567	time 0.4629 (0.4690)	loss 3.9612 (3.3925)	grad_norm 1.6927 (inf)	mem 14852MB
[2022-11-06 18:44:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [137/300][1250/1251]	eta 0:00:00 lr 0.000567	time 0.4571 (0.4688)	loss 2.9131 (3.3914)	grad_norm 1.4748 (inf)	mem 14852MB
[2022-11-06 18:44:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 137 training takes 0:09:46
[2022-11-06 18:44:50 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_137.pth saving......
[2022-11-06 18:44:51 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_137.pth saved !!!
[2022-11-06 18:44:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.561 (1.561)	Loss 0.9102 (0.9102)	Acc@1 80.273 (80.273)	Acc@5 94.629 (94.629)	Mem 14852MB
[2022-11-06 18:44:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.234 Acc@5 94.042
[2022-11-06 18:44:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.2%
[2022-11-06 18:45:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.656 (1.656)	Loss 0.8808 (0.8808)	Acc@1 76.660 (76.660)	Acc@5 95.215 (95.215)	Mem 14852MB
[2022-11-06 18:45:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.706 Acc@5 95.102
[2022-11-06 18:45:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.7%
[2022-11-06 18:45:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.71% at 137 epoch
[2022-11-06 18:45:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][0/1251]	eta 0:42:42 lr 0.000567	time 2.0483 (2.0483)	loss 3.6840 (3.6840)	grad_norm 1.7000 (1.7000)	mem 14852MB
[2022-11-06 18:45:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][50/1251]	eta 0:10:01 lr 0.000567	time 0.4643 (0.5006)	loss 3.3586 (3.3010)	grad_norm 1.4188 (1.4811)	mem 14852MB
[2022-11-06 18:45:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][100/1251]	eta 0:09:20 lr 0.000567	time 0.4624 (0.4870)	loss 2.5755 (3.3981)	grad_norm 1.4616 (1.4781)	mem 14852MB
[2022-11-06 18:46:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][150/1251]	eta 0:08:49 lr 0.000566	time 0.4618 (0.4812)	loss 3.5116 (3.4070)	grad_norm 1.4503 (1.4796)	mem 14852MB
[2022-11-06 18:46:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][200/1251]	eta 0:08:21 lr 0.000566	time 0.4628 (0.4774)	loss 3.5534 (3.4160)	grad_norm 1.3917 (1.4815)	mem 14852MB
[2022-11-06 18:47:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][250/1251]	eta 0:07:55 lr 0.000566	time 0.4580 (0.4753)	loss 2.8640 (3.4047)	grad_norm 1.3999 (1.4951)	mem 14852MB
[2022-11-06 18:47:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][300/1251]	eta 0:07:30 lr 0.000566	time 0.4663 (0.4736)	loss 2.4367 (3.4036)	grad_norm 1.4033 (1.4987)	mem 14852MB
[2022-11-06 18:47:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][350/1251]	eta 0:07:05 lr 0.000566	time 0.4608 (0.4726)	loss 3.4969 (3.3829)	grad_norm 1.4899 (1.4982)	mem 14852MB
[2022-11-06 18:48:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][400/1251]	eta 0:06:41 lr 0.000565	time 0.4703 (0.4719)	loss 2.6085 (3.3669)	grad_norm 1.7317 (1.4982)	mem 14852MB
[2022-11-06 18:48:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][450/1251]	eta 0:06:17 lr 0.000565	time 0.4591 (0.4712)	loss 3.5172 (3.3666)	grad_norm 1.3162 (1.4981)	mem 14852MB
[2022-11-06 18:49:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][500/1251]	eta 0:05:53 lr 0.000565	time 0.4668 (0.4707)	loss 3.8147 (3.3663)	grad_norm 1.3073 (1.5005)	mem 14852MB
[2022-11-06 18:49:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][550/1251]	eta 0:05:29 lr 0.000565	time 0.4706 (0.4707)	loss 2.1760 (3.3669)	grad_norm 1.4015 (1.4975)	mem 14852MB
[2022-11-06 18:49:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][600/1251]	eta 0:05:06 lr 0.000565	time 0.4707 (0.4702)	loss 3.7663 (3.3506)	grad_norm 1.5184 (1.4991)	mem 14852MB
[2022-11-06 18:50:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][650/1251]	eta 0:04:42 lr 0.000564	time 0.4645 (0.4700)	loss 4.1321 (3.3545)	grad_norm 1.6212 (1.5012)	mem 14852MB
[2022-11-06 18:50:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][700/1251]	eta 0:04:18 lr 0.000564	time 0.4730 (0.4699)	loss 3.2538 (3.3662)	grad_norm 1.6129 (1.5007)	mem 14852MB
[2022-11-06 18:51:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][750/1251]	eta 0:03:55 lr 0.000564	time 0.4744 (0.4697)	loss 2.4489 (3.3647)	grad_norm 1.3930 (1.5028)	mem 14852MB
[2022-11-06 18:51:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][800/1251]	eta 0:03:31 lr 0.000564	time 0.4715 (0.4698)	loss 2.6145 (3.3691)	grad_norm 1.5563 (1.5057)	mem 14852MB
[2022-11-06 18:51:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][850/1251]	eta 0:03:08 lr 0.000564	time 0.4606 (0.4696)	loss 4.0193 (3.3755)	grad_norm 1.5458 (1.5049)	mem 14852MB
[2022-11-06 18:52:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][900/1251]	eta 0:02:44 lr 0.000563	time 0.4637 (0.4694)	loss 2.6416 (3.3818)	grad_norm 1.5533 (1.5060)	mem 14852MB
[2022-11-06 18:52:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][950/1251]	eta 0:02:21 lr 0.000563	time 0.4723 (0.4693)	loss 3.5321 (3.3879)	grad_norm 1.5255 (1.5072)	mem 14852MB
[2022-11-06 18:52:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][1000/1251]	eta 0:01:57 lr 0.000563	time 0.4608 (0.4691)	loss 4.1584 (3.3899)	grad_norm 1.5122 (1.5082)	mem 14852MB
[2022-11-06 18:53:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][1050/1251]	eta 0:01:34 lr 0.000563	time 0.4590 (0.4693)	loss 2.6237 (3.3906)	grad_norm 1.5459 (1.5084)	mem 14852MB
[2022-11-06 18:53:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][1100/1251]	eta 0:01:10 lr 0.000563	time 0.4683 (0.4692)	loss 3.6136 (3.3924)	grad_norm 1.4573 (1.5082)	mem 14852MB
[2022-11-06 18:54:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][1150/1251]	eta 0:00:47 lr 0.000562	time 0.4577 (0.4691)	loss 4.3031 (3.3942)	grad_norm 1.4381 (1.5088)	mem 14852MB
[2022-11-06 18:54:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][1200/1251]	eta 0:00:23 lr 0.000562	time 0.4597 (0.4690)	loss 3.6694 (3.3904)	grad_norm 1.5499 (1.5095)	mem 14852MB
[2022-11-06 18:54:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [138/300][1250/1251]	eta 0:00:00 lr 0.000562	time 0.4576 (0.4687)	loss 3.5941 (3.3944)	grad_norm 1.4020 (1.5083)	mem 14852MB
[2022-11-06 18:54:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 138 training takes 0:09:46
[2022-11-06 18:54:55 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_138.pth saving......
[2022-11-06 18:54:55 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_138.pth saved !!!
[2022-11-06 18:54:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.536 (1.536)	Loss 0.9600 (0.9600)	Acc@1 77.832 (77.832)	Acc@5 94.238 (94.238)	Mem 14852MB
[2022-11-06 18:55:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.474 Acc@5 94.104
[2022-11-06 18:55:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.5%
[2022-11-06 18:55:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.619 (1.619)	Loss 0.8969 (0.8969)	Acc@1 79.883 (79.883)	Acc@5 93.262 (93.262)	Mem 14852MB
[2022-11-06 18:55:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.774 Acc@5 95.126
[2022-11-06 18:55:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.8%
[2022-11-06 18:55:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.77% at 138 epoch
[2022-11-06 18:55:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][0/1251]	eta 0:40:01 lr 0.000562	time 1.9194 (1.9194)	loss 2.5239 (2.5239)	grad_norm 1.4744 (1.4744)	mem 14852MB
[2022-11-06 18:55:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][50/1251]	eta 0:10:00 lr 0.000562	time 0.4707 (0.5001)	loss 3.6603 (3.3376)	grad_norm 1.4654 (1.5341)	mem 14852MB
[2022-11-06 18:56:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][100/1251]	eta 0:09:18 lr 0.000561	time 0.4733 (0.4855)	loss 3.5810 (3.3305)	grad_norm 1.3556 (1.5341)	mem 14852MB
[2022-11-06 18:56:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][150/1251]	eta 0:08:48 lr 0.000561	time 0.4648 (0.4797)	loss 3.8116 (3.3870)	grad_norm 1.4530 (1.5296)	mem 14852MB
[2022-11-06 18:56:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][200/1251]	eta 0:08:20 lr 0.000561	time 0.4626 (0.4760)	loss 2.2439 (3.3848)	grad_norm 1.3982 (1.5244)	mem 14852MB
[2022-11-06 18:57:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][250/1251]	eta 0:07:54 lr 0.000561	time 0.4634 (0.4743)	loss 3.5266 (3.3950)	grad_norm 1.7818 (1.5177)	mem 14852MB
[2022-11-06 18:57:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][300/1251]	eta 0:07:29 lr 0.000561	time 0.4540 (0.4728)	loss 4.0300 (3.3837)	grad_norm 1.7544 (1.5132)	mem 14852MB
[2022-11-06 18:57:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][350/1251]	eta 0:07:05 lr 0.000560	time 0.4660 (0.4718)	loss 3.6394 (3.3744)	grad_norm 1.4905 (1.5117)	mem 14852MB
[2022-11-06 18:58:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][400/1251]	eta 0:06:40 lr 0.000560	time 0.4783 (0.4710)	loss 3.1373 (3.3895)	grad_norm 1.7394 (1.5181)	mem 14852MB
[2022-11-06 18:58:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][450/1251]	eta 0:06:16 lr 0.000560	time 0.4633 (0.4703)	loss 2.7570 (3.3929)	grad_norm 1.3412 (1.5158)	mem 14852MB
[2022-11-06 18:59:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][500/1251]	eta 0:05:52 lr 0.000560	time 0.4648 (0.4700)	loss 3.0470 (3.3929)	grad_norm 1.6115 (1.5168)	mem 14852MB
[2022-11-06 18:59:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][550/1251]	eta 0:05:29 lr 0.000560	time 0.4685 (0.4704)	loss 3.2710 (3.3919)	grad_norm 1.3645 (nan)	mem 14852MB
[2022-11-06 18:59:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][600/1251]	eta 0:05:05 lr 0.000559	time 0.4617 (0.4699)	loss 3.7187 (3.3917)	grad_norm 1.3729 (nan)	mem 14852MB
[2022-11-06 19:00:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][650/1251]	eta 0:04:42 lr 0.000559	time 0.4567 (0.4697)	loss 2.4624 (3.3931)	grad_norm 1.6664 (nan)	mem 14852MB
[2022-11-06 19:00:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][700/1251]	eta 0:04:18 lr 0.000559	time 0.4636 (0.4695)	loss 4.0441 (3.3965)	grad_norm 1.5144 (nan)	mem 14852MB
[2022-11-06 19:01:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][750/1251]	eta 0:03:55 lr 0.000559	time 0.4679 (0.4692)	loss 3.7860 (3.3894)	grad_norm 1.3782 (nan)	mem 14852MB
[2022-11-06 19:01:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][800/1251]	eta 0:03:31 lr 0.000559	time 0.4666 (0.4694)	loss 3.2247 (3.3848)	grad_norm 1.4959 (nan)	mem 14852MB
[2022-11-06 19:01:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][850/1251]	eta 0:03:08 lr 0.000558	time 0.4601 (0.4693)	loss 4.0034 (3.3808)	grad_norm 1.5978 (nan)	mem 14852MB
[2022-11-06 19:02:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][900/1251]	eta 0:02:44 lr 0.000558	time 0.4636 (0.4692)	loss 4.1098 (3.3826)	grad_norm 1.9081 (nan)	mem 14852MB
[2022-11-06 19:02:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][950/1251]	eta 0:02:21 lr 0.000558	time 0.4685 (0.4689)	loss 3.9456 (3.3832)	grad_norm 1.4930 (nan)	mem 14852MB
[2022-11-06 19:03:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][1000/1251]	eta 0:01:57 lr 0.000558	time 0.4641 (0.4688)	loss 3.8463 (3.3809)	grad_norm 1.3913 (nan)	mem 14852MB
[2022-11-06 19:03:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][1050/1251]	eta 0:01:34 lr 0.000558	time 0.4645 (0.4690)	loss 2.9744 (3.3776)	grad_norm 1.5496 (nan)	mem 14852MB
[2022-11-06 19:03:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][1100/1251]	eta 0:01:10 lr 0.000557	time 0.4664 (0.4688)	loss 3.5197 (3.3791)	grad_norm 1.3460 (nan)	mem 14852MB
[2022-11-06 19:04:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][1150/1251]	eta 0:00:47 lr 0.000557	time 0.4637 (0.4688)	loss 3.4214 (3.3796)	grad_norm 1.3518 (nan)	mem 14852MB
[2022-11-06 19:04:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][1200/1251]	eta 0:00:23 lr 0.000557	time 0.4687 (0.4687)	loss 4.0871 (3.3815)	grad_norm 1.4683 (nan)	mem 14852MB
[2022-11-06 19:04:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [139/300][1250/1251]	eta 0:00:00 lr 0.000557	time 0.4596 (0.4685)	loss 3.9557 (3.3864)	grad_norm 1.4297 (nan)	mem 14852MB
[2022-11-06 19:04:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 139 training takes 0:09:46
[2022-11-06 19:04:59 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_139.pth saving......
[2022-11-06 19:05:00 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_139.pth saved !!!
[2022-11-06 19:05:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.487 (1.487)	Loss 1.0176 (1.0176)	Acc@1 76.172 (76.172)	Acc@5 91.797 (91.797)	Mem 14852MB
[2022-11-06 19:05:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.624 Acc@5 94.028
[2022-11-06 19:05:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.6%
[2022-11-06 19:05:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.644 (1.644)	Loss 0.8278 (0.8278)	Acc@1 80.273 (80.273)	Acc@5 95.410 (95.410)	Mem 14852MB
[2022-11-06 19:05:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.818 Acc@5 95.132
[2022-11-06 19:05:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.8%
[2022-11-06 19:05:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.82% at 139 epoch
[2022-11-06 19:05:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][0/1251]	eta 0:41:21 lr 0.000557	time 1.9834 (1.9834)	loss 4.1241 (4.1241)	grad_norm 1.5746 (1.5746)	mem 14852MB
[2022-11-06 19:05:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][50/1251]	eta 0:10:05 lr 0.000557	time 0.4653 (0.5039)	loss 3.9049 (3.4205)	grad_norm 1.4259 (1.4866)	mem 14852MB
[2022-11-06 19:06:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][100/1251]	eta 0:09:21 lr 0.000556	time 0.4551 (0.4880)	loss 2.8085 (3.4148)	grad_norm 1.3369 (1.4998)	mem 14852MB
[2022-11-06 19:06:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][150/1251]	eta 0:08:49 lr 0.000556	time 0.4778 (0.4811)	loss 3.6927 (3.3821)	grad_norm 1.4439 (1.4977)	mem 14852MB
[2022-11-06 19:06:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][200/1251]	eta 0:08:21 lr 0.000556	time 0.4600 (0.4773)	loss 3.5181 (3.3998)	grad_norm 1.5963 (1.4996)	mem 14852MB
[2022-11-06 19:07:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][250/1251]	eta 0:07:55 lr 0.000556	time 0.4557 (0.4750)	loss 3.2043 (3.3897)	grad_norm 1.3223 (1.5034)	mem 14852MB
[2022-11-06 19:07:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][300/1251]	eta 0:07:30 lr 0.000556	time 0.4657 (0.4736)	loss 3.8813 (3.3928)	grad_norm 1.5150 (1.5107)	mem 14852MB
[2022-11-06 19:08:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][350/1251]	eta 0:07:05 lr 0.000555	time 0.4720 (0.4727)	loss 3.2112 (3.3884)	grad_norm 1.3308 (1.5115)	mem 14852MB
[2022-11-06 19:08:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][400/1251]	eta 0:06:41 lr 0.000555	time 0.4669 (0.4722)	loss 3.3292 (3.3999)	grad_norm 1.4173 (1.5150)	mem 14852MB
[2022-11-06 19:08:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][450/1251]	eta 0:06:17 lr 0.000555	time 0.4803 (0.4715)	loss 3.0469 (3.3985)	grad_norm 1.5458 (1.5127)	mem 14852MB
[2022-11-06 19:09:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][500/1251]	eta 0:05:53 lr 0.000555	time 0.4668 (0.4709)	loss 3.3594 (3.4051)	grad_norm 1.4388 (1.5135)	mem 14852MB
[2022-11-06 19:09:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][550/1251]	eta 0:05:30 lr 0.000554	time 0.4657 (0.4712)	loss 4.0282 (3.4049)	grad_norm 1.5536 (1.5111)	mem 14852MB
[2022-11-06 19:10:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][600/1251]	eta 0:05:06 lr 0.000554	time 0.4648 (0.4709)	loss 2.2440 (3.3996)	grad_norm 1.5121 (1.5121)	mem 14852MB
[2022-11-06 19:10:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][650/1251]	eta 0:04:42 lr 0.000554	time 0.4647 (0.4705)	loss 3.5415 (3.4070)	grad_norm 1.5319 (1.5102)	mem 14852MB
[2022-11-06 19:10:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][700/1251]	eta 0:04:19 lr 0.000554	time 0.4689 (0.4703)	loss 3.2671 (3.4079)	grad_norm 1.4858 (1.5108)	mem 14852MB
[2022-11-06 19:11:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][750/1251]	eta 0:03:55 lr 0.000554	time 0.4621 (0.4701)	loss 4.0855 (3.4048)	grad_norm 1.6737 (1.5113)	mem 14852MB
[2022-11-06 19:11:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][800/1251]	eta 0:03:32 lr 0.000553	time 0.4622 (0.4703)	loss 3.7709 (3.3952)	grad_norm 1.6234 (1.5144)	mem 14852MB
[2022-11-06 19:11:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][850/1251]	eta 0:03:08 lr 0.000553	time 0.4598 (0.4702)	loss 3.1661 (3.3973)	grad_norm 1.5907 (1.5140)	mem 14852MB
[2022-11-06 19:12:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][900/1251]	eta 0:02:44 lr 0.000553	time 0.4633 (0.4700)	loss 2.7948 (3.3996)	grad_norm 1.6082 (1.5156)	mem 14852MB
[2022-11-06 19:12:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][950/1251]	eta 0:02:21 lr 0.000553	time 0.4604 (0.4699)	loss 3.8293 (3.3946)	grad_norm 1.6772 (1.5166)	mem 14852MB
[2022-11-06 19:13:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][1000/1251]	eta 0:01:57 lr 0.000553	time 0.4666 (0.4697)	loss 3.2586 (3.3946)	grad_norm 1.5836 (1.5173)	mem 14852MB
[2022-11-06 19:13:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][1050/1251]	eta 0:01:34 lr 0.000552	time 0.4693 (0.4698)	loss 3.7039 (3.3942)	grad_norm 1.6246 (1.5159)	mem 14852MB
[2022-11-06 19:13:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][1100/1251]	eta 0:01:10 lr 0.000552	time 0.4528 (0.4698)	loss 3.7205 (3.3951)	grad_norm 1.3785 (1.5174)	mem 14852MB
[2022-11-06 19:14:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][1150/1251]	eta 0:00:47 lr 0.000552	time 0.4652 (0.4696)	loss 3.6776 (3.3980)	grad_norm 1.5992 (1.5177)	mem 14852MB
[2022-11-06 19:14:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][1200/1251]	eta 0:00:23 lr 0.000552	time 0.4686 (0.4695)	loss 3.8423 (3.4020)	grad_norm 1.6179 (1.5180)	mem 14852MB
[2022-11-06 19:15:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [140/300][1250/1251]	eta 0:00:00 lr 0.000552	time 0.4565 (0.4694)	loss 3.1294 (3.4056)	grad_norm 1.5960 (1.5178)	mem 14852MB
[2022-11-06 19:15:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 140 training takes 0:09:47
[2022-11-06 19:15:04 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_140.pth saving......
[2022-11-06 19:15:05 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_140.pth saved !!!
[2022-11-06 19:15:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.541 (1.541)	Loss 0.9982 (0.9982)	Acc@1 77.539 (77.539)	Acc@5 93.848 (93.848)	Mem 14852MB
[2022-11-06 19:15:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.714 Acc@5 94.274
[2022-11-06 19:15:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.7%
[2022-11-06 19:15:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.618 (1.618)	Loss 0.8534 (0.8534)	Acc@1 78.809 (78.809)	Acc@5 94.922 (94.922)	Mem 14852MB
[2022-11-06 19:15:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.884 Acc@5 95.168
[2022-11-06 19:15:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.9%
[2022-11-06 19:15:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.88% at 140 epoch
[2022-11-06 19:15:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][0/1251]	eta 0:41:49 lr 0.000552	time 2.0062 (2.0062)	loss 2.4507 (2.4507)	grad_norm 1.4593 (1.4593)	mem 14852MB
[2022-11-06 19:15:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][50/1251]	eta 0:10:01 lr 0.000551	time 0.4656 (0.5009)	loss 3.7540 (3.4025)	grad_norm 1.3968 (1.4917)	mem 14852MB
[2022-11-06 19:16:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][100/1251]	eta 0:09:19 lr 0.000551	time 0.4568 (0.4859)	loss 2.6760 (3.3475)	grad_norm 1.5714 (1.5066)	mem 14852MB
[2022-11-06 19:16:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][150/1251]	eta 0:08:47 lr 0.000551	time 0.4611 (0.4790)	loss 2.4544 (3.2891)	grad_norm 1.4316 (1.5166)	mem 14852MB
[2022-11-06 19:16:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][200/1251]	eta 0:08:20 lr 0.000551	time 0.4676 (0.4758)	loss 3.4550 (3.2768)	grad_norm 1.4166 (1.5100)	mem 14852MB
[2022-11-06 19:17:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][250/1251]	eta 0:07:54 lr 0.000551	time 0.4544 (0.4743)	loss 2.9287 (3.2941)	grad_norm 1.5918 (1.5077)	mem 14852MB
[2022-11-06 19:17:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][300/1251]	eta 0:07:29 lr 0.000550	time 0.4655 (0.4728)	loss 2.3810 (3.3004)	grad_norm 1.3023 (1.5131)	mem 14852MB
[2022-11-06 19:18:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][350/1251]	eta 0:07:04 lr 0.000550	time 0.4827 (0.4717)	loss 3.6567 (3.2977)	grad_norm 1.5489 (inf)	mem 14852MB
[2022-11-06 19:18:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][400/1251]	eta 0:06:40 lr 0.000550	time 0.4635 (0.4708)	loss 3.6673 (3.3108)	grad_norm 1.3931 (inf)	mem 14852MB
[2022-11-06 19:18:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][450/1251]	eta 0:06:16 lr 0.000550	time 0.4652 (0.4702)	loss 3.8898 (3.3104)	grad_norm 1.5960 (inf)	mem 14852MB
[2022-11-06 19:19:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][500/1251]	eta 0:05:52 lr 0.000550	time 0.4563 (0.4697)	loss 3.0273 (3.3147)	grad_norm 1.4015 (inf)	mem 14852MB
[2022-11-06 19:19:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][550/1251]	eta 0:05:29 lr 0.000549	time 0.4705 (0.4699)	loss 3.9656 (3.3226)	grad_norm 1.7028 (inf)	mem 14852MB
[2022-11-06 19:20:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][600/1251]	eta 0:05:05 lr 0.000549	time 0.4617 (0.4696)	loss 3.8450 (3.3285)	grad_norm 1.4411 (inf)	mem 14852MB
[2022-11-06 19:20:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][650/1251]	eta 0:04:42 lr 0.000549	time 0.4685 (0.4693)	loss 3.1337 (3.3384)	grad_norm 1.4436 (inf)	mem 14852MB
[2022-11-06 19:20:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][700/1251]	eta 0:04:18 lr 0.000549	time 0.4588 (0.4690)	loss 3.7105 (3.3419)	grad_norm 1.6361 (inf)	mem 14852MB
[2022-11-06 19:21:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][750/1251]	eta 0:03:54 lr 0.000548	time 0.4705 (0.4689)	loss 2.7913 (3.3448)	grad_norm 1.4297 (inf)	mem 14852MB
[2022-11-06 19:21:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][800/1251]	eta 0:03:31 lr 0.000548	time 0.4633 (0.4691)	loss 3.1760 (3.3406)	grad_norm 1.6395 (inf)	mem 14852MB
[2022-11-06 19:22:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][850/1251]	eta 0:03:08 lr 0.000548	time 0.4681 (0.4690)	loss 2.7990 (3.3534)	grad_norm 1.6207 (inf)	mem 14852MB
[2022-11-06 19:22:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][900/1251]	eta 0:02:44 lr 0.000548	time 0.4614 (0.4688)	loss 2.9130 (3.3544)	grad_norm 1.5086 (inf)	mem 14852MB
[2022-11-06 19:22:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][950/1251]	eta 0:02:21 lr 0.000548	time 0.4622 (0.4687)	loss 2.9829 (3.3585)	grad_norm 1.8548 (inf)	mem 14852MB
[2022-11-06 19:23:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][1000/1251]	eta 0:01:57 lr 0.000547	time 0.4712 (0.4686)	loss 3.8671 (3.3653)	grad_norm 1.5921 (inf)	mem 14852MB
[2022-11-06 19:23:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][1050/1251]	eta 0:01:34 lr 0.000547	time 0.4655 (0.4687)	loss 3.5741 (3.3749)	grad_norm 1.5855 (inf)	mem 14852MB
[2022-11-06 19:23:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][1100/1251]	eta 0:01:10 lr 0.000547	time 0.4713 (0.4688)	loss 2.9987 (3.3781)	grad_norm 1.5870 (inf)	mem 14852MB
[2022-11-06 19:24:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][1150/1251]	eta 0:00:47 lr 0.000547	time 0.4619 (0.4686)	loss 2.8410 (3.3865)	grad_norm 1.8897 (inf)	mem 14852MB
[2022-11-06 19:24:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][1200/1251]	eta 0:00:23 lr 0.000547	time 0.4650 (0.4686)	loss 3.2715 (3.3871)	grad_norm 1.7325 (inf)	mem 14852MB
[2022-11-06 19:25:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [141/300][1250/1251]	eta 0:00:00 lr 0.000546	time 0.4582 (0.4684)	loss 4.0157 (3.3894)	grad_norm 1.4717 (inf)	mem 14852MB
[2022-11-06 19:25:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 141 training takes 0:09:46
[2022-11-06 19:25:08 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_141.pth saving......
[2022-11-06 19:25:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_141.pth saved !!!
[2022-11-06 19:25:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.654 (1.654)	Loss 0.9420 (0.9420)	Acc@1 77.930 (77.930)	Acc@5 94.434 (94.434)	Mem 14852MB
[2022-11-06 19:25:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.736 Acc@5 94.196
[2022-11-06 19:25:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.7%
[2022-11-06 19:25:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.792 (1.792)	Loss 0.8875 (0.8875)	Acc@1 78.613 (78.613)	Acc@5 95.508 (95.508)	Mem 14852MB
[2022-11-06 19:25:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.930 Acc@5 95.176
[2022-11-06 19:25:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.9%
[2022-11-06 19:25:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.93% at 141 epoch
[2022-11-06 19:25:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][0/1251]	eta 0:41:04 lr 0.000546	time 1.9703 (1.9703)	loss 2.5548 (2.5548)	grad_norm 1.4621 (1.4621)	mem 14852MB
[2022-11-06 19:25:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][50/1251]	eta 0:10:06 lr 0.000546	time 0.4524 (0.5047)	loss 3.4391 (3.3760)	grad_norm 1.6295 (1.5210)	mem 14852MB
[2022-11-06 19:26:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][100/1251]	eta 0:09:19 lr 0.000546	time 0.4615 (0.4865)	loss 3.4749 (3.3555)	grad_norm 1.5472 (1.5303)	mem 14852MB
[2022-11-06 19:26:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][150/1251]	eta 0:08:49 lr 0.000546	time 0.4649 (0.4805)	loss 3.1088 (3.3485)	grad_norm 1.3940 (1.5305)	mem 14852MB
[2022-11-06 19:27:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][200/1251]	eta 0:08:21 lr 0.000546	time 0.4619 (0.4767)	loss 3.9012 (3.3437)	grad_norm 1.5333 (1.5340)	mem 14852MB
[2022-11-06 19:27:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][250/1251]	eta 0:07:54 lr 0.000545	time 0.4603 (0.4743)	loss 3.4345 (3.3535)	grad_norm 1.5128 (1.5269)	mem 14852MB
[2022-11-06 19:27:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][300/1251]	eta 0:07:29 lr 0.000545	time 0.4541 (0.4727)	loss 3.4423 (3.3626)	grad_norm 1.6626 (1.5214)	mem 14852MB
[2022-11-06 19:28:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][350/1251]	eta 0:07:05 lr 0.000545	time 0.4589 (0.4720)	loss 2.8036 (3.3568)	grad_norm 1.5020 (1.5218)	mem 14852MB
[2022-11-06 19:28:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][400/1251]	eta 0:06:40 lr 0.000545	time 0.4668 (0.4711)	loss 3.5112 (3.3512)	grad_norm 1.5151 (1.5267)	mem 14852MB
[2022-11-06 19:28:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][450/1251]	eta 0:06:16 lr 0.000545	time 0.4652 (0.4704)	loss 3.0915 (3.3444)	grad_norm 1.3956 (1.5293)	mem 14852MB
[2022-11-06 19:29:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][500/1251]	eta 0:05:52 lr 0.000544	time 0.4682 (0.4699)	loss 3.1803 (3.3552)	grad_norm 1.6270 (1.5306)	mem 14852MB
[2022-11-06 19:29:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][550/1251]	eta 0:05:29 lr 0.000544	time 0.4543 (0.4699)	loss 3.5019 (3.3478)	grad_norm 1.3869 (1.5327)	mem 14852MB
[2022-11-06 19:30:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][600/1251]	eta 0:05:05 lr 0.000544	time 0.4720 (0.4697)	loss 3.5481 (3.3523)	grad_norm 1.4895 (1.5307)	mem 14852MB
[2022-11-06 19:30:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][650/1251]	eta 0:04:42 lr 0.000544	time 0.4715 (0.4697)	loss 3.3521 (3.3551)	grad_norm 1.5591 (1.5281)	mem 14852MB
[2022-11-06 19:30:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][700/1251]	eta 0:04:18 lr 0.000544	time 0.4647 (0.4694)	loss 2.9721 (3.3550)	grad_norm 1.4964 (1.5266)	mem 14852MB
[2022-11-06 19:31:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][750/1251]	eta 0:03:55 lr 0.000543	time 0.4659 (0.4691)	loss 2.8159 (3.3570)	grad_norm 1.3540 (1.5305)	mem 14852MB
[2022-11-06 19:31:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][800/1251]	eta 0:03:31 lr 0.000543	time 0.4654 (0.4691)	loss 3.6559 (3.3616)	grad_norm 1.4872 (1.5302)	mem 14852MB
[2022-11-06 19:32:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][850/1251]	eta 0:03:08 lr 0.000543	time 0.4593 (0.4690)	loss 3.5274 (3.3614)	grad_norm 1.4724 (1.5307)	mem 14852MB
[2022-11-06 19:32:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][900/1251]	eta 0:02:44 lr 0.000543	time 0.4735 (0.4688)	loss 3.1731 (3.3694)	grad_norm 1.4138 (1.5297)	mem 14852MB
[2022-11-06 19:32:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][950/1251]	eta 0:02:21 lr 0.000542	time 0.4628 (0.4688)	loss 3.8139 (3.3740)	grad_norm 1.4871 (1.5282)	mem 14852MB
[2022-11-06 19:33:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][1000/1251]	eta 0:01:57 lr 0.000542	time 0.4771 (0.4686)	loss 3.4909 (3.3737)	grad_norm 1.5124 (1.5273)	mem 14852MB
[2022-11-06 19:33:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][1050/1251]	eta 0:01:34 lr 0.000542	time 0.4661 (0.4686)	loss 2.6932 (3.3743)	grad_norm 1.5051 (1.5279)	mem 14852MB
[2022-11-06 19:34:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][1100/1251]	eta 0:01:10 lr 0.000542	time 0.4607 (0.4686)	loss 3.6856 (3.3737)	grad_norm 1.4463 (1.5288)	mem 14852MB
[2022-11-06 19:34:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][1150/1251]	eta 0:00:47 lr 0.000542	time 0.4521 (0.4686)	loss 3.4837 (3.3700)	grad_norm 1.6268 (1.5291)	mem 14852MB
[2022-11-06 19:34:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][1200/1251]	eta 0:00:23 lr 0.000541	time 0.4614 (0.4684)	loss 2.4094 (3.3711)	grad_norm 1.3576 (1.5289)	mem 14852MB
[2022-11-06 19:35:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [142/300][1250/1251]	eta 0:00:00 lr 0.000541	time 0.4606 (0.4682)	loss 3.8844 (3.3713)	grad_norm 1.5071 (1.5283)	mem 14852MB
[2022-11-06 19:35:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 142 training takes 0:09:45
[2022-11-06 19:35:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_142.pth saving......
[2022-11-06 19:35:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_142.pth saved !!!
[2022-11-06 19:35:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.535 (1.535)	Loss 0.9480 (0.9480)	Acc@1 77.930 (77.930)	Acc@5 94.629 (94.629)	Mem 14852MB
[2022-11-06 19:35:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.758 Acc@5 94.126
[2022-11-06 19:35:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.8%
[2022-11-06 19:35:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.703 (1.703)	Loss 0.7406 (0.7406)	Acc@1 82.910 (82.910)	Acc@5 95.703 (95.703)	Mem 14852MB
[2022-11-06 19:35:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.936 Acc@5 95.188
[2022-11-06 19:35:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.9%
[2022-11-06 19:35:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.94% at 142 epoch
[2022-11-06 19:35:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][0/1251]	eta 0:41:48 lr 0.000541	time 2.0051 (2.0051)	loss 2.9361 (2.9361)	grad_norm 1.3952 (1.3952)	mem 14852MB
[2022-11-06 19:35:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][50/1251]	eta 0:10:00 lr 0.000541	time 0.4682 (0.5004)	loss 3.3396 (3.4058)	grad_norm 1.6379 (1.5282)	mem 14852MB
[2022-11-06 19:36:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][100/1251]	eta 0:09:18 lr 0.000541	time 0.4601 (0.4853)	loss 3.4174 (3.4086)	grad_norm 1.3860 (1.5309)	mem 14852MB
[2022-11-06 19:36:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][150/1251]	eta 0:08:48 lr 0.000541	time 0.4553 (0.4799)	loss 3.7595 (3.3775)	grad_norm 1.4402 (1.5293)	mem 14852MB
[2022-11-06 19:37:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][200/1251]	eta 0:08:21 lr 0.000540	time 0.4569 (0.4769)	loss 2.8522 (3.3964)	grad_norm 1.3531 (1.5252)	mem 14852MB
[2022-11-06 19:37:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][250/1251]	eta 0:07:55 lr 0.000540	time 0.4692 (0.4749)	loss 3.7177 (3.3967)	grad_norm 1.6723 (1.5264)	mem 14852MB
[2022-11-06 19:37:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][300/1251]	eta 0:07:30 lr 0.000540	time 0.4590 (0.4735)	loss 3.5538 (3.3746)	grad_norm 1.5908 (1.5279)	mem 14852MB
[2022-11-06 19:38:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][350/1251]	eta 0:07:05 lr 0.000540	time 0.4731 (0.4724)	loss 2.3649 (3.3742)	grad_norm 1.4293 (1.5261)	mem 14852MB
[2022-11-06 19:38:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][400/1251]	eta 0:06:41 lr 0.000540	time 0.4693 (0.4715)	loss 3.8640 (3.3672)	grad_norm 1.3327 (1.5265)	mem 14852MB
[2022-11-06 19:39:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][450/1251]	eta 0:06:17 lr 0.000539	time 0.4596 (0.4709)	loss 3.9386 (3.3565)	grad_norm 1.6654 (1.5234)	mem 14852MB
[2022-11-06 19:39:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][500/1251]	eta 0:05:53 lr 0.000539	time 0.4587 (0.4706)	loss 3.7884 (3.3526)	grad_norm 1.5263 (1.5225)	mem 14852MB
[2022-11-06 19:39:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][550/1251]	eta 0:05:29 lr 0.000539	time 0.4665 (0.4705)	loss 3.3312 (3.3576)	grad_norm 1.3861 (1.5219)	mem 14852MB
[2022-11-06 19:40:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][600/1251]	eta 0:05:06 lr 0.000539	time 0.4613 (0.4702)	loss 3.6904 (3.3641)	grad_norm 1.5171 (1.5224)	mem 14852MB
[2022-11-06 19:40:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][650/1251]	eta 0:04:42 lr 0.000539	time 0.4628 (0.4699)	loss 3.8486 (3.3562)	grad_norm 1.5823 (1.5253)	mem 14852MB
[2022-11-06 19:41:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][700/1251]	eta 0:04:18 lr 0.000538	time 0.5401 (0.4697)	loss 3.9737 (3.3533)	grad_norm 1.4734 (1.5282)	mem 14852MB
[2022-11-06 19:41:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][750/1251]	eta 0:03:55 lr 0.000538	time 0.4530 (0.4696)	loss 2.7269 (3.3460)	grad_norm 1.3536 (1.5296)	mem 14852MB
[2022-11-06 19:41:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][800/1251]	eta 0:03:31 lr 0.000538	time 0.4632 (0.4695)	loss 3.8184 (3.3429)	grad_norm 1.6974 (1.5294)	mem 14852MB
[2022-11-06 19:42:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][850/1251]	eta 0:03:08 lr 0.000538	time 0.4658 (0.4694)	loss 3.3350 (3.3487)	grad_norm 1.5418 (1.5289)	mem 14852MB
[2022-11-06 19:42:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][900/1251]	eta 0:02:44 lr 0.000538	time 0.4622 (0.4691)	loss 3.9585 (3.3496)	grad_norm 1.5300 (1.5276)	mem 14852MB
[2022-11-06 19:42:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][950/1251]	eta 0:02:21 lr 0.000537	time 0.4697 (0.4691)	loss 2.4212 (3.3508)	grad_norm 1.4626 (1.5283)	mem 14852MB
[2022-11-06 19:43:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][1000/1251]	eta 0:01:57 lr 0.000537	time 0.4650 (0.4690)	loss 2.2939 (3.3460)	grad_norm 1.4739 (1.5293)	mem 14852MB
[2022-11-06 19:43:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][1050/1251]	eta 0:01:34 lr 0.000537	time 0.4634 (0.4691)	loss 3.1799 (3.3422)	grad_norm 1.4748 (1.5297)	mem 14852MB
[2022-11-06 19:44:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][1100/1251]	eta 0:01:10 lr 0.000537	time 0.4769 (0.4691)	loss 3.5187 (3.3438)	grad_norm 1.5596 (1.5298)	mem 14852MB
[2022-11-06 19:44:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][1150/1251]	eta 0:00:47 lr 0.000536	time 0.4547 (0.4690)	loss 3.0174 (3.3473)	grad_norm 1.4444 (1.5303)	mem 14852MB
[2022-11-06 19:44:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][1200/1251]	eta 0:00:23 lr 0.000536	time 0.4666 (0.4689)	loss 4.0182 (3.3518)	grad_norm 1.6307 (1.5316)	mem 14852MB
[2022-11-06 19:45:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [143/300][1250/1251]	eta 0:00:00 lr 0.000536	time 0.4579 (0.4687)	loss 3.2087 (3.3564)	grad_norm 1.6037 (1.5326)	mem 14852MB
[2022-11-06 19:45:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 143 training takes 0:09:46
[2022-11-06 19:45:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_143.pth saving......
[2022-11-06 19:45:18 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_143.pth saved !!!
[2022-11-06 19:45:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.478 (1.478)	Loss 1.0237 (1.0237)	Acc@1 74.512 (74.512)	Acc@5 93.848 (93.848)	Mem 14852MB
[2022-11-06 19:45:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.730 Acc@5 94.322
[2022-11-06 19:45:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.7%
[2022-11-06 19:45:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.563 (1.563)	Loss 0.8760 (0.8760)	Acc@1 78.516 (78.516)	Acc@5 94.531 (94.531)	Mem 14852MB
[2022-11-06 19:45:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.990 Acc@5 95.214
[2022-11-06 19:45:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.0%
[2022-11-06 19:45:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.99% at 143 epoch
[2022-11-06 19:45:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][0/1251]	eta 0:40:59 lr 0.000536	time 1.9664 (1.9664)	loss 3.5881 (3.5881)	grad_norm 1.6043 (1.6043)	mem 14852MB
[2022-11-06 19:46:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][50/1251]	eta 0:10:04 lr 0.000536	time 0.4554 (0.5030)	loss 3.3111 (3.2463)	grad_norm 1.3703 (1.5430)	mem 14852MB
[2022-11-06 19:46:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][100/1251]	eta 0:09:20 lr 0.000536	time 0.4724 (0.4870)	loss 3.9107 (3.3587)	grad_norm 1.4523 (1.5561)	mem 14852MB
[2022-11-06 19:46:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][150/1251]	eta 0:08:48 lr 0.000535	time 0.4599 (0.4803)	loss 3.6770 (3.3145)	grad_norm 1.6162 (1.5463)	mem 14852MB
[2022-11-06 19:47:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][200/1251]	eta 0:08:21 lr 0.000535	time 0.4701 (0.4767)	loss 3.1774 (3.3112)	grad_norm 1.4993 (1.5441)	mem 14852MB
[2022-11-06 19:47:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][250/1251]	eta 0:07:55 lr 0.000535	time 0.4682 (0.4746)	loss 3.7131 (3.3213)	grad_norm 1.4366 (1.5452)	mem 14852MB
[2022-11-06 19:47:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][300/1251]	eta 0:07:30 lr 0.000535	time 0.4577 (0.4733)	loss 3.4571 (3.3258)	grad_norm 1.4065 (inf)	mem 14852MB
[2022-11-06 19:48:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][350/1251]	eta 0:07:05 lr 0.000535	time 0.4735 (0.4722)	loss 3.4901 (3.3380)	grad_norm 1.6982 (inf)	mem 14852MB
[2022-11-06 19:48:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][400/1251]	eta 0:06:41 lr 0.000534	time 0.4590 (0.4715)	loss 3.8752 (3.3393)	grad_norm 1.4582 (inf)	mem 14852MB
[2022-11-06 19:49:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][450/1251]	eta 0:06:17 lr 0.000534	time 0.4736 (0.4710)	loss 4.2812 (3.3500)	grad_norm 1.5981 (inf)	mem 14852MB
[2022-11-06 19:49:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][500/1251]	eta 0:05:53 lr 0.000534	time 0.4643 (0.4704)	loss 2.6339 (3.3448)	grad_norm 1.7212 (inf)	mem 14852MB
[2022-11-06 19:49:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][550/1251]	eta 0:05:29 lr 0.000534	time 0.4632 (0.4705)	loss 3.7721 (3.3505)	grad_norm 1.3641 (inf)	mem 14852MB
[2022-11-06 19:50:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][600/1251]	eta 0:05:06 lr 0.000534	time 0.4573 (0.4704)	loss 2.9959 (3.3571)	grad_norm 1.5693 (inf)	mem 14852MB
[2022-11-06 19:50:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][650/1251]	eta 0:04:42 lr 0.000533	time 0.4627 (0.4699)	loss 4.1839 (3.3505)	grad_norm 1.6474 (inf)	mem 14852MB
[2022-11-06 19:51:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][700/1251]	eta 0:04:18 lr 0.000533	time 0.4639 (0.4696)	loss 3.3398 (3.3445)	grad_norm 1.5632 (inf)	mem 14852MB
[2022-11-06 19:51:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][750/1251]	eta 0:03:55 lr 0.000533	time 0.4637 (0.4694)	loss 2.2111 (3.3414)	grad_norm 1.4928 (inf)	mem 14852MB
[2022-11-06 19:51:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][800/1251]	eta 0:03:31 lr 0.000533	time 0.4715 (0.4694)	loss 3.8092 (3.3497)	grad_norm 1.4227 (inf)	mem 14852MB
[2022-11-06 19:52:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][850/1251]	eta 0:03:08 lr 0.000533	time 0.4626 (0.4694)	loss 2.5676 (3.3467)	grad_norm 1.4781 (inf)	mem 14852MB
[2022-11-06 19:52:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][900/1251]	eta 0:02:44 lr 0.000532	time 0.4558 (0.4694)	loss 3.9540 (3.3471)	grad_norm 1.5724 (inf)	mem 14852MB
[2022-11-06 19:53:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][950/1251]	eta 0:02:21 lr 0.000532	time 0.4873 (0.4692)	loss 2.5542 (3.3542)	grad_norm 1.3620 (inf)	mem 14852MB
[2022-11-06 19:53:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][1000/1251]	eta 0:01:57 lr 0.000532	time 0.4654 (0.4690)	loss 3.6609 (3.3484)	grad_norm 1.5450 (inf)	mem 14852MB
[2022-11-06 19:53:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][1050/1251]	eta 0:01:34 lr 0.000532	time 0.4628 (0.4690)	loss 3.5698 (3.3492)	grad_norm 1.4229 (inf)	mem 14852MB
[2022-11-06 19:54:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][1100/1251]	eta 0:01:10 lr 0.000532	time 0.4650 (0.4690)	loss 3.1804 (3.3537)	grad_norm 1.7298 (inf)	mem 14852MB
[2022-11-06 19:54:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][1150/1251]	eta 0:00:47 lr 0.000531	time 0.4589 (0.4690)	loss 3.5019 (3.3521)	grad_norm 1.3538 (inf)	mem 14852MB
[2022-11-06 19:54:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][1200/1251]	eta 0:00:23 lr 0.000531	time 0.4630 (0.4688)	loss 4.2365 (3.3544)	grad_norm 1.6709 (inf)	mem 14852MB
[2022-11-06 19:55:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [144/300][1250/1251]	eta 0:00:00 lr 0.000531	time 0.4567 (0.4686)	loss 2.8221 (3.3551)	grad_norm 1.5944 (inf)	mem 14852MB
[2022-11-06 19:55:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 144 training takes 0:09:46
[2022-11-06 19:55:22 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_144.pth saving......
[2022-11-06 19:55:22 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_144.pth saved !!!
[2022-11-06 19:55:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.519 (1.519)	Loss 0.9058 (0.9058)	Acc@1 78.516 (78.516)	Acc@5 94.629 (94.629)	Mem 14852MB
[2022-11-06 19:55:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.712 Acc@5 94.226
[2022-11-06 19:55:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.7%
[2022-11-06 19:55:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.686 (1.686)	Loss 0.8284 (0.8284)	Acc@1 80.469 (80.469)	Acc@5 94.922 (94.922)	Mem 14852MB
[2022-11-06 19:55:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.004 Acc@5 95.228
[2022-11-06 19:55:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.0%
[2022-11-06 19:55:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.00% at 144 epoch
[2022-11-06 19:55:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][0/1251]	eta 0:41:46 lr 0.000531	time 2.0035 (2.0035)	loss 2.3729 (2.3729)	grad_norm 1.4993 (1.4993)	mem 14852MB
[2022-11-06 19:56:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][50/1251]	eta 0:10:03 lr 0.000531	time 0.4640 (0.5026)	loss 3.3323 (3.2701)	grad_norm 1.6703 (1.5359)	mem 14852MB
[2022-11-06 19:56:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][100/1251]	eta 0:09:20 lr 0.000530	time 0.4827 (0.4870)	loss 2.6418 (3.3130)	grad_norm 1.5218 (1.5526)	mem 14852MB
[2022-11-06 19:56:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][150/1251]	eta 0:08:48 lr 0.000530	time 0.4793 (0.4800)	loss 3.1546 (3.3722)	grad_norm 1.6496 (1.5485)	mem 14852MB
[2022-11-06 19:57:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][200/1251]	eta 0:08:21 lr 0.000530	time 0.4705 (0.4770)	loss 2.8608 (3.3759)	grad_norm 1.4218 (1.5469)	mem 14852MB
[2022-11-06 19:57:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][250/1251]	eta 0:07:55 lr 0.000530	time 0.4720 (0.4747)	loss 2.4404 (3.3693)	grad_norm 1.4395 (1.5457)	mem 14852MB
[2022-11-06 19:58:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][300/1251]	eta 0:07:30 lr 0.000530	time 0.4625 (0.4734)	loss 3.8223 (3.3905)	grad_norm 1.4158 (1.5488)	mem 14852MB
[2022-11-06 19:58:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][350/1251]	eta 0:07:05 lr 0.000529	time 0.4632 (0.4723)	loss 3.4837 (3.3756)	grad_norm 1.6322 (1.5499)	mem 14852MB
[2022-11-06 19:58:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][400/1251]	eta 0:06:41 lr 0.000529	time 0.4619 (0.4715)	loss 3.5202 (3.3806)	grad_norm 1.4771 (1.5442)	mem 14852MB
[2022-11-06 19:59:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][450/1251]	eta 0:06:17 lr 0.000529	time 0.4672 (0.4709)	loss 2.8804 (3.3683)	grad_norm 1.8956 (1.5413)	mem 14852MB
[2022-11-06 19:59:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][500/1251]	eta 0:05:53 lr 0.000529	time 0.4754 (0.4706)	loss 2.9508 (3.3647)	grad_norm 1.5034 (1.5423)	mem 14852MB
[2022-11-06 19:59:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][550/1251]	eta 0:05:29 lr 0.000529	time 0.4642 (0.4706)	loss 3.6204 (3.3738)	grad_norm 1.6789 (1.5421)	mem 14852MB
[2022-11-06 20:00:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][600/1251]	eta 0:05:06 lr 0.000528	time 0.4723 (0.4702)	loss 3.5093 (3.3691)	grad_norm 1.4907 (1.5416)	mem 14852MB
[2022-11-06 20:00:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][650/1251]	eta 0:04:42 lr 0.000528	time 0.4598 (0.4700)	loss 3.8537 (3.3699)	grad_norm 1.4449 (1.5422)	mem 14852MB
[2022-11-06 20:01:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][700/1251]	eta 0:04:18 lr 0.000528	time 0.4603 (0.4697)	loss 3.6673 (3.3691)	grad_norm 1.5594 (1.5459)	mem 14852MB
[2022-11-06 20:01:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][750/1251]	eta 0:03:55 lr 0.000528	time 0.4713 (0.4695)	loss 3.9210 (3.3653)	grad_norm 1.5864 (1.5429)	mem 14852MB
[2022-11-06 20:01:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][800/1251]	eta 0:03:31 lr 0.000528	time 0.4696 (0.4698)	loss 2.5730 (3.3681)	grad_norm 1.6071 (1.5459)	mem 14852MB
[2022-11-06 20:02:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][850/1251]	eta 0:03:08 lr 0.000527	time 0.4587 (0.4694)	loss 3.2780 (3.3655)	grad_norm 1.4365 (1.5481)	mem 14852MB
[2022-11-06 20:02:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][900/1251]	eta 0:02:44 lr 0.000527	time 0.4655 (0.4692)	loss 3.4920 (3.3617)	grad_norm 1.4433 (1.5473)	mem 14852MB
[2022-11-06 20:03:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][950/1251]	eta 0:02:21 lr 0.000527	time 0.4623 (0.4692)	loss 3.5649 (3.3616)	grad_norm 1.6457 (1.5465)	mem 14852MB
[2022-11-06 20:03:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][1000/1251]	eta 0:01:57 lr 0.000527	time 0.4552 (0.4691)	loss 2.6348 (3.3614)	grad_norm 1.5049 (1.5479)	mem 14852MB
[2022-11-06 20:03:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][1050/1251]	eta 0:01:34 lr 0.000527	time 0.4731 (0.4693)	loss 3.7256 (3.3564)	grad_norm 2.2718 (1.5474)	mem 14852MB
[2022-11-06 20:04:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][1100/1251]	eta 0:01:10 lr 0.000526	time 0.4550 (0.4691)	loss 3.9583 (3.3584)	grad_norm 1.6120 (1.5471)	mem 14852MB
[2022-11-06 20:04:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][1150/1251]	eta 0:00:47 lr 0.000526	time 0.4661 (0.4689)	loss 3.5664 (3.3564)	grad_norm 1.5684 (1.5468)	mem 14852MB
[2022-11-06 20:05:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][1200/1251]	eta 0:00:23 lr 0.000526	time 0.4627 (0.4688)	loss 3.6613 (3.3618)	grad_norm 1.6297 (1.5483)	mem 14852MB
[2022-11-06 20:05:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [145/300][1250/1251]	eta 0:00:00 lr 0.000526	time 0.4584 (0.4687)	loss 3.6719 (3.3632)	grad_norm 1.5698 (1.5498)	mem 14852MB
[2022-11-06 20:05:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 145 training takes 0:09:46
[2022-11-06 20:05:26 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_145.pth saving......
[2022-11-06 20:05:27 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_145.pth saved !!!
[2022-11-06 20:05:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.683 (1.683)	Loss 0.9610 (0.9610)	Acc@1 77.148 (77.148)	Acc@5 93.066 (93.066)	Mem 14852MB
[2022-11-06 20:05:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.864 Acc@5 94.226
[2022-11-06 20:05:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.9%
[2022-11-06 20:05:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.741 (1.741)	Loss 0.9318 (0.9318)	Acc@1 78.613 (78.613)	Acc@5 94.141 (94.141)	Mem 14852MB
[2022-11-06 20:05:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.018 Acc@5 95.238
[2022-11-06 20:05:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.0%
[2022-11-06 20:05:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.02% at 145 epoch
[2022-11-06 20:05:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][0/1251]	eta 0:40:07 lr 0.000526	time 1.9243 (1.9243)	loss 3.8033 (3.8033)	grad_norm 1.5587 (1.5587)	mem 14852MB
[2022-11-06 20:06:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][50/1251]	eta 0:10:05 lr 0.000526	time 0.4693 (0.5040)	loss 3.9181 (3.2881)	grad_norm 1.5312 (1.5210)	mem 14852MB
[2022-11-06 20:06:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][100/1251]	eta 0:09:19 lr 0.000525	time 0.4625 (0.4861)	loss 2.3300 (3.2838)	grad_norm 1.6567 (1.5381)	mem 14852MB
[2022-11-06 20:06:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][150/1251]	eta 0:08:48 lr 0.000525	time 0.4741 (0.4799)	loss 3.2029 (3.2932)	grad_norm 1.8713 (1.5542)	mem 14852MB
[2022-11-06 20:07:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][200/1251]	eta 0:08:21 lr 0.000525	time 0.4577 (0.4775)	loss 2.1088 (3.3006)	grad_norm 1.9263 (1.5516)	mem 14852MB
[2022-11-06 20:07:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][250/1251]	eta 0:07:56 lr 0.000525	time 0.4694 (0.4756)	loss 3.2853 (3.3122)	grad_norm 1.5534 (1.5523)	mem 14852MB
[2022-11-06 20:08:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][300/1251]	eta 0:07:30 lr 0.000524	time 0.4658 (0.4739)	loss 2.6850 (3.3061)	grad_norm 1.5449 (1.5532)	mem 14852MB
[2022-11-06 20:08:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][350/1251]	eta 0:07:05 lr 0.000524	time 0.4540 (0.4727)	loss 2.2743 (3.3091)	grad_norm 1.6087 (1.5512)	mem 14852MB
[2022-11-06 20:08:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][400/1251]	eta 0:06:41 lr 0.000524	time 0.4785 (0.4721)	loss 4.1305 (3.3108)	grad_norm 1.5105 (1.5547)	mem 14852MB
[2022-11-06 20:09:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][450/1251]	eta 0:06:17 lr 0.000524	time 0.4667 (0.4714)	loss 2.9444 (3.3141)	grad_norm 1.4906 (1.5525)	mem 14852MB
[2022-11-06 20:09:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][500/1251]	eta 0:05:53 lr 0.000524	time 0.4647 (0.4714)	loss 2.5882 (3.3284)	grad_norm 1.5957 (1.5534)	mem 14852MB
[2022-11-06 20:10:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][550/1251]	eta 0:05:30 lr 0.000523	time 0.4629 (0.4714)	loss 2.8625 (3.3282)	grad_norm 1.5405 (1.5535)	mem 14852MB
[2022-11-06 20:10:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][600/1251]	eta 0:05:06 lr 0.000523	time 0.4637 (0.4709)	loss 3.9738 (3.3312)	grad_norm 2.0136 (1.5580)	mem 14852MB
[2022-11-06 20:10:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][650/1251]	eta 0:04:42 lr 0.000523	time 0.4735 (0.4706)	loss 3.8387 (3.3321)	grad_norm 1.8854 (1.5588)	mem 14852MB
[2022-11-06 20:11:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][700/1251]	eta 0:04:19 lr 0.000523	time 0.5362 (0.4703)	loss 2.3344 (3.3307)	grad_norm 1.7260 (1.5602)	mem 14852MB
[2022-11-06 20:11:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][750/1251]	eta 0:03:55 lr 0.000523	time 0.4694 (0.4701)	loss 3.6480 (3.3315)	grad_norm 1.4063 (1.5572)	mem 14852MB
[2022-11-06 20:12:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][800/1251]	eta 0:03:32 lr 0.000522	time 0.4597 (0.4702)	loss 3.5837 (3.3337)	grad_norm 1.6640 (1.5562)	mem 14852MB
[2022-11-06 20:12:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][850/1251]	eta 0:03:08 lr 0.000522	time 0.4624 (0.4700)	loss 3.1208 (3.3288)	grad_norm 1.5663 (1.5561)	mem 14852MB
[2022-11-06 20:12:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][900/1251]	eta 0:02:44 lr 0.000522	time 0.4831 (0.4698)	loss 2.7630 (3.3337)	grad_norm 1.5027 (1.5565)	mem 14852MB
[2022-11-06 20:13:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][950/1251]	eta 0:02:21 lr 0.000522	time 0.4703 (0.4697)	loss 2.3459 (3.3376)	grad_norm 1.5644 (1.5554)	mem 14852MB
[2022-11-06 20:13:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][1000/1251]	eta 0:01:57 lr 0.000522	time 0.4598 (0.4696)	loss 3.8451 (3.3392)	grad_norm 1.5628 (1.5545)	mem 14852MB
[2022-11-06 20:13:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][1050/1251]	eta 0:01:34 lr 0.000521	time 0.4631 (0.4697)	loss 3.5636 (3.3393)	grad_norm 1.7409 (1.5540)	mem 14852MB
[2022-11-06 20:14:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][1100/1251]	eta 0:01:10 lr 0.000521	time 0.4633 (0.4695)	loss 2.0128 (3.3402)	grad_norm 1.5443 (1.5541)	mem 14852MB
[2022-11-06 20:14:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][1150/1251]	eta 0:00:47 lr 0.000521	time 0.4678 (0.4694)	loss 3.1816 (3.3437)	grad_norm 1.4577 (1.5519)	mem 14852MB
[2022-11-06 20:15:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][1200/1251]	eta 0:00:23 lr 0.000521	time 0.5360 (0.4694)	loss 4.1447 (3.3445)	grad_norm 1.5657 (1.5521)	mem 14852MB
[2022-11-06 20:15:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [146/300][1250/1251]	eta 0:00:00 lr 0.000521	time 0.4635 (0.4691)	loss 3.4685 (3.3453)	grad_norm 1.4908 (1.5518)	mem 14852MB
[2022-11-06 20:15:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 146 training takes 0:09:47
[2022-11-06 20:15:32 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_146.pth saving......
[2022-11-06 20:15:32 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_146.pth saved !!!
[2022-11-06 20:15:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.613 (1.613)	Loss 0.8957 (0.8957)	Acc@1 79.297 (79.297)	Acc@5 94.629 (94.629)	Mem 14852MB
[2022-11-06 20:15:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.882 Acc@5 94.214
[2022-11-06 20:15:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.9%
[2022-11-06 20:15:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.644 (1.644)	Loss 0.8934 (0.8934)	Acc@1 79.102 (79.102)	Acc@5 94.922 (94.922)	Mem 14852MB
[2022-11-06 20:15:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.034 Acc@5 95.266
[2022-11-06 20:15:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.0%
[2022-11-06 20:15:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.03% at 146 epoch
[2022-11-06 20:15:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][0/1251]	eta 0:40:47 lr 0.000521	time 1.9567 (1.9567)	loss 3.6087 (3.6087)	grad_norm 1.8818 (1.8818)	mem 14852MB
[2022-11-06 20:16:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][50/1251]	eta 0:10:04 lr 0.000520	time 0.4870 (0.5031)	loss 2.6605 (3.2663)	grad_norm 1.6218 (1.5714)	mem 14852MB
[2022-11-06 20:16:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][100/1251]	eta 0:09:20 lr 0.000520	time 0.4654 (0.4870)	loss 3.6451 (3.3375)	grad_norm 1.5403 (1.5537)	mem 14852MB
[2022-11-06 20:17:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][150/1251]	eta 0:08:48 lr 0.000520	time 0.4590 (0.4800)	loss 3.4782 (3.2898)	grad_norm 1.4586 (1.5554)	mem 14852MB
[2022-11-06 20:17:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][200/1251]	eta 0:08:20 lr 0.000520	time 0.4638 (0.4764)	loss 3.1397 (3.3264)	grad_norm 1.5484 (1.5527)	mem 14852MB
[2022-11-06 20:17:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][250/1251]	eta 0:07:54 lr 0.000520	time 0.4657 (0.4745)	loss 3.0531 (3.3242)	grad_norm 1.4340 (1.5536)	mem 14852MB
[2022-11-06 20:18:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][300/1251]	eta 0:07:29 lr 0.000519	time 0.4641 (0.4732)	loss 3.1009 (3.3133)	grad_norm 1.6095 (1.5531)	mem 14852MB
[2022-11-06 20:18:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][350/1251]	eta 0:07:05 lr 0.000519	time 0.4599 (0.4721)	loss 2.8347 (3.3132)	grad_norm 1.5202 (1.5453)	mem 14852MB
[2022-11-06 20:18:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][400/1251]	eta 0:06:41 lr 0.000519	time 0.4578 (0.4712)	loss 2.1570 (3.3079)	grad_norm 1.4012 (1.5467)	mem 14852MB
[2022-11-06 20:19:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][450/1251]	eta 0:06:16 lr 0.000519	time 0.4581 (0.4705)	loss 3.9820 (3.3316)	grad_norm 1.4943 (1.5461)	mem 14852MB
[2022-11-06 20:19:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][500/1251]	eta 0:05:52 lr 0.000518	time 0.4631 (0.4700)	loss 3.3420 (3.3269)	grad_norm 1.6213 (1.5453)	mem 14852MB
[2022-11-06 20:20:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][550/1251]	eta 0:05:29 lr 0.000518	time 0.4525 (0.4703)	loss 3.4606 (3.3317)	grad_norm 1.6310 (1.5489)	mem 14852MB
[2022-11-06 20:20:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][600/1251]	eta 0:05:05 lr 0.000518	time 0.4720 (0.4700)	loss 2.3714 (3.3349)	grad_norm 1.4903 (1.5484)	mem 14852MB
[2022-11-06 20:20:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][650/1251]	eta 0:04:42 lr 0.000518	time 0.4678 (0.4697)	loss 2.5148 (3.3357)	grad_norm 1.8085 (nan)	mem 14852MB
[2022-11-06 20:21:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][700/1251]	eta 0:04:18 lr 0.000518	time 0.4593 (0.4693)	loss 3.9822 (3.3285)	grad_norm 1.5986 (nan)	mem 14852MB
[2022-11-06 20:21:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][750/1251]	eta 0:03:55 lr 0.000517	time 0.4634 (0.4691)	loss 3.0430 (3.3431)	grad_norm 1.4188 (nan)	mem 14852MB
[2022-11-06 20:22:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][800/1251]	eta 0:03:31 lr 0.000517	time 0.4663 (0.4692)	loss 3.8568 (3.3465)	grad_norm 1.6391 (nan)	mem 14852MB
[2022-11-06 20:22:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][850/1251]	eta 0:03:08 lr 0.000517	time 0.4615 (0.4691)	loss 3.6194 (3.3518)	grad_norm 1.4526 (nan)	mem 14852MB
[2022-11-06 20:22:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][900/1251]	eta 0:02:44 lr 0.000517	time 0.4607 (0.4690)	loss 3.6811 (3.3533)	grad_norm 1.4564 (nan)	mem 14852MB
[2022-11-06 20:23:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][950/1251]	eta 0:02:21 lr 0.000517	time 0.4728 (0.4688)	loss 2.3675 (3.3501)	grad_norm 1.5047 (nan)	mem 14852MB
[2022-11-06 20:23:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][1000/1251]	eta 0:01:57 lr 0.000516	time 0.4631 (0.4686)	loss 2.8025 (3.3460)	grad_norm 1.3810 (nan)	mem 14852MB
[2022-11-06 20:24:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][1050/1251]	eta 0:01:34 lr 0.000516	time 0.4642 (0.4688)	loss 3.6343 (3.3482)	grad_norm 2.0594 (nan)	mem 14852MB
[2022-11-06 20:24:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][1100/1251]	eta 0:01:10 lr 0.000516	time 0.4545 (0.4687)	loss 3.5682 (3.3462)	grad_norm 1.4327 (nan)	mem 14852MB
[2022-11-06 20:24:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][1150/1251]	eta 0:00:47 lr 0.000516	time 0.4642 (0.4686)	loss 3.7481 (3.3437)	grad_norm 1.8105 (nan)	mem 14852MB
[2022-11-06 20:25:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][1200/1251]	eta 0:00:23 lr 0.000516	time 0.4568 (0.4685)	loss 3.3877 (3.3470)	grad_norm 1.6773 (nan)	mem 14852MB
[2022-11-06 20:25:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [147/300][1250/1251]	eta 0:00:00 lr 0.000515	time 0.4575 (0.4683)	loss 4.0849 (3.3492)	grad_norm 1.7888 (nan)	mem 14852MB
[2022-11-06 20:25:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 147 training takes 0:09:46
[2022-11-06 20:25:36 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_147.pth saving......
[2022-11-06 20:25:36 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_147.pth saved !!!
[2022-11-06 20:25:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.481 (1.481)	Loss 0.9281 (0.9281)	Acc@1 79.688 (79.688)	Acc@5 94.727 (94.727)	Mem 14852MB
[2022-11-06 20:25:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.750 Acc@5 94.312
[2022-11-06 20:25:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.8%
[2022-11-06 20:25:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.603 (1.603)	Loss 0.8445 (0.8445)	Acc@1 78.223 (78.223)	Acc@5 95.703 (95.703)	Mem 14852MB
[2022-11-06 20:25:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.066 Acc@5 95.292
[2022-11-06 20:25:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.1%
[2022-11-06 20:25:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.07% at 147 epoch
[2022-11-06 20:25:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][0/1251]	eta 0:42:08 lr 0.000515	time 2.0210 (2.0210)	loss 3.4849 (3.4849)	grad_norm 1.6289 (1.6289)	mem 14853MB
[2022-11-06 20:26:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][50/1251]	eta 0:10:05 lr 0.000515	time 0.4803 (0.5044)	loss 3.1127 (3.3990)	grad_norm 1.3926 (1.5491)	mem 14853MB
[2022-11-06 20:26:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][100/1251]	eta 0:09:21 lr 0.000515	time 0.4680 (0.4878)	loss 2.3318 (3.3887)	grad_norm 1.5519 (1.5483)	mem 14853MB
[2022-11-06 20:27:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][150/1251]	eta 0:08:49 lr 0.000515	time 0.4589 (0.4808)	loss 2.1274 (3.3483)	grad_norm 1.5245 (1.5598)	mem 14853MB
[2022-11-06 20:27:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][200/1251]	eta 0:08:21 lr 0.000515	time 0.4627 (0.4773)	loss 3.6453 (3.3295)	grad_norm 1.4231 (1.5656)	mem 14853MB
[2022-11-06 20:27:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][250/1251]	eta 0:07:55 lr 0.000514	time 0.4641 (0.4751)	loss 3.8118 (3.3157)	grad_norm 1.5898 (1.5642)	mem 14853MB
[2022-11-06 20:28:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][300/1251]	eta 0:07:30 lr 0.000514	time 0.4596 (0.4735)	loss 3.6872 (3.3253)	grad_norm 2.0533 (1.5710)	mem 14853MB
[2022-11-06 20:28:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][350/1251]	eta 0:07:05 lr 0.000514	time 0.4619 (0.4723)	loss 3.8357 (3.3481)	grad_norm 1.4962 (1.5698)	mem 14853MB
[2022-11-06 20:29:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][400/1251]	eta 0:06:41 lr 0.000514	time 0.4698 (0.4714)	loss 3.7432 (3.3425)	grad_norm 1.5506 (1.5686)	mem 14853MB
[2022-11-06 20:29:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][450/1251]	eta 0:06:17 lr 0.000514	time 0.4741 (0.4711)	loss 3.6926 (3.3294)	grad_norm 1.5218 (1.5661)	mem 14853MB
[2022-11-06 20:29:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][500/1251]	eta 0:05:53 lr 0.000513	time 0.4689 (0.4704)	loss 3.4665 (3.3406)	grad_norm 1.5599 (1.5673)	mem 14853MB
[2022-11-06 20:30:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][550/1251]	eta 0:05:29 lr 0.000513	time 0.4730 (0.4706)	loss 2.5435 (3.3450)	grad_norm 1.6993 (1.5681)	mem 14853MB
[2022-11-06 20:30:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][600/1251]	eta 0:05:06 lr 0.000513	time 0.4510 (0.4703)	loss 3.7424 (3.3591)	grad_norm 1.5349 (1.5668)	mem 14853MB
[2022-11-06 20:30:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][650/1251]	eta 0:04:42 lr 0.000513	time 0.4810 (0.4699)	loss 4.1145 (3.3564)	grad_norm 1.8416 (1.5654)	mem 14853MB
[2022-11-06 20:31:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][700/1251]	eta 0:04:18 lr 0.000512	time 0.4749 (0.4697)	loss 3.6196 (3.3553)	grad_norm 1.5647 (1.5658)	mem 14853MB
[2022-11-06 20:31:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][750/1251]	eta 0:03:55 lr 0.000512	time 0.4707 (0.4695)	loss 3.6262 (3.3558)	grad_norm 1.7999 (1.5649)	mem 14853MB
[2022-11-06 20:32:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][800/1251]	eta 0:03:31 lr 0.000512	time 0.4711 (0.4697)	loss 3.8675 (3.3510)	grad_norm 1.3660 (1.5635)	mem 14853MB
[2022-11-06 20:32:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][850/1251]	eta 0:03:08 lr 0.000512	time 0.4539 (0.4695)	loss 3.8190 (3.3575)	grad_norm 1.4435 (1.5619)	mem 14853MB
[2022-11-06 20:32:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][900/1251]	eta 0:02:44 lr 0.000512	time 0.4886 (0.4692)	loss 2.7198 (3.3580)	grad_norm 1.3687 (1.5593)	mem 14853MB
[2022-11-06 20:33:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][950/1251]	eta 0:02:21 lr 0.000511	time 0.4679 (0.4691)	loss 3.3206 (3.3559)	grad_norm 1.4845 (1.5617)	mem 14853MB
[2022-11-06 20:33:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][1000/1251]	eta 0:01:57 lr 0.000511	time 0.4712 (0.4689)	loss 2.1629 (3.3623)	grad_norm 1.5659 (1.5632)	mem 14853MB
[2022-11-06 20:34:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][1050/1251]	eta 0:01:34 lr 0.000511	time 0.4627 (0.4690)	loss 3.6829 (3.3651)	grad_norm 1.9776 (1.5653)	mem 14853MB
[2022-11-06 20:34:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][1100/1251]	eta 0:01:10 lr 0.000511	time 0.4611 (0.4691)	loss 3.5934 (3.3637)	grad_norm 1.6036 (1.5666)	mem 14853MB
[2022-11-06 20:34:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][1150/1251]	eta 0:00:47 lr 0.000511	time 0.4696 (0.4689)	loss 2.9450 (3.3608)	grad_norm 1.7408 (1.5653)	mem 14853MB
[2022-11-06 20:35:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][1200/1251]	eta 0:00:23 lr 0.000510	time 0.4568 (0.4689)	loss 3.7313 (3.3649)	grad_norm 1.5713 (1.5664)	mem 14853MB
[2022-11-06 20:35:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [148/300][1250/1251]	eta 0:00:00 lr 0.000510	time 0.4575 (0.4687)	loss 3.8027 (3.3649)	grad_norm 1.5457 (1.5651)	mem 14853MB
[2022-11-06 20:35:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 148 training takes 0:09:46
[2022-11-06 20:35:40 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_148.pth saving......
[2022-11-06 20:35:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_148.pth saved !!!
[2022-11-06 20:35:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.597 (1.597)	Loss 0.8882 (0.8882)	Acc@1 78.711 (78.711)	Acc@5 94.336 (94.336)	Mem 14853MB
[2022-11-06 20:35:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.032 Acc@5 94.354
[2022-11-06 20:35:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.0%
[2022-11-06 20:35:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.735 (1.735)	Loss 0.8788 (0.8788)	Acc@1 79.492 (79.492)	Acc@5 94.141 (94.141)	Mem 14853MB
[2022-11-06 20:35:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.108 Acc@5 95.298
[2022-11-06 20:35:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.1%
[2022-11-06 20:35:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.11% at 148 epoch
[2022-11-06 20:36:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][0/1251]	eta 0:46:07 lr 0.000510	time 2.2119 (2.2119)	loss 3.7602 (3.7602)	grad_norm 1.7205 (1.7205)	mem 14853MB
[2022-11-06 20:36:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][50/1251]	eta 0:10:04 lr 0.000510	time 0.4624 (0.5036)	loss 3.4902 (3.2683)	grad_norm 1.5751 (1.5347)	mem 14853MB
[2022-11-06 20:36:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][100/1251]	eta 0:09:22 lr 0.000510	time 0.4627 (0.4885)	loss 3.4463 (3.3372)	grad_norm 1.7482 (1.5747)	mem 14853MB
[2022-11-06 20:37:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][150/1251]	eta 0:08:49 lr 0.000510	time 0.4702 (0.4809)	loss 2.8087 (3.3389)	grad_norm 1.7938 (1.5714)	mem 14853MB
[2022-11-06 20:37:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][200/1251]	eta 0:08:21 lr 0.000509	time 0.4604 (0.4773)	loss 3.2008 (3.3520)	grad_norm 1.3141 (1.5662)	mem 14853MB
[2022-11-06 20:37:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][250/1251]	eta 0:07:55 lr 0.000509	time 0.4616 (0.4749)	loss 3.9215 (3.3682)	grad_norm 1.5774 (1.5697)	mem 14853MB
[2022-11-06 20:38:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][300/1251]	eta 0:07:30 lr 0.000509	time 0.4701 (0.4734)	loss 2.8690 (3.3635)	grad_norm 1.4435 (1.5711)	mem 14853MB
[2022-11-06 20:38:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][350/1251]	eta 0:07:05 lr 0.000509	time 0.4634 (0.4728)	loss 3.6234 (3.3595)	grad_norm 1.6339 (1.5793)	mem 14853MB
[2022-11-06 20:39:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][400/1251]	eta 0:06:41 lr 0.000509	time 0.4608 (0.4718)	loss 3.7889 (3.3691)	grad_norm 1.4189 (1.5761)	mem 14853MB
[2022-11-06 20:39:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][450/1251]	eta 0:06:17 lr 0.000508	time 0.4652 (0.4712)	loss 2.5440 (3.3694)	grad_norm 1.7542 (1.5779)	mem 14853MB
[2022-11-06 20:39:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][500/1251]	eta 0:05:53 lr 0.000508	time 0.4641 (0.4705)	loss 3.7472 (3.3682)	grad_norm 1.6669 (1.5769)	mem 14853MB
[2022-11-06 20:40:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][550/1251]	eta 0:05:29 lr 0.000508	time 0.4608 (0.4707)	loss 3.9947 (3.3627)	grad_norm 1.6060 (1.5785)	mem 14853MB
[2022-11-06 20:40:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][600/1251]	eta 0:05:06 lr 0.000508	time 0.4619 (0.4704)	loss 3.0585 (3.3584)	grad_norm 1.6321 (1.5805)	mem 14853MB
[2022-11-06 20:41:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][650/1251]	eta 0:04:42 lr 0.000507	time 0.4641 (0.4700)	loss 2.1175 (3.3566)	grad_norm 1.8860 (1.5819)	mem 14853MB
[2022-11-06 20:41:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][700/1251]	eta 0:04:18 lr 0.000507	time 0.4602 (0.4697)	loss 2.3649 (3.3516)	grad_norm 1.5707 (nan)	mem 14853MB
[2022-11-06 20:41:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][750/1251]	eta 0:03:55 lr 0.000507	time 0.4625 (0.4695)	loss 3.9537 (3.3495)	grad_norm 1.6463 (nan)	mem 14853MB
[2022-11-06 20:42:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][800/1251]	eta 0:03:31 lr 0.000507	time 0.4697 (0.4696)	loss 3.3332 (3.3414)	grad_norm 1.5886 (nan)	mem 14853MB
[2022-11-06 20:42:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][850/1251]	eta 0:03:08 lr 0.000507	time 0.4658 (0.4694)	loss 2.2689 (3.3391)	grad_norm 1.6459 (nan)	mem 14853MB
[2022-11-06 20:43:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][900/1251]	eta 0:02:44 lr 0.000506	time 0.4856 (0.4692)	loss 3.7697 (3.3390)	grad_norm 1.6260 (nan)	mem 14853MB
[2022-11-06 20:43:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][950/1251]	eta 0:02:21 lr 0.000506	time 0.4744 (0.4691)	loss 3.3544 (3.3420)	grad_norm 1.4785 (nan)	mem 14853MB
[2022-11-06 20:43:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][1000/1251]	eta 0:01:57 lr 0.000506	time 0.4745 (0.4690)	loss 2.9564 (3.3463)	grad_norm 1.4065 (nan)	mem 14853MB
[2022-11-06 20:44:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][1050/1251]	eta 0:01:34 lr 0.000506	time 0.4670 (0.4691)	loss 3.8106 (3.3449)	grad_norm 1.7771 (nan)	mem 14853MB
[2022-11-06 20:44:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][1100/1251]	eta 0:01:10 lr 0.000506	time 0.4780 (0.4689)	loss 3.2098 (3.3452)	grad_norm 1.4197 (nan)	mem 14853MB
[2022-11-06 20:44:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][1150/1251]	eta 0:00:47 lr 0.000505	time 0.4628 (0.4688)	loss 3.7674 (3.3465)	grad_norm 1.7679 (nan)	mem 14853MB
[2022-11-06 20:45:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][1200/1251]	eta 0:00:23 lr 0.000505	time 0.4662 (0.4687)	loss 3.3411 (3.3426)	grad_norm 1.5257 (nan)	mem 14853MB
[2022-11-06 20:45:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [149/300][1250/1251]	eta 0:00:00 lr 0.000505	time 0.4569 (0.4685)	loss 3.6859 (3.3405)	grad_norm 1.6445 (nan)	mem 14853MB
[2022-11-06 20:45:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 149 training takes 0:09:46
[2022-11-06 20:45:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_149.pth saving......
[2022-11-06 20:45:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_149.pth saved !!!
[2022-11-06 20:45:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.545 (1.545)	Loss 1.0140 (1.0140)	Acc@1 76.074 (76.074)	Acc@5 93.262 (93.262)	Mem 14853MB
[2022-11-06 20:45:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.970 Acc@5 94.450
[2022-11-06 20:45:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.0%
[2022-11-06 20:45:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.635 (1.635)	Loss 0.8589 (0.8589)	Acc@1 80.469 (80.469)	Acc@5 95.117 (95.117)	Mem 14853MB
[2022-11-06 20:46:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.162 Acc@5 95.334
[2022-11-06 20:46:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.2%
[2022-11-06 20:46:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.16% at 149 epoch
[2022-11-06 20:46:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][0/1251]	eta 0:42:20 lr 0.000505	time 2.0305 (2.0305)	loss 4.0843 (4.0843)	grad_norm 1.5891 (1.5891)	mem 14853MB
[2022-11-06 20:46:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][50/1251]	eta 0:10:04 lr 0.000505	time 0.4604 (0.5037)	loss 3.9755 (3.4288)	grad_norm 1.6549 (1.5849)	mem 14853MB
[2022-11-06 20:46:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][100/1251]	eta 0:09:19 lr 0.000505	time 0.4689 (0.4860)	loss 2.4191 (3.3720)	grad_norm 1.6450 (1.5706)	mem 14853MB
[2022-11-06 20:47:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][150/1251]	eta 0:08:48 lr 0.000504	time 0.4723 (0.4799)	loss 3.3125 (3.3373)	grad_norm 1.6117 (1.5687)	mem 14853MB
[2022-11-06 20:47:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][200/1251]	eta 0:08:21 lr 0.000504	time 0.4617 (0.4772)	loss 2.9032 (3.3495)	grad_norm 1.8371 (1.5700)	mem 14853MB
[2022-11-06 20:48:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][250/1251]	eta 0:07:55 lr 0.000504	time 0.4641 (0.4749)	loss 3.0472 (3.3309)	grad_norm 1.4399 (1.5732)	mem 14853MB
[2022-11-06 20:48:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][300/1251]	eta 0:07:30 lr 0.000504	time 0.4711 (0.4733)	loss 3.4848 (3.3260)	grad_norm 1.6389 (1.5743)	mem 14853MB
[2022-11-06 20:48:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][350/1251]	eta 0:07:05 lr 0.000504	time 0.4624 (0.4723)	loss 3.7126 (3.3367)	grad_norm 1.5372 (1.5749)	mem 14853MB
[2022-11-06 20:49:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][400/1251]	eta 0:06:41 lr 0.000503	time 0.4554 (0.4716)	loss 2.7254 (3.3127)	grad_norm 1.7801 (1.5718)	mem 14853MB
[2022-11-06 20:49:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][450/1251]	eta 0:06:17 lr 0.000503	time 0.4691 (0.4709)	loss 3.2393 (3.3082)	grad_norm 1.6529 (1.5680)	mem 14853MB
[2022-11-06 20:49:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][500/1251]	eta 0:05:53 lr 0.000503	time 0.4570 (0.4705)	loss 3.8228 (3.2908)	grad_norm 1.5001 (1.5700)	mem 14853MB
[2022-11-06 20:50:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][550/1251]	eta 0:05:29 lr 0.000503	time 0.4597 (0.4704)	loss 3.0483 (3.2950)	grad_norm 1.5926 (1.5740)	mem 14853MB
[2022-11-06 20:50:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][600/1251]	eta 0:05:05 lr 0.000503	time 0.4876 (0.4700)	loss 2.3122 (3.2974)	grad_norm 1.5426 (1.5702)	mem 14853MB
[2022-11-06 20:51:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][650/1251]	eta 0:04:42 lr 0.000502	time 0.4710 (0.4697)	loss 3.8962 (3.3031)	grad_norm 1.5702 (1.5714)	mem 14853MB
[2022-11-06 20:51:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][700/1251]	eta 0:04:18 lr 0.000502	time 0.4789 (0.4696)	loss 2.9857 (3.3091)	grad_norm 1.4752 (1.5721)	mem 14853MB
[2022-11-06 20:51:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][750/1251]	eta 0:03:55 lr 0.000502	time 0.4507 (0.4694)	loss 3.3027 (3.3152)	grad_norm 1.6375 (1.5704)	mem 14853MB
[2022-11-06 20:52:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][800/1251]	eta 0:03:31 lr 0.000502	time 0.4684 (0.4694)	loss 2.7617 (3.3153)	grad_norm 1.6356 (1.5695)	mem 14853MB
[2022-11-06 20:52:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][850/1251]	eta 0:03:08 lr 0.000501	time 0.5388 (0.4693)	loss 2.9556 (3.3161)	grad_norm 1.5844 (1.5724)	mem 14853MB
[2022-11-06 20:53:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][900/1251]	eta 0:02:44 lr 0.000501	time 0.4641 (0.4690)	loss 3.7640 (3.3211)	grad_norm 1.5015 (1.5713)	mem 14853MB
[2022-11-06 20:53:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][950/1251]	eta 0:02:21 lr 0.000501	time 0.4658 (0.4688)	loss 2.8336 (3.3203)	grad_norm 1.5221 (1.5712)	mem 14853MB
[2022-11-06 20:53:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][1000/1251]	eta 0:01:57 lr 0.000501	time 0.4691 (0.4686)	loss 3.4145 (3.3190)	grad_norm 1.6291 (1.5717)	mem 14853MB
[2022-11-06 20:54:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][1050/1251]	eta 0:01:34 lr 0.000501	time 0.4593 (0.4687)	loss 2.2864 (3.3216)	grad_norm 1.6838 (1.5725)	mem 14853MB
[2022-11-06 20:54:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][1100/1251]	eta 0:01:10 lr 0.000500	time 0.4722 (0.4687)	loss 3.6628 (3.3270)	grad_norm 1.5709 (1.5717)	mem 14853MB
[2022-11-06 20:55:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][1150/1251]	eta 0:00:47 lr 0.000500	time 0.4613 (0.4685)	loss 2.8872 (3.3182)	grad_norm 1.6420 (1.5731)	mem 14853MB
[2022-11-06 20:55:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][1200/1251]	eta 0:00:23 lr 0.000500	time 0.5419 (0.4685)	loss 3.5166 (3.3214)	grad_norm 1.7061 (1.5721)	mem 14853MB
[2022-11-06 20:55:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [150/300][1250/1251]	eta 0:00:00 lr 0.000500	time 0.4577 (0.4682)	loss 3.8339 (3.3214)	grad_norm 1.5126 (1.5722)	mem 14853MB
[2022-11-06 20:55:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 150 training takes 0:09:45
[2022-11-06 20:55:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_150.pth saving......
[2022-11-06 20:55:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_150.pth saved !!!
[2022-11-06 20:55:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.447 (1.447)	Loss 0.9794 (0.9794)	Acc@1 78.027 (78.027)	Acc@5 93.750 (93.750)	Mem 14853MB
[2022-11-06 20:55:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.086 Acc@5 94.412
[2022-11-06 20:55:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.1%
[2022-11-06 20:55:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.615 (1.615)	Loss 0.8886 (0.8886)	Acc@1 77.539 (77.539)	Acc@5 94.629 (94.629)	Mem 14853MB
[2022-11-06 20:56:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.152 Acc@5 95.314
[2022-11-06 20:56:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.2%
[2022-11-06 20:56:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.16% at 149 epoch
[2022-11-06 20:56:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][0/1251]	eta 0:40:56 lr 0.000500	time 1.9635 (1.9635)	loss 3.5657 (3.5657)	grad_norm 1.4397 (1.4397)	mem 14853MB
[2022-11-06 20:56:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][50/1251]	eta 0:10:02 lr 0.000500	time 0.4677 (0.5018)	loss 3.4596 (3.3380)	grad_norm 1.6030 (1.5774)	mem 14853MB
[2022-11-06 20:56:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][100/1251]	eta 0:09:19 lr 0.000499	time 0.4548 (0.4859)	loss 3.1899 (3.3674)	grad_norm 1.3963 (1.5910)	mem 14853MB
[2022-11-06 20:57:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][150/1251]	eta 0:08:46 lr 0.000499	time 0.4551 (0.4786)	loss 3.3730 (3.3905)	grad_norm 1.5933 (1.5931)	mem 14853MB
[2022-11-06 20:57:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][200/1251]	eta 0:08:20 lr 0.000499	time 0.4628 (0.4760)	loss 2.8451 (3.3659)	grad_norm 1.6773 (1.5906)	mem 14853MB
[2022-11-06 20:58:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][250/1251]	eta 0:07:54 lr 0.000499	time 0.4612 (0.4742)	loss 4.1715 (3.3866)	grad_norm 1.7892 (1.5911)	mem 14853MB
[2022-11-06 20:58:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][300/1251]	eta 0:07:29 lr 0.000499	time 0.4645 (0.4730)	loss 3.5771 (3.3625)	grad_norm 1.6991 (1.5850)	mem 14853MB
[2022-11-06 20:58:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][350/1251]	eta 0:07:05 lr 0.000498	time 0.4707 (0.4721)	loss 3.6748 (3.3613)	grad_norm 1.5298 (1.5826)	mem 14853MB
[2022-11-06 20:59:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][400/1251]	eta 0:06:41 lr 0.000498	time 0.4691 (0.4713)	loss 2.4676 (3.3690)	grad_norm 1.3192 (1.5808)	mem 14853MB
[2022-11-06 20:59:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][450/1251]	eta 0:06:17 lr 0.000498	time 0.4641 (0.4710)	loss 3.4430 (3.3626)	grad_norm 1.4631 (1.5776)	mem 14853MB
[2022-11-06 21:00:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][500/1251]	eta 0:05:53 lr 0.000498	time 0.4733 (0.4704)	loss 3.6871 (3.3562)	grad_norm 1.4996 (1.5752)	mem 14853MB
[2022-11-06 21:00:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][550/1251]	eta 0:05:29 lr 0.000498	time 0.4697 (0.4705)	loss 3.7710 (3.3618)	grad_norm 1.5313 (1.5789)	mem 14853MB
[2022-11-06 21:00:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][600/1251]	eta 0:05:06 lr 0.000497	time 0.4780 (0.4703)	loss 2.9185 (3.3585)	grad_norm 1.7223 (1.5797)	mem 14853MB
[2022-11-06 21:01:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][650/1251]	eta 0:04:42 lr 0.000497	time 0.4728 (0.4699)	loss 3.6162 (3.3594)	grad_norm 1.4089 (1.5793)	mem 14853MB
[2022-11-06 21:01:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][700/1251]	eta 0:04:18 lr 0.000497	time 0.4698 (0.4699)	loss 2.8607 (3.3579)	grad_norm 1.9444 (1.5794)	mem 14853MB
[2022-11-06 21:01:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][750/1251]	eta 0:03:55 lr 0.000497	time 0.4724 (0.4696)	loss 3.5361 (3.3606)	grad_norm 1.4592 (1.5823)	mem 14853MB
[2022-11-06 21:02:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][800/1251]	eta 0:03:31 lr 0.000497	time 0.4634 (0.4697)	loss 3.4799 (3.3652)	grad_norm 1.4896 (1.5813)	mem 14853MB
[2022-11-06 21:02:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][850/1251]	eta 0:03:08 lr 0.000496	time 0.4625 (0.4694)	loss 3.0232 (3.3650)	grad_norm 1.8468 (1.5810)	mem 14853MB
[2022-11-06 21:03:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][900/1251]	eta 0:02:44 lr 0.000496	time 0.4625 (0.4692)	loss 3.9176 (3.3674)	grad_norm 1.5471 (1.5812)	mem 14853MB
[2022-11-06 21:03:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][950/1251]	eta 0:02:21 lr 0.000496	time 0.4644 (0.4691)	loss 3.3349 (3.3656)	grad_norm 1.4374 (1.5795)	mem 14853MB
[2022-11-06 21:03:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][1000/1251]	eta 0:01:57 lr 0.000496	time 0.4522 (0.4689)	loss 3.5287 (3.3642)	grad_norm 1.5300 (inf)	mem 14853MB
[2022-11-06 21:04:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][1050/1251]	eta 0:01:34 lr 0.000495	time 0.4613 (0.4690)	loss 3.0603 (3.3714)	grad_norm 1.4898 (inf)	mem 14853MB
[2022-11-06 21:04:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][1100/1251]	eta 0:01:10 lr 0.000495	time 0.4684 (0.4689)	loss 2.9405 (3.3724)	grad_norm 1.3251 (inf)	mem 14853MB
[2022-11-06 21:05:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][1150/1251]	eta 0:00:47 lr 0.000495	time 0.4708 (0.4687)	loss 3.3288 (3.3701)	grad_norm 1.9807 (inf)	mem 14853MB
[2022-11-06 21:05:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][1200/1251]	eta 0:00:23 lr 0.000495	time 0.4570 (0.4688)	loss 2.1633 (3.3747)	grad_norm 1.4163 (inf)	mem 14853MB
[2022-11-06 21:05:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [151/300][1250/1251]	eta 0:00:00 lr 0.000495	time 0.4576 (0.4686)	loss 3.5447 (3.3767)	grad_norm 1.3225 (inf)	mem 14853MB
[2022-11-06 21:05:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 151 training takes 0:09:46
[2022-11-06 21:05:53 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_151.pth saving......
[2022-11-06 21:05:53 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_151.pth saved !!!
[2022-11-06 21:05:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.612 (1.612)	Loss 0.9152 (0.9152)	Acc@1 78.223 (78.223)	Acc@5 94.141 (94.141)	Mem 14853MB
[2022-11-06 21:06:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.934 Acc@5 94.268
[2022-11-06 21:06:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.9%
[2022-11-06 21:06:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.608 (1.608)	Loss 0.8405 (0.8405)	Acc@1 80.762 (80.762)	Acc@5 95.020 (95.020)	Mem 14853MB
[2022-11-06 21:06:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.212 Acc@5 95.284
[2022-11-06 21:06:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.2%
[2022-11-06 21:06:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.21% at 151 epoch
[2022-11-06 21:06:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][0/1251]	eta 0:39:27 lr 0.000495	time 1.8926 (1.8926)	loss 3.7624 (3.7624)	grad_norm 1.5493 (1.5493)	mem 14853MB
[2022-11-06 21:06:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][50/1251]	eta 0:09:59 lr 0.000494	time 0.4619 (0.4991)	loss 2.9190 (3.3577)	grad_norm 1.6086 (1.5524)	mem 14853MB
[2022-11-06 21:07:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][100/1251]	eta 0:09:16 lr 0.000494	time 0.4675 (0.4836)	loss 3.8910 (3.3426)	grad_norm 1.6130 (1.5679)	mem 14853MB
[2022-11-06 21:07:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][150/1251]	eta 0:08:47 lr 0.000494	time 0.4525 (0.4788)	loss 3.4951 (3.3480)	grad_norm 1.7868 (1.5768)	mem 14853MB
[2022-11-06 21:07:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][200/1251]	eta 0:08:20 lr 0.000494	time 0.4631 (0.4763)	loss 2.2507 (3.3526)	grad_norm 1.7028 (1.5807)	mem 14853MB
[2022-11-06 21:08:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][250/1251]	eta 0:07:54 lr 0.000494	time 0.4635 (0.4743)	loss 3.4102 (3.3414)	grad_norm 1.5615 (1.5777)	mem 14853MB
[2022-11-06 21:08:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][300/1251]	eta 0:07:29 lr 0.000493	time 0.4638 (0.4730)	loss 2.7581 (3.3563)	grad_norm 1.6106 (1.5889)	mem 14853MB
[2022-11-06 21:08:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][350/1251]	eta 0:07:05 lr 0.000493	time 0.4735 (0.4718)	loss 2.6951 (3.3541)	grad_norm 1.4717 (1.5884)	mem 14853MB
[2022-11-06 21:09:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][400/1251]	eta 0:06:41 lr 0.000493	time 0.4600 (0.4712)	loss 3.5504 (3.3448)	grad_norm 1.6551 (1.5951)	mem 14853MB
[2022-11-06 21:09:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][450/1251]	eta 0:06:17 lr 0.000493	time 0.4613 (0.4709)	loss 3.4393 (3.3336)	grad_norm 1.5960 (1.5931)	mem 14853MB
[2022-11-06 21:10:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][500/1251]	eta 0:05:53 lr 0.000493	time 0.4773 (0.4704)	loss 3.9914 (3.3381)	grad_norm 1.6904 (1.5922)	mem 14853MB
[2022-11-06 21:10:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][550/1251]	eta 0:05:29 lr 0.000492	time 0.4630 (0.4701)	loss 3.2175 (3.3353)	grad_norm 1.5669 (1.5968)	mem 14853MB
[2022-11-06 21:10:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][600/1251]	eta 0:05:05 lr 0.000492	time 0.4649 (0.4698)	loss 3.4097 (3.3227)	grad_norm 1.3327 (1.5967)	mem 14853MB
[2022-11-06 21:11:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][650/1251]	eta 0:04:42 lr 0.000492	time 0.4645 (0.4695)	loss 2.7320 (3.3240)	grad_norm 1.5151 (1.5941)	mem 14853MB
[2022-11-06 21:11:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][700/1251]	eta 0:04:18 lr 0.000492	time 0.4609 (0.4694)	loss 2.6365 (3.3277)	grad_norm 1.5470 (1.5912)	mem 14853MB
[2022-11-06 21:12:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][750/1251]	eta 0:03:55 lr 0.000492	time 0.4643 (0.4692)	loss 3.3382 (3.3274)	grad_norm 1.7519 (1.5922)	mem 14853MB
[2022-11-06 21:12:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][800/1251]	eta 0:03:31 lr 0.000491	time 0.4644 (0.4693)	loss 4.1049 (3.3238)	grad_norm 1.5496 (1.5923)	mem 14853MB
[2022-11-06 21:12:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][850/1251]	eta 0:03:08 lr 0.000491	time 0.4541 (0.4690)	loss 3.2005 (3.3350)	grad_norm 1.4757 (1.5917)	mem 14853MB
[2022-11-06 21:13:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][900/1251]	eta 0:02:44 lr 0.000491	time 0.4636 (0.4689)	loss 3.3456 (3.3423)	grad_norm 1.6659 (1.5903)	mem 14853MB
[2022-11-06 21:13:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][950/1251]	eta 0:02:21 lr 0.000491	time 0.4692 (0.4687)	loss 3.2085 (3.3436)	grad_norm 1.6323 (1.5887)	mem 14853MB
[2022-11-06 21:14:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][1000/1251]	eta 0:01:57 lr 0.000490	time 0.4770 (0.4687)	loss 3.5725 (3.3453)	grad_norm 1.4481 (1.5882)	mem 14853MB
[2022-11-06 21:14:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][1050/1251]	eta 0:01:34 lr 0.000490	time 0.4678 (0.4687)	loss 3.4112 (3.3365)	grad_norm 1.6451 (1.5876)	mem 14853MB
[2022-11-06 21:14:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][1100/1251]	eta 0:01:10 lr 0.000490	time 0.4655 (0.4687)	loss 3.9275 (3.3412)	grad_norm 1.7222 (1.5878)	mem 14853MB
[2022-11-06 21:15:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][1150/1251]	eta 0:00:47 lr 0.000490	time 0.4671 (0.4685)	loss 3.3559 (3.3417)	grad_norm 1.7647 (1.5876)	mem 14853MB
[2022-11-06 21:15:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][1200/1251]	eta 0:00:23 lr 0.000490	time 0.4623 (0.4685)	loss 3.7765 (3.3466)	grad_norm 1.6307 (1.5891)	mem 14853MB
[2022-11-06 21:15:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [152/300][1250/1251]	eta 0:00:00 lr 0.000489	time 0.4571 (0.4683)	loss 3.0845 (3.3460)	grad_norm 1.5603 (1.5878)	mem 14853MB
[2022-11-06 21:15:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 152 training takes 0:09:45
[2022-11-06 21:15:57 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_152.pth saving......
[2022-11-06 21:15:58 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_152.pth saved !!!
[2022-11-06 21:15:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.555 (1.555)	Loss 0.8991 (0.8991)	Acc@1 78.613 (78.613)	Acc@5 94.727 (94.727)	Mem 14853MB
[2022-11-06 21:16:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.232 Acc@5 94.366
[2022-11-06 21:16:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.2%
[2022-11-06 21:16:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.677 (1.677)	Loss 0.9045 (0.9045)	Acc@1 77.051 (77.051)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-06 21:16:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.210 Acc@5 95.308
[2022-11-06 21:16:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.2%
[2022-11-06 21:16:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.21% at 151 epoch
[2022-11-06 21:16:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][0/1251]	eta 0:41:22 lr 0.000489	time 1.9846 (1.9846)	loss 3.0720 (3.0720)	grad_norm 1.6774 (1.6774)	mem 14853MB
[2022-11-06 21:16:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][50/1251]	eta 0:10:01 lr 0.000489	time 0.4650 (0.5012)	loss 3.4470 (3.1334)	grad_norm 1.4307 (1.6057)	mem 14853MB
[2022-11-06 21:17:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][100/1251]	eta 0:09:18 lr 0.000489	time 0.4707 (0.4852)	loss 2.6052 (3.2344)	grad_norm 1.5360 (1.6056)	mem 14853MB
[2022-11-06 21:17:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][150/1251]	eta 0:08:47 lr 0.000489	time 0.4638 (0.4788)	loss 3.6019 (3.2530)	grad_norm 1.5982 (1.5910)	mem 14853MB
[2022-11-06 21:17:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][200/1251]	eta 0:08:20 lr 0.000489	time 0.4632 (0.4765)	loss 3.0511 (3.2563)	grad_norm 1.4783 (1.5964)	mem 14853MB
[2022-11-06 21:18:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][250/1251]	eta 0:07:55 lr 0.000488	time 0.4732 (0.4748)	loss 2.9654 (3.2773)	grad_norm 1.4893 (1.5990)	mem 14853MB
[2022-11-06 21:18:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][300/1251]	eta 0:07:30 lr 0.000488	time 0.4703 (0.4732)	loss 3.3557 (3.2854)	grad_norm 1.7682 (1.6025)	mem 14853MB
[2022-11-06 21:19:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][350/1251]	eta 0:07:05 lr 0.000488	time 0.4666 (0.4722)	loss 3.3845 (3.2920)	grad_norm 1.5157 (1.6066)	mem 14853MB
[2022-11-06 21:19:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][400/1251]	eta 0:06:41 lr 0.000488	time 0.4575 (0.4718)	loss 3.3474 (3.3011)	grad_norm 1.4806 (1.6066)	mem 14853MB
[2022-11-06 21:19:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][450/1251]	eta 0:06:17 lr 0.000488	time 0.4796 (0.4711)	loss 3.7368 (3.3097)	grad_norm 1.6010 (1.6044)	mem 14853MB
[2022-11-06 21:20:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][500/1251]	eta 0:05:53 lr 0.000487	time 0.4510 (0.4706)	loss 3.3008 (3.3045)	grad_norm 1.9036 (1.6052)	mem 14853MB
[2022-11-06 21:20:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][550/1251]	eta 0:05:29 lr 0.000487	time 0.4643 (0.4705)	loss 3.6619 (3.3072)	grad_norm 1.7124 (1.6043)	mem 14853MB
[2022-11-06 21:20:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][600/1251]	eta 0:05:06 lr 0.000487	time 0.4738 (0.4701)	loss 2.5892 (3.3149)	grad_norm 1.5846 (1.6023)	mem 14853MB
[2022-11-06 21:21:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][650/1251]	eta 0:04:42 lr 0.000487	time 0.4735 (0.4697)	loss 3.4256 (3.3182)	grad_norm 1.7790 (1.6039)	mem 14853MB
[2022-11-06 21:21:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][700/1251]	eta 0:04:18 lr 0.000487	time 0.4693 (0.4697)	loss 3.2614 (3.3136)	grad_norm 1.6557 (1.6036)	mem 14853MB
[2022-11-06 21:22:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][750/1251]	eta 0:03:55 lr 0.000486	time 0.4680 (0.4694)	loss 3.3836 (3.3203)	grad_norm 1.5994 (1.6054)	mem 14853MB
[2022-11-06 21:22:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][800/1251]	eta 0:03:31 lr 0.000486	time 0.4599 (0.4694)	loss 3.7258 (3.3165)	grad_norm 1.6106 (1.6019)	mem 14853MB
[2022-11-06 21:22:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][850/1251]	eta 0:03:08 lr 0.000486	time 0.4604 (0.4694)	loss 3.0825 (3.3215)	grad_norm 1.5165 (nan)	mem 14853MB
[2022-11-06 21:23:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][900/1251]	eta 0:02:44 lr 0.000486	time 0.4574 (0.4691)	loss 3.6615 (3.3142)	grad_norm 1.5700 (nan)	mem 14853MB
[2022-11-06 21:23:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][950/1251]	eta 0:02:21 lr 0.000486	time 0.4721 (0.4689)	loss 3.6439 (3.3210)	grad_norm 1.6154 (nan)	mem 14853MB
[2022-11-06 21:24:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][1000/1251]	eta 0:01:57 lr 0.000485	time 0.4747 (0.4689)	loss 3.2690 (3.3228)	grad_norm 1.7298 (nan)	mem 14853MB
[2022-11-06 21:24:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][1050/1251]	eta 0:01:34 lr 0.000485	time 0.4604 (0.4688)	loss 4.1270 (3.3263)	grad_norm 1.5972 (nan)	mem 14853MB
[2022-11-06 21:24:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][1100/1251]	eta 0:01:10 lr 0.000485	time 0.4650 (0.4687)	loss 3.4764 (3.3244)	grad_norm 1.4992 (nan)	mem 14853MB
[2022-11-06 21:25:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][1150/1251]	eta 0:00:47 lr 0.000485	time 0.4628 (0.4686)	loss 4.0330 (3.3232)	grad_norm 1.5327 (nan)	mem 14853MB
[2022-11-06 21:25:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][1200/1251]	eta 0:00:23 lr 0.000484	time 0.4619 (0.4686)	loss 3.0367 (3.3178)	grad_norm 1.4135 (nan)	mem 14853MB
[2022-11-06 21:26:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [153/300][1250/1251]	eta 0:00:00 lr 0.000484	time 0.4571 (0.4684)	loss 3.3773 (3.3219)	grad_norm 1.4887 (nan)	mem 14853MB
[2022-11-06 21:26:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 153 training takes 0:09:46
[2022-11-06 21:26:01 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_153.pth saving......
[2022-11-06 21:26:02 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_153.pth saved !!!
[2022-11-06 21:26:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.579 (1.579)	Loss 0.9844 (0.9844)	Acc@1 75.684 (75.684)	Acc@5 93.848 (93.848)	Mem 14853MB
[2022-11-06 21:26:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.128 Acc@5 94.302
[2022-11-06 21:26:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.1%
[2022-11-06 21:26:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.644 (1.644)	Loss 0.8626 (0.8626)	Acc@1 79.297 (79.297)	Acc@5 94.824 (94.824)	Mem 14853MB
[2022-11-06 21:26:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.242 Acc@5 95.328
[2022-11-06 21:26:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.2%
[2022-11-06 21:26:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.24% at 153 epoch
[2022-11-06 21:26:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][0/1251]	eta 0:43:07 lr 0.000484	time 2.0682 (2.0682)	loss 2.7660 (2.7660)	grad_norm 1.6464 (1.6464)	mem 14853MB
[2022-11-06 21:26:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][50/1251]	eta 0:10:06 lr 0.000484	time 0.4719 (0.5046)	loss 3.7678 (3.1568)	grad_norm 1.5897 (1.5783)	mem 14853MB
[2022-11-06 21:27:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][100/1251]	eta 0:09:19 lr 0.000484	time 0.4568 (0.4861)	loss 3.9059 (3.2569)	grad_norm 1.4907 (1.5855)	mem 14853MB
[2022-11-06 21:27:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][150/1251]	eta 0:08:48 lr 0.000484	time 0.4621 (0.4799)	loss 3.7199 (3.2949)	grad_norm 1.5232 (1.5822)	mem 14853MB
[2022-11-06 21:27:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][200/1251]	eta 0:08:20 lr 0.000483	time 0.4604 (0.4766)	loss 3.4672 (3.3126)	grad_norm 1.5248 (1.5826)	mem 14853MB
[2022-11-06 21:28:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][250/1251]	eta 0:07:55 lr 0.000483	time 0.4657 (0.4747)	loss 3.8081 (3.3084)	grad_norm 1.7014 (1.5942)	mem 14853MB
[2022-11-06 21:28:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][300/1251]	eta 0:07:29 lr 0.000483	time 0.4626 (0.4730)	loss 3.4447 (3.2922)	grad_norm 1.6811 (1.5939)	mem 14853MB
[2022-11-06 21:29:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][350/1251]	eta 0:07:05 lr 0.000483	time 0.4660 (0.4722)	loss 3.0378 (3.2854)	grad_norm 1.4066 (1.5900)	mem 14853MB
[2022-11-06 21:29:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][400/1251]	eta 0:06:41 lr 0.000483	time 0.4653 (0.4714)	loss 3.5841 (3.2963)	grad_norm 1.6587 (1.5902)	mem 14853MB
[2022-11-06 21:29:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][450/1251]	eta 0:06:17 lr 0.000482	time 0.4668 (0.4710)	loss 3.7922 (3.2991)	grad_norm 1.7299 (1.5911)	mem 14853MB
[2022-11-06 21:30:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][500/1251]	eta 0:05:53 lr 0.000482	time 0.5407 (0.4706)	loss 3.1515 (3.2895)	grad_norm 1.6715 (1.5904)	mem 14853MB
[2022-11-06 21:30:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][550/1251]	eta 0:05:29 lr 0.000482	time 0.4711 (0.4703)	loss 3.3176 (3.2945)	grad_norm 1.6836 (1.5927)	mem 14853MB
[2022-11-06 21:31:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][600/1251]	eta 0:05:05 lr 0.000482	time 0.4623 (0.4699)	loss 2.7016 (3.2955)	grad_norm 1.4884 (1.5967)	mem 14853MB
[2022-11-06 21:31:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][650/1251]	eta 0:04:42 lr 0.000482	time 0.4793 (0.4697)	loss 3.0423 (3.3066)	grad_norm 1.6479 (1.5950)	mem 14853MB
[2022-11-06 21:31:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][700/1251]	eta 0:04:18 lr 0.000481	time 0.4789 (0.4695)	loss 3.6834 (3.3095)	grad_norm 1.5989 (1.5978)	mem 14853MB
[2022-11-06 21:32:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][750/1251]	eta 0:03:55 lr 0.000481	time 0.4723 (0.4694)	loss 3.7997 (3.3127)	grad_norm 1.7225 (1.5977)	mem 14853MB
[2022-11-06 21:32:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][800/1251]	eta 0:03:31 lr 0.000481	time 0.4686 (0.4692)	loss 3.9999 (3.3092)	grad_norm 1.7255 (1.5986)	mem 14853MB
[2022-11-06 21:32:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][850/1251]	eta 0:03:08 lr 0.000481	time 0.4625 (0.4689)	loss 3.7580 (3.3094)	grad_norm 1.5586 (1.6004)	mem 14853MB
[2022-11-06 21:33:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][900/1251]	eta 0:02:44 lr 0.000481	time 0.4659 (0.4688)	loss 2.8952 (3.3014)	grad_norm 1.6906 (1.6010)	mem 14853MB
[2022-11-06 21:33:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][950/1251]	eta 0:02:21 lr 0.000480	time 0.4602 (0.4686)	loss 3.8742 (3.2998)	grad_norm 1.5313 (1.6000)	mem 14853MB
[2022-11-06 21:34:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][1000/1251]	eta 0:01:57 lr 0.000480	time 0.4624 (0.4685)	loss 3.9178 (3.2966)	grad_norm 1.8818 (1.6006)	mem 14853MB
[2022-11-06 21:34:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][1050/1251]	eta 0:01:34 lr 0.000480	time 0.4706 (0.4686)	loss 3.8409 (3.2998)	grad_norm 1.5118 (1.6023)	mem 14853MB
[2022-11-06 21:34:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][1100/1251]	eta 0:01:10 lr 0.000480	time 0.4644 (0.4684)	loss 3.9126 (3.3024)	grad_norm 1.5822 (1.6042)	mem 14853MB
[2022-11-06 21:35:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][1150/1251]	eta 0:00:47 lr 0.000480	time 0.5441 (0.4683)	loss 2.7515 (3.3047)	grad_norm 1.7244 (1.6042)	mem 14853MB
[2022-11-06 21:35:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][1200/1251]	eta 0:00:23 lr 0.000479	time 0.4706 (0.4683)	loss 2.7871 (3.3057)	grad_norm 1.7137 (1.6047)	mem 14853MB
[2022-11-06 21:36:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [154/300][1250/1251]	eta 0:00:00 lr 0.000479	time 0.4567 (0.4681)	loss 2.3353 (3.3082)	grad_norm 1.6471 (1.6046)	mem 14853MB
[2022-11-06 21:36:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 154 training takes 0:09:45
[2022-11-06 21:36:05 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_154.pth saving......
[2022-11-06 21:36:06 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_154.pth saved !!!
[2022-11-06 21:36:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.531 (1.531)	Loss 0.8979 (0.8979)	Acc@1 78.711 (78.711)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-06 21:36:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.218 Acc@5 94.502
[2022-11-06 21:36:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.2%
[2022-11-06 21:36:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.600 (1.600)	Loss 0.8400 (0.8400)	Acc@1 79.102 (79.102)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-06 21:36:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.232 Acc@5 95.340
[2022-11-06 21:36:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.2%
[2022-11-06 21:36:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.24% at 153 epoch
[2022-11-06 21:36:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][0/1251]	eta 0:41:10 lr 0.000479	time 1.9747 (1.9747)	loss 3.4331 (3.4331)	grad_norm 1.5282 (1.5282)	mem 14853MB
[2022-11-06 21:36:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][50/1251]	eta 0:10:01 lr 0.000479	time 0.4718 (0.5012)	loss 3.9455 (3.3898)	grad_norm 1.6838 (1.6147)	mem 14853MB
[2022-11-06 21:37:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][100/1251]	eta 0:09:17 lr 0.000479	time 0.4609 (0.4845)	loss 3.6241 (3.3131)	grad_norm 1.7111 (1.6271)	mem 14853MB
[2022-11-06 21:37:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][150/1251]	eta 0:08:47 lr 0.000478	time 0.4731 (0.4788)	loss 3.4399 (3.3236)	grad_norm 1.6502 (1.6132)	mem 14853MB
[2022-11-06 21:37:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][200/1251]	eta 0:08:20 lr 0.000478	time 0.4574 (0.4763)	loss 3.2130 (3.3150)	grad_norm 1.6008 (1.6053)	mem 14853MB
[2022-11-06 21:38:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][250/1251]	eta 0:07:54 lr 0.000478	time 0.4629 (0.4743)	loss 2.5524 (3.3033)	grad_norm 1.6665 (1.6151)	mem 14853MB
[2022-11-06 21:38:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][300/1251]	eta 0:07:29 lr 0.000478	time 0.4620 (0.4728)	loss 2.9904 (3.3191)	grad_norm 1.4059 (1.6052)	mem 14853MB
[2022-11-06 21:39:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][350/1251]	eta 0:07:04 lr 0.000478	time 0.4769 (0.4715)	loss 3.4057 (3.3276)	grad_norm 1.5350 (1.6065)	mem 14853MB
[2022-11-06 21:39:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][400/1251]	eta 0:06:40 lr 0.000477	time 0.4585 (0.4711)	loss 3.3713 (3.3185)	grad_norm 1.6775 (1.6091)	mem 14853MB
[2022-11-06 21:39:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][450/1251]	eta 0:06:17 lr 0.000477	time 0.4588 (0.4707)	loss 3.4793 (3.3115)	grad_norm 1.4824 (1.6115)	mem 14853MB
[2022-11-06 21:40:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][500/1251]	eta 0:05:53 lr 0.000477	time 0.4712 (0.4703)	loss 2.8325 (3.3178)	grad_norm 1.7310 (1.6132)	mem 14853MB
[2022-11-06 21:40:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][550/1251]	eta 0:05:29 lr 0.000477	time 0.4584 (0.4700)	loss 2.6413 (3.3066)	grad_norm 1.7985 (1.6148)	mem 14853MB
[2022-11-06 21:41:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][600/1251]	eta 0:05:05 lr 0.000477	time 0.4586 (0.4699)	loss 3.4126 (3.3110)	grad_norm 2.0406 (1.6144)	mem 14853MB
[2022-11-06 21:41:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][650/1251]	eta 0:04:42 lr 0.000476	time 0.4604 (0.4695)	loss 3.0270 (3.3102)	grad_norm 2.0295 (1.6131)	mem 14853MB
[2022-11-06 21:41:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][700/1251]	eta 0:04:18 lr 0.000476	time 0.4663 (0.4693)	loss 2.7351 (3.3142)	grad_norm 1.7831 (1.6147)	mem 14853MB
[2022-11-06 21:42:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][750/1251]	eta 0:03:55 lr 0.000476	time 0.4637 (0.4692)	loss 2.1950 (3.3205)	grad_norm 1.4997 (1.6116)	mem 14853MB
[2022-11-06 21:42:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][800/1251]	eta 0:03:31 lr 0.000476	time 0.4637 (0.4691)	loss 3.0931 (3.3210)	grad_norm 1.4833 (1.6101)	mem 14853MB
[2022-11-06 21:43:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][850/1251]	eta 0:03:08 lr 0.000476	time 0.4609 (0.4690)	loss 3.3274 (3.3236)	grad_norm 2.0091 (1.6129)	mem 14853MB
[2022-11-06 21:43:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][900/1251]	eta 0:02:44 lr 0.000475	time 0.4750 (0.4688)	loss 2.0268 (3.3267)	grad_norm 1.5198 (1.6122)	mem 14853MB
[2022-11-06 21:43:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][950/1251]	eta 0:02:21 lr 0.000475	time 0.4558 (0.4686)	loss 3.3934 (3.3242)	grad_norm 1.5991 (1.6146)	mem 14853MB
[2022-11-06 21:44:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][1000/1251]	eta 0:01:57 lr 0.000475	time 0.4593 (0.4685)	loss 3.7166 (3.3280)	grad_norm 1.6220 (1.6132)	mem 14853MB
[2022-11-06 21:44:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][1050/1251]	eta 0:01:34 lr 0.000475	time 0.4642 (0.4685)	loss 3.9466 (3.3324)	grad_norm 1.6839 (1.6128)	mem 14853MB
[2022-11-06 21:44:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][1100/1251]	eta 0:01:10 lr 0.000475	time 0.4585 (0.4685)	loss 2.8335 (3.3296)	grad_norm 1.6584 (1.6134)	mem 14853MB
[2022-11-06 21:45:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][1150/1251]	eta 0:00:47 lr 0.000474	time 0.4695 (0.4683)	loss 3.8430 (3.3291)	grad_norm 1.4777 (1.6148)	mem 14853MB
[2022-11-06 21:45:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][1200/1251]	eta 0:00:23 lr 0.000474	time 0.4721 (0.4682)	loss 3.0105 (3.3289)	grad_norm 1.4998 (1.6141)	mem 14853MB
[2022-11-06 21:46:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [155/300][1250/1251]	eta 0:00:00 lr 0.000474	time 0.4564 (0.4680)	loss 3.6303 (3.3237)	grad_norm 1.3865 (1.6139)	mem 14853MB
[2022-11-06 21:46:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 155 training takes 0:09:45
[2022-11-06 21:46:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_155.pth saving......
[2022-11-06 21:46:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_155.pth saved !!!
[2022-11-06 21:46:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.556 (1.556)	Loss 0.9070 (0.9070)	Acc@1 78.418 (78.418)	Acc@5 95.020 (95.020)	Mem 14853MB
[2022-11-06 21:46:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.900 Acc@5 94.454
[2022-11-06 21:46:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.9%
[2022-11-06 21:46:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.792 (1.792)	Loss 0.7704 (0.7704)	Acc@1 81.152 (81.152)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-06 21:46:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.274 Acc@5 95.360
[2022-11-06 21:46:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.3%
[2022-11-06 21:46:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.27% at 155 epoch
[2022-11-06 21:46:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][0/1251]	eta 0:42:17 lr 0.000474	time 2.0285 (2.0285)	loss 3.8188 (3.8188)	grad_norm 1.6660 (1.6660)	mem 14853MB
[2022-11-06 21:46:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][50/1251]	eta 0:10:01 lr 0.000474	time 0.4586 (0.5007)	loss 3.8143 (3.4170)	grad_norm 1.4508 (1.5656)	mem 14853MB
[2022-11-06 21:47:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][100/1251]	eta 0:09:18 lr 0.000474	time 0.4683 (0.4852)	loss 3.6758 (3.3568)	grad_norm 1.5514 (1.5871)	mem 14853MB
[2022-11-06 21:47:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][150/1251]	eta 0:08:47 lr 0.000473	time 0.4728 (0.4794)	loss 3.0374 (3.3531)	grad_norm 1.4817 (1.5977)	mem 14853MB
[2022-11-06 21:48:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][200/1251]	eta 0:08:20 lr 0.000473	time 0.4692 (0.4759)	loss 3.6197 (3.3287)	grad_norm 1.6855 (1.6034)	mem 14853MB
[2022-11-06 21:48:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][250/1251]	eta 0:07:54 lr 0.000473	time 0.4747 (0.4737)	loss 3.2014 (3.3017)	grad_norm 1.5816 (1.6041)	mem 14853MB
[2022-11-06 21:48:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][300/1251]	eta 0:07:29 lr 0.000473	time 0.4656 (0.4726)	loss 3.6931 (3.3088)	grad_norm 1.9191 (1.6061)	mem 14853MB
[2022-11-06 21:49:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][350/1251]	eta 0:07:05 lr 0.000472	time 0.4602 (0.4718)	loss 3.5215 (3.3063)	grad_norm 1.7849 (1.6119)	mem 14853MB
[2022-11-06 21:49:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][400/1251]	eta 0:06:40 lr 0.000472	time 0.4582 (0.4710)	loss 3.6392 (3.2977)	grad_norm 1.4973 (1.6120)	mem 14853MB
[2022-11-06 21:49:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][450/1251]	eta 0:06:16 lr 0.000472	time 0.4575 (0.4703)	loss 3.4775 (3.3297)	grad_norm 1.5994 (1.6168)	mem 14853MB
[2022-11-06 21:50:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][500/1251]	eta 0:05:52 lr 0.000472	time 0.4652 (0.4699)	loss 3.3575 (3.3195)	grad_norm 1.5513 (1.6217)	mem 14853MB
[2022-11-06 21:50:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][550/1251]	eta 0:05:29 lr 0.000472	time 0.4650 (0.4698)	loss 3.8038 (3.3154)	grad_norm 1.5853 (1.6197)	mem 14853MB
[2022-11-06 21:51:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][600/1251]	eta 0:05:05 lr 0.000471	time 0.4632 (0.4695)	loss 3.4847 (3.3155)	grad_norm 2.0941 (1.6196)	mem 14853MB
[2022-11-06 21:51:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][650/1251]	eta 0:04:42 lr 0.000471	time 0.4780 (0.4695)	loss 3.7729 (3.3109)	grad_norm 1.5166 (1.6172)	mem 14853MB
[2022-11-06 21:51:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][700/1251]	eta 0:04:18 lr 0.000471	time 0.4535 (0.4691)	loss 3.6139 (3.3126)	grad_norm 1.5267 (1.6147)	mem 14853MB
[2022-11-06 21:52:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][750/1251]	eta 0:03:54 lr 0.000471	time 0.4649 (0.4689)	loss 2.8298 (3.3158)	grad_norm 1.6915 (1.6147)	mem 14853MB
[2022-11-06 21:52:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][800/1251]	eta 0:03:31 lr 0.000471	time 0.4665 (0.4690)	loss 3.6880 (3.3158)	grad_norm 1.7225 (1.6182)	mem 14853MB
[2022-11-06 21:53:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][850/1251]	eta 0:03:07 lr 0.000470	time 0.4674 (0.4688)	loss 3.9573 (3.3240)	grad_norm 1.5958 (1.6195)	mem 14853MB
[2022-11-06 21:53:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][900/1251]	eta 0:02:44 lr 0.000470	time 0.4672 (0.4687)	loss 3.7152 (3.3273)	grad_norm 1.6841 (1.6212)	mem 14853MB
[2022-11-06 21:53:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][950/1251]	eta 0:02:21 lr 0.000470	time 0.4618 (0.4685)	loss 2.7576 (3.3336)	grad_norm 1.4070 (1.6198)	mem 14853MB
[2022-11-06 21:54:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][1000/1251]	eta 0:01:57 lr 0.000470	time 0.4557 (0.4684)	loss 3.7452 (3.3277)	grad_norm 1.6734 (1.6206)	mem 14853MB
[2022-11-06 21:54:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][1050/1251]	eta 0:01:34 lr 0.000470	time 0.4614 (0.4686)	loss 3.5883 (3.3297)	grad_norm 1.4998 (1.6193)	mem 14853MB
[2022-11-06 21:55:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][1100/1251]	eta 0:01:10 lr 0.000469	time 0.4734 (0.4685)	loss 3.0385 (3.3281)	grad_norm 1.7131 (1.6209)	mem 14853MB
[2022-11-06 21:55:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][1150/1251]	eta 0:00:47 lr 0.000469	time 0.4620 (0.4684)	loss 2.2258 (3.3234)	grad_norm 1.5846 (nan)	mem 14853MB
[2022-11-06 21:55:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][1200/1251]	eta 0:00:23 lr 0.000469	time 0.4646 (0.4682)	loss 2.4667 (3.3255)	grad_norm 1.6462 (nan)	mem 14853MB
[2022-11-06 21:56:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [156/300][1250/1251]	eta 0:00:00 lr 0.000469	time 0.4580 (0.4681)	loss 2.9629 (3.3294)	grad_norm 1.6744 (nan)	mem 14853MB
[2022-11-06 21:56:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 156 training takes 0:09:45
[2022-11-06 21:56:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_156.pth saving......
[2022-11-06 21:56:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_156.pth saved !!!
[2022-11-06 21:56:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.505 (1.505)	Loss 0.8679 (0.8679)	Acc@1 79.883 (79.883)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-06 21:56:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.270 Acc@5 94.558
[2022-11-06 21:56:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.3%
[2022-11-06 21:56:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.612 (1.612)	Loss 0.8389 (0.8389)	Acc@1 79.199 (79.199)	Acc@5 94.922 (94.922)	Mem 14853MB
[2022-11-06 21:56:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.270 Acc@5 95.374
[2022-11-06 21:56:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.3%
[2022-11-06 21:56:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.27% at 155 epoch
[2022-11-06 21:56:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][0/1251]	eta 0:41:29 lr 0.000469	time 1.9903 (1.9903)	loss 2.3908 (2.3908)	grad_norm 1.5574 (1.5574)	mem 14853MB
[2022-11-06 21:56:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][50/1251]	eta 0:09:59 lr 0.000469	time 0.4583 (0.4990)	loss 3.3818 (3.3404)	grad_norm 1.5894 (1.6191)	mem 14853MB
[2022-11-06 21:57:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][100/1251]	eta 0:09:16 lr 0.000468	time 0.4736 (0.4834)	loss 3.0634 (3.3210)	grad_norm 1.6111 (1.6031)	mem 14853MB
[2022-11-06 21:57:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][150/1251]	eta 0:08:46 lr 0.000468	time 0.4686 (0.4785)	loss 2.7876 (3.2959)	grad_norm 1.5043 (1.6041)	mem 14853MB
[2022-11-06 21:58:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][200/1251]	eta 0:08:19 lr 0.000468	time 0.4590 (0.4755)	loss 3.8315 (3.3003)	grad_norm 1.4855 (1.6045)	mem 14853MB
[2022-11-06 21:58:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][250/1251]	eta 0:07:53 lr 0.000468	time 0.4667 (0.4735)	loss 3.6771 (3.3317)	grad_norm 1.5827 (1.6080)	mem 14853MB
[2022-11-06 21:58:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][300/1251]	eta 0:07:29 lr 0.000468	time 0.4754 (0.4725)	loss 2.1490 (3.3185)	grad_norm 1.6723 (1.6170)	mem 14853MB
[2022-11-06 21:59:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][350/1251]	eta 0:07:04 lr 0.000467	time 0.4584 (0.4714)	loss 3.5940 (3.3157)	grad_norm 1.5209 (1.6167)	mem 14853MB
[2022-11-06 21:59:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][400/1251]	eta 0:06:40 lr 0.000467	time 0.4702 (0.4708)	loss 3.4962 (3.3229)	grad_norm 1.7865 (1.6186)	mem 14853MB
[2022-11-06 22:00:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][450/1251]	eta 0:06:16 lr 0.000467	time 0.4650 (0.4701)	loss 2.6339 (3.3074)	grad_norm 1.8708 (1.6192)	mem 14853MB
[2022-11-06 22:00:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][500/1251]	eta 0:05:52 lr 0.000467	time 0.4667 (0.4697)	loss 3.4106 (3.3116)	grad_norm 1.6148 (1.6233)	mem 14853MB
[2022-11-06 22:00:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][550/1251]	eta 0:05:29 lr 0.000466	time 0.4610 (0.4697)	loss 3.4544 (3.3082)	grad_norm 1.7985 (1.6240)	mem 14853MB
[2022-11-06 22:01:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][600/1251]	eta 0:05:05 lr 0.000466	time 0.4632 (0.4695)	loss 3.3461 (3.3102)	grad_norm 1.4859 (1.6238)	mem 14853MB
[2022-11-06 22:01:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][650/1251]	eta 0:04:41 lr 0.000466	time 0.4606 (0.4692)	loss 3.5187 (3.3127)	grad_norm 2.1079 (1.6274)	mem 14853MB
[2022-11-06 22:01:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][700/1251]	eta 0:04:18 lr 0.000466	time 0.4667 (0.4690)	loss 3.4557 (3.3168)	grad_norm 1.7488 (1.6267)	mem 14853MB
[2022-11-06 22:02:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][750/1251]	eta 0:03:54 lr 0.000466	time 0.4620 (0.4689)	loss 3.7459 (3.3122)	grad_norm 1.8809 (1.6296)	mem 14853MB
[2022-11-06 22:02:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][800/1251]	eta 0:03:31 lr 0.000465	time 0.4714 (0.4689)	loss 2.2896 (3.3098)	grad_norm 1.5831 (1.6321)	mem 14853MB
[2022-11-06 22:03:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][850/1251]	eta 0:03:07 lr 0.000465	time 0.4642 (0.4688)	loss 3.9117 (3.3138)	grad_norm 1.5079 (1.6314)	mem 14853MB
[2022-11-06 22:03:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][900/1251]	eta 0:02:44 lr 0.000465	time 0.4638 (0.4686)	loss 3.6698 (3.3156)	grad_norm 1.6855 (1.6306)	mem 14853MB
[2022-11-06 22:03:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][950/1251]	eta 0:02:21 lr 0.000465	time 0.4618 (0.4685)	loss 3.5807 (3.3150)	grad_norm 1.4589 (1.6298)	mem 14853MB
[2022-11-06 22:04:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][1000/1251]	eta 0:01:57 lr 0.000465	time 0.4742 (0.4683)	loss 3.7348 (3.3101)	grad_norm 1.5794 (nan)	mem 14853MB
[2022-11-06 22:04:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][1050/1251]	eta 0:01:34 lr 0.000464	time 0.4618 (0.4683)	loss 3.3657 (3.3089)	grad_norm 1.5654 (nan)	mem 14853MB
[2022-11-06 22:05:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][1100/1251]	eta 0:01:10 lr 0.000464	time 0.4736 (0.4683)	loss 3.8770 (3.3041)	grad_norm 1.6962 (nan)	mem 14853MB
[2022-11-06 22:05:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][1150/1251]	eta 0:00:47 lr 0.000464	time 0.4626 (0.4682)	loss 2.6978 (3.3055)	grad_norm 1.5480 (nan)	mem 14853MB
[2022-11-06 22:05:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][1200/1251]	eta 0:00:23 lr 0.000464	time 0.4535 (0.4681)	loss 3.5511 (3.3054)	grad_norm 1.4347 (nan)	mem 14853MB
[2022-11-06 22:06:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [157/300][1250/1251]	eta 0:00:00 lr 0.000464	time 0.4571 (0.4679)	loss 3.2172 (3.3041)	grad_norm 1.5583 (nan)	mem 14853MB
[2022-11-06 22:06:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 157 training takes 0:09:45
[2022-11-06 22:06:16 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_157.pth saving......
[2022-11-06 22:06:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_157.pth saved !!!
[2022-11-06 22:06:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.478 (1.478)	Loss 0.9334 (0.9334)	Acc@1 77.246 (77.246)	Acc@5 93.945 (93.945)	Mem 14853MB
[2022-11-06 22:06:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.262 Acc@5 94.460
[2022-11-06 22:06:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.3%
[2022-11-06 22:06:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.707 (1.707)	Loss 0.8434 (0.8434)	Acc@1 80.469 (80.469)	Acc@5 94.922 (94.922)	Mem 14853MB
[2022-11-06 22:06:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.314 Acc@5 95.378
[2022-11-06 22:06:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.3%
[2022-11-06 22:06:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.31% at 157 epoch
[2022-11-06 22:06:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][0/1251]	eta 0:43:57 lr 0.000464	time 2.1087 (2.1087)	loss 2.6899 (2.6899)	grad_norm 1.4228 (1.4228)	mem 14853MB
[2022-11-06 22:07:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][50/1251]	eta 0:10:06 lr 0.000463	time 0.4687 (0.5051)	loss 3.9404 (3.2877)	grad_norm 1.6135 (1.6034)	mem 14853MB
[2022-11-06 22:07:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][100/1251]	eta 0:09:20 lr 0.000463	time 0.4657 (0.4865)	loss 3.7860 (3.3587)	grad_norm 1.6447 (1.6224)	mem 14853MB
[2022-11-06 22:07:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][150/1251]	eta 0:08:48 lr 0.000463	time 0.4619 (0.4800)	loss 3.6873 (3.3407)	grad_norm 2.0216 (1.6376)	mem 14853MB
[2022-11-06 22:08:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][200/1251]	eta 0:08:20 lr 0.000463	time 0.4671 (0.4766)	loss 3.1993 (3.3298)	grad_norm 1.6237 (1.6403)	mem 14853MB
[2022-11-06 22:08:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][250/1251]	eta 0:07:55 lr 0.000463	time 0.4647 (0.4748)	loss 2.8673 (3.3228)	grad_norm 1.5937 (1.6402)	mem 14853MB
[2022-11-06 22:08:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][300/1251]	eta 0:07:30 lr 0.000462	time 0.4715 (0.4735)	loss 3.5183 (3.3026)	grad_norm 1.5603 (1.6348)	mem 14853MB
[2022-11-06 22:09:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][350/1251]	eta 0:07:05 lr 0.000462	time 0.4635 (0.4725)	loss 3.8764 (3.3038)	grad_norm 1.8704 (1.6318)	mem 14853MB
[2022-11-06 22:09:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][400/1251]	eta 0:06:41 lr 0.000462	time 0.4667 (0.4716)	loss 3.0603 (3.3046)	grad_norm 1.5146 (1.6282)	mem 14853MB
[2022-11-06 22:10:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][450/1251]	eta 0:06:17 lr 0.000462	time 0.4696 (0.4709)	loss 4.1007 (3.3005)	grad_norm 1.6617 (1.6295)	mem 14853MB
[2022-11-06 22:10:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][500/1251]	eta 0:05:53 lr 0.000462	time 0.4588 (0.4706)	loss 3.3255 (3.3096)	grad_norm 1.6000 (1.6271)	mem 14853MB
[2022-11-06 22:10:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][550/1251]	eta 0:05:29 lr 0.000461	time 0.4722 (0.4704)	loss 3.5203 (3.3089)	grad_norm 1.7134 (1.6253)	mem 14853MB
[2022-11-06 22:11:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][600/1251]	eta 0:05:05 lr 0.000461	time 0.4713 (0.4700)	loss 3.8591 (3.3191)	grad_norm 1.4151 (1.6226)	mem 14853MB
[2022-11-06 22:11:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][650/1251]	eta 0:04:42 lr 0.000461	time 0.4540 (0.4698)	loss 3.9069 (3.3313)	grad_norm 1.4566 (1.6224)	mem 14853MB
[2022-11-06 22:12:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][700/1251]	eta 0:04:18 lr 0.000461	time 0.4585 (0.4694)	loss 3.0007 (3.3284)	grad_norm 1.6817 (1.6222)	mem 14853MB
[2022-11-06 22:12:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][750/1251]	eta 0:03:55 lr 0.000460	time 0.4624 (0.4694)	loss 3.1119 (3.3220)	grad_norm 1.4610 (1.6193)	mem 14853MB
[2022-11-06 22:12:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][800/1251]	eta 0:03:31 lr 0.000460	time 0.4601 (0.4694)	loss 2.4603 (3.3270)	grad_norm 1.4877 (1.6200)	mem 14853MB
[2022-11-06 22:13:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][850/1251]	eta 0:03:08 lr 0.000460	time 0.4717 (0.4692)	loss 3.8962 (3.3301)	grad_norm 1.8690 (1.6225)	mem 14853MB
[2022-11-06 22:13:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][900/1251]	eta 0:02:44 lr 0.000460	time 0.4619 (0.4690)	loss 4.2524 (3.3324)	grad_norm 1.7222 (1.6233)	mem 14853MB
[2022-11-06 22:14:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][950/1251]	eta 0:02:21 lr 0.000460	time 0.4725 (0.4689)	loss 4.0318 (3.3334)	grad_norm 1.6667 (1.6253)	mem 14853MB
[2022-11-06 22:14:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][1000/1251]	eta 0:01:57 lr 0.000459	time 0.4676 (0.4689)	loss 3.8379 (3.3358)	grad_norm 1.6801 (1.6268)	mem 14853MB
[2022-11-06 22:14:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][1050/1251]	eta 0:01:34 lr 0.000459	time 0.4559 (0.4690)	loss 2.5445 (3.3342)	grad_norm 1.5130 (1.6288)	mem 14853MB
[2022-11-06 22:15:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][1100/1251]	eta 0:01:10 lr 0.000459	time 0.4668 (0.4688)	loss 3.9311 (3.3391)	grad_norm 1.7425 (1.6281)	mem 14853MB
[2022-11-06 22:15:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][1150/1251]	eta 0:00:47 lr 0.000459	time 0.4648 (0.4687)	loss 2.4091 (3.3372)	grad_norm 1.5193 (1.6299)	mem 14853MB
[2022-11-06 22:15:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][1200/1251]	eta 0:00:23 lr 0.000459	time 0.4614 (0.4685)	loss 3.6922 (3.3375)	grad_norm 1.5012 (1.6287)	mem 14853MB
[2022-11-06 22:16:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [158/300][1250/1251]	eta 0:00:00 lr 0.000458	time 0.5417 (0.4684)	loss 3.4871 (3.3339)	grad_norm 1.7441 (1.6299)	mem 14853MB
[2022-11-06 22:16:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 158 training takes 0:09:46
[2022-11-06 22:16:21 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_158.pth saving......
[2022-11-06 22:16:21 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_158.pth saved !!!
[2022-11-06 22:16:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.533 (1.533)	Loss 0.9578 (0.9578)	Acc@1 77.930 (77.930)	Acc@5 93.848 (93.848)	Mem 14853MB
[2022-11-06 22:16:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.506 Acc@5 94.620
[2022-11-06 22:16:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.5%
[2022-11-06 22:16:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.746 (1.746)	Loss 0.8008 (0.8008)	Acc@1 81.250 (81.250)	Acc@5 94.727 (94.727)	Mem 14853MB
[2022-11-06 22:16:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.296 Acc@5 95.386
[2022-11-06 22:16:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.3%
[2022-11-06 22:16:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.31% at 157 epoch
[2022-11-06 22:16:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][0/1251]	eta 0:42:10 lr 0.000458	time 2.0227 (2.0227)	loss 3.4810 (3.4810)	grad_norm 1.6737 (1.6737)	mem 14853MB
[2022-11-06 22:17:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][50/1251]	eta 0:10:03 lr 0.000458	time 0.4696 (0.5028)	loss 3.4362 (3.3918)	grad_norm 1.4833 (1.6364)	mem 14853MB
[2022-11-06 22:17:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][100/1251]	eta 0:09:19 lr 0.000458	time 0.4670 (0.4857)	loss 4.0235 (3.3372)	grad_norm 1.6994 (1.6205)	mem 14853MB
[2022-11-06 22:17:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][150/1251]	eta 0:08:47 lr 0.000458	time 0.4658 (0.4794)	loss 3.7378 (3.2893)	grad_norm 1.7755 (1.6281)	mem 14853MB
[2022-11-06 22:18:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][200/1251]	eta 0:08:21 lr 0.000458	time 0.4747 (0.4767)	loss 3.4607 (3.2996)	grad_norm 1.8789 (1.6289)	mem 14853MB
[2022-11-06 22:18:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][250/1251]	eta 0:07:54 lr 0.000457	time 0.4724 (0.4744)	loss 3.0208 (3.2991)	grad_norm 1.6638 (1.6276)	mem 14853MB
[2022-11-06 22:19:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][300/1251]	eta 0:07:29 lr 0.000457	time 0.4700 (0.4729)	loss 3.9404 (3.3129)	grad_norm 1.7169 (1.6233)	mem 14853MB
[2022-11-06 22:19:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][350/1251]	eta 0:07:04 lr 0.000457	time 0.4640 (0.4717)	loss 3.5386 (3.3226)	grad_norm 1.6513 (1.6284)	mem 14853MB
[2022-11-06 22:19:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][400/1251]	eta 0:06:40 lr 0.000457	time 0.4619 (0.4708)	loss 3.0023 (3.3053)	grad_norm 1.3328 (1.6297)	mem 14853MB
[2022-11-06 22:20:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][450/1251]	eta 0:06:16 lr 0.000457	time 0.4665 (0.4702)	loss 3.0518 (3.2976)	grad_norm 1.7833 (1.6307)	mem 14853MB
[2022-11-06 22:20:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][500/1251]	eta 0:05:53 lr 0.000456	time 0.4650 (0.4701)	loss 2.2476 (3.2894)	grad_norm 1.4892 (1.6331)	mem 14853MB
[2022-11-06 22:20:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][550/1251]	eta 0:05:29 lr 0.000456	time 0.4714 (0.4701)	loss 3.1482 (3.2890)	grad_norm 1.5052 (1.6330)	mem 14853MB
[2022-11-06 22:21:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][600/1251]	eta 0:05:05 lr 0.000456	time 0.4579 (0.4698)	loss 2.8859 (3.2846)	grad_norm 1.5234 (nan)	mem 14853MB
[2022-11-06 22:21:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][650/1251]	eta 0:04:42 lr 0.000456	time 0.4665 (0.4696)	loss 3.3808 (3.2873)	grad_norm 1.4293 (nan)	mem 14853MB
[2022-11-06 22:22:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][700/1251]	eta 0:04:18 lr 0.000456	time 0.4826 (0.4693)	loss 2.9575 (3.2928)	grad_norm 1.8554 (nan)	mem 14853MB
[2022-11-06 22:22:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][750/1251]	eta 0:03:55 lr 0.000455	time 0.4661 (0.4691)	loss 3.4754 (3.2993)	grad_norm 1.6746 (nan)	mem 14853MB
[2022-11-06 22:22:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][800/1251]	eta 0:03:31 lr 0.000455	time 0.4644 (0.4692)	loss 3.6187 (3.3004)	grad_norm 1.8297 (nan)	mem 14853MB
[2022-11-06 22:23:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][850/1251]	eta 0:03:08 lr 0.000455	time 0.4667 (0.4691)	loss 3.3086 (3.2981)	grad_norm 1.6073 (nan)	mem 14853MB
[2022-11-06 22:23:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][900/1251]	eta 0:02:44 lr 0.000455	time 0.4648 (0.4688)	loss 3.7358 (3.3058)	grad_norm 1.6786 (nan)	mem 14853MB
[2022-11-06 22:24:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][950/1251]	eta 0:02:21 lr 0.000454	time 0.4576 (0.4687)	loss 4.1729 (3.3022)	grad_norm 1.6740 (nan)	mem 14853MB
[2022-11-06 22:24:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][1000/1251]	eta 0:01:57 lr 0.000454	time 0.4669 (0.4686)	loss 3.7611 (3.3060)	grad_norm 1.5455 (nan)	mem 14853MB
[2022-11-06 22:24:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][1050/1251]	eta 0:01:34 lr 0.000454	time 0.4622 (0.4686)	loss 2.4456 (3.3029)	grad_norm 1.7597 (nan)	mem 14853MB
[2022-11-06 22:25:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][1100/1251]	eta 0:01:10 lr 0.000454	time 0.4644 (0.4686)	loss 3.6573 (3.3092)	grad_norm 1.5329 (nan)	mem 14853MB
[2022-11-06 22:25:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][1150/1251]	eta 0:00:47 lr 0.000454	time 0.4619 (0.4685)	loss 2.1162 (3.3077)	grad_norm 1.7600 (nan)	mem 14853MB
[2022-11-06 22:26:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][1200/1251]	eta 0:00:23 lr 0.000453	time 0.4699 (0.4684)	loss 2.7770 (3.3062)	grad_norm 1.8937 (nan)	mem 14853MB
[2022-11-06 22:26:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [159/300][1250/1251]	eta 0:00:00 lr 0.000453	time 0.4574 (0.4682)	loss 2.5708 (3.3051)	grad_norm 1.7316 (nan)	mem 14853MB
[2022-11-06 22:26:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 159 training takes 0:09:45
[2022-11-06 22:26:25 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_159.pth saving......
[2022-11-06 22:26:25 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_159.pth saved !!!
[2022-11-06 22:26:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.446 (1.446)	Loss 0.9141 (0.9141)	Acc@1 79.102 (79.102)	Acc@5 94.727 (94.727)	Mem 14853MB
[2022-11-06 22:26:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.430 Acc@5 94.530
[2022-11-06 22:26:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.4%
[2022-11-06 22:26:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.566 (1.566)	Loss 0.8387 (0.8387)	Acc@1 79.199 (79.199)	Acc@5 95.215 (95.215)	Mem 14853MB
[2022-11-06 22:26:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.328 Acc@5 95.402
[2022-11-06 22:26:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.3%
[2022-11-06 22:26:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.33% at 159 epoch
[2022-11-06 22:26:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][0/1251]	eta 0:42:19 lr 0.000453	time 2.0302 (2.0302)	loss 3.9705 (3.9705)	grad_norm 1.6391 (1.6391)	mem 14853MB
[2022-11-06 22:27:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][50/1251]	eta 0:10:05 lr 0.000453	time 0.4598 (0.5038)	loss 3.5208 (3.3512)	grad_norm 1.6214 (1.6929)	mem 14853MB
[2022-11-06 22:27:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][100/1251]	eta 0:09:21 lr 0.000453	time 0.4619 (0.4879)	loss 3.6002 (3.3080)	grad_norm 1.6109 (1.6729)	mem 14853MB
[2022-11-06 22:27:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][150/1251]	eta 0:08:48 lr 0.000453	time 0.4627 (0.4802)	loss 3.6434 (3.2794)	grad_norm 1.5292 (1.6496)	mem 14853MB
[2022-11-06 22:28:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][200/1251]	eta 0:08:20 lr 0.000452	time 0.4643 (0.4766)	loss 3.6571 (3.3000)	grad_norm 1.6433 (1.6534)	mem 14853MB
[2022-11-06 22:28:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][250/1251]	eta 0:07:54 lr 0.000452	time 0.4960 (0.4745)	loss 2.7519 (3.3030)	grad_norm 1.6180 (1.6504)	mem 14853MB
[2022-11-06 22:29:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][300/1251]	eta 0:07:30 lr 0.000452	time 0.4619 (0.4735)	loss 2.6710 (3.2882)	grad_norm 1.6768 (1.6485)	mem 14853MB
[2022-11-06 22:29:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][350/1251]	eta 0:07:05 lr 0.000452	time 0.4636 (0.4726)	loss 3.2535 (3.2966)	grad_norm 1.7190 (1.6501)	mem 14853MB
[2022-11-06 22:29:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][400/1251]	eta 0:06:41 lr 0.000452	time 0.4711 (0.4718)	loss 3.8040 (3.2901)	grad_norm 1.6910 (1.6467)	mem 14853MB
[2022-11-06 22:30:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][450/1251]	eta 0:06:17 lr 0.000451	time 0.4624 (0.4709)	loss 2.9002 (3.2894)	grad_norm 1.5726 (1.6456)	mem 14853MB
[2022-11-06 22:30:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][500/1251]	eta 0:05:53 lr 0.000451	time 0.4629 (0.4703)	loss 2.7878 (3.2794)	grad_norm 1.5447 (1.6453)	mem 14853MB
[2022-11-06 22:31:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][550/1251]	eta 0:05:29 lr 0.000451	time 0.4527 (0.4706)	loss 2.8632 (3.2781)	grad_norm 1.6541 (1.6438)	mem 14853MB
[2022-11-06 22:31:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][600/1251]	eta 0:05:06 lr 0.000451	time 0.4743 (0.4702)	loss 3.8059 (3.2805)	grad_norm 1.7845 (1.6455)	mem 14853MB
[2022-11-06 22:31:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][650/1251]	eta 0:04:42 lr 0.000451	time 0.4548 (0.4698)	loss 2.6996 (3.2828)	grad_norm 1.9659 (1.6442)	mem 14853MB
[2022-11-06 22:32:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][700/1251]	eta 0:04:18 lr 0.000450	time 0.4667 (0.4694)	loss 2.3244 (3.2789)	grad_norm 1.6334 (1.6436)	mem 14853MB
[2022-11-06 22:32:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][750/1251]	eta 0:03:55 lr 0.000450	time 0.4590 (0.4693)	loss 3.0582 (3.2735)	grad_norm 1.6502 (1.6434)	mem 14853MB
[2022-11-06 22:32:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][800/1251]	eta 0:03:31 lr 0.000450	time 0.4698 (0.4693)	loss 3.8070 (3.2728)	grad_norm 1.5646 (1.6401)	mem 14853MB
[2022-11-06 22:33:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][850/1251]	eta 0:03:08 lr 0.000450	time 0.4552 (0.4692)	loss 3.1662 (3.2686)	grad_norm 1.7374 (1.6393)	mem 14853MB
[2022-11-06 22:33:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][900/1251]	eta 0:02:44 lr 0.000450	time 0.4656 (0.4691)	loss 3.7940 (3.2759)	grad_norm 1.5429 (1.6414)	mem 14853MB
[2022-11-06 22:34:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][950/1251]	eta 0:02:21 lr 0.000449	time 0.4781 (0.4689)	loss 4.0777 (3.2762)	grad_norm 1.5604 (1.6433)	mem 14853MB
[2022-11-06 22:34:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][1000/1251]	eta 0:01:57 lr 0.000449	time 0.4655 (0.4687)	loss 4.0614 (3.2804)	grad_norm 1.8272 (1.6445)	mem 14853MB
[2022-11-06 22:34:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][1050/1251]	eta 0:01:34 lr 0.000449	time 0.4637 (0.4689)	loss 3.5997 (3.2750)	grad_norm 1.5322 (1.6436)	mem 14853MB
[2022-11-06 22:35:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][1100/1251]	eta 0:01:10 lr 0.000449	time 0.4668 (0.4689)	loss 3.9207 (3.2729)	grad_norm 1.7223 (1.6427)	mem 14853MB
[2022-11-06 22:35:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][1150/1251]	eta 0:00:47 lr 0.000449	time 0.4729 (0.4688)	loss 2.4142 (3.2770)	grad_norm 1.5817 (1.6422)	mem 14853MB
[2022-11-06 22:36:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][1200/1251]	eta 0:00:23 lr 0.000448	time 0.4644 (0.4686)	loss 3.6947 (3.2790)	grad_norm 1.7591 (1.6410)	mem 14853MB
[2022-11-06 22:36:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [160/300][1250/1251]	eta 0:00:00 lr 0.000448	time 0.4570 (0.4684)	loss 3.6768 (3.2725)	grad_norm 1.9190 (1.6414)	mem 14853MB
[2022-11-06 22:36:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 160 training takes 0:09:46
[2022-11-06 22:36:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_160.pth saving......
[2022-11-06 22:36:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_160.pth saved !!!
[2022-11-06 22:36:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.573 (1.573)	Loss 0.8357 (0.8357)	Acc@1 80.273 (80.273)	Acc@5 95.312 (95.312)	Mem 14853MB
[2022-11-06 22:36:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.224 Acc@5 94.540
[2022-11-06 22:36:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.2%
[2022-11-06 22:36:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.650 (1.650)	Loss 0.9112 (0.9112)	Acc@1 78.027 (78.027)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-06 22:36:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.388 Acc@5 95.446
[2022-11-06 22:36:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.4%
[2022-11-06 22:36:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.39% at 160 epoch
[2022-11-06 22:36:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][0/1251]	eta 0:41:04 lr 0.000448	time 1.9699 (1.9699)	loss 3.2652 (3.2652)	grad_norm 1.7814 (1.7814)	mem 14853MB
[2022-11-06 22:37:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][50/1251]	eta 0:10:02 lr 0.000448	time 0.4678 (0.5021)	loss 3.5424 (3.2320)	grad_norm 1.6975 (1.6725)	mem 14853MB
[2022-11-06 22:37:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][100/1251]	eta 0:09:21 lr 0.000448	time 0.4613 (0.4875)	loss 3.1485 (3.2073)	grad_norm 1.5511 (1.6336)	mem 14853MB
[2022-11-06 22:37:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][150/1251]	eta 0:08:48 lr 0.000447	time 0.4819 (0.4802)	loss 3.1483 (3.2184)	grad_norm 1.4808 (1.6339)	mem 14853MB
[2022-11-06 22:38:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][200/1251]	eta 0:08:20 lr 0.000447	time 0.4631 (0.4765)	loss 2.9806 (3.2212)	grad_norm 1.7863 (1.6358)	mem 14853MB
[2022-11-06 22:38:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][250/1251]	eta 0:07:55 lr 0.000447	time 0.4612 (0.4746)	loss 2.8219 (3.2506)	grad_norm 1.6593 (1.6304)	mem 14853MB
[2022-11-06 22:39:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][300/1251]	eta 0:07:29 lr 0.000447	time 0.4591 (0.4730)	loss 3.4934 (3.2545)	grad_norm 1.6003 (1.6357)	mem 14853MB
[2022-11-06 22:39:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][350/1251]	eta 0:07:05 lr 0.000447	time 0.4630 (0.4722)	loss 3.3718 (3.2638)	grad_norm 1.7339 (1.6294)	mem 14853MB
[2022-11-06 22:39:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][400/1251]	eta 0:06:41 lr 0.000446	time 0.4658 (0.4715)	loss 4.1087 (3.2822)	grad_norm 1.4686 (1.6304)	mem 14853MB
[2022-11-06 22:40:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][450/1251]	eta 0:06:17 lr 0.000446	time 0.4725 (0.4708)	loss 3.5305 (3.2955)	grad_norm 1.4749 (1.6276)	mem 14853MB
[2022-11-06 22:40:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][500/1251]	eta 0:05:53 lr 0.000446	time 0.4679 (0.4702)	loss 3.0003 (3.2890)	grad_norm 1.7201 (1.6306)	mem 14853MB
[2022-11-06 22:41:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][550/1251]	eta 0:05:29 lr 0.000446	time 0.4587 (0.4701)	loss 3.0784 (3.2875)	grad_norm 1.5998 (1.6324)	mem 14853MB
[2022-11-06 22:41:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][600/1251]	eta 0:05:06 lr 0.000446	time 0.4594 (0.4701)	loss 3.8240 (3.2830)	grad_norm 1.6968 (1.6317)	mem 14853MB
[2022-11-06 22:41:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][650/1251]	eta 0:04:42 lr 0.000445	time 0.4682 (0.4700)	loss 3.4711 (3.2868)	grad_norm 1.6996 (1.6341)	mem 14853MB
[2022-11-06 22:42:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][700/1251]	eta 0:04:18 lr 0.000445	time 0.4647 (0.4697)	loss 4.0780 (3.2859)	grad_norm 1.6230 (1.6374)	mem 14853MB
[2022-11-06 22:42:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][750/1251]	eta 0:03:55 lr 0.000445	time 0.4621 (0.4695)	loss 2.9445 (3.2919)	grad_norm 1.5450 (1.6376)	mem 14853MB
[2022-11-06 22:43:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][800/1251]	eta 0:03:31 lr 0.000445	time 0.5582 (0.4696)	loss 3.5036 (3.2957)	grad_norm 1.8302 (1.6401)	mem 14853MB
[2022-11-06 22:43:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][850/1251]	eta 0:03:08 lr 0.000445	time 0.4617 (0.4694)	loss 3.5868 (3.2948)	grad_norm 1.6690 (1.6414)	mem 14853MB
[2022-11-06 22:43:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][900/1251]	eta 0:02:44 lr 0.000444	time 0.4708 (0.4692)	loss 3.7997 (3.2965)	grad_norm 1.4823 (nan)	mem 14853MB
[2022-11-06 22:44:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][950/1251]	eta 0:02:21 lr 0.000444	time 0.4592 (0.4692)	loss 3.4124 (3.2986)	grad_norm 1.6341 (nan)	mem 14853MB
[2022-11-06 22:44:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][1000/1251]	eta 0:01:57 lr 0.000444	time 0.4522 (0.4690)	loss 3.5385 (3.3004)	grad_norm 1.8610 (nan)	mem 14853MB
[2022-11-06 22:45:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][1050/1251]	eta 0:01:34 lr 0.000444	time 0.4621 (0.4691)	loss 3.7040 (3.3020)	grad_norm 1.6609 (nan)	mem 14853MB
[2022-11-06 22:45:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][1100/1251]	eta 0:01:10 lr 0.000444	time 0.4669 (0.4689)	loss 3.7523 (3.3005)	grad_norm 1.6680 (nan)	mem 14853MB
[2022-11-06 22:45:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][1150/1251]	eta 0:00:47 lr 0.000443	time 0.4631 (0.4688)	loss 3.5922 (3.3034)	grad_norm 1.6030 (nan)	mem 14853MB
[2022-11-06 22:46:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][1200/1251]	eta 0:00:23 lr 0.000443	time 0.4611 (0.4687)	loss 3.4453 (3.3004)	grad_norm 1.4861 (nan)	mem 14853MB
[2022-11-06 22:46:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [161/300][1250/1251]	eta 0:00:00 lr 0.000443	time 0.4584 (0.4685)	loss 3.1118 (3.3019)	grad_norm 1.6828 (nan)	mem 14853MB
[2022-11-06 22:46:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 161 training takes 0:09:46
[2022-11-06 22:46:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_161.pth saving......
[2022-11-06 22:46:34 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_161.pth saved !!!
[2022-11-06 22:46:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.479 (1.479)	Loss 0.8375 (0.8375)	Acc@1 79.883 (79.883)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-06 22:46:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.650 Acc@5 94.544
[2022-11-06 22:46:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.7%
[2022-11-06 22:46:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.538 (1.538)	Loss 0.7439 (0.7439)	Acc@1 82.227 (82.227)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-06 22:46:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.446 Acc@5 95.434
[2022-11-06 22:46:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.4%
[2022-11-06 22:46:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.45% at 161 epoch
[2022-11-06 22:46:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][0/1251]	eta 0:40:09 lr 0.000443	time 1.9259 (1.9259)	loss 2.5332 (2.5332)	grad_norm 1.5910 (1.5910)	mem 14853MB
[2022-11-06 22:47:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][50/1251]	eta 0:10:00 lr 0.000443	time 0.4611 (0.5002)	loss 2.8121 (3.2481)	grad_norm 1.8177 (1.6757)	mem 14853MB
[2022-11-06 22:47:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][100/1251]	eta 0:09:19 lr 0.000443	time 0.4669 (0.4859)	loss 2.9802 (3.2146)	grad_norm 1.6885 (1.6648)	mem 14853MB
[2022-11-06 22:48:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][150/1251]	eta 0:08:48 lr 0.000442	time 0.4690 (0.4800)	loss 3.1059 (3.2424)	grad_norm 1.5038 (1.6534)	mem 14853MB
[2022-11-06 22:48:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][200/1251]	eta 0:08:20 lr 0.000442	time 0.4727 (0.4763)	loss 3.1497 (3.2398)	grad_norm 1.6457 (1.6418)	mem 14853MB
[2022-11-06 22:48:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][250/1251]	eta 0:07:54 lr 0.000442	time 0.4633 (0.4743)	loss 3.4130 (3.2360)	grad_norm 2.0802 (1.6531)	mem 14853MB
[2022-11-06 22:49:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][300/1251]	eta 0:07:30 lr 0.000442	time 0.4579 (0.4734)	loss 3.4586 (3.2394)	grad_norm 1.6773 (1.6570)	mem 14853MB
[2022-11-06 22:49:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][350/1251]	eta 0:07:05 lr 0.000442	time 0.4594 (0.4722)	loss 3.1629 (3.2486)	grad_norm 1.5601 (1.6536)	mem 14853MB
[2022-11-06 22:50:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][400/1251]	eta 0:06:41 lr 0.000441	time 0.4666 (0.4714)	loss 3.3156 (3.2576)	grad_norm 1.5561 (1.6509)	mem 14853MB
[2022-11-06 22:50:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][450/1251]	eta 0:06:17 lr 0.000441	time 0.4695 (0.4709)	loss 3.7979 (3.2651)	grad_norm 1.7006 (1.6464)	mem 14853MB
[2022-11-06 22:50:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][500/1251]	eta 0:05:53 lr 0.000441	time 0.4636 (0.4704)	loss 3.5746 (3.2655)	grad_norm 1.3450 (1.6429)	mem 14853MB
[2022-11-06 22:51:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][550/1251]	eta 0:05:29 lr 0.000441	time 0.4630 (0.4703)	loss 4.0240 (3.2831)	grad_norm 1.5941 (1.6418)	mem 14853MB
[2022-11-06 22:51:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][600/1251]	eta 0:05:06 lr 0.000440	time 0.4696 (0.4701)	loss 2.7605 (3.2778)	grad_norm 1.4778 (1.6396)	mem 14853MB
[2022-11-06 22:51:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][650/1251]	eta 0:04:42 lr 0.000440	time 0.4559 (0.4698)	loss 3.8762 (3.2762)	grad_norm 1.5932 (1.6418)	mem 14853MB
[2022-11-06 22:52:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][700/1251]	eta 0:04:18 lr 0.000440	time 0.4558 (0.4694)	loss 3.5137 (3.2826)	grad_norm 1.5619 (1.6429)	mem 14853MB
[2022-11-06 22:52:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][750/1251]	eta 0:03:55 lr 0.000440	time 0.4801 (0.4693)	loss 2.9358 (3.2836)	grad_norm 1.6326 (1.6454)	mem 14853MB
[2022-11-06 22:53:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][800/1251]	eta 0:03:31 lr 0.000440	time 0.4632 (0.4694)	loss 3.4738 (3.2884)	grad_norm 1.5046 (1.6438)	mem 14853MB
[2022-11-06 22:53:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][850/1251]	eta 0:03:08 lr 0.000439	time 0.4694 (0.4692)	loss 3.6036 (3.2885)	grad_norm 1.4365 (1.6408)	mem 14853MB
[2022-11-06 22:53:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][900/1251]	eta 0:02:44 lr 0.000439	time 0.4599 (0.4690)	loss 3.3702 (3.2872)	grad_norm 1.5480 (1.6423)	mem 14853MB
[2022-11-06 22:54:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][950/1251]	eta 0:02:21 lr 0.000439	time 0.4666 (0.4688)	loss 3.1240 (3.2911)	grad_norm 1.5858 (1.6423)	mem 14853MB
[2022-11-06 22:54:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][1000/1251]	eta 0:01:57 lr 0.000439	time 0.4518 (0.4687)	loss 2.3101 (3.2858)	grad_norm 1.7329 (1.6422)	mem 14853MB
[2022-11-06 22:55:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][1050/1251]	eta 0:01:34 lr 0.000439	time 0.4587 (0.4688)	loss 2.4111 (3.2925)	grad_norm 1.6589 (1.6429)	mem 14853MB
[2022-11-06 22:55:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][1100/1251]	eta 0:01:10 lr 0.000438	time 0.4633 (0.4687)	loss 3.6617 (3.2934)	grad_norm 1.6404 (1.6441)	mem 14853MB
[2022-11-06 22:55:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][1150/1251]	eta 0:00:47 lr 0.000438	time 0.4638 (0.4687)	loss 3.8786 (3.2948)	grad_norm 1.6260 (1.6450)	mem 14853MB
[2022-11-06 22:56:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][1200/1251]	eta 0:00:23 lr 0.000438	time 0.4615 (0.4686)	loss 3.9692 (3.3035)	grad_norm 1.6640 (1.6443)	mem 14853MB
[2022-11-06 22:56:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [162/300][1250/1251]	eta 0:00:00 lr 0.000438	time 0.4575 (0.4683)	loss 3.7307 (3.3026)	grad_norm 1.8102 (1.6457)	mem 14853MB
[2022-11-06 22:56:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 162 training takes 0:09:46
[2022-11-06 22:56:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_162.pth saving......
[2022-11-06 22:56:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_162.pth saved !!!
[2022-11-06 22:56:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.491 (1.491)	Loss 0.9195 (0.9195)	Acc@1 77.734 (77.734)	Acc@5 94.043 (94.043)	Mem 14853MB
[2022-11-06 22:56:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.622 Acc@5 94.678
[2022-11-06 22:56:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.6%
[2022-11-06 22:56:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.699 (1.699)	Loss 0.8400 (0.8400)	Acc@1 80.176 (80.176)	Acc@5 94.922 (94.922)	Mem 14853MB
[2022-11-06 22:56:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.480 Acc@5 95.456
[2022-11-06 22:56:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.5%
[2022-11-06 22:56:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.48% at 162 epoch
[2022-11-06 22:56:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][0/1251]	eta 0:40:24 lr 0.000438	time 1.9382 (1.9382)	loss 3.7210 (3.7210)	grad_norm 1.5569 (1.5569)	mem 14853MB
[2022-11-06 22:57:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][50/1251]	eta 0:10:02 lr 0.000438	time 0.4722 (0.5019)	loss 3.7694 (3.3746)	grad_norm 1.6916 (1.6448)	mem 14853MB
[2022-11-06 22:57:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][100/1251]	eta 0:09:17 lr 0.000437	time 0.4747 (0.4847)	loss 2.3579 (3.3438)	grad_norm 1.6715 (1.6678)	mem 14853MB
[2022-11-06 22:58:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][150/1251]	eta 0:08:47 lr 0.000437	time 0.4684 (0.4788)	loss 3.7506 (3.3370)	grad_norm 1.5764 (1.6718)	mem 14853MB
[2022-11-06 22:58:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][200/1251]	eta 0:08:20 lr 0.000437	time 0.4620 (0.4761)	loss 3.3981 (3.3080)	grad_norm 1.5484 (1.6746)	mem 14853MB
[2022-11-06 22:58:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][250/1251]	eta 0:07:54 lr 0.000437	time 0.4719 (0.4738)	loss 4.1738 (3.3237)	grad_norm 1.7745 (1.6748)	mem 14853MB
[2022-11-06 22:59:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][300/1251]	eta 0:07:29 lr 0.000437	time 0.4781 (0.4728)	loss 2.2867 (3.2948)	grad_norm 1.7197 (1.6679)	mem 14853MB
[2022-11-06 22:59:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][350/1251]	eta 0:07:05 lr 0.000436	time 0.4672 (0.4718)	loss 3.4929 (3.3097)	grad_norm 1.4605 (1.6682)	mem 14853MB
[2022-11-06 23:00:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][400/1251]	eta 0:06:40 lr 0.000436	time 0.4627 (0.4708)	loss 2.9120 (3.3127)	grad_norm 1.5943 (1.6658)	mem 14853MB
[2022-11-06 23:00:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][450/1251]	eta 0:06:16 lr 0.000436	time 0.4586 (0.4702)	loss 2.6942 (3.3061)	grad_norm 1.5538 (1.6642)	mem 14853MB
[2022-11-06 23:00:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][500/1251]	eta 0:05:52 lr 0.000436	time 0.4674 (0.4697)	loss 2.0372 (3.3002)	grad_norm 1.4741 (1.6623)	mem 14853MB
[2022-11-06 23:01:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][550/1251]	eta 0:05:29 lr 0.000436	time 0.4694 (0.4701)	loss 3.0989 (3.3036)	grad_norm 1.5345 (1.6644)	mem 14853MB
[2022-11-06 23:01:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][600/1251]	eta 0:05:05 lr 0.000435	time 0.4618 (0.4698)	loss 3.1307 (3.3011)	grad_norm 1.6609 (1.6642)	mem 14853MB
[2022-11-06 23:02:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][650/1251]	eta 0:04:42 lr 0.000435	time 0.4523 (0.4694)	loss 2.8527 (3.3040)	grad_norm 1.5568 (1.6645)	mem 14853MB
[2022-11-06 23:02:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][700/1251]	eta 0:04:18 lr 0.000435	time 0.4662 (0.4692)	loss 3.7699 (3.3021)	grad_norm 1.7089 (1.6635)	mem 14853MB
[2022-11-06 23:02:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][750/1251]	eta 0:03:54 lr 0.000435	time 0.4689 (0.4690)	loss 3.2796 (3.2983)	grad_norm 1.5469 (1.6625)	mem 14853MB
[2022-11-06 23:03:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][800/1251]	eta 0:03:31 lr 0.000435	time 0.4700 (0.4692)	loss 3.5907 (3.2974)	grad_norm 1.6953 (1.6643)	mem 14853MB
[2022-11-06 23:03:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][850/1251]	eta 0:03:08 lr 0.000434	time 0.4620 (0.4691)	loss 3.2885 (3.3029)	grad_norm 1.9577 (1.6658)	mem 14853MB
[2022-11-06 23:03:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][900/1251]	eta 0:02:44 lr 0.000434	time 0.4634 (0.4689)	loss 3.8393 (3.3023)	grad_norm 1.4281 (1.6640)	mem 14853MB
[2022-11-06 23:04:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][950/1251]	eta 0:02:21 lr 0.000434	time 0.4650 (0.4688)	loss 3.6799 (3.3017)	grad_norm 1.8639 (1.6619)	mem 14853MB
[2022-11-06 23:04:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][1000/1251]	eta 0:01:57 lr 0.000434	time 0.4631 (0.4687)	loss 1.8868 (3.3010)	grad_norm 1.5512 (1.6613)	mem 14853MB
[2022-11-06 23:05:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][1050/1251]	eta 0:01:34 lr 0.000434	time 0.4577 (0.4689)	loss 3.7212 (3.3007)	grad_norm 1.8744 (1.6608)	mem 14853MB
[2022-11-06 23:05:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][1100/1251]	eta 0:01:10 lr 0.000433	time 0.4714 (0.4687)	loss 3.6652 (3.2994)	grad_norm 1.5296 (1.6610)	mem 14853MB
[2022-11-06 23:05:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][1150/1251]	eta 0:00:47 lr 0.000433	time 0.4712 (0.4686)	loss 3.5939 (3.2987)	grad_norm 1.5423 (1.6605)	mem 14853MB
[2022-11-06 23:06:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][1200/1251]	eta 0:00:23 lr 0.000433	time 0.4533 (0.4686)	loss 3.7631 (3.2987)	grad_norm 1.5033 (1.6595)	mem 14853MB
[2022-11-06 23:06:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [163/300][1250/1251]	eta 0:00:00 lr 0.000433	time 0.4726 (0.4684)	loss 2.6843 (3.2985)	grad_norm 1.6180 (1.6584)	mem 14853MB
[2022-11-06 23:06:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 163 training takes 0:09:46
[2022-11-06 23:06:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_163.pth saving......
[2022-11-06 23:06:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_163.pth saved !!!
[2022-11-06 23:06:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.594 (1.594)	Loss 0.9244 (0.9244)	Acc@1 79.004 (79.004)	Acc@5 95.312 (95.312)	Mem 14853MB
[2022-11-06 23:06:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.874 Acc@5 94.720
[2022-11-06 23:06:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.9%
[2022-11-06 23:06:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.637 (1.637)	Loss 0.7689 (0.7689)	Acc@1 82.227 (82.227)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-06 23:06:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.494 Acc@5 95.480
[2022-11-06 23:06:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.5%
[2022-11-06 23:06:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.49% at 163 epoch
[2022-11-06 23:07:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][0/1251]	eta 0:42:07 lr 0.000433	time 2.0200 (2.0200)	loss 3.6733 (3.6733)	grad_norm 1.6163 (1.6163)	mem 14853MB
[2022-11-06 23:07:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][50/1251]	eta 0:10:03 lr 0.000432	time 0.4577 (0.5026)	loss 4.0411 (3.3931)	grad_norm 1.5133 (1.6885)	mem 14853MB
[2022-11-06 23:07:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][100/1251]	eta 0:09:20 lr 0.000432	time 0.4655 (0.4869)	loss 2.3135 (3.3767)	grad_norm 1.5424 (1.6779)	mem 14853MB
[2022-11-06 23:08:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][150/1251]	eta 0:08:48 lr 0.000432	time 0.4741 (0.4801)	loss 3.8463 (3.3621)	grad_norm 1.6291 (1.6755)	mem 14853MB
[2022-11-06 23:08:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][200/1251]	eta 0:08:20 lr 0.000432	time 0.4622 (0.4764)	loss 3.3459 (3.3645)	grad_norm 2.1395 (1.6785)	mem 14853MB
[2022-11-06 23:08:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][250/1251]	eta 0:07:54 lr 0.000432	time 0.4610 (0.4740)	loss 3.5291 (3.3563)	grad_norm 1.6812 (1.6754)	mem 14853MB
[2022-11-06 23:09:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][300/1251]	eta 0:07:29 lr 0.000431	time 0.4617 (0.4723)	loss 2.6348 (3.3283)	grad_norm 1.5465 (1.6676)	mem 14853MB
[2022-11-06 23:09:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][350/1251]	eta 0:07:05 lr 0.000431	time 0.4529 (0.4718)	loss 3.5753 (3.3263)	grad_norm 1.6858 (1.6652)	mem 14853MB
[2022-11-06 23:10:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][400/1251]	eta 0:06:40 lr 0.000431	time 0.4736 (0.4710)	loss 3.6885 (3.3182)	grad_norm 1.4254 (1.6596)	mem 14853MB
[2022-11-06 23:10:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][450/1251]	eta 0:06:16 lr 0.000431	time 0.4715 (0.4702)	loss 2.6810 (3.3241)	grad_norm 1.6720 (1.6575)	mem 14853MB
[2022-11-06 23:10:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][500/1251]	eta 0:05:52 lr 0.000431	time 0.4686 (0.4697)	loss 3.1539 (3.3170)	grad_norm 1.6175 (1.6597)	mem 14853MB
[2022-11-06 23:11:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][550/1251]	eta 0:05:29 lr 0.000430	time 0.4700 (0.4697)	loss 3.9567 (3.3136)	grad_norm 1.5938 (1.6554)	mem 14853MB
[2022-11-06 23:11:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][600/1251]	eta 0:05:05 lr 0.000430	time 0.4759 (0.4693)	loss 4.0706 (3.3151)	grad_norm 1.7607 (1.6605)	mem 14853MB
[2022-11-06 23:12:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][650/1251]	eta 0:04:41 lr 0.000430	time 0.4633 (0.4691)	loss 3.1968 (3.3166)	grad_norm 1.5311 (1.6609)	mem 14853MB
[2022-11-06 23:12:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][700/1251]	eta 0:04:18 lr 0.000430	time 0.4618 (0.4690)	loss 3.5596 (3.3086)	grad_norm 1.5103 (1.6600)	mem 14853MB
[2022-11-06 23:12:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][750/1251]	eta 0:03:54 lr 0.000430	time 0.4581 (0.4687)	loss 3.3078 (3.3173)	grad_norm 1.6111 (1.6606)	mem 14853MB
[2022-11-06 23:13:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][800/1251]	eta 0:03:31 lr 0.000429	time 0.4728 (0.4687)	loss 3.3522 (3.3141)	grad_norm 1.6292 (1.6586)	mem 14853MB
[2022-11-06 23:13:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][850/1251]	eta 0:03:07 lr 0.000429	time 0.4594 (0.4687)	loss 3.6841 (3.3129)	grad_norm 1.7229 (1.6605)	mem 14853MB
[2022-11-06 23:14:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][900/1251]	eta 0:02:44 lr 0.000429	time 0.4594 (0.4686)	loss 3.6793 (3.3195)	grad_norm 1.4687 (inf)	mem 14853MB
[2022-11-06 23:14:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][950/1251]	eta 0:02:20 lr 0.000429	time 0.4598 (0.4683)	loss 2.0700 (3.3196)	grad_norm 1.5391 (inf)	mem 14853MB
[2022-11-06 23:14:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][1000/1251]	eta 0:01:57 lr 0.000429	time 0.4655 (0.4683)	loss 2.2398 (3.3114)	grad_norm 1.5018 (inf)	mem 14853MB
[2022-11-06 23:15:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][1050/1251]	eta 0:01:34 lr 0.000428	time 0.4608 (0.4683)	loss 2.3953 (3.3119)	grad_norm 1.5436 (inf)	mem 14853MB
[2022-11-06 23:15:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][1100/1251]	eta 0:01:10 lr 0.000428	time 0.4633 (0.4683)	loss 3.8898 (3.3114)	grad_norm 1.6886 (inf)	mem 14853MB
[2022-11-06 23:15:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][1150/1251]	eta 0:00:47 lr 0.000428	time 0.4603 (0.4682)	loss 2.8005 (3.3152)	grad_norm 1.5748 (inf)	mem 14853MB
[2022-11-06 23:16:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][1200/1251]	eta 0:00:23 lr 0.000428	time 0.4616 (0.4681)	loss 2.8959 (3.3128)	grad_norm 1.7842 (inf)	mem 14853MB
[2022-11-06 23:16:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [164/300][1250/1251]	eta 0:00:00 lr 0.000428	time 0.4583 (0.4679)	loss 2.7190 (3.3177)	grad_norm 1.6499 (inf)	mem 14853MB
[2022-11-06 23:16:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 164 training takes 0:09:45
[2022-11-06 23:16:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_164.pth saving......
[2022-11-06 23:16:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_164.pth saved !!!
[2022-11-06 23:16:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.543 (1.543)	Loss 0.8622 (0.8622)	Acc@1 78.613 (78.613)	Acc@5 95.312 (95.312)	Mem 14853MB
[2022-11-06 23:16:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.434 Acc@5 94.714
[2022-11-06 23:16:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.4%
[2022-11-06 23:16:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.822 (1.822)	Loss 0.8226 (0.8226)	Acc@1 81.055 (81.055)	Acc@5 95.117 (95.117)	Mem 14853MB
[2022-11-06 23:17:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.538 Acc@5 95.494
[2022-11-06 23:17:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.5%
[2022-11-06 23:17:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.54% at 164 epoch
[2022-11-06 23:17:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][0/1251]	eta 0:39:37 lr 0.000428	time 1.9007 (1.9007)	loss 2.8340 (2.8340)	grad_norm 1.9327 (1.9327)	mem 14853MB
[2022-11-06 23:17:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][50/1251]	eta 0:09:58 lr 0.000427	time 0.4614 (0.4983)	loss 3.0172 (3.2647)	grad_norm 1.6562 (1.6190)	mem 14853MB
[2022-11-06 23:17:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][100/1251]	eta 0:09:18 lr 0.000427	time 0.4633 (0.4850)	loss 3.2904 (3.3423)	grad_norm 1.7565 (1.6585)	mem 14853MB
[2022-11-06 23:18:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][150/1251]	eta 0:08:46 lr 0.000427	time 0.4701 (0.4786)	loss 2.8299 (3.2834)	grad_norm 1.7927 (1.6581)	mem 14853MB
[2022-11-06 23:18:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][200/1251]	eta 0:08:19 lr 0.000427	time 0.4630 (0.4757)	loss 3.0537 (3.2989)	grad_norm 1.6561 (1.6696)	mem 14853MB
[2022-11-06 23:19:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][250/1251]	eta 0:07:54 lr 0.000427	time 0.4631 (0.4741)	loss 3.2896 (3.3094)	grad_norm 1.6178 (1.6662)	mem 14853MB
[2022-11-06 23:19:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][300/1251]	eta 0:07:29 lr 0.000426	time 0.4807 (0.4728)	loss 2.2996 (3.3153)	grad_norm 1.6020 (1.6659)	mem 14853MB
[2022-11-06 23:19:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][350/1251]	eta 0:07:04 lr 0.000426	time 0.4658 (0.4716)	loss 3.5884 (3.3139)	grad_norm 1.6407 (1.6689)	mem 14853MB
[2022-11-06 23:20:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][400/1251]	eta 0:06:40 lr 0.000426	time 0.4689 (0.4709)	loss 2.9431 (3.3091)	grad_norm 1.5716 (1.6652)	mem 14853MB
[2022-11-06 23:20:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][450/1251]	eta 0:06:16 lr 0.000426	time 0.4646 (0.4704)	loss 2.4018 (3.3085)	grad_norm 1.6154 (1.6649)	mem 14853MB
[2022-11-06 23:20:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][500/1251]	eta 0:05:53 lr 0.000426	time 0.4562 (0.4702)	loss 3.8725 (3.3091)	grad_norm 1.8835 (1.6685)	mem 14853MB
[2022-11-06 23:21:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][550/1251]	eta 0:05:29 lr 0.000425	time 0.4736 (0.4704)	loss 4.0733 (3.3105)	grad_norm 1.7394 (1.6685)	mem 14853MB
[2022-11-06 23:21:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][600/1251]	eta 0:05:06 lr 0.000425	time 0.4584 (0.4701)	loss 3.4274 (3.3149)	grad_norm 1.5491 (1.6720)	mem 14853MB
[2022-11-06 23:22:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][650/1251]	eta 0:04:42 lr 0.000425	time 0.4556 (0.4698)	loss 2.3238 (3.3150)	grad_norm 1.6420 (1.6758)	mem 14853MB
[2022-11-06 23:22:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][700/1251]	eta 0:04:18 lr 0.000425	time 0.4617 (0.4695)	loss 3.4701 (3.3243)	grad_norm 1.6600 (1.6746)	mem 14853MB
[2022-11-06 23:22:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][750/1251]	eta 0:03:55 lr 0.000424	time 0.4747 (0.4693)	loss 3.1602 (3.3231)	grad_norm 1.6136 (1.6750)	mem 14853MB
[2022-11-06 23:23:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][800/1251]	eta 0:03:31 lr 0.000424	time 0.4612 (0.4693)	loss 3.0387 (3.3209)	grad_norm 1.5631 (1.6754)	mem 14853MB
[2022-11-06 23:23:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][850/1251]	eta 0:03:08 lr 0.000424	time 0.4743 (0.4692)	loss 3.2395 (3.3165)	grad_norm 1.7228 (1.6739)	mem 14853MB
[2022-11-06 23:24:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][900/1251]	eta 0:02:44 lr 0.000424	time 0.4650 (0.4690)	loss 3.0897 (3.3121)	grad_norm 1.5519 (1.6734)	mem 14853MB
[2022-11-06 23:24:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][950/1251]	eta 0:02:21 lr 0.000424	time 0.4614 (0.4689)	loss 3.4508 (3.3058)	grad_norm 1.5632 (1.6732)	mem 14853MB
[2022-11-06 23:24:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][1000/1251]	eta 0:01:57 lr 0.000423	time 0.4718 (0.4688)	loss 3.3703 (3.3012)	grad_norm 1.4693 (1.6741)	mem 14853MB
[2022-11-06 23:25:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][1050/1251]	eta 0:01:34 lr 0.000423	time 0.4599 (0.4689)	loss 3.7485 (3.3043)	grad_norm 1.6175 (1.6744)	mem 14853MB
[2022-11-06 23:25:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][1100/1251]	eta 0:01:10 lr 0.000423	time 0.4625 (0.4688)	loss 3.6651 (3.2979)	grad_norm 1.6798 (1.6744)	mem 14853MB
[2022-11-06 23:26:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][1150/1251]	eta 0:00:47 lr 0.000423	time 0.4782 (0.4687)	loss 3.5806 (3.2983)	grad_norm 1.6402 (1.6744)	mem 14853MB
[2022-11-06 23:26:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][1200/1251]	eta 0:00:23 lr 0.000423	time 0.4643 (0.4686)	loss 3.6458 (3.3011)	grad_norm 1.6498 (1.6741)	mem 14853MB
[2022-11-06 23:26:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [165/300][1250/1251]	eta 0:00:00 lr 0.000422	time 0.4584 (0.4684)	loss 4.0395 (3.2949)	grad_norm 1.8647 (1.6728)	mem 14853MB
[2022-11-06 23:26:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 165 training takes 0:09:46
[2022-11-06 23:26:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_165.pth saving......
[2022-11-06 23:26:50 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_165.pth saved !!!
[2022-11-06 23:26:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.568 (1.568)	Loss 0.8478 (0.8478)	Acc@1 80.957 (80.957)	Acc@5 94.824 (94.824)	Mem 14853MB
[2022-11-06 23:26:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.572 Acc@5 94.676
[2022-11-06 23:26:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.6%
[2022-11-06 23:27:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.579 (1.579)	Loss 0.7558 (0.7558)	Acc@1 83.398 (83.398)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-06 23:27:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.556 Acc@5 95.526
[2022-11-06 23:27:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.6%
[2022-11-06 23:27:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.56% at 165 epoch
[2022-11-06 23:27:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][0/1251]	eta 0:41:55 lr 0.000422	time 2.0112 (2.0112)	loss 3.9839 (3.9839)	grad_norm 1.7487 (1.7487)	mem 14853MB
[2022-11-06 23:27:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][50/1251]	eta 0:10:06 lr 0.000422	time 0.5632 (0.5047)	loss 3.6745 (3.2197)	grad_norm 1.8060 (1.7042)	mem 14853MB
[2022-11-06 23:27:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][100/1251]	eta 0:09:19 lr 0.000422	time 0.4537 (0.4857)	loss 3.5091 (3.3048)	grad_norm 2.2753 (1.7224)	mem 14853MB
[2022-11-06 23:28:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][150/1251]	eta 0:08:48 lr 0.000422	time 0.4663 (0.4797)	loss 2.7876 (3.2752)	grad_norm 1.6053 (1.7075)	mem 14853MB
[2022-11-06 23:28:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][200/1251]	eta 0:08:20 lr 0.000422	time 0.4523 (0.4767)	loss 3.2032 (3.2483)	grad_norm 1.8148 (1.6971)	mem 14853MB
[2022-11-06 23:29:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][250/1251]	eta 0:07:54 lr 0.000421	time 0.4688 (0.4744)	loss 3.6716 (3.2312)	grad_norm 1.7170 (1.6985)	mem 14853MB
[2022-11-06 23:29:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][300/1251]	eta 0:07:29 lr 0.000421	time 0.4638 (0.4729)	loss 3.4485 (3.2527)	grad_norm 1.6378 (1.6977)	mem 14853MB
[2022-11-06 23:29:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][350/1251]	eta 0:07:05 lr 0.000421	time 0.4523 (0.4723)	loss 3.7703 (3.2656)	grad_norm 1.6961 (1.6987)	mem 14853MB
[2022-11-06 23:30:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][400/1251]	eta 0:06:41 lr 0.000421	time 0.4630 (0.4716)	loss 3.1933 (3.2579)	grad_norm 2.0131 (1.6973)	mem 14853MB
[2022-11-06 23:30:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][450/1251]	eta 0:06:17 lr 0.000421	time 0.4577 (0.4709)	loss 3.6918 (3.2583)	grad_norm 1.6081 (1.6948)	mem 14853MB
[2022-11-06 23:31:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][500/1251]	eta 0:05:53 lr 0.000420	time 0.4658 (0.4704)	loss 3.4546 (3.2638)	grad_norm 1.7173 (1.6934)	mem 14853MB
[2022-11-06 23:31:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][550/1251]	eta 0:05:29 lr 0.000420	time 0.4665 (0.4700)	loss 2.5398 (3.2592)	grad_norm 1.9495 (1.6955)	mem 14853MB
[2022-11-06 23:31:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][600/1251]	eta 0:05:05 lr 0.000420	time 0.4519 (0.4698)	loss 3.0542 (3.2649)	grad_norm 1.7343 (1.6912)	mem 14853MB
[2022-11-06 23:32:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][650/1251]	eta 0:04:42 lr 0.000420	time 0.4692 (0.4699)	loss 3.9370 (3.2664)	grad_norm 1.6510 (1.6889)	mem 14853MB
[2022-11-06 23:32:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][700/1251]	eta 0:04:18 lr 0.000420	time 0.4711 (0.4697)	loss 3.7936 (3.2728)	grad_norm 1.5475 (inf)	mem 14853MB
[2022-11-06 23:33:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][750/1251]	eta 0:03:55 lr 0.000419	time 0.4683 (0.4694)	loss 3.7779 (3.2722)	grad_norm 1.7042 (inf)	mem 14853MB
[2022-11-06 23:33:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][800/1251]	eta 0:03:31 lr 0.000419	time 0.4664 (0.4693)	loss 2.3123 (3.2652)	grad_norm 1.6130 (inf)	mem 14853MB
[2022-11-06 23:33:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][850/1251]	eta 0:03:08 lr 0.000419	time 0.4672 (0.4692)	loss 3.6973 (3.2595)	grad_norm 1.6903 (inf)	mem 14853MB
[2022-11-06 23:34:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][900/1251]	eta 0:02:44 lr 0.000419	time 0.4584 (0.4692)	loss 3.6188 (3.2596)	grad_norm 1.7879 (inf)	mem 14853MB
[2022-11-06 23:34:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][950/1251]	eta 0:02:21 lr 0.000419	time 0.4634 (0.4690)	loss 3.5014 (3.2633)	grad_norm 1.7063 (inf)	mem 14853MB
[2022-11-06 23:34:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][1000/1251]	eta 0:01:57 lr 0.000418	time 0.4647 (0.4688)	loss 3.5860 (3.2631)	grad_norm 1.6259 (inf)	mem 14853MB
[2022-11-06 23:35:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][1050/1251]	eta 0:01:34 lr 0.000418	time 0.4757 (0.4687)	loss 3.6124 (3.2568)	grad_norm 1.6513 (inf)	mem 14853MB
[2022-11-06 23:35:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][1100/1251]	eta 0:01:10 lr 0.000418	time 0.4704 (0.4687)	loss 3.7216 (3.2616)	grad_norm 1.6054 (inf)	mem 14853MB
[2022-11-06 23:36:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][1150/1251]	eta 0:00:47 lr 0.000418	time 0.4629 (0.4687)	loss 3.3936 (3.2650)	grad_norm 1.9191 (inf)	mem 14853MB
[2022-11-06 23:36:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][1200/1251]	eta 0:00:23 lr 0.000418	time 0.4532 (0.4685)	loss 3.7172 (3.2724)	grad_norm 1.6716 (inf)	mem 14853MB
[2022-11-06 23:36:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [166/300][1250/1251]	eta 0:00:00 lr 0.000417	time 0.4578 (0.4684)	loss 3.7672 (3.2748)	grad_norm 1.7315 (inf)	mem 14853MB
[2022-11-06 23:36:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 166 training takes 0:09:46
[2022-11-06 23:36:53 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_166.pth saving......
[2022-11-06 23:36:54 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_166.pth saved !!!
[2022-11-06 23:36:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.510 (1.510)	Loss 0.9351 (0.9351)	Acc@1 78.223 (78.223)	Acc@5 94.531 (94.531)	Mem 14853MB
[2022-11-06 23:37:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.626 Acc@5 94.660
[2022-11-06 23:37:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.6%
[2022-11-06 23:37:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.630 (1.630)	Loss 0.8798 (0.8798)	Acc@1 78.809 (78.809)	Acc@5 94.629 (94.629)	Mem 14853MB
[2022-11-06 23:37:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.582 Acc@5 95.534
[2022-11-06 23:37:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.6%
[2022-11-06 23:37:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.58% at 166 epoch
[2022-11-06 23:37:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][0/1251]	eta 0:42:00 lr 0.000417	time 2.0150 (2.0150)	loss 3.9085 (3.9085)	grad_norm 1.5440 (1.5440)	mem 14853MB
[2022-11-06 23:37:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][50/1251]	eta 0:09:59 lr 0.000417	time 0.4646 (0.4992)	loss 3.4635 (3.2865)	grad_norm 1.5448 (1.6818)	mem 14853MB
[2022-11-06 23:38:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][100/1251]	eta 0:09:17 lr 0.000417	time 0.4682 (0.4843)	loss 3.3208 (3.2721)	grad_norm 1.5579 (1.6660)	mem 14853MB
[2022-11-06 23:38:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][150/1251]	eta 0:08:47 lr 0.000417	time 0.4684 (0.4794)	loss 2.4917 (3.2911)	grad_norm 1.7259 (1.6732)	mem 14853MB
[2022-11-06 23:38:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][200/1251]	eta 0:08:20 lr 0.000417	time 0.4639 (0.4760)	loss 3.4978 (3.3308)	grad_norm 1.8144 (1.6781)	mem 14853MB
[2022-11-06 23:39:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][250/1251]	eta 0:07:54 lr 0.000416	time 0.4660 (0.4744)	loss 2.6468 (3.3371)	grad_norm 2.0691 (1.6888)	mem 14853MB
[2022-11-06 23:39:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][300/1251]	eta 0:07:29 lr 0.000416	time 0.4747 (0.4731)	loss 3.7878 (3.3378)	grad_norm 1.7982 (1.6843)	mem 14853MB
[2022-11-06 23:39:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][350/1251]	eta 0:07:05 lr 0.000416	time 0.4644 (0.4721)	loss 3.2902 (3.3116)	grad_norm 1.6836 (nan)	mem 14853MB
[2022-11-06 23:40:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][400/1251]	eta 0:06:40 lr 0.000416	time 0.4559 (0.4712)	loss 3.7729 (3.3040)	grad_norm 1.9378 (nan)	mem 14853MB
[2022-11-06 23:40:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][450/1251]	eta 0:06:16 lr 0.000416	time 0.4594 (0.4705)	loss 3.1523 (3.3028)	grad_norm 1.7539 (nan)	mem 14853MB
[2022-11-06 23:41:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][500/1251]	eta 0:05:53 lr 0.000415	time 0.4605 (0.4702)	loss 3.7056 (3.3039)	grad_norm 1.8811 (nan)	mem 14853MB
[2022-11-06 23:41:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][550/1251]	eta 0:05:29 lr 0.000415	time 0.4721 (0.4699)	loss 3.8676 (3.3070)	grad_norm 1.5884 (nan)	mem 14853MB
[2022-11-06 23:41:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][600/1251]	eta 0:05:05 lr 0.000415	time 0.4597 (0.4697)	loss 4.2681 (3.3089)	grad_norm 1.7753 (nan)	mem 14853MB
[2022-11-06 23:42:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][650/1251]	eta 0:04:42 lr 0.000415	time 0.4537 (0.4693)	loss 3.7163 (3.3103)	grad_norm 1.7895 (nan)	mem 14853MB
[2022-11-06 23:42:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][700/1251]	eta 0:04:18 lr 0.000414	time 0.4623 (0.4691)	loss 4.0095 (3.3084)	grad_norm 1.5922 (nan)	mem 14853MB
[2022-11-06 23:43:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][750/1251]	eta 0:03:54 lr 0.000414	time 0.4624 (0.4690)	loss 3.2286 (3.3022)	grad_norm 1.7870 (nan)	mem 14853MB
[2022-11-06 23:43:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][800/1251]	eta 0:03:31 lr 0.000414	time 0.5401 (0.4691)	loss 3.2380 (3.3109)	grad_norm 1.4922 (nan)	mem 14853MB
[2022-11-06 23:43:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][850/1251]	eta 0:03:08 lr 0.000414	time 0.4801 (0.4690)	loss 3.7345 (3.3064)	grad_norm 1.6030 (nan)	mem 14853MB
[2022-11-06 23:44:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][900/1251]	eta 0:02:44 lr 0.000414	time 0.4624 (0.4689)	loss 2.4176 (3.3009)	grad_norm 1.5755 (nan)	mem 14853MB
[2022-11-06 23:44:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][950/1251]	eta 0:02:21 lr 0.000413	time 0.4780 (0.4687)	loss 2.5418 (3.3011)	grad_norm 1.8665 (nan)	mem 14853MB
[2022-11-06 23:45:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][1000/1251]	eta 0:01:57 lr 0.000413	time 0.4690 (0.4686)	loss 2.4515 (3.3014)	grad_norm 1.6976 (nan)	mem 14853MB
[2022-11-06 23:45:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][1050/1251]	eta 0:01:34 lr 0.000413	time 0.4733 (0.4689)	loss 2.8536 (3.2980)	grad_norm 1.9262 (nan)	mem 14853MB
[2022-11-06 23:45:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][1100/1251]	eta 0:01:10 lr 0.000413	time 0.4583 (0.4688)	loss 3.1677 (3.2991)	grad_norm 1.5537 (nan)	mem 14853MB
[2022-11-06 23:46:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][1150/1251]	eta 0:00:47 lr 0.000413	time 0.4613 (0.4686)	loss 2.7059 (3.2973)	grad_norm 1.5558 (nan)	mem 14853MB
[2022-11-06 23:46:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][1200/1251]	eta 0:00:23 lr 0.000412	time 0.4579 (0.4685)	loss 4.0376 (3.2945)	grad_norm 1.7488 (nan)	mem 14853MB
[2022-11-06 23:46:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [167/300][1250/1251]	eta 0:00:00 lr 0.000412	time 0.4568 (0.4683)	loss 2.8519 (3.2916)	grad_norm 1.5219 (nan)	mem 14853MB
[2022-11-06 23:46:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 167 training takes 0:09:45
[2022-11-06 23:46:57 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_167.pth saving......
[2022-11-06 23:46:58 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_167.pth saved !!!
[2022-11-06 23:46:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.507 (1.507)	Loss 0.8977 (0.8977)	Acc@1 78.516 (78.516)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-06 23:47:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.836 Acc@5 94.750
[2022-11-06 23:47:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.8%
[2022-11-06 23:47:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.600 (1.600)	Loss 0.7436 (0.7436)	Acc@1 82.324 (82.324)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-06 23:47:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.620 Acc@5 95.586
[2022-11-06 23:47:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.6%
[2022-11-06 23:47:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.62% at 167 epoch
[2022-11-06 23:47:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][0/1251]	eta 0:41:54 lr 0.000412	time 2.0101 (2.0101)	loss 3.2253 (3.2253)	grad_norm 1.7625 (1.7625)	mem 14853MB
[2022-11-06 23:47:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][50/1251]	eta 0:10:04 lr 0.000412	time 0.4600 (0.5035)	loss 3.4018 (3.2965)	grad_norm 1.9571 (1.6857)	mem 14853MB
[2022-11-06 23:48:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][100/1251]	eta 0:09:18 lr 0.000412	time 0.4738 (0.4853)	loss 3.8252 (3.3219)	grad_norm 1.5242 (1.6864)	mem 14853MB
[2022-11-06 23:48:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][150/1251]	eta 0:08:47 lr 0.000412	time 0.4703 (0.4790)	loss 3.3839 (3.2629)	grad_norm 1.6236 (1.6751)	mem 14853MB
[2022-11-06 23:48:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][200/1251]	eta 0:08:20 lr 0.000411	time 0.4619 (0.4762)	loss 2.7950 (3.2568)	grad_norm 1.6606 (1.6832)	mem 14853MB
[2022-11-06 23:49:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][250/1251]	eta 0:07:55 lr 0.000411	time 0.4591 (0.4745)	loss 3.1847 (3.2641)	grad_norm 1.5644 (1.6760)	mem 14853MB
[2022-11-06 23:49:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][300/1251]	eta 0:07:29 lr 0.000411	time 0.4596 (0.4729)	loss 3.0982 (3.2547)	grad_norm 1.7811 (1.6790)	mem 14853MB
[2022-11-06 23:50:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][350/1251]	eta 0:07:05 lr 0.000411	time 0.4681 (0.4718)	loss 2.7606 (3.2716)	grad_norm 1.7303 (1.6748)	mem 14853MB
[2022-11-06 23:50:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][400/1251]	eta 0:06:40 lr 0.000411	time 0.4577 (0.4710)	loss 3.5034 (3.2697)	grad_norm 1.5829 (1.6782)	mem 14853MB
[2022-11-06 23:50:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][450/1251]	eta 0:06:16 lr 0.000410	time 0.4656 (0.4703)	loss 3.6075 (3.2620)	grad_norm 1.6751 (1.6758)	mem 14853MB
[2022-11-06 23:51:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][500/1251]	eta 0:05:52 lr 0.000410	time 0.4556 (0.4697)	loss 3.0726 (3.2638)	grad_norm 1.5739 (1.6768)	mem 14853MB
[2022-11-06 23:51:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][550/1251]	eta 0:05:29 lr 0.000410	time 0.4584 (0.4698)	loss 3.9687 (3.2515)	grad_norm 1.7380 (1.6785)	mem 14853MB
[2022-11-06 23:51:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][600/1251]	eta 0:05:05 lr 0.000410	time 0.4620 (0.4695)	loss 3.5991 (3.2600)	grad_norm 1.7711 (1.6802)	mem 14853MB
[2022-11-06 23:52:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][650/1251]	eta 0:04:41 lr 0.000410	time 0.4666 (0.4691)	loss 3.8903 (3.2551)	grad_norm 1.7265 (1.6806)	mem 14853MB
[2022-11-06 23:52:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][700/1251]	eta 0:04:18 lr 0.000409	time 0.4710 (0.4691)	loss 2.6790 (3.2547)	grad_norm 1.6665 (1.6829)	mem 14853MB
[2022-11-06 23:53:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][750/1251]	eta 0:03:54 lr 0.000409	time 0.4757 (0.4689)	loss 2.7372 (3.2594)	grad_norm 1.6425 (1.6896)	mem 14853MB
[2022-11-06 23:53:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][800/1251]	eta 0:03:31 lr 0.000409	time 0.5526 (0.4690)	loss 2.4737 (3.2599)	grad_norm 1.5334 (1.6880)	mem 14853MB
[2022-11-06 23:53:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][850/1251]	eta 0:03:07 lr 0.000409	time 0.4693 (0.4688)	loss 3.9331 (3.2741)	grad_norm 1.5884 (1.6907)	mem 14853MB
[2022-11-06 23:54:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][900/1251]	eta 0:02:44 lr 0.000409	time 0.5469 (0.4687)	loss 3.4211 (3.2758)	grad_norm 1.7130 (1.6905)	mem 14853MB
[2022-11-06 23:54:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][950/1251]	eta 0:02:21 lr 0.000408	time 0.4630 (0.4686)	loss 3.5858 (3.2718)	grad_norm 1.6312 (1.6891)	mem 14853MB
[2022-11-06 23:55:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][1000/1251]	eta 0:01:57 lr 0.000408	time 0.4613 (0.4684)	loss 2.5003 (3.2751)	grad_norm 2.0662 (1.6906)	mem 14853MB
[2022-11-06 23:55:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][1050/1251]	eta 0:01:34 lr 0.000408	time 0.4543 (0.4685)	loss 3.6543 (3.2773)	grad_norm 1.6458 (1.6921)	mem 14853MB
[2022-11-06 23:55:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][1100/1251]	eta 0:01:10 lr 0.000408	time 0.4716 (0.4684)	loss 3.1435 (3.2750)	grad_norm 1.6236 (1.6926)	mem 14853MB
[2022-11-06 23:56:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][1150/1251]	eta 0:00:47 lr 0.000408	time 0.4646 (0.4684)	loss 3.1980 (3.2678)	grad_norm 1.5105 (1.6918)	mem 14853MB
[2022-11-06 23:56:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][1200/1251]	eta 0:00:23 lr 0.000407	time 0.4588 (0.4683)	loss 3.3641 (3.2611)	grad_norm 1.5999 (1.6908)	mem 14853MB
[2022-11-06 23:57:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [168/300][1250/1251]	eta 0:00:00 lr 0.000407	time 0.4556 (0.4681)	loss 3.9780 (3.2657)	grad_norm 1.5858 (1.6908)	mem 14853MB
[2022-11-06 23:57:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 168 training takes 0:09:45
[2022-11-06 23:57:01 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_168.pth saving......
[2022-11-06 23:57:02 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_168.pth saved !!!
[2022-11-06 23:57:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.618 (1.618)	Loss 0.9047 (0.9047)	Acc@1 79.492 (79.492)	Acc@5 94.141 (94.141)	Mem 14853MB
[2022-11-06 23:57:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.774 Acc@5 94.766
[2022-11-06 23:57:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.8%
[2022-11-06 23:57:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.642 (1.642)	Loss 0.8134 (0.8134)	Acc@1 81.250 (81.250)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-06 23:57:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.626 Acc@5 95.588
[2022-11-06 23:57:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.6%
[2022-11-06 23:57:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.63% at 168 epoch
[2022-11-06 23:57:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][0/1251]	eta 0:42:32 lr 0.000407	time 2.0404 (2.0404)	loss 3.4581 (3.4581)	grad_norm 1.6145 (1.6145)	mem 14853MB
[2022-11-06 23:57:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][50/1251]	eta 0:10:01 lr 0.000407	time 0.4663 (0.5011)	loss 3.3915 (3.1138)	grad_norm 1.7309 (1.7020)	mem 14853MB
[2022-11-06 23:58:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][100/1251]	eta 0:09:19 lr 0.000407	time 0.4507 (0.4859)	loss 3.4229 (3.2299)	grad_norm 1.7074 (1.7125)	mem 14853MB
[2022-11-06 23:58:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][150/1251]	eta 0:08:48 lr 0.000407	time 0.4595 (0.4802)	loss 2.8748 (3.2311)	grad_norm 1.7652 (1.7039)	mem 14853MB
[2022-11-06 23:58:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][200/1251]	eta 0:08:21 lr 0.000406	time 0.4688 (0.4769)	loss 3.8563 (3.2071)	grad_norm 1.9805 (1.6955)	mem 14853MB
[2022-11-06 23:59:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][250/1251]	eta 0:07:55 lr 0.000406	time 0.4632 (0.4748)	loss 3.3050 (3.2372)	grad_norm 1.6386 (1.6970)	mem 14853MB
[2022-11-06 23:59:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][300/1251]	eta 0:07:30 lr 0.000406	time 0.4647 (0.4735)	loss 3.1213 (3.2388)	grad_norm 1.5142 (1.6888)	mem 14853MB
[2022-11-07 00:00:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][350/1251]	eta 0:07:06 lr 0.000406	time 0.4725 (0.4729)	loss 2.7146 (3.2364)	grad_norm 1.6202 (1.6865)	mem 14853MB
[2022-11-07 00:00:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][400/1251]	eta 0:06:41 lr 0.000406	time 0.4614 (0.4719)	loss 3.5959 (3.2460)	grad_norm 1.7627 (1.6904)	mem 14853MB
[2022-11-07 00:00:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][450/1251]	eta 0:06:17 lr 0.000405	time 0.4634 (0.4711)	loss 2.6663 (3.2535)	grad_norm 1.6059 (1.6941)	mem 14853MB
[2022-11-07 00:01:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][500/1251]	eta 0:05:53 lr 0.000405	time 0.4701 (0.4703)	loss 3.2450 (3.2583)	grad_norm 1.6023 (1.6905)	mem 14853MB
[2022-11-07 00:01:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][550/1251]	eta 0:05:29 lr 0.000405	time 0.4634 (0.4701)	loss 3.2327 (3.2634)	grad_norm 1.6585 (1.6902)	mem 14853MB
[2022-11-07 00:02:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][600/1251]	eta 0:05:05 lr 0.000405	time 0.4581 (0.4698)	loss 2.8007 (3.2741)	grad_norm 2.2572 (1.6942)	mem 14853MB
[2022-11-07 00:02:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][650/1251]	eta 0:04:42 lr 0.000405	time 0.4832 (0.4696)	loss 3.7198 (3.2731)	grad_norm 1.4633 (1.6925)	mem 14853MB
[2022-11-07 00:02:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][700/1251]	eta 0:04:18 lr 0.000404	time 0.4618 (0.4693)	loss 3.3942 (3.2729)	grad_norm 1.6109 (1.6936)	mem 14853MB
[2022-11-07 00:03:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][750/1251]	eta 0:03:55 lr 0.000404	time 0.4587 (0.4691)	loss 3.8872 (3.2752)	grad_norm 1.6002 (1.6943)	mem 14853MB
[2022-11-07 00:03:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][800/1251]	eta 0:03:31 lr 0.000404	time 0.4636 (0.4691)	loss 3.6566 (3.2746)	grad_norm 1.6355 (1.6944)	mem 14853MB
[2022-11-07 00:03:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][850/1251]	eta 0:03:08 lr 0.000404	time 0.4694 (0.4690)	loss 3.2218 (3.2720)	grad_norm 1.6380 (1.6936)	mem 14853MB
[2022-11-07 00:04:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][900/1251]	eta 0:02:44 lr 0.000404	time 0.4695 (0.4689)	loss 3.8685 (3.2679)	grad_norm 1.6446 (1.6940)	mem 14853MB
[2022-11-07 00:04:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][950/1251]	eta 0:02:21 lr 0.000403	time 0.4623 (0.4687)	loss 3.5878 (3.2682)	grad_norm 1.6224 (1.6940)	mem 14853MB
[2022-11-07 00:05:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][1000/1251]	eta 0:01:57 lr 0.000403	time 0.4725 (0.4686)	loss 3.8234 (3.2753)	grad_norm 1.5507 (1.6930)	mem 14853MB
[2022-11-07 00:05:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][1050/1251]	eta 0:01:34 lr 0.000403	time 0.4603 (0.4687)	loss 3.0814 (3.2789)	grad_norm 1.4139 (1.6911)	mem 14853MB
[2022-11-07 00:05:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][1100/1251]	eta 0:01:10 lr 0.000403	time 0.4679 (0.4686)	loss 3.8128 (3.2774)	grad_norm 1.6078 (1.6901)	mem 14853MB
[2022-11-07 00:06:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][1150/1251]	eta 0:00:47 lr 0.000402	time 0.4786 (0.4686)	loss 3.9802 (3.2791)	grad_norm 1.7764 (1.6921)	mem 14853MB
[2022-11-07 00:06:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][1200/1251]	eta 0:00:23 lr 0.000402	time 0.4719 (0.4684)	loss 2.3788 (3.2767)	grad_norm 1.6483 (1.6930)	mem 14853MB
[2022-11-07 00:07:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [169/300][1250/1251]	eta 0:00:00 lr 0.000402	time 0.4575 (0.4682)	loss 2.8564 (3.2779)	grad_norm 1.8734 (1.6926)	mem 14853MB
[2022-11-07 00:07:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 169 training takes 0:09:45
[2022-11-07 00:07:05 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_169.pth saving......
[2022-11-07 00:07:06 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_169.pth saved !!!
[2022-11-07 00:07:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.664 (1.664)	Loss 0.9377 (0.9377)	Acc@1 79.199 (79.199)	Acc@5 93.848 (93.848)	Mem 14853MB
[2022-11-07 00:07:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.738 Acc@5 94.660
[2022-11-07 00:07:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.7%
[2022-11-07 00:07:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.654 (1.654)	Loss 0.7809 (0.7809)	Acc@1 80.762 (80.762)	Acc@5 96.289 (96.289)	Mem 14853MB
[2022-11-07 00:07:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.694 Acc@5 95.594
[2022-11-07 00:07:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.7%
[2022-11-07 00:07:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.69% at 169 epoch
[2022-11-07 00:07:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][0/1251]	eta 0:41:09 lr 0.000402	time 1.9741 (1.9741)	loss 2.5259 (2.5259)	grad_norm 1.6931 (1.6931)	mem 14853MB
[2022-11-07 00:07:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][50/1251]	eta 0:09:58 lr 0.000402	time 0.4619 (0.4983)	loss 3.5481 (3.2736)	grad_norm 1.7797 (1.6763)	mem 14853MB
[2022-11-07 00:08:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][100/1251]	eta 0:09:16 lr 0.000402	time 0.4594 (0.4836)	loss 3.4940 (3.2864)	grad_norm 1.6421 (1.6824)	mem 14853MB
[2022-11-07 00:08:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][150/1251]	eta 0:08:46 lr 0.000401	time 0.4692 (0.4784)	loss 3.2284 (3.2970)	grad_norm 1.6034 (1.6832)	mem 14853MB
[2022-11-07 00:08:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][200/1251]	eta 0:08:19 lr 0.000401	time 0.4633 (0.4753)	loss 2.5928 (3.2843)	grad_norm 1.5777 (1.6857)	mem 14853MB
[2022-11-07 00:09:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][250/1251]	eta 0:07:54 lr 0.000401	time 0.4630 (0.4736)	loss 4.1548 (3.2697)	grad_norm 1.9282 (1.6891)	mem 14853MB
[2022-11-07 00:09:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][300/1251]	eta 0:07:29 lr 0.000401	time 0.4585 (0.4724)	loss 3.4601 (3.2749)	grad_norm 1.7737 (1.7046)	mem 14853MB
[2022-11-07 00:10:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][350/1251]	eta 0:07:04 lr 0.000401	time 0.4829 (0.4715)	loss 3.0059 (3.2733)	grad_norm 1.7358 (1.7114)	mem 14853MB
[2022-11-07 00:10:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][400/1251]	eta 0:06:40 lr 0.000400	time 0.4651 (0.4707)	loss 3.9147 (3.2694)	grad_norm 1.9077 (1.7096)	mem 14853MB
[2022-11-07 00:10:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][450/1251]	eta 0:06:16 lr 0.000400	time 0.4579 (0.4702)	loss 2.9570 (3.2609)	grad_norm 1.6077 (1.7073)	mem 14853MB
[2022-11-07 00:11:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][500/1251]	eta 0:05:52 lr 0.000400	time 0.4663 (0.4699)	loss 3.2684 (3.2631)	grad_norm 1.5177 (1.7114)	mem 14853MB
[2022-11-07 00:11:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][550/1251]	eta 0:05:29 lr 0.000400	time 0.4627 (0.4696)	loss 2.8725 (3.2648)	grad_norm 1.6290 (1.7105)	mem 14853MB
[2022-11-07 00:12:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][600/1251]	eta 0:05:05 lr 0.000400	time 0.4564 (0.4695)	loss 3.7430 (3.2651)	grad_norm 1.7419 (1.7058)	mem 14853MB
[2022-11-07 00:12:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][650/1251]	eta 0:04:42 lr 0.000399	time 0.4687 (0.4693)	loss 3.7233 (3.2614)	grad_norm 1.8080 (1.7090)	mem 14853MB
[2022-11-07 00:12:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][700/1251]	eta 0:04:18 lr 0.000399	time 0.4645 (0.4692)	loss 3.5480 (3.2639)	grad_norm 1.6923 (1.7113)	mem 14853MB
[2022-11-07 00:13:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][750/1251]	eta 0:03:54 lr 0.000399	time 0.4604 (0.4690)	loss 2.3268 (3.2673)	grad_norm 1.6546 (1.7146)	mem 14853MB
[2022-11-07 00:13:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][800/1251]	eta 0:03:31 lr 0.000399	time 0.4639 (0.4689)	loss 3.5248 (3.2667)	grad_norm 1.7508 (1.7130)	mem 14853MB
[2022-11-07 00:14:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][850/1251]	eta 0:03:08 lr 0.000399	time 0.4530 (0.4689)	loss 3.1854 (3.2760)	grad_norm 1.5921 (1.7143)	mem 14853MB
[2022-11-07 00:14:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][900/1251]	eta 0:02:44 lr 0.000398	time 0.4653 (0.4688)	loss 2.6108 (3.2759)	grad_norm 1.7756 (1.7131)	mem 14853MB
[2022-11-07 00:14:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][950/1251]	eta 0:02:21 lr 0.000398	time 0.4587 (0.4687)	loss 3.8181 (3.2767)	grad_norm 1.9218 (1.7171)	mem 14853MB
[2022-11-07 00:15:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][1000/1251]	eta 0:01:57 lr 0.000398	time 0.4632 (0.4685)	loss 3.8246 (3.2721)	grad_norm 1.6223 (1.7163)	mem 14853MB
[2022-11-07 00:15:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][1050/1251]	eta 0:01:34 lr 0.000398	time 0.4651 (0.4684)	loss 2.3317 (3.2676)	grad_norm 1.5097 (1.7148)	mem 14853MB
[2022-11-07 00:15:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][1100/1251]	eta 0:01:10 lr 0.000398	time 0.4650 (0.4684)	loss 3.7512 (3.2644)	grad_norm 1.6120 (1.7146)	mem 14853MB
[2022-11-07 00:16:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][1150/1251]	eta 0:00:47 lr 0.000397	time 0.4727 (0.4683)	loss 3.6845 (3.2625)	grad_norm 1.7359 (1.7142)	mem 14853MB
[2022-11-07 00:16:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][1200/1251]	eta 0:00:23 lr 0.000397	time 0.4588 (0.4684)	loss 3.3978 (3.2681)	grad_norm 1.5120 (1.7154)	mem 14853MB
[2022-11-07 00:17:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [170/300][1250/1251]	eta 0:00:00 lr 0.000397	time 0.4575 (0.4682)	loss 3.1955 (3.2650)	grad_norm 1.6703 (1.7160)	mem 14853MB
[2022-11-07 00:17:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 170 training takes 0:09:45
[2022-11-07 00:17:10 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_170.pth saving......
[2022-11-07 00:17:10 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_170.pth saved !!!
[2022-11-07 00:17:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.535 (1.535)	Loss 0.8737 (0.8737)	Acc@1 80.566 (80.566)	Acc@5 95.020 (95.020)	Mem 14853MB
[2022-11-07 00:17:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.790 Acc@5 94.642
[2022-11-07 00:17:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.8%
[2022-11-07 00:17:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.728 (1.728)	Loss 0.8117 (0.8117)	Acc@1 81.250 (81.250)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 00:17:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.768 Acc@5 95.628
[2022-11-07 00:17:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.8%
[2022-11-07 00:17:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.77% at 170 epoch
[2022-11-07 00:17:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][0/1251]	eta 0:39:55 lr 0.000397	time 1.9146 (1.9146)	loss 4.3367 (4.3367)	grad_norm 2.2117 (2.2117)	mem 14853MB
[2022-11-07 00:17:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][50/1251]	eta 0:09:55 lr 0.000397	time 0.4537 (0.4958)	loss 2.2177 (3.1837)	grad_norm 1.6946 (1.6694)	mem 14853MB
[2022-11-07 00:18:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][100/1251]	eta 0:09:18 lr 0.000397	time 0.4658 (0.4856)	loss 2.1593 (3.1730)	grad_norm 1.6419 (1.7212)	mem 14853MB
[2022-11-07 00:18:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][150/1251]	eta 0:08:48 lr 0.000396	time 0.4623 (0.4799)	loss 3.0949 (3.2219)	grad_norm 1.8481 (1.7451)	mem 14853MB
[2022-11-07 00:19:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][200/1251]	eta 0:08:20 lr 0.000396	time 0.4584 (0.4764)	loss 2.9482 (3.2217)	grad_norm 1.6852 (1.7422)	mem 14853MB
[2022-11-07 00:19:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][250/1251]	eta 0:07:54 lr 0.000396	time 0.4704 (0.4742)	loss 3.7362 (3.2285)	grad_norm 1.5156 (nan)	mem 14853MB
[2022-11-07 00:19:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][300/1251]	eta 0:07:29 lr 0.000396	time 0.4575 (0.4730)	loss 3.1492 (3.2043)	grad_norm 1.5967 (nan)	mem 14853MB
[2022-11-07 00:20:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][350/1251]	eta 0:07:05 lr 0.000396	time 0.4575 (0.4720)	loss 3.1816 (3.2136)	grad_norm 1.6431 (nan)	mem 14853MB
[2022-11-07 00:20:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][400/1251]	eta 0:06:41 lr 0.000395	time 0.4728 (0.4716)	loss 3.8024 (3.2283)	grad_norm 1.6902 (nan)	mem 14853MB
[2022-11-07 00:21:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][450/1251]	eta 0:06:17 lr 0.000395	time 0.4580 (0.4711)	loss 3.2801 (3.2271)	grad_norm 1.8836 (nan)	mem 14853MB
[2022-11-07 00:21:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][500/1251]	eta 0:05:53 lr 0.000395	time 0.4610 (0.4705)	loss 3.4430 (3.2330)	grad_norm 1.7166 (nan)	mem 14853MB
[2022-11-07 00:21:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][550/1251]	eta 0:05:29 lr 0.000395	time 0.4670 (0.4704)	loss 3.0916 (3.2456)	grad_norm 1.7491 (nan)	mem 14853MB
[2022-11-07 00:22:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][600/1251]	eta 0:05:06 lr 0.000395	time 0.4675 (0.4701)	loss 2.5840 (3.2461)	grad_norm 1.5203 (nan)	mem 14853MB
[2022-11-07 00:22:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][650/1251]	eta 0:04:42 lr 0.000394	time 0.4670 (0.4699)	loss 3.7263 (3.2433)	grad_norm 1.6736 (nan)	mem 14853MB
[2022-11-07 00:22:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][700/1251]	eta 0:04:18 lr 0.000394	time 0.4651 (0.4696)	loss 3.6262 (3.2533)	grad_norm 1.8214 (nan)	mem 14853MB
[2022-11-07 00:23:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][750/1251]	eta 0:03:55 lr 0.000394	time 0.4657 (0.4693)	loss 3.0105 (3.2551)	grad_norm 1.8198 (nan)	mem 14853MB
[2022-11-07 00:23:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][800/1251]	eta 0:03:31 lr 0.000394	time 0.4625 (0.4694)	loss 3.4901 (3.2518)	grad_norm 1.7557 (nan)	mem 14853MB
[2022-11-07 00:24:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][850/1251]	eta 0:03:08 lr 0.000394	time 0.4732 (0.4694)	loss 2.5348 (3.2584)	grad_norm 1.5977 (nan)	mem 14853MB
[2022-11-07 00:24:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][900/1251]	eta 0:02:44 lr 0.000393	time 0.4706 (0.4692)	loss 3.6878 (3.2605)	grad_norm 1.7713 (nan)	mem 14853MB
[2022-11-07 00:24:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][950/1251]	eta 0:02:21 lr 0.000393	time 0.4639 (0.4690)	loss 3.9509 (3.2622)	grad_norm 1.7907 (nan)	mem 14853MB
[2022-11-07 00:25:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][1000/1251]	eta 0:01:57 lr 0.000393	time 0.4632 (0.4688)	loss 2.8630 (3.2602)	grad_norm 1.6790 (nan)	mem 14853MB
[2022-11-07 00:25:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][1050/1251]	eta 0:01:34 lr 0.000393	time 0.4739 (0.4690)	loss 3.3454 (3.2616)	grad_norm 1.7800 (nan)	mem 14853MB
[2022-11-07 00:26:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][1100/1251]	eta 0:01:10 lr 0.000393	time 0.4635 (0.4688)	loss 3.0096 (3.2605)	grad_norm 1.8178 (nan)	mem 14853MB
[2022-11-07 00:26:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][1150/1251]	eta 0:00:47 lr 0.000392	time 0.4619 (0.4688)	loss 3.5912 (3.2668)	grad_norm 1.6847 (nan)	mem 14853MB
[2022-11-07 00:26:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][1200/1251]	eta 0:00:23 lr 0.000392	time 0.4641 (0.4687)	loss 3.9326 (3.2662)	grad_norm 1.7777 (nan)	mem 14853MB
[2022-11-07 00:27:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [171/300][1250/1251]	eta 0:00:00 lr 0.000392	time 0.4571 (0.4685)	loss 3.2987 (3.2683)	grad_norm 1.5092 (nan)	mem 14853MB
[2022-11-07 00:27:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 171 training takes 0:09:46
[2022-11-07 00:27:14 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_171.pth saving......
[2022-11-07 00:27:15 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_171.pth saved !!!
[2022-11-07 00:27:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.608 (1.608)	Loss 0.9081 (0.9081)	Acc@1 78.516 (78.516)	Acc@5 94.629 (94.629)	Mem 14853MB
[2022-11-07 00:27:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.984 Acc@5 94.864
[2022-11-07 00:27:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.0%
[2022-11-07 00:27:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.664 (1.664)	Loss 0.7631 (0.7631)	Acc@1 82.422 (82.422)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 00:27:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.772 Acc@5 95.618
[2022-11-07 00:27:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.8%
[2022-11-07 00:27:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.77% at 171 epoch
[2022-11-07 00:27:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][0/1251]	eta 0:41:00 lr 0.000392	time 1.9672 (1.9672)	loss 2.2559 (2.2559)	grad_norm 1.8263 (1.8263)	mem 14853MB
[2022-11-07 00:27:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][50/1251]	eta 0:09:58 lr 0.000392	time 0.4527 (0.4984)	loss 2.4714 (3.2207)	grad_norm 1.7005 (1.7040)	mem 14853MB
[2022-11-07 00:28:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][100/1251]	eta 0:09:17 lr 0.000392	time 0.4718 (0.4840)	loss 3.4213 (3.1811)	grad_norm 1.5674 (1.7062)	mem 14853MB
[2022-11-07 00:28:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][150/1251]	eta 0:08:45 lr 0.000391	time 0.4666 (0.4775)	loss 2.9264 (3.1892)	grad_norm 1.5020 (1.6929)	mem 14853MB
[2022-11-07 00:29:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][200/1251]	eta 0:08:18 lr 0.000391	time 0.4687 (0.4746)	loss 2.9355 (3.1756)	grad_norm 1.9061 (1.6982)	mem 14853MB
[2022-11-07 00:29:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][250/1251]	eta 0:07:54 lr 0.000391	time 0.4685 (0.4739)	loss 2.0516 (3.1763)	grad_norm 1.7694 (1.7007)	mem 14853MB
[2022-11-07 00:29:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][300/1251]	eta 0:07:29 lr 0.000391	time 0.4597 (0.4726)	loss 3.6763 (3.1916)	grad_norm 1.6397 (1.7065)	mem 14853MB
[2022-11-07 00:30:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][350/1251]	eta 0:07:05 lr 0.000391	time 0.4534 (0.4718)	loss 3.6053 (3.1937)	grad_norm 1.7753 (1.7042)	mem 14853MB
[2022-11-07 00:30:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][400/1251]	eta 0:06:40 lr 0.000390	time 0.4750 (0.4710)	loss 3.6182 (3.1984)	grad_norm 1.5278 (1.7119)	mem 14853MB
[2022-11-07 00:31:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][450/1251]	eta 0:06:16 lr 0.000390	time 0.4521 (0.4703)	loss 2.1496 (3.2096)	grad_norm 1.6022 (1.7112)	mem 14853MB
[2022-11-07 00:31:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][500/1251]	eta 0:05:52 lr 0.000390	time 0.4627 (0.4699)	loss 3.5396 (3.2116)	grad_norm 2.0672 (1.7119)	mem 14853MB
[2022-11-07 00:31:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][550/1251]	eta 0:05:29 lr 0.000390	time 0.4630 (0.4700)	loss 3.4811 (3.2133)	grad_norm 1.4863 (1.7108)	mem 14853MB
[2022-11-07 00:32:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][600/1251]	eta 0:05:05 lr 0.000390	time 0.4627 (0.4697)	loss 3.3729 (3.2204)	grad_norm 1.6800 (1.7108)	mem 14853MB
[2022-11-07 00:32:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][650/1251]	eta 0:04:42 lr 0.000389	time 0.4672 (0.4694)	loss 3.2272 (3.2257)	grad_norm 1.6422 (1.7117)	mem 14853MB
[2022-11-07 00:33:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][700/1251]	eta 0:04:18 lr 0.000389	time 0.4807 (0.4691)	loss 4.1744 (3.2225)	grad_norm 1.7567 (1.7112)	mem 14853MB
[2022-11-07 00:33:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][750/1251]	eta 0:03:54 lr 0.000389	time 0.4661 (0.4690)	loss 3.8434 (3.2313)	grad_norm 1.9008 (1.7110)	mem 14853MB
[2022-11-07 00:33:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][800/1251]	eta 0:03:31 lr 0.000389	time 0.4533 (0.4691)	loss 3.4595 (3.2311)	grad_norm 1.7917 (1.7125)	mem 14853MB
[2022-11-07 00:34:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][850/1251]	eta 0:03:08 lr 0.000389	time 0.4516 (0.4690)	loss 3.7599 (3.2270)	grad_norm 1.9164 (1.7119)	mem 14853MB
[2022-11-07 00:34:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][900/1251]	eta 0:02:44 lr 0.000388	time 0.4652 (0.4688)	loss 3.7420 (3.2327)	grad_norm 1.6466 (1.7142)	mem 14853MB
[2022-11-07 00:34:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][950/1251]	eta 0:02:21 lr 0.000388	time 0.4752 (0.4685)	loss 2.0524 (3.2333)	grad_norm 1.7538 (1.7173)	mem 14853MB
[2022-11-07 00:35:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][1000/1251]	eta 0:01:57 lr 0.000388	time 0.4637 (0.4684)	loss 3.5485 (3.2371)	grad_norm 1.6907 (nan)	mem 14853MB
[2022-11-07 00:35:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][1050/1251]	eta 0:01:34 lr 0.000388	time 0.4555 (0.4684)	loss 3.7807 (3.2400)	grad_norm 1.7591 (nan)	mem 14853MB
[2022-11-07 00:36:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][1100/1251]	eta 0:01:10 lr 0.000388	time 0.4612 (0.4684)	loss 3.6142 (3.2386)	grad_norm 1.6646 (nan)	mem 14853MB
[2022-11-07 00:36:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][1150/1251]	eta 0:00:47 lr 0.000387	time 0.4643 (0.4683)	loss 2.7591 (3.2362)	grad_norm 1.8306 (nan)	mem 14853MB
[2022-11-07 00:36:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][1200/1251]	eta 0:00:23 lr 0.000387	time 0.4623 (0.4682)	loss 4.1414 (3.2370)	grad_norm 1.8757 (nan)	mem 14853MB
[2022-11-07 00:37:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [172/300][1250/1251]	eta 0:00:00 lr 0.000387	time 0.4598 (0.4681)	loss 3.0546 (3.2368)	grad_norm 1.8139 (nan)	mem 14853MB
[2022-11-07 00:37:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 172 training takes 0:09:45
[2022-11-07 00:37:18 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_172.pth saving......
[2022-11-07 00:37:19 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_172.pth saved !!!
[2022-11-07 00:37:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.548 (1.548)	Loss 0.8308 (0.8308)	Acc@1 79.590 (79.590)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 00:37:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.964 Acc@5 94.718
[2022-11-07 00:37:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.0%
[2022-11-07 00:37:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.629 (1.629)	Loss 0.8200 (0.8200)	Acc@1 79.004 (79.004)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 00:37:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.790 Acc@5 95.634
[2022-11-07 00:37:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.8%
[2022-11-07 00:37:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.79% at 172 epoch
[2022-11-07 00:37:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][0/1251]	eta 0:42:40 lr 0.000387	time 2.0466 (2.0466)	loss 3.6042 (3.6042)	grad_norm 1.6344 (1.6344)	mem 14853MB
[2022-11-07 00:38:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][50/1251]	eta 0:10:03 lr 0.000387	time 0.4672 (0.5027)	loss 2.5654 (3.2115)	grad_norm 1.7664 (1.6910)	mem 14853MB
[2022-11-07 00:38:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][100/1251]	eta 0:09:19 lr 0.000387	time 0.4674 (0.4858)	loss 2.6730 (3.2080)	grad_norm 1.5042 (1.6869)	mem 14853MB
[2022-11-07 00:38:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][150/1251]	eta 0:08:47 lr 0.000386	time 0.4535 (0.4794)	loss 3.5563 (3.2339)	grad_norm 1.6606 (1.7063)	mem 14853MB
[2022-11-07 00:39:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][200/1251]	eta 0:08:20 lr 0.000386	time 0.4654 (0.4765)	loss 2.4304 (3.2564)	grad_norm 1.5841 (1.7141)	mem 14853MB
[2022-11-07 00:39:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][250/1251]	eta 0:07:55 lr 0.000386	time 0.4704 (0.4746)	loss 3.4542 (3.2693)	grad_norm 1.9931 (1.7222)	mem 14853MB
[2022-11-07 00:39:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][300/1251]	eta 0:07:30 lr 0.000386	time 0.4664 (0.4735)	loss 3.4183 (3.2632)	grad_norm 1.9465 (1.7273)	mem 14853MB
[2022-11-07 00:40:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][350/1251]	eta 0:07:05 lr 0.000386	time 0.4791 (0.4723)	loss 2.9299 (3.2690)	grad_norm 2.0708 (1.7353)	mem 14853MB
[2022-11-07 00:40:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][400/1251]	eta 0:06:41 lr 0.000385	time 0.4652 (0.4714)	loss 3.3168 (3.2507)	grad_norm 1.8216 (1.7340)	mem 14853MB
[2022-11-07 00:41:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][450/1251]	eta 0:06:17 lr 0.000385	time 0.4666 (0.4708)	loss 2.7986 (3.2538)	grad_norm 1.8119 (1.7364)	mem 14853MB
[2022-11-07 00:41:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][500/1251]	eta 0:05:53 lr 0.000385	time 0.4591 (0.4704)	loss 3.2722 (3.2498)	grad_norm 1.7941 (1.7356)	mem 14853MB
[2022-11-07 00:41:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][550/1251]	eta 0:05:29 lr 0.000385	time 0.4647 (0.4704)	loss 3.4513 (3.2504)	grad_norm 1.7053 (1.7341)	mem 14853MB
[2022-11-07 00:42:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][600/1251]	eta 0:05:06 lr 0.000385	time 0.4693 (0.4702)	loss 3.1596 (3.2549)	grad_norm 1.6444 (1.7346)	mem 14853MB
[2022-11-07 00:42:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][650/1251]	eta 0:04:42 lr 0.000384	time 0.4768 (0.4699)	loss 3.2815 (3.2646)	grad_norm 1.7359 (1.7318)	mem 14853MB
[2022-11-07 00:43:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][700/1251]	eta 0:04:18 lr 0.000384	time 0.4625 (0.4696)	loss 2.4498 (3.2663)	grad_norm 1.8910 (1.7371)	mem 14853MB
[2022-11-07 00:43:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][750/1251]	eta 0:03:55 lr 0.000384	time 0.4650 (0.4694)	loss 2.1566 (3.2595)	grad_norm 1.6441 (1.7368)	mem 14853MB
[2022-11-07 00:43:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][800/1251]	eta 0:03:31 lr 0.000384	time 0.5365 (0.4696)	loss 3.8451 (3.2631)	grad_norm 1.5739 (1.7373)	mem 14853MB
[2022-11-07 00:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][850/1251]	eta 0:03:08 lr 0.000384	time 0.4660 (0.4694)	loss 3.6766 (3.2706)	grad_norm 1.8972 (1.7377)	mem 14853MB
[2022-11-07 00:44:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][900/1251]	eta 0:02:44 lr 0.000383	time 0.4603 (0.4692)	loss 2.5613 (3.2689)	grad_norm 1.6252 (1.7371)	mem 14853MB
[2022-11-07 00:45:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][950/1251]	eta 0:02:21 lr 0.000383	time 0.4694 (0.4690)	loss 3.7902 (3.2766)	grad_norm 1.8581 (1.7372)	mem 14853MB
[2022-11-07 00:45:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][1000/1251]	eta 0:01:57 lr 0.000383	time 0.4614 (0.4689)	loss 3.5829 (3.2773)	grad_norm 1.6929 (1.7375)	mem 14853MB
[2022-11-07 00:45:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][1050/1251]	eta 0:01:34 lr 0.000383	time 0.4615 (0.4689)	loss 3.4677 (3.2767)	grad_norm 1.5378 (1.7387)	mem 14853MB
[2022-11-07 00:46:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][1100/1251]	eta 0:01:10 lr 0.000383	time 0.4693 (0.4687)	loss 2.8823 (3.2718)	grad_norm 1.7268 (1.7375)	mem 14853MB
[2022-11-07 00:46:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][1150/1251]	eta 0:00:47 lr 0.000382	time 0.4575 (0.4686)	loss 3.3815 (3.2686)	grad_norm 2.1003 (1.7364)	mem 14853MB
[2022-11-07 00:46:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][1200/1251]	eta 0:00:23 lr 0.000382	time 0.4606 (0.4685)	loss 3.2459 (3.2690)	grad_norm 1.7654 (1.7353)	mem 14853MB
[2022-11-07 00:47:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [173/300][1250/1251]	eta 0:00:00 lr 0.000382	time 0.4561 (0.4684)	loss 3.8011 (3.2701)	grad_norm 1.7969 (1.7330)	mem 14853MB
[2022-11-07 00:47:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 173 training takes 0:09:46
[2022-11-07 00:47:22 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_173.pth saving......
[2022-11-07 00:47:23 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_173.pth saved !!!
[2022-11-07 00:47:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.463 (1.463)	Loss 0.8828 (0.8828)	Acc@1 79.590 (79.590)	Acc@5 94.629 (94.629)	Mem 14853MB
[2022-11-07 00:47:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.938 Acc@5 94.878
[2022-11-07 00:47:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.9%
[2022-11-07 00:47:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.614 (1.614)	Loss 0.8010 (0.8010)	Acc@1 81.641 (81.641)	Acc@5 94.922 (94.922)	Mem 14853MB
[2022-11-07 00:47:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.796 Acc@5 95.654
[2022-11-07 00:47:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.8%
[2022-11-07 00:47:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.80% at 173 epoch
[2022-11-07 00:47:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][0/1251]	eta 0:40:30 lr 0.000382	time 1.9427 (1.9427)	loss 3.0904 (3.0904)	grad_norm 1.6401 (1.6401)	mem 14853MB
[2022-11-07 00:48:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][50/1251]	eta 0:09:59 lr 0.000382	time 0.4748 (0.4995)	loss 2.8401 (3.2543)	grad_norm 2.2711 (1.7616)	mem 14853MB
[2022-11-07 00:48:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][100/1251]	eta 0:09:17 lr 0.000381	time 0.4671 (0.4841)	loss 3.0169 (3.2084)	grad_norm 1.6861 (1.7373)	mem 14853MB
[2022-11-07 00:48:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][150/1251]	eta 0:08:47 lr 0.000381	time 0.4666 (0.4795)	loss 3.9822 (3.2236)	grad_norm 1.9386 (1.7367)	mem 14853MB
[2022-11-07 00:49:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][200/1251]	eta 0:08:20 lr 0.000381	time 0.4771 (0.4764)	loss 3.3125 (3.2489)	grad_norm 1.6477 (1.7376)	mem 14853MB
[2022-11-07 00:49:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][250/1251]	eta 0:07:54 lr 0.000381	time 0.4653 (0.4745)	loss 3.2201 (3.2491)	grad_norm 1.7142 (1.7463)	mem 14853MB
[2022-11-07 00:50:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][300/1251]	eta 0:07:29 lr 0.000381	time 0.4639 (0.4729)	loss 2.3029 (3.2393)	grad_norm 1.7222 (1.7503)	mem 14853MB
[2022-11-07 00:50:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][350/1251]	eta 0:07:05 lr 0.000380	time 0.4691 (0.4719)	loss 3.1541 (3.2464)	grad_norm 1.5613 (1.7506)	mem 14853MB
[2022-11-07 00:50:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][400/1251]	eta 0:06:40 lr 0.000380	time 0.4709 (0.4711)	loss 4.0628 (3.2579)	grad_norm 1.7053 (1.7449)	mem 14853MB
[2022-11-07 00:51:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][450/1251]	eta 0:06:16 lr 0.000380	time 0.4634 (0.4704)	loss 3.7909 (3.2622)	grad_norm 1.5208 (1.7442)	mem 14853MB
[2022-11-07 00:51:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][500/1251]	eta 0:05:53 lr 0.000380	time 0.4659 (0.4701)	loss 3.2131 (3.2715)	grad_norm 1.6063 (1.7407)	mem 14853MB
[2022-11-07 00:51:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][550/1251]	eta 0:05:29 lr 0.000380	time 0.4618 (0.4701)	loss 3.1010 (3.2645)	grad_norm 1.4776 (1.7437)	mem 14853MB
[2022-11-07 00:52:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][600/1251]	eta 0:05:05 lr 0.000379	time 0.4606 (0.4698)	loss 3.1543 (3.2668)	grad_norm 1.6144 (1.7409)	mem 14853MB
[2022-11-07 00:52:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][650/1251]	eta 0:04:42 lr 0.000379	time 0.4634 (0.4695)	loss 3.2695 (3.2589)	grad_norm 1.7386 (1.7405)	mem 14853MB
[2022-11-07 00:53:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][700/1251]	eta 0:04:18 lr 0.000379	time 0.4693 (0.4692)	loss 2.1262 (3.2572)	grad_norm 1.9686 (1.7390)	mem 14853MB
[2022-11-07 00:53:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][750/1251]	eta 0:03:54 lr 0.000379	time 0.4672 (0.4690)	loss 3.6310 (3.2534)	grad_norm 1.8258 (1.7390)	mem 14853MB
[2022-11-07 00:53:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][800/1251]	eta 0:03:31 lr 0.000379	time 0.4752 (0.4692)	loss 2.7480 (3.2522)	grad_norm 1.5928 (1.7391)	mem 14853MB
[2022-11-07 00:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][850/1251]	eta 0:03:08 lr 0.000378	time 0.4627 (0.4692)	loss 2.1189 (3.2544)	grad_norm 1.7189 (1.7377)	mem 14853MB
[2022-11-07 00:54:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][900/1251]	eta 0:02:44 lr 0.000378	time 0.4697 (0.4689)	loss 3.9366 (3.2592)	grad_norm 1.7265 (1.7383)	mem 14853MB
[2022-11-07 00:55:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][950/1251]	eta 0:02:21 lr 0.000378	time 0.4634 (0.4687)	loss 3.6232 (3.2620)	grad_norm 1.5729 (1.7380)	mem 14853MB
[2022-11-07 00:55:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][1000/1251]	eta 0:01:57 lr 0.000378	time 0.4672 (0.4685)	loss 3.1313 (3.2584)	grad_norm 1.7902 (1.7378)	mem 14853MB
[2022-11-07 00:55:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][1050/1251]	eta 0:01:34 lr 0.000378	time 0.4640 (0.4685)	loss 2.7444 (3.2583)	grad_norm 1.6939 (1.7390)	mem 14853MB
[2022-11-07 00:56:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][1100/1251]	eta 0:01:10 lr 0.000377	time 0.4749 (0.4685)	loss 2.3921 (3.2588)	grad_norm 1.6801 (1.7390)	mem 14853MB
[2022-11-07 00:56:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][1150/1251]	eta 0:00:47 lr 0.000377	time 0.4627 (0.4684)	loss 2.8770 (3.2628)	grad_norm 1.5967 (1.7399)	mem 14853MB
[2022-11-07 00:57:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][1200/1251]	eta 0:00:23 lr 0.000377	time 0.4617 (0.4683)	loss 3.3004 (3.2610)	grad_norm 1.7911 (1.7400)	mem 14853MB
[2022-11-07 00:57:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [174/300][1250/1251]	eta 0:00:00 lr 0.000377	time 0.4554 (0.4681)	loss 3.1975 (3.2615)	grad_norm 2.0838 (1.7410)	mem 14853MB
[2022-11-07 00:57:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 174 training takes 0:09:45
[2022-11-07 00:57:26 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_174.pth saving......
[2022-11-07 00:57:26 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_174.pth saved !!!
[2022-11-07 00:57:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.451 (1.451)	Loss 0.9078 (0.9078)	Acc@1 78.809 (78.809)	Acc@5 94.434 (94.434)	Mem 14853MB
[2022-11-07 00:57:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.246 Acc@5 94.850
[2022-11-07 00:57:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.2%
[2022-11-07 00:57:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.677 (1.677)	Loss 0.7895 (0.7895)	Acc@1 80.957 (80.957)	Acc@5 96.191 (96.191)	Mem 14853MB
[2022-11-07 00:57:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.794 Acc@5 95.668
[2022-11-07 00:57:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.8%
[2022-11-07 00:57:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.80% at 173 epoch
[2022-11-07 00:57:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][0/1251]	eta 0:42:33 lr 0.000377	time 2.0415 (2.0415)	loss 3.6125 (3.6125)	grad_norm 1.6287 (1.6287)	mem 14853MB
[2022-11-07 00:58:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][50/1251]	eta 0:10:07 lr 0.000377	time 0.4569 (0.5054)	loss 2.8043 (3.2794)	grad_norm 1.9173 (1.7216)	mem 14853MB
[2022-11-07 00:58:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][100/1251]	eta 0:09:20 lr 0.000376	time 0.4642 (0.4867)	loss 3.6443 (3.3220)	grad_norm 1.5812 (1.7113)	mem 14853MB
[2022-11-07 00:58:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][150/1251]	eta 0:08:49 lr 0.000376	time 0.4708 (0.4807)	loss 3.4019 (3.2780)	grad_norm 2.3963 (1.7321)	mem 14853MB
[2022-11-07 00:59:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][200/1251]	eta 0:08:21 lr 0.000376	time 0.4696 (0.4773)	loss 3.6345 (3.2647)	grad_norm 1.6449 (1.7426)	mem 14853MB
[2022-11-07 00:59:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][250/1251]	eta 0:07:55 lr 0.000376	time 0.4796 (0.4751)	loss 2.4542 (3.2504)	grad_norm 2.2846 (1.7447)	mem 14853MB
[2022-11-07 01:00:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][300/1251]	eta 0:07:30 lr 0.000376	time 0.4662 (0.4734)	loss 3.1122 (3.2586)	grad_norm 1.6651 (1.7357)	mem 14853MB
[2022-11-07 01:00:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][350/1251]	eta 0:07:05 lr 0.000375	time 0.4659 (0.4725)	loss 3.8993 (3.2550)	grad_norm 1.7640 (1.7409)	mem 14853MB
[2022-11-07 01:00:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][400/1251]	eta 0:06:41 lr 0.000375	time 0.4613 (0.4716)	loss 2.4238 (3.2430)	grad_norm 1.9140 (1.7396)	mem 14853MB
[2022-11-07 01:01:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][450/1251]	eta 0:06:17 lr 0.000375	time 0.4545 (0.4711)	loss 2.4839 (3.2457)	grad_norm 1.9117 (1.7439)	mem 14853MB
[2022-11-07 01:01:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][500/1251]	eta 0:05:53 lr 0.000375	time 0.4673 (0.4707)	loss 3.5360 (3.2514)	grad_norm 1.9598 (1.7421)	mem 14853MB
[2022-11-07 01:02:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][550/1251]	eta 0:05:29 lr 0.000375	time 0.4615 (0.4705)	loss 3.2732 (3.2536)	grad_norm 1.6722 (1.7489)	mem 14853MB
[2022-11-07 01:02:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][600/1251]	eta 0:05:06 lr 0.000374	time 0.4685 (0.4704)	loss 2.8396 (3.2602)	grad_norm 1.9091 (1.7496)	mem 14853MB
[2022-11-07 01:02:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][650/1251]	eta 0:04:42 lr 0.000374	time 0.4700 (0.4702)	loss 3.7302 (3.2530)	grad_norm 1.6629 (1.7486)	mem 14853MB
[2022-11-07 01:03:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][700/1251]	eta 0:04:18 lr 0.000374	time 0.4762 (0.4699)	loss 3.4043 (3.2491)	grad_norm 1.7094 (1.7453)	mem 14853MB
[2022-11-07 01:03:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][750/1251]	eta 0:03:55 lr 0.000374	time 0.4554 (0.4696)	loss 3.3293 (3.2453)	grad_norm 1.8287 (1.7453)	mem 14853MB
[2022-11-07 01:04:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][800/1251]	eta 0:03:31 lr 0.000374	time 0.4656 (0.4697)	loss 3.5898 (3.2439)	grad_norm 1.8199 (inf)	mem 14853MB
[2022-11-07 01:04:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][850/1251]	eta 0:03:08 lr 0.000373	time 0.4725 (0.4696)	loss 3.6559 (3.2408)	grad_norm 1.8041 (inf)	mem 14853MB
[2022-11-07 01:04:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][900/1251]	eta 0:02:44 lr 0.000373	time 0.4762 (0.4695)	loss 4.0980 (3.2443)	grad_norm 1.5451 (inf)	mem 14853MB
[2022-11-07 01:05:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][950/1251]	eta 0:02:21 lr 0.000373	time 0.4744 (0.4693)	loss 3.2501 (3.2397)	grad_norm 1.5659 (inf)	mem 14853MB
[2022-11-07 01:05:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][1000/1251]	eta 0:01:57 lr 0.000373	time 0.4612 (0.4692)	loss 3.5538 (3.2355)	grad_norm 1.7430 (inf)	mem 14853MB
[2022-11-07 01:05:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][1050/1251]	eta 0:01:34 lr 0.000373	time 0.4608 (0.4692)	loss 2.2641 (3.2357)	grad_norm 1.9378 (inf)	mem 14853MB
[2022-11-07 01:06:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][1100/1251]	eta 0:01:10 lr 0.000372	time 0.4650 (0.4691)	loss 3.4461 (3.2375)	grad_norm 1.8202 (inf)	mem 14853MB
[2022-11-07 01:06:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][1150/1251]	eta 0:00:47 lr 0.000372	time 0.4646 (0.4691)	loss 3.6245 (3.2385)	grad_norm 1.8522 (inf)	mem 14853MB
[2022-11-07 01:07:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][1200/1251]	eta 0:00:23 lr 0.000372	time 0.4621 (0.4690)	loss 3.7337 (3.2352)	grad_norm 1.7360 (inf)	mem 14853MB
[2022-11-07 01:07:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [175/300][1250/1251]	eta 0:00:00 lr 0.000372	time 0.4730 (0.4688)	loss 3.0337 (3.2330)	grad_norm 1.9794 (inf)	mem 14853MB
[2022-11-07 01:07:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 175 training takes 0:09:46
[2022-11-07 01:07:31 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_175.pth saving......
[2022-11-07 01:07:31 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_175.pth saved !!!
[2022-11-07 01:07:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.552 (1.552)	Loss 0.8764 (0.8764)	Acc@1 80.859 (80.859)	Acc@5 95.020 (95.020)	Mem 14853MB
[2022-11-07 01:07:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.248 Acc@5 94.924
[2022-11-07 01:07:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.2%
[2022-11-07 01:07:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.601 (1.601)	Loss 0.8543 (0.8543)	Acc@1 79.883 (79.883)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 01:07:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.832 Acc@5 95.660
[2022-11-07 01:07:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.8%
[2022-11-07 01:07:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.83% at 175 epoch
[2022-11-07 01:07:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][0/1251]	eta 0:40:06 lr 0.000372	time 1.9237 (1.9237)	loss 3.3919 (3.3919)	grad_norm 1.7667 (1.7667)	mem 14853MB
[2022-11-07 01:08:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][50/1251]	eta 0:10:04 lr 0.000372	time 0.4682 (0.5034)	loss 3.1880 (3.2413)	grad_norm 1.8266 (1.7265)	mem 14853MB
[2022-11-07 01:08:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][100/1251]	eta 0:09:22 lr 0.000371	time 0.4538 (0.4886)	loss 3.9550 (3.2764)	grad_norm 1.7841 (1.7457)	mem 14853MB
[2022-11-07 01:09:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][150/1251]	eta 0:08:49 lr 0.000371	time 0.4679 (0.4812)	loss 3.2839 (3.2516)	grad_norm 1.7498 (1.7405)	mem 14853MB
[2022-11-07 01:09:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][200/1251]	eta 0:08:21 lr 0.000371	time 0.4566 (0.4771)	loss 3.6045 (3.2279)	grad_norm 1.7996 (1.7344)	mem 14853MB
[2022-11-07 01:09:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][250/1251]	eta 0:07:55 lr 0.000371	time 0.4708 (0.4747)	loss 3.2360 (3.2070)	grad_norm 1.9470 (1.7406)	mem 14853MB
[2022-11-07 01:10:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][300/1251]	eta 0:07:29 lr 0.000371	time 0.4530 (0.4729)	loss 3.2918 (3.2242)	grad_norm 1.5639 (1.7442)	mem 14853MB
[2022-11-07 01:10:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][350/1251]	eta 0:07:04 lr 0.000370	time 0.4678 (0.4716)	loss 3.4548 (3.2392)	grad_norm 1.7985 (1.7397)	mem 14853MB
[2022-11-07 01:10:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][400/1251]	eta 0:06:41 lr 0.000370	time 0.4635 (0.4712)	loss 2.7295 (3.2513)	grad_norm 1.7827 (1.7473)	mem 14853MB
[2022-11-07 01:11:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][450/1251]	eta 0:06:17 lr 0.000370	time 0.4696 (0.4707)	loss 3.3297 (3.2607)	grad_norm 1.7549 (1.7462)	mem 14853MB
[2022-11-07 01:11:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][500/1251]	eta 0:05:53 lr 0.000370	time 0.4837 (0.4704)	loss 3.1186 (3.2676)	grad_norm 1.7813 (1.7472)	mem 14853MB
[2022-11-07 01:12:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][550/1251]	eta 0:05:29 lr 0.000370	time 0.4622 (0.4704)	loss 3.7675 (3.2602)	grad_norm 1.7040 (1.7477)	mem 14853MB
[2022-11-07 01:12:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][600/1251]	eta 0:05:06 lr 0.000369	time 0.4759 (0.4701)	loss 3.2204 (3.2543)	grad_norm 1.7258 (1.7505)	mem 14853MB
[2022-11-07 01:12:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][650/1251]	eta 0:04:42 lr 0.000369	time 0.4665 (0.4697)	loss 2.9881 (3.2507)	grad_norm 1.9527 (1.7517)	mem 14853MB
[2022-11-07 01:13:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][700/1251]	eta 0:04:18 lr 0.000369	time 0.4590 (0.4694)	loss 2.5195 (3.2485)	grad_norm 1.6890 (1.7498)	mem 14853MB
[2022-11-07 01:13:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][750/1251]	eta 0:03:55 lr 0.000369	time 0.4639 (0.4694)	loss 3.4660 (3.2487)	grad_norm 1.6071 (1.7510)	mem 14853MB
[2022-11-07 01:14:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][800/1251]	eta 0:03:31 lr 0.000369	time 0.4779 (0.4695)	loss 2.3174 (3.2495)	grad_norm 1.8864 (1.7498)	mem 14853MB
[2022-11-07 01:14:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][850/1251]	eta 0:03:08 lr 0.000368	time 0.4687 (0.4693)	loss 3.5193 (3.2405)	grad_norm 1.8404 (1.7478)	mem 14853MB
[2022-11-07 01:14:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][900/1251]	eta 0:02:44 lr 0.000368	time 0.4643 (0.4690)	loss 3.2105 (3.2417)	grad_norm 1.7667 (1.7491)	mem 14853MB
[2022-11-07 01:15:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][950/1251]	eta 0:02:21 lr 0.000368	time 0.4648 (0.4688)	loss 3.4849 (3.2447)	grad_norm 1.7279 (1.7526)	mem 14853MB
[2022-11-07 01:15:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][1000/1251]	eta 0:01:57 lr 0.000368	time 0.4540 (0.4688)	loss 3.0926 (3.2425)	grad_norm 1.6955 (1.7517)	mem 14853MB
[2022-11-07 01:16:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][1050/1251]	eta 0:01:34 lr 0.000368	time 0.4583 (0.4688)	loss 3.6560 (3.2428)	grad_norm 1.7398 (1.7529)	mem 14853MB
[2022-11-07 01:16:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][1100/1251]	eta 0:01:10 lr 0.000368	time 0.4654 (0.4688)	loss 3.3401 (3.2406)	grad_norm 1.6457 (1.7548)	mem 14853MB
[2022-11-07 01:16:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][1150/1251]	eta 0:00:47 lr 0.000367	time 0.4634 (0.4686)	loss 3.0871 (3.2406)	grad_norm 1.8009 (1.7553)	mem 14853MB
[2022-11-07 01:17:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][1200/1251]	eta 0:00:23 lr 0.000367	time 0.4783 (0.4685)	loss 2.6055 (3.2378)	grad_norm 1.5483 (1.7567)	mem 14853MB
[2022-11-07 01:17:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [176/300][1250/1251]	eta 0:00:00 lr 0.000367	time 0.4575 (0.4682)	loss 2.5103 (3.2385)	grad_norm 1.9282 (1.7580)	mem 14853MB
[2022-11-07 01:17:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 176 training takes 0:09:45
[2022-11-07 01:17:35 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_176.pth saving......
[2022-11-07 01:17:35 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_176.pth saved !!!
[2022-11-07 01:17:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.575 (1.575)	Loss 0.8505 (0.8505)	Acc@1 79.395 (79.395)	Acc@5 95.020 (95.020)	Mem 14853MB
[2022-11-07 01:17:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.928 Acc@5 94.740
[2022-11-07 01:17:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.9%
[2022-11-07 01:17:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.648 (1.648)	Loss 0.8159 (0.8159)	Acc@1 80.957 (80.957)	Acc@5 95.020 (95.020)	Mem 14853MB
[2022-11-07 01:17:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.902 Acc@5 95.658
[2022-11-07 01:17:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.9%
[2022-11-07 01:17:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.90% at 176 epoch
[2022-11-07 01:17:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][0/1251]	eta 0:41:11 lr 0.000367	time 1.9759 (1.9759)	loss 3.7573 (3.7573)	grad_norm 1.8448 (1.8448)	mem 14853MB
[2022-11-07 01:18:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][50/1251]	eta 0:10:02 lr 0.000367	time 0.4678 (0.5012)	loss 2.1734 (3.1730)	grad_norm 1.8184 (1.8353)	mem 14853MB
[2022-11-07 01:18:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][100/1251]	eta 0:09:19 lr 0.000367	time 0.4562 (0.4860)	loss 2.7948 (3.2050)	grad_norm 1.7657 (1.8064)	mem 14853MB
[2022-11-07 01:19:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][150/1251]	eta 0:08:47 lr 0.000366	time 0.4641 (0.4791)	loss 3.7466 (3.2088)	grad_norm 1.7820 (1.7861)	mem 14853MB
[2022-11-07 01:19:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][200/1251]	eta 0:08:20 lr 0.000366	time 0.4642 (0.4760)	loss 3.3747 (3.2245)	grad_norm 1.6180 (1.7793)	mem 14853MB
[2022-11-07 01:19:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][250/1251]	eta 0:07:54 lr 0.000366	time 0.4640 (0.4740)	loss 2.2640 (3.2576)	grad_norm 1.9519 (1.7692)	mem 14853MB
[2022-11-07 01:20:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][300/1251]	eta 0:07:29 lr 0.000366	time 0.4620 (0.4728)	loss 3.7812 (3.2553)	grad_norm 1.7162 (1.7683)	mem 14853MB
[2022-11-07 01:20:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][350/1251]	eta 0:07:05 lr 0.000366	time 0.4654 (0.4718)	loss 3.6780 (3.2264)	grad_norm 1.6269 (1.7644)	mem 14853MB
[2022-11-07 01:21:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][400/1251]	eta 0:06:40 lr 0.000365	time 0.4597 (0.4710)	loss 3.7680 (3.2160)	grad_norm 1.8836 (1.7703)	mem 14853MB
[2022-11-07 01:21:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][450/1251]	eta 0:06:16 lr 0.000365	time 0.4606 (0.4705)	loss 3.1712 (3.2175)	grad_norm 1.7741 (1.7721)	mem 14853MB
[2022-11-07 01:21:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][500/1251]	eta 0:05:53 lr 0.000365	time 0.4632 (0.4704)	loss 3.5310 (3.2100)	grad_norm 1.7624 (1.7748)	mem 14853MB
[2022-11-07 01:22:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][550/1251]	eta 0:05:29 lr 0.000365	time 0.4664 (0.4703)	loss 2.4907 (3.2267)	grad_norm 2.0209 (1.7759)	mem 14853MB
[2022-11-07 01:22:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][600/1251]	eta 0:05:05 lr 0.000365	time 0.4637 (0.4699)	loss 3.6692 (3.2385)	grad_norm 1.8384 (nan)	mem 14853MB
[2022-11-07 01:22:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][650/1251]	eta 0:04:42 lr 0.000364	time 0.4655 (0.4696)	loss 3.9535 (3.2469)	grad_norm 1.7671 (nan)	mem 14853MB
[2022-11-07 01:23:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][700/1251]	eta 0:04:18 lr 0.000364	time 0.4599 (0.4692)	loss 2.2227 (3.2463)	grad_norm 1.6227 (nan)	mem 14853MB
[2022-11-07 01:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][750/1251]	eta 0:03:55 lr 0.000364	time 0.4653 (0.4692)	loss 3.6497 (3.2459)	grad_norm 1.6394 (nan)	mem 14853MB
[2022-11-07 01:24:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][800/1251]	eta 0:03:31 lr 0.000364	time 0.5361 (0.4692)	loss 3.8717 (3.2444)	grad_norm 1.9916 (nan)	mem 14853MB
[2022-11-07 01:24:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][850/1251]	eta 0:03:08 lr 0.000364	time 0.4519 (0.4690)	loss 3.5227 (3.2409)	grad_norm 1.6848 (nan)	mem 14853MB
[2022-11-07 01:24:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][900/1251]	eta 0:02:44 lr 0.000363	time 0.4612 (0.4689)	loss 3.5256 (3.2443)	grad_norm 1.7881 (nan)	mem 14853MB
[2022-11-07 01:25:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][950/1251]	eta 0:02:21 lr 0.000363	time 0.4725 (0.4687)	loss 2.6879 (3.2456)	grad_norm 1.5516 (nan)	mem 14853MB
[2022-11-07 01:25:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][1000/1251]	eta 0:01:57 lr 0.000363	time 0.4659 (0.4687)	loss 3.5750 (3.2455)	grad_norm 1.6540 (nan)	mem 14853MB
[2022-11-07 01:26:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][1050/1251]	eta 0:01:34 lr 0.000363	time 0.4726 (0.4687)	loss 3.1509 (3.2433)	grad_norm 1.7576 (nan)	mem 14853MB
[2022-11-07 01:26:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][1100/1251]	eta 0:01:10 lr 0.000363	time 0.4666 (0.4686)	loss 2.6814 (3.2424)	grad_norm 1.7867 (nan)	mem 14853MB
[2022-11-07 01:26:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][1150/1251]	eta 0:00:47 lr 0.000362	time 0.4667 (0.4685)	loss 3.3207 (3.2407)	grad_norm 1.7889 (nan)	mem 14853MB
[2022-11-07 01:27:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][1200/1251]	eta 0:00:23 lr 0.000362	time 0.4873 (0.4685)	loss 3.2155 (3.2379)	grad_norm 1.8625 (nan)	mem 14853MB
[2022-11-07 01:27:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [177/300][1250/1251]	eta 0:00:00 lr 0.000362	time 0.4562 (0.4683)	loss 2.7863 (3.2393)	grad_norm 1.6183 (nan)	mem 14853MB
[2022-11-07 01:27:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 177 training takes 0:09:45
[2022-11-07 01:27:39 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_177.pth saving......
[2022-11-07 01:27:39 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_177.pth saved !!!
[2022-11-07 01:27:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.602 (1.602)	Loss 0.9203 (0.9203)	Acc@1 78.711 (78.711)	Acc@5 94.531 (94.531)	Mem 14853MB
[2022-11-07 01:27:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.000 Acc@5 94.862
[2022-11-07 01:27:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.0%
[2022-11-07 01:27:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.689 (1.689)	Loss 0.7944 (0.7944)	Acc@1 79.883 (79.883)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 01:27:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.926 Acc@5 95.642
[2022-11-07 01:27:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.9%
[2022-11-07 01:27:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.93% at 177 epoch
[2022-11-07 01:27:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][0/1251]	eta 0:42:00 lr 0.000362	time 2.0146 (2.0146)	loss 2.1594 (2.1594)	grad_norm 1.8469 (1.8469)	mem 14853MB
[2022-11-07 01:28:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][50/1251]	eta 0:10:05 lr 0.000362	time 0.4635 (0.5043)	loss 3.1955 (3.2280)	grad_norm 1.7118 (1.7741)	mem 14853MB
[2022-11-07 01:28:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][100/1251]	eta 0:09:20 lr 0.000362	time 0.4725 (0.4869)	loss 3.8261 (3.2599)	grad_norm 1.8090 (1.7700)	mem 14853MB
[2022-11-07 01:29:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][150/1251]	eta 0:08:48 lr 0.000361	time 0.4640 (0.4803)	loss 3.5744 (3.2675)	grad_norm 1.9231 (1.7706)	mem 14853MB
[2022-11-07 01:29:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][200/1251]	eta 0:08:20 lr 0.000361	time 0.4547 (0.4765)	loss 3.4273 (3.2663)	grad_norm 1.8859 (1.7656)	mem 14853MB
[2022-11-07 01:29:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][250/1251]	eta 0:07:54 lr 0.000361	time 0.4682 (0.4745)	loss 2.2824 (3.2281)	grad_norm 1.7493 (1.7592)	mem 14853MB
[2022-11-07 01:30:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][300/1251]	eta 0:07:29 lr 0.000361	time 0.4640 (0.4730)	loss 3.2482 (3.2371)	grad_norm 1.7423 (1.7562)	mem 14853MB
[2022-11-07 01:30:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][350/1251]	eta 0:07:05 lr 0.000361	time 0.4756 (0.4719)	loss 3.0968 (3.2497)	grad_norm 1.8476 (1.7555)	mem 14853MB
[2022-11-07 01:31:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][400/1251]	eta 0:06:41 lr 0.000360	time 0.4609 (0.4712)	loss 2.7650 (3.2374)	grad_norm 1.6283 (1.7530)	mem 14853MB
[2022-11-07 01:31:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][450/1251]	eta 0:06:16 lr 0.000360	time 0.4613 (0.4705)	loss 3.3092 (3.2459)	grad_norm 2.0190 (1.7530)	mem 14853MB
[2022-11-07 01:31:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][500/1251]	eta 0:05:53 lr 0.000360	time 0.4599 (0.4701)	loss 2.3950 (3.2421)	grad_norm 1.8576 (1.7534)	mem 14853MB
[2022-11-07 01:32:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][550/1251]	eta 0:05:29 lr 0.000360	time 0.4715 (0.4702)	loss 2.3865 (3.2385)	grad_norm 1.7057 (1.7572)	mem 14853MB
[2022-11-07 01:32:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][600/1251]	eta 0:05:05 lr 0.000360	time 0.4700 (0.4698)	loss 3.5209 (3.2336)	grad_norm 1.6079 (1.7590)	mem 14853MB
[2022-11-07 01:33:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][650/1251]	eta 0:04:42 lr 0.000359	time 0.4543 (0.4697)	loss 3.4104 (3.2329)	grad_norm 1.6456 (1.7633)	mem 14853MB
[2022-11-07 01:33:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][700/1251]	eta 0:04:18 lr 0.000359	time 0.4576 (0.4695)	loss 2.9525 (3.2341)	grad_norm 1.7450 (1.7659)	mem 14853MB
[2022-11-07 01:33:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][750/1251]	eta 0:03:55 lr 0.000359	time 0.4685 (0.4692)	loss 3.5909 (3.2393)	grad_norm 1.5803 (1.7668)	mem 14853MB
[2022-11-07 01:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][800/1251]	eta 0:03:31 lr 0.000359	time 0.4590 (0.4692)	loss 3.1849 (3.2464)	grad_norm 1.6992 (1.7677)	mem 14853MB
[2022-11-07 01:34:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][850/1251]	eta 0:03:08 lr 0.000359	time 0.4780 (0.4691)	loss 3.5215 (3.2490)	grad_norm 1.6490 (1.7653)	mem 14853MB
[2022-11-07 01:34:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][900/1251]	eta 0:02:44 lr 0.000358	time 0.4730 (0.4690)	loss 3.5668 (3.2519)	grad_norm 1.6662 (1.7645)	mem 14853MB
[2022-11-07 01:35:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][950/1251]	eta 0:02:21 lr 0.000358	time 0.4597 (0.4689)	loss 3.5219 (3.2454)	grad_norm 1.7954 (1.7629)	mem 14853MB
[2022-11-07 01:35:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][1000/1251]	eta 0:01:57 lr 0.000358	time 0.4562 (0.4688)	loss 2.5020 (3.2419)	grad_norm 1.5203 (1.7630)	mem 14853MB
[2022-11-07 01:36:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][1050/1251]	eta 0:01:34 lr 0.000358	time 0.4737 (0.4689)	loss 3.3910 (3.2467)	grad_norm 1.9628 (1.7659)	mem 14853MB
[2022-11-07 01:36:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][1100/1251]	eta 0:01:10 lr 0.000358	time 0.4628 (0.4688)	loss 3.4252 (3.2461)	grad_norm 1.7002 (1.7677)	mem 14853MB
[2022-11-07 01:36:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][1150/1251]	eta 0:00:47 lr 0.000357	time 0.4644 (0.4687)	loss 3.1235 (3.2436)	grad_norm 1.6083 (1.7668)	mem 14853MB
[2022-11-07 01:37:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][1200/1251]	eta 0:00:23 lr 0.000357	time 0.4634 (0.4686)	loss 3.8961 (3.2424)	grad_norm 1.8605 (1.7669)	mem 14853MB
[2022-11-07 01:37:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [178/300][1250/1251]	eta 0:00:00 lr 0.000357	time 0.4588 (0.4684)	loss 3.0802 (3.2415)	grad_norm 1.7135 (1.7672)	mem 14853MB
[2022-11-07 01:37:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 178 training takes 0:09:46
[2022-11-07 01:37:43 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_178.pth saving......
[2022-11-07 01:37:44 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_178.pth saved !!!
[2022-11-07 01:37:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.775 (1.775)	Loss 0.8863 (0.8863)	Acc@1 79.785 (79.785)	Acc@5 95.117 (95.117)	Mem 14853MB
[2022-11-07 01:37:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.964 Acc@5 94.994
[2022-11-07 01:37:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.0%
[2022-11-07 01:37:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.003 (2.003)	Loss 0.8552 (0.8552)	Acc@1 79.590 (79.590)	Acc@5 95.215 (95.215)	Mem 14853MB
[2022-11-07 01:38:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.904 Acc@5 95.660
[2022-11-07 01:38:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.9%
[2022-11-07 01:38:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.93% at 177 epoch
[2022-11-07 01:38:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][0/1251]	eta 0:43:21 lr 0.000357	time 2.0799 (2.0799)	loss 3.4196 (3.4196)	grad_norm 1.6410 (1.6410)	mem 14853MB
[2022-11-07 01:38:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][50/1251]	eta 0:10:06 lr 0.000357	time 0.4628 (0.5048)	loss 2.4011 (3.2862)	grad_norm 1.6571 (1.7884)	mem 14853MB
[2022-11-07 01:38:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][100/1251]	eta 0:09:20 lr 0.000357	time 0.4649 (0.4868)	loss 3.0505 (3.2237)	grad_norm 1.8551 (1.7624)	mem 14853MB
[2022-11-07 01:39:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][150/1251]	eta 0:08:50 lr 0.000356	time 0.4663 (0.4815)	loss 3.8382 (3.2451)	grad_norm 1.6987 (1.7742)	mem 14853MB
[2022-11-07 01:39:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][200/1251]	eta 0:08:22 lr 0.000356	time 0.4769 (0.4779)	loss 2.6213 (3.2444)	grad_norm 1.7646 (1.7701)	mem 14853MB
[2022-11-07 01:40:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][250/1251]	eta 0:07:55 lr 0.000356	time 0.4703 (0.4755)	loss 3.7592 (3.2480)	grad_norm 1.8759 (1.7703)	mem 14853MB
[2022-11-07 01:40:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][300/1251]	eta 0:07:30 lr 0.000356	time 0.4776 (0.4740)	loss 2.7564 (3.2373)	grad_norm 1.7354 (1.7846)	mem 14853MB
[2022-11-07 01:40:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][350/1251]	eta 0:07:05 lr 0.000356	time 0.4624 (0.4728)	loss 3.6573 (3.2301)	grad_norm 1.8425 (1.7855)	mem 14853MB
[2022-11-07 01:41:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][400/1251]	eta 0:06:41 lr 0.000355	time 0.4677 (0.4721)	loss 2.9500 (3.2251)	grad_norm 1.8381 (1.7820)	mem 14853MB
[2022-11-07 01:41:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][450/1251]	eta 0:06:18 lr 0.000355	time 0.4653 (0.4720)	loss 3.5628 (3.2250)	grad_norm 1.5890 (1.7820)	mem 14853MB
[2022-11-07 01:41:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][500/1251]	eta 0:05:53 lr 0.000355	time 0.4572 (0.4713)	loss 3.7460 (3.2234)	grad_norm 1.8263 (1.7781)	mem 14853MB
[2022-11-07 01:42:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][550/1251]	eta 0:05:30 lr 0.000355	time 0.4656 (0.4712)	loss 3.1138 (3.2282)	grad_norm 1.8607 (1.7750)	mem 14853MB
[2022-11-07 01:42:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][600/1251]	eta 0:05:06 lr 0.000355	time 0.4741 (0.4709)	loss 3.1671 (3.2296)	grad_norm 1.6387 (1.7734)	mem 14853MB
[2022-11-07 01:43:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][650/1251]	eta 0:04:42 lr 0.000354	time 0.4651 (0.4707)	loss 4.0080 (3.2342)	grad_norm 1.9469 (1.7735)	mem 14853MB
[2022-11-07 01:43:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][700/1251]	eta 0:04:19 lr 0.000354	time 0.4652 (0.4705)	loss 3.1873 (3.2327)	grad_norm 1.8071 (1.7725)	mem 14853MB
[2022-11-07 01:43:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][750/1251]	eta 0:03:55 lr 0.000354	time 0.4623 (0.4703)	loss 3.8053 (3.2265)	grad_norm 1.7274 (1.7710)	mem 14853MB
[2022-11-07 01:44:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][800/1251]	eta 0:03:32 lr 0.000354	time 0.4611 (0.4702)	loss 3.5096 (3.2234)	grad_norm 1.8952 (1.7716)	mem 14853MB
[2022-11-07 01:44:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][850/1251]	eta 0:03:08 lr 0.000354	time 0.4682 (0.4699)	loss 2.9057 (3.2186)	grad_norm 1.6753 (1.7720)	mem 14853MB
[2022-11-07 01:45:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][900/1251]	eta 0:02:44 lr 0.000353	time 0.4648 (0.4699)	loss 3.6993 (3.2155)	grad_norm 1.7800 (1.7745)	mem 14853MB
[2022-11-07 01:45:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][950/1251]	eta 0:02:21 lr 0.000353	time 0.4736 (0.4698)	loss 2.7970 (3.2137)	grad_norm 1.6646 (1.7741)	mem 14853MB
[2022-11-07 01:45:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][1000/1251]	eta 0:01:57 lr 0.000353	time 0.4581 (0.4695)	loss 3.4349 (3.2196)	grad_norm 1.5776 (1.7727)	mem 14853MB
[2022-11-07 01:46:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][1050/1251]	eta 0:01:34 lr 0.000353	time 0.4588 (0.4696)	loss 3.7320 (3.2149)	grad_norm 1.5826 (1.7731)	mem 14853MB
[2022-11-07 01:46:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][1100/1251]	eta 0:01:10 lr 0.000353	time 0.4629 (0.4695)	loss 3.8056 (3.2132)	grad_norm 1.7480 (1.7736)	mem 14853MB
[2022-11-07 01:47:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][1150/1251]	eta 0:00:47 lr 0.000352	time 0.4620 (0.4694)	loss 3.5368 (3.2151)	grad_norm 1.7546 (1.7756)	mem 14853MB
[2022-11-07 01:47:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][1200/1251]	eta 0:00:23 lr 0.000352	time 0.4636 (0.4692)	loss 3.5858 (3.2159)	grad_norm 1.5230 (1.7763)	mem 14853MB
[2022-11-07 01:47:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [179/300][1250/1251]	eta 0:00:00 lr 0.000352	time 0.4577 (0.4690)	loss 3.2925 (3.2147)	grad_norm 1.6275 (1.7775)	mem 14853MB
[2022-11-07 01:47:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 179 training takes 0:09:46
[2022-11-07 01:47:48 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_179.pth saving......
[2022-11-07 01:47:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_179.pth saved !!!
[2022-11-07 01:47:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.591 (1.591)	Loss 0.9370 (0.9370)	Acc@1 77.441 (77.441)	Acc@5 94.727 (94.727)	Mem 14853MB
[2022-11-07 01:47:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.422 Acc@5 95.060
[2022-11-07 01:47:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.4%
[2022-11-07 01:47:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.661 (1.661)	Loss 0.8179 (0.8179)	Acc@1 80.176 (80.176)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 01:48:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.942 Acc@5 95.686
[2022-11-07 01:48:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.9%
[2022-11-07 01:48:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.94% at 179 epoch
[2022-11-07 01:48:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][0/1251]	eta 0:41:05 lr 0.000352	time 1.9710 (1.9710)	loss 3.4187 (3.4187)	grad_norm 1.6554 (1.6554)	mem 14853MB
[2022-11-07 01:48:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][50/1251]	eta 0:10:03 lr 0.000352	time 0.4797 (0.5027)	loss 2.5803 (3.2253)	grad_norm 1.8219 (1.7885)	mem 14853MB
[2022-11-07 01:48:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][100/1251]	eta 0:09:18 lr 0.000352	time 0.4671 (0.4853)	loss 2.4045 (3.2073)	grad_norm 1.9508 (nan)	mem 14853MB
[2022-11-07 01:49:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][150/1251]	eta 0:08:47 lr 0.000351	time 0.4648 (0.4791)	loss 2.3588 (3.1865)	grad_norm 1.5832 (nan)	mem 14853MB
[2022-11-07 01:49:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][200/1251]	eta 0:08:19 lr 0.000351	time 0.4644 (0.4756)	loss 2.6314 (3.2136)	grad_norm 1.7909 (nan)	mem 14853MB
[2022-11-07 01:50:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][250/1251]	eta 0:07:53 lr 0.000351	time 0.4680 (0.4735)	loss 4.0070 (3.2042)	grad_norm 1.6870 (nan)	mem 14853MB
[2022-11-07 01:50:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][300/1251]	eta 0:07:29 lr 0.000351	time 0.4638 (0.4730)	loss 3.1723 (3.2004)	grad_norm 1.6567 (nan)	mem 14853MB
[2022-11-07 01:50:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][350/1251]	eta 0:07:05 lr 0.000351	time 0.4716 (0.4719)	loss 2.8536 (3.2041)	grad_norm 1.7285 (nan)	mem 14853MB
[2022-11-07 01:51:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][400/1251]	eta 0:06:40 lr 0.000350	time 0.4696 (0.4710)	loss 3.5141 (3.2112)	grad_norm 1.7070 (nan)	mem 14853MB
[2022-11-07 01:51:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][450/1251]	eta 0:06:16 lr 0.000350	time 0.4697 (0.4703)	loss 2.7050 (3.2182)	grad_norm 1.7384 (nan)	mem 14853MB
[2022-11-07 01:52:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][500/1251]	eta 0:05:52 lr 0.000350	time 0.4669 (0.4698)	loss 2.2017 (3.2105)	grad_norm 1.8342 (nan)	mem 14853MB
[2022-11-07 01:52:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][550/1251]	eta 0:05:29 lr 0.000350	time 0.4568 (0.4699)	loss 3.6526 (3.2153)	grad_norm 1.5047 (nan)	mem 14853MB
[2022-11-07 01:52:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][600/1251]	eta 0:05:05 lr 0.000350	time 0.4577 (0.4697)	loss 2.8215 (3.2213)	grad_norm 1.7026 (nan)	mem 14853MB
[2022-11-07 01:53:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][650/1251]	eta 0:04:42 lr 0.000349	time 0.4785 (0.4693)	loss 2.8365 (3.2215)	grad_norm 1.8027 (nan)	mem 14853MB
[2022-11-07 01:53:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][700/1251]	eta 0:04:18 lr 0.000349	time 0.4600 (0.4691)	loss 3.2584 (3.2274)	grad_norm 1.6404 (nan)	mem 14853MB
[2022-11-07 01:53:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][750/1251]	eta 0:03:54 lr 0.000349	time 0.4568 (0.4689)	loss 3.1608 (3.2268)	grad_norm 1.5945 (nan)	mem 14853MB
[2022-11-07 01:54:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][800/1251]	eta 0:03:31 lr 0.000349	time 0.4636 (0.4691)	loss 2.9199 (3.2315)	grad_norm 1.7087 (nan)	mem 14853MB
[2022-11-07 01:54:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][850/1251]	eta 0:03:08 lr 0.000349	time 0.4600 (0.4689)	loss 3.0620 (3.2333)	grad_norm 1.9335 (nan)	mem 14853MB
[2022-11-07 01:55:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][900/1251]	eta 0:02:44 lr 0.000348	time 0.4715 (0.4688)	loss 3.6588 (3.2364)	grad_norm 1.7543 (nan)	mem 14853MB
[2022-11-07 01:55:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][950/1251]	eta 0:02:21 lr 0.000348	time 0.4644 (0.4686)	loss 3.4109 (3.2328)	grad_norm 1.7076 (nan)	mem 14853MB
[2022-11-07 01:55:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][1000/1251]	eta 0:01:57 lr 0.000348	time 0.4651 (0.4684)	loss 3.4547 (3.2313)	grad_norm 1.6811 (nan)	mem 14853MB
[2022-11-07 01:56:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][1050/1251]	eta 0:01:34 lr 0.000348	time 0.4677 (0.4687)	loss 4.1399 (3.2328)	grad_norm 1.7211 (nan)	mem 14853MB
[2022-11-07 01:56:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][1100/1251]	eta 0:01:10 lr 0.000348	time 0.4622 (0.4686)	loss 3.6198 (3.2362)	grad_norm 1.8007 (nan)	mem 14853MB
[2022-11-07 01:57:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][1150/1251]	eta 0:00:47 lr 0.000348	time 0.4642 (0.4684)	loss 3.2964 (3.2355)	grad_norm 1.8789 (nan)	mem 14853MB
[2022-11-07 01:57:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][1200/1251]	eta 0:00:23 lr 0.000347	time 0.4615 (0.4683)	loss 3.8076 (3.2351)	grad_norm 1.6398 (nan)	mem 14853MB
[2022-11-07 01:57:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [180/300][1250/1251]	eta 0:00:00 lr 0.000347	time 0.4572 (0.4681)	loss 3.2523 (3.2411)	grad_norm 1.6350 (nan)	mem 14853MB
[2022-11-07 01:57:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 180 training takes 0:09:45
[2022-11-07 01:57:52 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_180.pth saving......
[2022-11-07 01:57:53 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_180.pth saved !!!
[2022-11-07 01:57:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.586 (1.586)	Loss 0.8764 (0.8764)	Acc@1 78.711 (78.711)	Acc@5 95.020 (95.020)	Mem 14853MB
[2022-11-07 01:58:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.252 Acc@5 94.930
[2022-11-07 01:58:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.3%
[2022-11-07 01:58:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.614 (1.614)	Loss 0.7739 (0.7739)	Acc@1 80.957 (80.957)	Acc@5 96.387 (96.387)	Mem 14853MB
[2022-11-07 01:58:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.984 Acc@5 95.692
[2022-11-07 01:58:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.0%
[2022-11-07 01:58:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.98% at 180 epoch
[2022-11-07 01:58:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][0/1251]	eta 0:41:36 lr 0.000347	time 1.9953 (1.9953)	loss 3.6585 (3.6585)	grad_norm 1.9500 (1.9500)	mem 14853MB
[2022-11-07 01:58:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][50/1251]	eta 0:10:01 lr 0.000347	time 0.4608 (0.5011)	loss 3.6433 (3.3371)	grad_norm 1.6304 (1.7902)	mem 14853MB
[2022-11-07 01:58:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][100/1251]	eta 0:09:19 lr 0.000347	time 0.4674 (0.4863)	loss 2.6488 (3.3128)	grad_norm 1.8201 (1.8048)	mem 14853MB
[2022-11-07 01:59:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][150/1251]	eta 0:08:48 lr 0.000347	time 0.4690 (0.4804)	loss 2.8488 (3.2857)	grad_norm 1.7737 (1.7761)	mem 14853MB
[2022-11-07 01:59:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][200/1251]	eta 0:08:21 lr 0.000346	time 0.4627 (0.4769)	loss 3.5451 (3.2976)	grad_norm 1.8805 (1.7874)	mem 14853MB
[2022-11-07 02:00:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][250/1251]	eta 0:07:55 lr 0.000346	time 0.4668 (0.4747)	loss 2.1493 (3.2697)	grad_norm 1.7423 (1.7885)	mem 14853MB
[2022-11-07 02:00:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][300/1251]	eta 0:07:30 lr 0.000346	time 0.4660 (0.4733)	loss 2.2088 (3.2631)	grad_norm 1.8715 (1.7845)	mem 14853MB
[2022-11-07 02:00:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][350/1251]	eta 0:07:05 lr 0.000346	time 0.4668 (0.4724)	loss 2.6640 (3.2479)	grad_norm 1.8261 (1.7854)	mem 14853MB
[2022-11-07 02:01:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][400/1251]	eta 0:06:41 lr 0.000346	time 0.4689 (0.4716)	loss 3.0860 (3.2383)	grad_norm 1.7364 (1.7891)	mem 14853MB
[2022-11-07 02:01:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][450/1251]	eta 0:06:17 lr 0.000345	time 0.4764 (0.4710)	loss 2.4755 (3.2444)	grad_norm 1.7237 (1.7866)	mem 14853MB
[2022-11-07 02:02:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][500/1251]	eta 0:05:53 lr 0.000345	time 0.4632 (0.4706)	loss 3.3661 (3.2509)	grad_norm 1.8083 (1.7863)	mem 14853MB
[2022-11-07 02:02:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][550/1251]	eta 0:05:29 lr 0.000345	time 0.4560 (0.4705)	loss 2.5854 (3.2509)	grad_norm 1.8401 (1.7923)	mem 14853MB
[2022-11-07 02:02:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][600/1251]	eta 0:05:06 lr 0.000345	time 0.4723 (0.4702)	loss 3.5880 (3.2477)	grad_norm 1.6390 (1.7921)	mem 14853MB
[2022-11-07 02:03:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][650/1251]	eta 0:04:42 lr 0.000345	time 0.4597 (0.4698)	loss 3.3114 (3.2438)	grad_norm 1.7812 (1.7908)	mem 14853MB
[2022-11-07 02:03:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][700/1251]	eta 0:04:18 lr 0.000344	time 0.4639 (0.4696)	loss 3.3645 (3.2465)	grad_norm 1.7004 (1.7909)	mem 14853MB
[2022-11-07 02:04:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][750/1251]	eta 0:03:55 lr 0.000344	time 0.4681 (0.4694)	loss 3.6961 (3.2385)	grad_norm 2.1126 (1.7956)	mem 14853MB
[2022-11-07 02:04:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][800/1251]	eta 0:03:31 lr 0.000344	time 0.4767 (0.4695)	loss 3.5984 (3.2353)	grad_norm 1.9800 (1.7917)	mem 14853MB
[2022-11-07 02:04:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][850/1251]	eta 0:03:08 lr 0.000344	time 0.4728 (0.4694)	loss 3.3548 (3.2329)	grad_norm 1.5696 (1.7914)	mem 14853MB
[2022-11-07 02:05:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][900/1251]	eta 0:02:44 lr 0.000344	time 0.4566 (0.4691)	loss 3.6565 (3.2277)	grad_norm 1.7104 (inf)	mem 14853MB
[2022-11-07 02:05:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][950/1251]	eta 0:02:21 lr 0.000343	time 0.4591 (0.4691)	loss 3.5495 (3.2265)	grad_norm 1.5898 (inf)	mem 14853MB
[2022-11-07 02:06:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][1000/1251]	eta 0:01:57 lr 0.000343	time 0.4663 (0.4689)	loss 2.9085 (3.2254)	grad_norm 1.7088 (inf)	mem 14853MB
[2022-11-07 02:06:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][1050/1251]	eta 0:01:34 lr 0.000343	time 0.4580 (0.4689)	loss 3.9176 (3.2275)	grad_norm 1.6460 (inf)	mem 14853MB
[2022-11-07 02:06:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][1100/1251]	eta 0:01:10 lr 0.000343	time 0.4607 (0.4689)	loss 2.6569 (3.2293)	grad_norm 1.6095 (inf)	mem 14853MB
[2022-11-07 02:07:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][1150/1251]	eta 0:00:47 lr 0.000343	time 0.4750 (0.4688)	loss 3.3310 (3.2279)	grad_norm 1.7860 (inf)	mem 14853MB
[2022-11-07 02:07:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][1200/1251]	eta 0:00:23 lr 0.000342	time 0.4546 (0.4688)	loss 2.8525 (3.2291)	grad_norm 1.8846 (inf)	mem 14853MB
[2022-11-07 02:07:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [181/300][1250/1251]	eta 0:00:00 lr 0.000342	time 0.4563 (0.4685)	loss 3.5163 (3.2279)	grad_norm 1.8776 (inf)	mem 14853MB
[2022-11-07 02:07:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 181 training takes 0:09:46
[2022-11-07 02:07:57 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_181.pth saving......
[2022-11-07 02:07:57 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_181.pth saved !!!
[2022-11-07 02:07:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.578 (1.578)	Loss 0.8036 (0.8036)	Acc@1 80.859 (80.859)	Acc@5 96.191 (96.191)	Mem 14853MB
[2022-11-07 02:08:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.466 Acc@5 94.948
[2022-11-07 02:08:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.5%
[2022-11-07 02:08:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.650 (1.650)	Loss 0.8564 (0.8564)	Acc@1 80.566 (80.566)	Acc@5 94.727 (94.727)	Mem 14853MB
[2022-11-07 02:08:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.020 Acc@5 95.686
[2022-11-07 02:08:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.0%
[2022-11-07 02:08:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.02% at 181 epoch
[2022-11-07 02:08:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][0/1251]	eta 0:39:38 lr 0.000342	time 1.9015 (1.9015)	loss 2.3304 (2.3304)	grad_norm 1.8641 (1.8641)	mem 14853MB
[2022-11-07 02:08:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][50/1251]	eta 0:09:57 lr 0.000342	time 0.4658 (0.4975)	loss 3.8792 (3.2181)	grad_norm 1.8666 (1.8431)	mem 14853MB
[2022-11-07 02:09:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][100/1251]	eta 0:09:17 lr 0.000342	time 0.4610 (0.4840)	loss 3.5322 (3.2130)	grad_norm 1.7832 (1.8437)	mem 14853MB
[2022-11-07 02:09:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][150/1251]	eta 0:08:46 lr 0.000342	time 0.4536 (0.4785)	loss 2.0135 (3.2287)	grad_norm 1.8232 (1.8236)	mem 14853MB
[2022-11-07 02:09:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][200/1251]	eta 0:08:19 lr 0.000341	time 0.4665 (0.4753)	loss 3.2169 (3.2138)	grad_norm 1.6373 (1.8334)	mem 14853MB
[2022-11-07 02:10:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][250/1251]	eta 0:07:54 lr 0.000341	time 0.4667 (0.4741)	loss 3.2485 (3.2251)	grad_norm 1.7380 (1.8302)	mem 14853MB
[2022-11-07 02:10:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][300/1251]	eta 0:07:29 lr 0.000341	time 0.5633 (0.4729)	loss 3.7641 (3.2213)	grad_norm 1.8685 (1.8174)	mem 14853MB
[2022-11-07 02:11:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][350/1251]	eta 0:07:05 lr 0.000341	time 0.4670 (0.4719)	loss 3.8342 (3.2444)	grad_norm 2.2031 (1.8167)	mem 14853MB
[2022-11-07 02:11:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][400/1251]	eta 0:06:40 lr 0.000341	time 0.4834 (0.4710)	loss 3.6271 (3.2493)	grad_norm 2.0608 (1.8221)	mem 14853MB
[2022-11-07 02:11:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][450/1251]	eta 0:06:16 lr 0.000340	time 0.4645 (0.4703)	loss 3.3336 (3.2398)	grad_norm 1.9019 (1.8208)	mem 14853MB
[2022-11-07 02:12:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][500/1251]	eta 0:05:52 lr 0.000340	time 0.4499 (0.4699)	loss 3.3975 (3.2398)	grad_norm 1.9818 (1.8191)	mem 14853MB
[2022-11-07 02:12:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][550/1251]	eta 0:05:29 lr 0.000340	time 0.4767 (0.4700)	loss 2.8915 (3.2410)	grad_norm 2.2269 (1.8197)	mem 14853MB
[2022-11-07 02:12:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][600/1251]	eta 0:05:05 lr 0.000340	time 0.4661 (0.4695)	loss 3.4902 (3.2390)	grad_norm 1.7860 (1.8202)	mem 14853MB
[2022-11-07 02:13:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][650/1251]	eta 0:04:42 lr 0.000340	time 0.4709 (0.4692)	loss 2.7035 (3.2387)	grad_norm 1.9123 (1.8202)	mem 14853MB
[2022-11-07 02:13:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][700/1251]	eta 0:04:18 lr 0.000339	time 0.4599 (0.4691)	loss 3.3583 (3.2369)	grad_norm 1.8540 (1.8241)	mem 14853MB
[2022-11-07 02:14:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][750/1251]	eta 0:03:54 lr 0.000339	time 0.4733 (0.4688)	loss 2.6171 (3.2302)	grad_norm 1.8279 (1.8250)	mem 14853MB
[2022-11-07 02:14:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][800/1251]	eta 0:03:31 lr 0.000339	time 0.4552 (0.4690)	loss 2.4651 (3.2192)	grad_norm 1.6583 (1.8211)	mem 14853MB
[2022-11-07 02:14:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][850/1251]	eta 0:03:07 lr 0.000339	time 0.4673 (0.4687)	loss 3.7112 (3.2156)	grad_norm 1.8680 (1.8212)	mem 14853MB
[2022-11-07 02:15:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][900/1251]	eta 0:02:44 lr 0.000339	time 0.4642 (0.4686)	loss 3.3793 (3.2134)	grad_norm 1.9533 (1.8199)	mem 14853MB
[2022-11-07 02:15:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][950/1251]	eta 0:02:21 lr 0.000338	time 0.4646 (0.4685)	loss 2.7757 (3.2180)	grad_norm 1.7458 (1.8182)	mem 14853MB
[2022-11-07 02:16:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][1000/1251]	eta 0:01:57 lr 0.000338	time 0.4740 (0.4684)	loss 3.6427 (3.2182)	grad_norm 1.6805 (1.8167)	mem 14853MB
[2022-11-07 02:16:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][1050/1251]	eta 0:01:34 lr 0.000338	time 0.4624 (0.4685)	loss 3.3806 (3.2195)	grad_norm 1.7799 (1.8164)	mem 14853MB
[2022-11-07 02:16:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][1100/1251]	eta 0:01:10 lr 0.000338	time 0.4638 (0.4683)	loss 3.4791 (3.2253)	grad_norm 1.8251 (1.8144)	mem 14853MB
[2022-11-07 02:17:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][1150/1251]	eta 0:00:47 lr 0.000338	time 0.4689 (0.4681)	loss 3.5207 (3.2259)	grad_norm 1.9749 (1.8143)	mem 14853MB
[2022-11-07 02:17:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][1200/1251]	eta 0:00:23 lr 0.000338	time 0.4672 (0.4681)	loss 3.3119 (3.2236)	grad_norm 1.8216 (1.8143)	mem 14853MB
[2022-11-07 02:18:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [182/300][1250/1251]	eta 0:00:00 lr 0.000337	time 0.4664 (0.4680)	loss 2.0698 (3.2208)	grad_norm 1.7243 (1.8126)	mem 14853MB
[2022-11-07 02:18:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 182 training takes 0:09:45
[2022-11-07 02:18:00 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_182.pth saving......
[2022-11-07 02:18:01 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_182.pth saved !!!
[2022-11-07 02:18:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.540 (1.540)	Loss 0.9224 (0.9224)	Acc@1 78.027 (78.027)	Acc@5 94.141 (94.141)	Mem 14853MB
[2022-11-07 02:18:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.282 Acc@5 94.978
[2022-11-07 02:18:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.3%
[2022-11-07 02:18:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.549 (1.549)	Loss 0.7372 (0.7372)	Acc@1 81.836 (81.836)	Acc@5 96.289 (96.289)	Mem 14853MB
[2022-11-07 02:18:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.014 Acc@5 95.704
[2022-11-07 02:18:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.0%
[2022-11-07 02:18:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.02% at 181 epoch
[2022-11-07 02:18:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][0/1251]	eta 0:41:22 lr 0.000337	time 1.9844 (1.9844)	loss 2.9994 (2.9994)	grad_norm 1.8231 (1.8231)	mem 14853MB
[2022-11-07 02:18:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][50/1251]	eta 0:10:02 lr 0.000337	time 0.4569 (0.5020)	loss 3.3670 (3.2043)	grad_norm 1.9425 (1.8191)	mem 14853MB
[2022-11-07 02:19:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][100/1251]	eta 0:09:18 lr 0.000337	time 0.4595 (0.4854)	loss 2.8305 (3.1634)	grad_norm 1.8395 (1.8144)	mem 14853MB
[2022-11-07 02:19:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][150/1251]	eta 0:08:48 lr 0.000337	time 0.4616 (0.4797)	loss 3.5287 (3.2391)	grad_norm 1.7222 (1.8188)	mem 14853MB
[2022-11-07 02:19:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][200/1251]	eta 0:08:20 lr 0.000337	time 0.4748 (0.4761)	loss 3.4112 (3.2527)	grad_norm 1.8190 (1.8115)	mem 14853MB
[2022-11-07 02:20:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][250/1251]	eta 0:07:54 lr 0.000336	time 0.4643 (0.4740)	loss 3.6578 (3.2609)	grad_norm 2.1044 (1.8098)	mem 14853MB
[2022-11-07 02:20:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][300/1251]	eta 0:07:29 lr 0.000336	time 0.4654 (0.4727)	loss 2.8366 (3.2481)	grad_norm 1.6086 (1.8073)	mem 14853MB
[2022-11-07 02:21:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][350/1251]	eta 0:07:04 lr 0.000336	time 0.4609 (0.4716)	loss 2.6280 (3.2362)	grad_norm 1.9343 (1.8043)	mem 14853MB
[2022-11-07 02:21:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][400/1251]	eta 0:06:40 lr 0.000336	time 0.4654 (0.4708)	loss 3.7474 (3.2340)	grad_norm 1.9049 (1.8118)	mem 14853MB
[2022-11-07 02:21:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][450/1251]	eta 0:06:16 lr 0.000336	time 0.4567 (0.4703)	loss 2.9143 (3.2148)	grad_norm 1.7265 (1.8110)	mem 14853MB
[2022-11-07 02:22:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][500/1251]	eta 0:05:52 lr 0.000335	time 0.4630 (0.4699)	loss 3.2648 (3.2072)	grad_norm 2.1566 (1.8119)	mem 14853MB
[2022-11-07 02:22:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][550/1251]	eta 0:05:29 lr 0.000335	time 0.4648 (0.4700)	loss 3.0428 (3.2126)	grad_norm 1.6529 (1.8124)	mem 14853MB
[2022-11-07 02:23:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][600/1251]	eta 0:05:05 lr 0.000335	time 0.4663 (0.4698)	loss 3.5886 (3.2105)	grad_norm 1.6702 (1.8121)	mem 14853MB
[2022-11-07 02:23:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][650/1251]	eta 0:04:42 lr 0.000335	time 0.4669 (0.4695)	loss 2.7596 (3.2100)	grad_norm 1.5493 (1.8128)	mem 14853MB
[2022-11-07 02:23:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][700/1251]	eta 0:04:18 lr 0.000335	time 0.4583 (0.4692)	loss 3.6090 (3.2209)	grad_norm 2.0205 (1.8150)	mem 14853MB
[2022-11-07 02:24:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][750/1251]	eta 0:03:55 lr 0.000334	time 0.4613 (0.4691)	loss 3.4211 (3.2229)	grad_norm 1.5770 (nan)	mem 14853MB
[2022-11-07 02:24:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][800/1251]	eta 0:03:31 lr 0.000334	time 0.4655 (0.4690)	loss 3.9326 (3.2175)	grad_norm 1.9143 (nan)	mem 14853MB
[2022-11-07 02:24:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][850/1251]	eta 0:03:08 lr 0.000334	time 0.4546 (0.4689)	loss 2.9823 (3.2196)	grad_norm 1.6483 (nan)	mem 14853MB
[2022-11-07 02:25:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][900/1251]	eta 0:02:44 lr 0.000334	time 0.4649 (0.4687)	loss 3.2898 (3.2149)	grad_norm 1.7995 (nan)	mem 14853MB
[2022-11-07 02:25:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][950/1251]	eta 0:02:21 lr 0.000334	time 0.4696 (0.4686)	loss 3.4542 (3.2118)	grad_norm 1.8425 (nan)	mem 14853MB
[2022-11-07 02:26:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][1000/1251]	eta 0:01:57 lr 0.000333	time 0.4657 (0.4684)	loss 2.1060 (3.2158)	grad_norm 1.8523 (nan)	mem 14853MB
[2022-11-07 02:26:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][1050/1251]	eta 0:01:34 lr 0.000333	time 0.4633 (0.4684)	loss 3.6333 (3.2181)	grad_norm 1.6336 (nan)	mem 14853MB
[2022-11-07 02:26:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][1100/1251]	eta 0:01:10 lr 0.000333	time 0.4658 (0.4685)	loss 3.0547 (3.2148)	grad_norm 1.9699 (nan)	mem 14853MB
[2022-11-07 02:27:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][1150/1251]	eta 0:00:47 lr 0.000333	time 0.4726 (0.4684)	loss 3.3412 (3.2067)	grad_norm 1.7570 (nan)	mem 14853MB
[2022-11-07 02:27:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][1200/1251]	eta 0:00:23 lr 0.000333	time 0.4631 (0.4683)	loss 3.5715 (3.2050)	grad_norm 1.7204 (nan)	mem 14853MB
[2022-11-07 02:28:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [183/300][1250/1251]	eta 0:00:00 lr 0.000332	time 0.4584 (0.4681)	loss 1.9361 (3.2082)	grad_norm 1.7470 (nan)	mem 14853MB
[2022-11-07 02:28:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 183 training takes 0:09:45
[2022-11-07 02:28:04 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_183.pth saving......
[2022-11-07 02:28:05 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_183.pth saved !!!
[2022-11-07 02:28:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.502 (1.502)	Loss 0.8881 (0.8881)	Acc@1 77.734 (77.734)	Acc@5 94.727 (94.727)	Mem 14853MB
[2022-11-07 02:28:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.268 Acc@5 94.944
[2022-11-07 02:28:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.3%
[2022-11-07 02:28:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.780 (1.780)	Loss 0.7900 (0.7900)	Acc@1 80.859 (80.859)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 02:28:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.036 Acc@5 95.710
[2022-11-07 02:28:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.0%
[2022-11-07 02:28:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.04% at 183 epoch
[2022-11-07 02:28:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][0/1251]	eta 0:39:34 lr 0.000332	time 1.8979 (1.8979)	loss 2.3048 (2.3048)	grad_norm 1.7316 (1.7316)	mem 14853MB
[2022-11-07 02:28:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][50/1251]	eta 0:10:02 lr 0.000332	time 0.4622 (0.5019)	loss 2.8339 (3.2113)	grad_norm 1.8615 (1.8112)	mem 14853MB
[2022-11-07 02:29:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][100/1251]	eta 0:09:19 lr 0.000332	time 0.4779 (0.4862)	loss 1.9648 (3.1930)	grad_norm 1.6793 (1.8117)	mem 14853MB
[2022-11-07 02:29:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][150/1251]	eta 0:08:48 lr 0.000332	time 0.4856 (0.4804)	loss 3.4071 (3.1781)	grad_norm 1.7020 (1.8095)	mem 14853MB
[2022-11-07 02:29:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][200/1251]	eta 0:08:20 lr 0.000332	time 0.4533 (0.4767)	loss 2.8152 (3.1922)	grad_norm 1.7016 (1.8047)	mem 14853MB
[2022-11-07 02:30:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][250/1251]	eta 0:07:55 lr 0.000331	time 0.4571 (0.4751)	loss 3.3925 (3.2239)	grad_norm 1.7243 (1.8036)	mem 14853MB
[2022-11-07 02:30:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][300/1251]	eta 0:07:30 lr 0.000331	time 0.4658 (0.4737)	loss 3.1690 (3.2249)	grad_norm 1.7325 (1.8160)	mem 14853MB
[2022-11-07 02:31:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][350/1251]	eta 0:07:05 lr 0.000331	time 0.4742 (0.4726)	loss 3.1772 (3.2161)	grad_norm 1.7310 (1.8154)	mem 14853MB
[2022-11-07 02:31:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][400/1251]	eta 0:06:41 lr 0.000331	time 0.4613 (0.4718)	loss 3.1307 (3.2243)	grad_norm 1.6219 (1.8122)	mem 14853MB
[2022-11-07 02:31:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][450/1251]	eta 0:06:17 lr 0.000331	time 0.4686 (0.4711)	loss 3.6151 (3.2254)	grad_norm 1.8270 (1.8090)	mem 14853MB
[2022-11-07 02:32:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][500/1251]	eta 0:05:53 lr 0.000331	time 0.4651 (0.4707)	loss 3.0029 (3.2308)	grad_norm 1.5299 (1.8144)	mem 14853MB
[2022-11-07 02:32:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][550/1251]	eta 0:05:30 lr 0.000330	time 0.4671 (0.4708)	loss 3.0576 (3.2386)	grad_norm 2.0369 (1.8174)	mem 14853MB
[2022-11-07 02:33:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][600/1251]	eta 0:05:06 lr 0.000330	time 0.4692 (0.4706)	loss 3.4317 (3.2404)	grad_norm 1.6574 (1.8204)	mem 14853MB
[2022-11-07 02:33:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][650/1251]	eta 0:04:42 lr 0.000330	time 0.4599 (0.4702)	loss 3.6300 (3.2406)	grad_norm 1.8885 (1.8239)	mem 14853MB
[2022-11-07 02:33:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][700/1251]	eta 0:04:18 lr 0.000330	time 0.4692 (0.4699)	loss 3.7215 (3.2443)	grad_norm 1.5771 (1.8248)	mem 14853MB
[2022-11-07 02:34:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][750/1251]	eta 0:03:55 lr 0.000330	time 0.4545 (0.4698)	loss 3.4673 (3.2403)	grad_norm 1.9529 (1.8241)	mem 14853MB
[2022-11-07 02:34:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][800/1251]	eta 0:03:31 lr 0.000329	time 0.4651 (0.4698)	loss 3.4134 (3.2429)	grad_norm 1.4950 (1.8242)	mem 14853MB
[2022-11-07 02:35:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][850/1251]	eta 0:03:08 lr 0.000329	time 0.4649 (0.4696)	loss 2.9821 (3.2384)	grad_norm 1.6660 (1.8270)	mem 14853MB
[2022-11-07 02:35:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][900/1251]	eta 0:02:44 lr 0.000329	time 0.4739 (0.4695)	loss 3.6235 (3.2392)	grad_norm 1.8983 (1.8303)	mem 14853MB
[2022-11-07 02:35:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][950/1251]	eta 0:02:21 lr 0.000329	time 0.4620 (0.4693)	loss 2.8463 (3.2345)	grad_norm 1.8095 (1.8308)	mem 14853MB
[2022-11-07 02:36:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][1000/1251]	eta 0:01:57 lr 0.000329	time 0.4666 (0.4692)	loss 2.3724 (3.2291)	grad_norm 1.6685 (1.8329)	mem 14853MB
[2022-11-07 02:36:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][1050/1251]	eta 0:01:34 lr 0.000328	time 0.4607 (0.4693)	loss 3.5370 (3.2282)	grad_norm 1.6755 (1.8326)	mem 14853MB
[2022-11-07 02:36:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][1100/1251]	eta 0:01:10 lr 0.000328	time 0.4617 (0.4692)	loss 2.9196 (3.2294)	grad_norm 2.0020 (1.8315)	mem 14853MB
[2022-11-07 02:37:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][1150/1251]	eta 0:00:47 lr 0.000328	time 0.4635 (0.4690)	loss 3.7445 (3.2270)	grad_norm 2.2559 (1.8326)	mem 14853MB
[2022-11-07 02:37:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][1200/1251]	eta 0:00:23 lr 0.000328	time 0.4691 (0.4689)	loss 3.3132 (3.2353)	grad_norm 1.7539 (1.8313)	mem 14853MB
[2022-11-07 02:38:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [184/300][1250/1251]	eta 0:00:00 lr 0.000328	time 0.4707 (0.4688)	loss 3.0148 (3.2362)	grad_norm 1.7562 (1.8294)	mem 14853MB
[2022-11-07 02:38:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 184 training takes 0:09:46
[2022-11-07 02:38:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_184.pth saving......
[2022-11-07 02:38:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_184.pth saved !!!
[2022-11-07 02:38:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.493 (1.493)	Loss 0.9716 (0.9716)	Acc@1 77.637 (77.637)	Acc@5 93.555 (93.555)	Mem 14853MB
[2022-11-07 02:38:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.342 Acc@5 94.950
[2022-11-07 02:38:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.3%
[2022-11-07 02:38:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.606 (1.606)	Loss 0.8078 (0.8078)	Acc@1 80.859 (80.859)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 02:38:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.082 Acc@5 95.752
[2022-11-07 02:38:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.1%
[2022-11-07 02:38:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.08% at 184 epoch
[2022-11-07 02:38:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][0/1251]	eta 0:44:34 lr 0.000328	time 2.1381 (2.1381)	loss 2.9989 (2.9989)	grad_norm 1.6711 (1.6711)	mem 14853MB
[2022-11-07 02:38:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][50/1251]	eta 0:10:05 lr 0.000327	time 0.4592 (0.5039)	loss 2.5104 (3.0792)	grad_norm 1.8493 (1.8084)	mem 14853MB
[2022-11-07 02:39:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][100/1251]	eta 0:09:21 lr 0.000327	time 0.4633 (0.4876)	loss 2.1982 (3.1301)	grad_norm 1.7708 (1.7991)	mem 14853MB
[2022-11-07 02:39:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][150/1251]	eta 0:08:49 lr 0.000327	time 0.4736 (0.4813)	loss 3.6061 (3.1580)	grad_norm 1.9421 (1.8102)	mem 14853MB
[2022-11-07 02:40:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][200/1251]	eta 0:08:21 lr 0.000327	time 0.4705 (0.4771)	loss 3.2211 (3.1420)	grad_norm 1.7749 (1.8218)	mem 14853MB
[2022-11-07 02:40:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][250/1251]	eta 0:07:55 lr 0.000327	time 0.4641 (0.4749)	loss 3.8526 (3.1775)	grad_norm 1.5782 (1.8333)	mem 14853MB
[2022-11-07 02:40:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][300/1251]	eta 0:07:30 lr 0.000326	time 0.4686 (0.4732)	loss 2.9427 (3.1736)	grad_norm 2.2344 (1.8321)	mem 14853MB
[2022-11-07 02:41:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][350/1251]	eta 0:07:05 lr 0.000326	time 0.4642 (0.4720)	loss 3.6532 (3.1788)	grad_norm 1.8011 (1.8415)	mem 14853MB
[2022-11-07 02:41:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][400/1251]	eta 0:06:41 lr 0.000326	time 0.4581 (0.4716)	loss 3.5351 (3.1847)	grad_norm 1.7142 (1.8423)	mem 14853MB
[2022-11-07 02:41:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][450/1251]	eta 0:06:17 lr 0.000326	time 0.4560 (0.4711)	loss 3.1039 (3.1820)	grad_norm 2.3585 (1.8372)	mem 14853MB
[2022-11-07 02:42:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][500/1251]	eta 0:05:53 lr 0.000326	time 0.4685 (0.4706)	loss 3.3144 (3.1853)	grad_norm 2.2492 (1.8402)	mem 14853MB
[2022-11-07 02:42:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][550/1251]	eta 0:05:29 lr 0.000325	time 0.4590 (0.4705)	loss 2.4509 (3.1845)	grad_norm 1.7438 (1.8378)	mem 14853MB
[2022-11-07 02:43:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][600/1251]	eta 0:05:06 lr 0.000325	time 0.4578 (0.4702)	loss 3.1586 (3.1915)	grad_norm 1.8477 (1.8350)	mem 14853MB
[2022-11-07 02:43:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][650/1251]	eta 0:04:42 lr 0.000325	time 0.4654 (0.4699)	loss 3.1474 (3.1916)	grad_norm 1.6249 (inf)	mem 14853MB
[2022-11-07 02:43:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][700/1251]	eta 0:04:18 lr 0.000325	time 0.4556 (0.4696)	loss 3.2207 (3.1995)	grad_norm 1.7008 (inf)	mem 14853MB
[2022-11-07 02:44:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][750/1251]	eta 0:03:55 lr 0.000325	time 0.4602 (0.4695)	loss 3.7876 (3.1990)	grad_norm 2.0462 (inf)	mem 14853MB
[2022-11-07 02:44:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][800/1251]	eta 0:03:31 lr 0.000325	time 0.4608 (0.4696)	loss 3.2530 (3.2061)	grad_norm 1.7106 (inf)	mem 14853MB
[2022-11-07 02:45:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][850/1251]	eta 0:03:08 lr 0.000324	time 0.4603 (0.4693)	loss 3.5980 (3.2099)	grad_norm 1.8412 (inf)	mem 14853MB
[2022-11-07 02:45:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][900/1251]	eta 0:02:44 lr 0.000324	time 0.4735 (0.4693)	loss 3.1006 (3.2040)	grad_norm 1.7946 (inf)	mem 14853MB
[2022-11-07 02:45:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][950/1251]	eta 0:02:21 lr 0.000324	time 0.4665 (0.4690)	loss 2.9742 (3.2034)	grad_norm 1.8007 (inf)	mem 14853MB
[2022-11-07 02:46:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][1000/1251]	eta 0:01:57 lr 0.000324	time 0.4685 (0.4690)	loss 3.3769 (3.2063)	grad_norm 1.9510 (inf)	mem 14853MB
[2022-11-07 02:46:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][1050/1251]	eta 0:01:34 lr 0.000324	time 0.4752 (0.4690)	loss 3.4404 (3.2084)	grad_norm 1.8262 (inf)	mem 14853MB
[2022-11-07 02:47:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][1100/1251]	eta 0:01:10 lr 0.000323	time 0.4629 (0.4689)	loss 2.3093 (3.2075)	grad_norm 2.2114 (inf)	mem 14853MB
[2022-11-07 02:47:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][1150/1251]	eta 0:00:47 lr 0.000323	time 0.4721 (0.4688)	loss 2.6454 (3.2069)	grad_norm 1.8371 (inf)	mem 14853MB
[2022-11-07 02:47:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][1200/1251]	eta 0:00:23 lr 0.000323	time 0.4764 (0.4686)	loss 3.5400 (3.2092)	grad_norm 1.7934 (inf)	mem 14853MB
[2022-11-07 02:48:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [185/300][1250/1251]	eta 0:00:00 lr 0.000323	time 0.4574 (0.4684)	loss 3.4277 (3.2134)	grad_norm 1.8980 (inf)	mem 14853MB
[2022-11-07 02:48:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 185 training takes 0:09:46
[2022-11-07 02:48:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_185.pth saving......
[2022-11-07 02:48:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_185.pth saved !!!
[2022-11-07 02:48:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.574 (1.574)	Loss 0.8151 (0.8151)	Acc@1 80.566 (80.566)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-07 02:48:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.516 Acc@5 95.108
[2022-11-07 02:48:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.5%
[2022-11-07 02:48:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.653 (1.653)	Loss 0.7224 (0.7224)	Acc@1 82.812 (82.812)	Acc@5 96.875 (96.875)	Mem 14853MB
[2022-11-07 02:48:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.112 Acc@5 95.770
[2022-11-07 02:48:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.1%
[2022-11-07 02:48:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.11% at 185 epoch
[2022-11-07 02:48:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][0/1251]	eta 0:45:07 lr 0.000323	time 2.1646 (2.1646)	loss 2.6873 (2.6873)	grad_norm 1.7578 (1.7578)	mem 14853MB
[2022-11-07 02:48:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][50/1251]	eta 0:10:08 lr 0.000323	time 0.4603 (0.5069)	loss 2.2865 (3.1757)	grad_norm 1.9082 (1.8676)	mem 14853MB
[2022-11-07 02:49:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][100/1251]	eta 0:09:20 lr 0.000322	time 0.4678 (0.4870)	loss 2.2393 (3.2270)	grad_norm 1.6935 (1.8331)	mem 14853MB
[2022-11-07 02:49:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][150/1251]	eta 0:08:47 lr 0.000322	time 0.4703 (0.4795)	loss 3.5001 (3.2251)	grad_norm 1.6448 (1.8245)	mem 14853MB
[2022-11-07 02:50:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][200/1251]	eta 0:08:20 lr 0.000322	time 0.4513 (0.4760)	loss 2.6540 (3.1946)	grad_norm 1.6816 (1.8157)	mem 14853MB
[2022-11-07 02:50:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][250/1251]	eta 0:07:54 lr 0.000322	time 0.4675 (0.4742)	loss 3.1601 (3.2082)	grad_norm 1.6747 (1.8331)	mem 14853MB
[2022-11-07 02:50:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][300/1251]	eta 0:07:29 lr 0.000322	time 0.4726 (0.4731)	loss 3.6050 (3.2109)	grad_norm 1.7473 (1.8417)	mem 14853MB
[2022-11-07 02:51:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][350/1251]	eta 0:07:05 lr 0.000321	time 0.4606 (0.4721)	loss 3.2957 (3.2223)	grad_norm 1.5289 (1.8362)	mem 14853MB
[2022-11-07 02:51:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][400/1251]	eta 0:06:41 lr 0.000321	time 0.4578 (0.4713)	loss 3.5010 (3.2212)	grad_norm 1.7725 (1.8378)	mem 14853MB
[2022-11-07 02:52:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][450/1251]	eta 0:06:16 lr 0.000321	time 0.4629 (0.4706)	loss 3.6294 (3.2182)	grad_norm 1.9489 (1.8382)	mem 14853MB
[2022-11-07 02:52:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][500/1251]	eta 0:05:53 lr 0.000321	time 0.4677 (0.4704)	loss 2.6469 (3.2084)	grad_norm 1.9483 (1.8369)	mem 14853MB
[2022-11-07 02:52:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][550/1251]	eta 0:05:29 lr 0.000321	time 0.4604 (0.4705)	loss 2.6157 (3.2064)	grad_norm 1.8174 (1.8343)	mem 14853MB
[2022-11-07 02:53:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][600/1251]	eta 0:05:05 lr 0.000320	time 0.4828 (0.4700)	loss 3.4000 (3.2052)	grad_norm 2.0897 (1.8348)	mem 14853MB
[2022-11-07 02:53:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][650/1251]	eta 0:04:42 lr 0.000320	time 0.4674 (0.4696)	loss 2.4689 (3.1963)	grad_norm 1.6468 (1.8346)	mem 14853MB
[2022-11-07 02:54:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][700/1251]	eta 0:04:18 lr 0.000320	time 0.4580 (0.4693)	loss 3.8498 (3.2079)	grad_norm 1.8564 (1.8358)	mem 14853MB
[2022-11-07 02:54:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][750/1251]	eta 0:03:55 lr 0.000320	time 0.4652 (0.4692)	loss 3.6049 (3.2125)	grad_norm 1.7741 (1.8356)	mem 14853MB
[2022-11-07 02:54:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][800/1251]	eta 0:03:31 lr 0.000320	time 0.4658 (0.4694)	loss 3.6439 (3.2147)	grad_norm 1.9073 (1.8348)	mem 14853MB
[2022-11-07 02:55:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][850/1251]	eta 0:03:08 lr 0.000320	time 0.4649 (0.4693)	loss 2.7005 (3.2165)	grad_norm 1.9474 (1.8374)	mem 14853MB
[2022-11-07 02:55:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][900/1251]	eta 0:02:44 lr 0.000319	time 0.4593 (0.4691)	loss 3.3754 (3.2189)	grad_norm 1.8488 (1.8374)	mem 14853MB
[2022-11-07 02:55:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][950/1251]	eta 0:02:21 lr 0.000319	time 0.4774 (0.4690)	loss 2.8196 (3.2100)	grad_norm 1.6938 (1.8351)	mem 14853MB
[2022-11-07 02:56:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][1000/1251]	eta 0:01:57 lr 0.000319	time 0.4680 (0.4688)	loss 2.0577 (3.2072)	grad_norm 1.6096 (1.8355)	mem 14853MB
[2022-11-07 02:56:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][1050/1251]	eta 0:01:34 lr 0.000319	time 0.4660 (0.4690)	loss 3.3275 (3.2051)	grad_norm 1.7224 (1.8332)	mem 14853MB
[2022-11-07 02:57:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][1100/1251]	eta 0:01:10 lr 0.000319	time 0.4764 (0.4688)	loss 3.6052 (3.2013)	grad_norm 1.7178 (1.8331)	mem 14853MB
[2022-11-07 02:57:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][1150/1251]	eta 0:00:47 lr 0.000318	time 0.4606 (0.4687)	loss 3.4205 (3.1982)	grad_norm 1.9722 (1.8333)	mem 14853MB
[2022-11-07 02:57:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][1200/1251]	eta 0:00:23 lr 0.000318	time 0.4737 (0.4686)	loss 3.7455 (3.1946)	grad_norm 2.0710 (1.8353)	mem 14853MB
[2022-11-07 02:58:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [186/300][1250/1251]	eta 0:00:00 lr 0.000318	time 0.4629 (0.4684)	loss 3.2060 (3.1996)	grad_norm 1.9140 (1.8355)	mem 14853MB
[2022-11-07 02:58:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 186 training takes 0:09:46
[2022-11-07 02:58:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_186.pth saving......
[2022-11-07 02:58:18 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_186.pth saved !!!
[2022-11-07 02:58:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.615 (1.615)	Loss 0.8405 (0.8405)	Acc@1 80.469 (80.469)	Acc@5 94.824 (94.824)	Mem 14853MB
[2022-11-07 02:58:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.442 Acc@5 94.968
[2022-11-07 02:58:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.4%
[2022-11-07 02:58:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.602 (1.602)	Loss 0.7932 (0.7932)	Acc@1 81.738 (81.738)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 02:58:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.132 Acc@5 95.790
[2022-11-07 02:58:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.1%
[2022-11-07 02:58:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.13% at 186 epoch
[2022-11-07 02:58:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][0/1251]	eta 0:41:45 lr 0.000318	time 2.0026 (2.0026)	loss 3.0047 (3.0047)	grad_norm 1.9132 (1.9132)	mem 14853MB
[2022-11-07 02:59:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][50/1251]	eta 0:10:06 lr 0.000318	time 0.4639 (0.5052)	loss 4.2252 (3.2694)	grad_norm 1.7666 (1.8435)	mem 14853MB
[2022-11-07 02:59:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][100/1251]	eta 0:09:19 lr 0.000318	time 0.4620 (0.4860)	loss 3.3567 (3.1992)	grad_norm 1.6804 (1.8493)	mem 14853MB
[2022-11-07 02:59:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][150/1251]	eta 0:08:47 lr 0.000317	time 0.4560 (0.4794)	loss 3.6183 (3.2022)	grad_norm 1.7958 (1.8538)	mem 14853MB
[2022-11-07 03:00:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][200/1251]	eta 0:08:20 lr 0.000317	time 0.4699 (0.4766)	loss 3.5258 (3.2192)	grad_norm 1.8481 (1.8666)	mem 14853MB
[2022-11-07 03:00:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][250/1251]	eta 0:07:55 lr 0.000317	time 0.4689 (0.4746)	loss 3.2001 (3.2376)	grad_norm 1.8920 (1.8701)	mem 14853MB
[2022-11-07 03:00:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][300/1251]	eta 0:07:30 lr 0.000317	time 0.4671 (0.4735)	loss 3.6138 (3.2364)	grad_norm 1.8023 (1.8643)	mem 14853MB
[2022-11-07 03:01:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][350/1251]	eta 0:07:05 lr 0.000317	time 0.4566 (0.4723)	loss 2.1833 (3.2155)	grad_norm 1.8879 (1.8644)	mem 14853MB
[2022-11-07 03:01:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][400/1251]	eta 0:06:41 lr 0.000316	time 0.4667 (0.4715)	loss 3.8861 (3.2137)	grad_norm 2.1006 (1.8665)	mem 14853MB
[2022-11-07 03:02:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][450/1251]	eta 0:06:17 lr 0.000316	time 0.4544 (0.4709)	loss 3.9235 (3.2245)	grad_norm 2.2533 (1.8681)	mem 14853MB
[2022-11-07 03:02:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][500/1251]	eta 0:05:53 lr 0.000316	time 0.4728 (0.4705)	loss 2.4576 (3.2136)	grad_norm 1.9990 (1.8645)	mem 14853MB
[2022-11-07 03:02:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][550/1251]	eta 0:05:29 lr 0.000316	time 0.4629 (0.4704)	loss 2.0715 (3.2188)	grad_norm 1.8846 (1.8618)	mem 14853MB
[2022-11-07 03:03:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][600/1251]	eta 0:05:06 lr 0.000316	time 0.5435 (0.4701)	loss 3.8512 (3.2260)	grad_norm 1.9386 (1.8595)	mem 14853MB
[2022-11-07 03:03:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][650/1251]	eta 0:04:42 lr 0.000315	time 0.4565 (0.4697)	loss 4.0210 (3.2132)	grad_norm 1.7942 (1.8574)	mem 14853MB
[2022-11-07 03:04:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][700/1251]	eta 0:04:18 lr 0.000315	time 0.4657 (0.4696)	loss 2.7973 (3.2088)	grad_norm 1.7240 (1.8616)	mem 14853MB
[2022-11-07 03:04:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][750/1251]	eta 0:03:55 lr 0.000315	time 0.4598 (0.4693)	loss 3.3686 (3.2050)	grad_norm 1.7100 (1.8607)	mem 14853MB
[2022-11-07 03:04:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][800/1251]	eta 0:03:31 lr 0.000315	time 0.4655 (0.4694)	loss 2.6729 (3.1992)	grad_norm 1.7245 (1.8626)	mem 14853MB
[2022-11-07 03:05:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][850/1251]	eta 0:03:08 lr 0.000315	time 0.4687 (0.4691)	loss 2.7254 (3.1953)	grad_norm 1.6663 (1.8606)	mem 14853MB
[2022-11-07 03:05:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][900/1251]	eta 0:02:44 lr 0.000315	time 0.4621 (0.4691)	loss 2.3202 (3.1847)	grad_norm 2.0194 (1.8575)	mem 14853MB
[2022-11-07 03:06:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][950/1251]	eta 0:02:21 lr 0.000314	time 0.4572 (0.4690)	loss 3.3259 (3.1859)	grad_norm 1.8520 (1.8592)	mem 14853MB
[2022-11-07 03:06:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][1000/1251]	eta 0:01:57 lr 0.000314	time 0.4682 (0.4688)	loss 3.4575 (3.1939)	grad_norm 1.6487 (1.8586)	mem 14853MB
[2022-11-07 03:06:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][1050/1251]	eta 0:01:34 lr 0.000314	time 0.4632 (0.4689)	loss 3.6397 (3.1920)	grad_norm 1.6900 (1.8609)	mem 14853MB
[2022-11-07 03:07:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][1100/1251]	eta 0:01:10 lr 0.000314	time 0.4643 (0.4687)	loss 3.9974 (3.1923)	grad_norm 1.9707 (1.8587)	mem 14853MB
[2022-11-07 03:07:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][1150/1251]	eta 0:00:47 lr 0.000314	time 0.4637 (0.4686)	loss 2.9316 (3.1926)	grad_norm 1.9388 (1.8579)	mem 14853MB
[2022-11-07 03:07:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][1200/1251]	eta 0:00:23 lr 0.000313	time 0.4715 (0.4686)	loss 3.2240 (3.1969)	grad_norm 1.7383 (nan)	mem 14853MB
[2022-11-07 03:08:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [187/300][1250/1251]	eta 0:00:00 lr 0.000313	time 0.4591 (0.4684)	loss 3.4531 (3.1930)	grad_norm 2.1121 (nan)	mem 14853MB
[2022-11-07 03:08:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 187 training takes 0:09:46
[2022-11-07 03:08:21 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_187.pth saving......
[2022-11-07 03:08:22 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_187.pth saved !!!
[2022-11-07 03:08:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.545 (1.545)	Loss 0.8350 (0.8350)	Acc@1 77.734 (77.734)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 03:08:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.578 Acc@5 94.952
[2022-11-07 03:08:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.6%
[2022-11-07 03:08:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.531 (1.531)	Loss 0.7727 (0.7727)	Acc@1 81.543 (81.543)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 03:08:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.164 Acc@5 95.786
[2022-11-07 03:08:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.2%
[2022-11-07 03:08:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.16% at 187 epoch
[2022-11-07 03:08:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][0/1251]	eta 0:40:07 lr 0.000313	time 1.9243 (1.9243)	loss 2.6556 (2.6556)	grad_norm 1.6755 (1.6755)	mem 14853MB
[2022-11-07 03:09:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][50/1251]	eta 0:10:05 lr 0.000313	time 0.4703 (0.5038)	loss 3.9601 (3.2237)	grad_norm 1.9115 (1.8535)	mem 14853MB
[2022-11-07 03:09:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][100/1251]	eta 0:09:20 lr 0.000313	time 0.4580 (0.4866)	loss 3.2875 (3.1673)	grad_norm 1.7870 (1.8301)	mem 14853MB
[2022-11-07 03:09:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][150/1251]	eta 0:08:48 lr 0.000313	time 0.4643 (0.4799)	loss 3.3345 (3.2079)	grad_norm 1.8964 (1.8450)	mem 14853MB
[2022-11-07 03:10:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][200/1251]	eta 0:08:20 lr 0.000312	time 0.4629 (0.4763)	loss 3.0392 (3.2131)	grad_norm 1.7150 (1.8497)	mem 14853MB
[2022-11-07 03:10:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][250/1251]	eta 0:07:54 lr 0.000312	time 0.4704 (0.4744)	loss 3.2477 (3.2154)	grad_norm 1.7667 (1.8537)	mem 14853MB
[2022-11-07 03:11:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][300/1251]	eta 0:07:29 lr 0.000312	time 0.4636 (0.4729)	loss 3.0931 (3.2034)	grad_norm 1.9081 (1.8556)	mem 14853MB
[2022-11-07 03:11:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][350/1251]	eta 0:07:05 lr 0.000312	time 0.4795 (0.4718)	loss 2.2603 (3.1921)	grad_norm 2.0228 (1.8542)	mem 14853MB
[2022-11-07 03:11:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][400/1251]	eta 0:06:40 lr 0.000312	time 0.4665 (0.4710)	loss 3.6586 (3.2147)	grad_norm 1.7659 (1.8576)	mem 14853MB
[2022-11-07 03:12:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][450/1251]	eta 0:06:16 lr 0.000311	time 0.4611 (0.4705)	loss 3.3689 (3.2191)	grad_norm 1.8963 (1.8626)	mem 14853MB
[2022-11-07 03:12:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][500/1251]	eta 0:05:53 lr 0.000311	time 0.4646 (0.4702)	loss 2.6018 (3.2113)	grad_norm 1.7535 (1.8630)	mem 14853MB
[2022-11-07 03:12:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][550/1251]	eta 0:05:29 lr 0.000311	time 0.4668 (0.4700)	loss 3.2257 (3.2045)	grad_norm 1.7444 (1.8600)	mem 14853MB
[2022-11-07 03:13:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][600/1251]	eta 0:05:05 lr 0.000311	time 0.4604 (0.4698)	loss 3.3987 (3.1979)	grad_norm 1.8605 (1.8575)	mem 14853MB
[2022-11-07 03:13:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][650/1251]	eta 0:04:42 lr 0.000311	time 0.4559 (0.4694)	loss 3.4877 (3.1968)	grad_norm 1.8000 (1.8583)	mem 14853MB
[2022-11-07 03:14:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][700/1251]	eta 0:04:18 lr 0.000311	time 0.4597 (0.4692)	loss 2.4881 (3.2028)	grad_norm 2.0973 (1.8564)	mem 14853MB
[2022-11-07 03:14:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][750/1251]	eta 0:03:55 lr 0.000310	time 0.4564 (0.4693)	loss 2.8661 (3.2007)	grad_norm 1.7532 (1.8571)	mem 14853MB
[2022-11-07 03:14:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][800/1251]	eta 0:03:31 lr 0.000310	time 0.4653 (0.4693)	loss 3.6978 (3.2024)	grad_norm 2.4194 (1.8573)	mem 14853MB
[2022-11-07 03:15:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][850/1251]	eta 0:03:08 lr 0.000310	time 0.4611 (0.4692)	loss 4.0020 (3.2015)	grad_norm 1.8005 (1.8558)	mem 14853MB
[2022-11-07 03:15:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][900/1251]	eta 0:02:44 lr 0.000310	time 0.4611 (0.4690)	loss 3.4791 (3.2012)	grad_norm 1.6877 (1.8567)	mem 14853MB
[2022-11-07 03:16:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][950/1251]	eta 0:02:21 lr 0.000310	time 0.4688 (0.4688)	loss 3.2104 (3.1982)	grad_norm 1.5930 (1.8586)	mem 14853MB
[2022-11-07 03:16:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][1000/1251]	eta 0:01:57 lr 0.000309	time 0.4707 (0.4689)	loss 2.4380 (3.1955)	grad_norm 1.5325 (1.8574)	mem 14853MB
[2022-11-07 03:16:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][1050/1251]	eta 0:01:34 lr 0.000309	time 0.4646 (0.4688)	loss 3.5120 (3.1911)	grad_norm 1.8891 (1.8561)	mem 14853MB
[2022-11-07 03:17:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][1100/1251]	eta 0:01:10 lr 0.000309	time 0.4659 (0.4688)	loss 2.3042 (3.1893)	grad_norm 1.7872 (1.8568)	mem 14853MB
[2022-11-07 03:17:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][1150/1251]	eta 0:00:47 lr 0.000309	time 0.4769 (0.4687)	loss 3.4702 (3.1900)	grad_norm 1.9547 (1.8565)	mem 14853MB
[2022-11-07 03:18:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][1200/1251]	eta 0:00:23 lr 0.000309	time 0.4764 (0.4685)	loss 3.5805 (3.1908)	grad_norm 2.2719 (1.8594)	mem 14853MB
[2022-11-07 03:18:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [188/300][1250/1251]	eta 0:00:00 lr 0.000308	time 0.4573 (0.4683)	loss 3.2536 (3.1943)	grad_norm 1.9989 (1.8603)	mem 14853MB
[2022-11-07 03:18:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 188 training takes 0:09:46
[2022-11-07 03:18:25 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_188.pth saving......
[2022-11-07 03:18:26 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_188.pth saved !!!
[2022-11-07 03:18:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.488 (1.488)	Loss 0.9413 (0.9413)	Acc@1 78.027 (78.027)	Acc@5 94.531 (94.531)	Mem 14853MB
[2022-11-07 03:18:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.298 Acc@5 95.028
[2022-11-07 03:18:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.3%
[2022-11-07 03:18:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.549 (1.549)	Loss 0.7828 (0.7828)	Acc@1 79.590 (79.590)	Acc@5 96.680 (96.680)	Mem 14853MB
[2022-11-07 03:18:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.176 Acc@5 95.772
[2022-11-07 03:18:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.2%
[2022-11-07 03:18:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.18% at 188 epoch
[2022-11-07 03:18:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][0/1251]	eta 0:41:34 lr 0.000308	time 1.9940 (1.9940)	loss 3.0426 (3.0426)	grad_norm 1.7261 (1.7261)	mem 14853MB
[2022-11-07 03:19:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][50/1251]	eta 0:10:02 lr 0.000308	time 0.4616 (0.5021)	loss 3.0015 (3.0798)	grad_norm 1.7536 (1.8621)	mem 14853MB
[2022-11-07 03:19:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][100/1251]	eta 0:09:20 lr 0.000308	time 0.4665 (0.4866)	loss 3.6543 (3.1431)	grad_norm 1.8530 (1.8615)	mem 14853MB
[2022-11-07 03:19:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][150/1251]	eta 0:08:48 lr 0.000308	time 0.4640 (0.4801)	loss 3.4548 (3.1734)	grad_norm 2.0979 (1.8912)	mem 14853MB
[2022-11-07 03:20:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][200/1251]	eta 0:08:20 lr 0.000308	time 0.4762 (0.4762)	loss 2.7164 (3.1979)	grad_norm 1.7510 (1.8900)	mem 14853MB
[2022-11-07 03:20:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][250/1251]	eta 0:07:54 lr 0.000307	time 0.4710 (0.4740)	loss 3.5964 (3.2029)	grad_norm 1.9444 (1.8894)	mem 14853MB
[2022-11-07 03:21:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][300/1251]	eta 0:07:29 lr 0.000307	time 0.4598 (0.4728)	loss 3.6459 (3.1863)	grad_norm 1.8016 (1.8905)	mem 14853MB
[2022-11-07 03:21:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][350/1251]	eta 0:07:05 lr 0.000307	time 0.4764 (0.4718)	loss 2.6882 (3.1782)	grad_norm 1.8886 (1.8822)	mem 14853MB
[2022-11-07 03:21:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][400/1251]	eta 0:06:40 lr 0.000307	time 0.4686 (0.4711)	loss 3.7572 (3.1720)	grad_norm 1.7782 (1.8736)	mem 14853MB
[2022-11-07 03:22:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][450/1251]	eta 0:06:16 lr 0.000307	time 0.4618 (0.4705)	loss 2.3428 (3.1645)	grad_norm 1.9172 (1.8746)	mem 14853MB
[2022-11-07 03:22:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][500/1251]	eta 0:05:52 lr 0.000307	time 0.4571 (0.4700)	loss 3.3917 (3.1647)	grad_norm 1.6897 (1.8827)	mem 14853MB
[2022-11-07 03:23:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][550/1251]	eta 0:05:29 lr 0.000306	time 0.4697 (0.4700)	loss 3.4834 (3.1653)	grad_norm 2.1458 (1.8774)	mem 14853MB
[2022-11-07 03:23:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][600/1251]	eta 0:05:05 lr 0.000306	time 0.4702 (0.4700)	loss 3.7462 (3.1570)	grad_norm 1.9489 (1.8744)	mem 14853MB
[2022-11-07 03:23:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][650/1251]	eta 0:04:42 lr 0.000306	time 0.4555 (0.4697)	loss 3.0068 (3.1501)	grad_norm 2.0639 (1.8767)	mem 14853MB
[2022-11-07 03:24:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][700/1251]	eta 0:04:18 lr 0.000306	time 0.4792 (0.4694)	loss 3.3299 (3.1507)	grad_norm 1.8763 (1.8762)	mem 14853MB
[2022-11-07 03:24:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][750/1251]	eta 0:03:55 lr 0.000306	time 0.5340 (0.4693)	loss 3.4294 (3.1486)	grad_norm 1.7421 (1.8773)	mem 14853MB
[2022-11-07 03:24:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][800/1251]	eta 0:03:31 lr 0.000305	time 0.5380 (0.4695)	loss 3.5449 (3.1574)	grad_norm 2.0005 (1.8804)	mem 14853MB
[2022-11-07 03:25:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][850/1251]	eta 0:03:08 lr 0.000305	time 0.4649 (0.4693)	loss 2.0439 (3.1532)	grad_norm 1.6672 (1.8793)	mem 14853MB
[2022-11-07 03:25:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][900/1251]	eta 0:02:44 lr 0.000305	time 0.4656 (0.4691)	loss 3.4073 (3.1527)	grad_norm 1.6981 (1.8793)	mem 14853MB
[2022-11-07 03:26:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][950/1251]	eta 0:02:21 lr 0.000305	time 0.4632 (0.4689)	loss 2.2076 (3.1568)	grad_norm 2.1492 (1.8780)	mem 14853MB
[2022-11-07 03:26:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][1000/1251]	eta 0:01:57 lr 0.000305	time 0.4587 (0.4688)	loss 2.7550 (3.1555)	grad_norm 1.7204 (1.8767)	mem 14853MB
[2022-11-07 03:26:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][1050/1251]	eta 0:01:34 lr 0.000304	time 0.4681 (0.4689)	loss 2.6951 (3.1531)	grad_norm 1.9168 (1.8771)	mem 14853MB
[2022-11-07 03:27:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][1100/1251]	eta 0:01:10 lr 0.000304	time 0.4627 (0.4689)	loss 3.4339 (3.1536)	grad_norm 2.0449 (1.8770)	mem 14853MB
[2022-11-07 03:27:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][1150/1251]	eta 0:00:47 lr 0.000304	time 0.4675 (0.4688)	loss 2.2731 (3.1564)	grad_norm 1.7608 (1.8810)	mem 14853MB
[2022-11-07 03:28:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][1200/1251]	eta 0:00:23 lr 0.000304	time 0.4695 (0.4687)	loss 3.3062 (3.1590)	grad_norm 1.8776 (1.8811)	mem 14853MB
[2022-11-07 03:28:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [189/300][1250/1251]	eta 0:00:00 lr 0.000304	time 0.4578 (0.4685)	loss 2.3562 (3.1626)	grad_norm 2.0376 (1.8812)	mem 14853MB
[2022-11-07 03:28:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 189 training takes 0:09:46
[2022-11-07 03:28:30 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_189.pth saving......
[2022-11-07 03:28:30 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_189.pth saved !!!
[2022-11-07 03:28:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.541 (1.541)	Loss 0.9549 (0.9549)	Acc@1 77.441 (77.441)	Acc@5 94.043 (94.043)	Mem 14853MB
[2022-11-07 03:28:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.658 Acc@5 95.148
[2022-11-07 03:28:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.7%
[2022-11-07 03:28:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.557 (1.557)	Loss 0.7523 (0.7523)	Acc@1 81.543 (81.543)	Acc@5 96.875 (96.875)	Mem 14853MB
[2022-11-07 03:28:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.182 Acc@5 95.776
[2022-11-07 03:28:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.2%
[2022-11-07 03:28:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.18% at 189 epoch
[2022-11-07 03:28:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][0/1251]	eta 0:41:40 lr 0.000304	time 1.9990 (1.9990)	loss 2.9488 (2.9488)	grad_norm 1.8021 (1.8021)	mem 14853MB
[2022-11-07 03:29:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][50/1251]	eta 0:10:07 lr 0.000303	time 0.4546 (0.5061)	loss 3.1894 (3.1466)	grad_norm 2.3501 (1.8896)	mem 14853MB
[2022-11-07 03:29:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][100/1251]	eta 0:09:22 lr 0.000303	time 0.4623 (0.4888)	loss 3.5046 (3.1471)	grad_norm 1.9050 (1.8656)	mem 14853MB
[2022-11-07 03:30:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][150/1251]	eta 0:08:49 lr 0.000303	time 0.4626 (0.4812)	loss 2.4404 (3.1716)	grad_norm 1.7178 (1.8669)	mem 14853MB
[2022-11-07 03:30:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][200/1251]	eta 0:08:21 lr 0.000303	time 0.4532 (0.4774)	loss 2.5569 (3.1877)	grad_norm 1.9737 (1.8725)	mem 14853MB
[2022-11-07 03:30:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][250/1251]	eta 0:07:55 lr 0.000303	time 0.4721 (0.4749)	loss 3.0861 (3.1685)	grad_norm 2.0189 (1.8751)	mem 14853MB
[2022-11-07 03:31:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][300/1251]	eta 0:07:30 lr 0.000303	time 0.4604 (0.4736)	loss 3.4147 (3.1549)	grad_norm 2.0521 (1.8806)	mem 14853MB
[2022-11-07 03:31:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][350/1251]	eta 0:07:05 lr 0.000302	time 0.4698 (0.4725)	loss 2.3584 (3.1563)	grad_norm 1.9679 (1.8828)	mem 14853MB
[2022-11-07 03:31:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][400/1251]	eta 0:06:41 lr 0.000302	time 0.4707 (0.4719)	loss 3.1534 (3.1641)	grad_norm 1.7416 (1.8833)	mem 14853MB
[2022-11-07 03:32:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][450/1251]	eta 0:06:17 lr 0.000302	time 0.4597 (0.4710)	loss 2.9976 (3.1640)	grad_norm 1.9455 (1.8857)	mem 14853MB
[2022-11-07 03:32:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][500/1251]	eta 0:05:53 lr 0.000302	time 0.4622 (0.4704)	loss 3.5333 (3.1743)	grad_norm 2.1950 (1.8871)	mem 14853MB
[2022-11-07 03:33:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][550/1251]	eta 0:05:29 lr 0.000302	time 0.4712 (0.4707)	loss 2.7966 (3.1705)	grad_norm 2.1689 (1.8918)	mem 14853MB
[2022-11-07 03:33:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][600/1251]	eta 0:05:06 lr 0.000301	time 0.4763 (0.4703)	loss 3.2304 (3.1669)	grad_norm 1.9573 (1.8969)	mem 14853MB
[2022-11-07 03:33:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][650/1251]	eta 0:04:42 lr 0.000301	time 0.4756 (0.4699)	loss 3.4778 (3.1698)	grad_norm 1.7344 (1.8940)	mem 14853MB
[2022-11-07 03:34:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][700/1251]	eta 0:04:18 lr 0.000301	time 0.4761 (0.4696)	loss 3.3092 (3.1742)	grad_norm 1.8870 (1.8980)	mem 14853MB
[2022-11-07 03:34:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][750/1251]	eta 0:03:55 lr 0.000301	time 0.4612 (0.4694)	loss 3.6874 (3.1677)	grad_norm 1.8978 (1.8946)	mem 14853MB
[2022-11-07 03:35:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][800/1251]	eta 0:03:31 lr 0.000301	time 0.5348 (0.4695)	loss 2.9963 (3.1698)	grad_norm 1.6901 (1.8946)	mem 14853MB
[2022-11-07 03:35:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][850/1251]	eta 0:03:08 lr 0.000300	time 0.4601 (0.4692)	loss 3.5757 (3.1690)	grad_norm 1.9492 (1.8949)	mem 14853MB
[2022-11-07 03:35:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][900/1251]	eta 0:02:44 lr 0.000300	time 0.4604 (0.4690)	loss 2.0384 (3.1698)	grad_norm 1.8768 (1.8950)	mem 14853MB
[2022-11-07 03:36:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][950/1251]	eta 0:02:21 lr 0.000300	time 0.4640 (0.4688)	loss 2.8638 (3.1661)	grad_norm 1.7166 (1.8926)	mem 14853MB
[2022-11-07 03:36:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][1000/1251]	eta 0:01:57 lr 0.000300	time 0.4683 (0.4687)	loss 3.3134 (3.1678)	grad_norm 2.1408 (1.8919)	mem 14853MB
[2022-11-07 03:37:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][1050/1251]	eta 0:01:34 lr 0.000300	time 0.4750 (0.4690)	loss 3.5710 (3.1676)	grad_norm 2.0297 (inf)	mem 14853MB
[2022-11-07 03:37:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][1100/1251]	eta 0:01:10 lr 0.000300	time 0.4628 (0.4689)	loss 4.0803 (3.1734)	grad_norm 1.9654 (inf)	mem 14853MB
[2022-11-07 03:37:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][1150/1251]	eta 0:00:47 lr 0.000299	time 0.4664 (0.4687)	loss 3.4310 (3.1707)	grad_norm 1.9188 (inf)	mem 14853MB
[2022-11-07 03:38:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][1200/1251]	eta 0:00:23 lr 0.000299	time 0.4706 (0.4686)	loss 1.9670 (3.1708)	grad_norm 1.9754 (inf)	mem 14853MB
[2022-11-07 03:38:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [190/300][1250/1251]	eta 0:00:00 lr 0.000299	time 0.4646 (0.4684)	loss 2.1893 (3.1689)	grad_norm 2.1249 (inf)	mem 14853MB
[2022-11-07 03:38:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 190 training takes 0:09:46
[2022-11-07 03:38:34 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_190.pth saving......
[2022-11-07 03:38:34 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_190.pth saved !!!
[2022-11-07 03:38:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.592 (1.592)	Loss 0.8338 (0.8338)	Acc@1 79.688 (79.688)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 03:38:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.594 Acc@5 95.010
[2022-11-07 03:38:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.6%
[2022-11-07 03:38:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.762 (1.762)	Loss 0.8585 (0.8585)	Acc@1 80.469 (80.469)	Acc@5 94.336 (94.336)	Mem 14853MB
[2022-11-07 03:38:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.180 Acc@5 95.790
[2022-11-07 03:38:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.2%
[2022-11-07 03:38:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.18% at 189 epoch
[2022-11-07 03:38:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][0/1251]	eta 0:41:54 lr 0.000299	time 2.0101 (2.0101)	loss 3.5987 (3.5987)	grad_norm 1.6818 (1.6818)	mem 14853MB
[2022-11-07 03:39:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][50/1251]	eta 0:10:03 lr 0.000299	time 0.4657 (0.5024)	loss 3.0967 (3.0668)	grad_norm 1.9163 (1.8591)	mem 14853MB
[2022-11-07 03:39:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][100/1251]	eta 0:09:18 lr 0.000299	time 0.4588 (0.4850)	loss 3.2998 (3.1168)	grad_norm 1.7727 (1.8691)	mem 14853MB
[2022-11-07 03:40:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][150/1251]	eta 0:08:46 lr 0.000298	time 0.4626 (0.4785)	loss 3.9117 (3.1425)	grad_norm 1.7638 (1.8645)	mem 14853MB
[2022-11-07 03:40:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][200/1251]	eta 0:08:20 lr 0.000298	time 0.4601 (0.4758)	loss 3.4652 (3.1596)	grad_norm 1.9418 (1.8674)	mem 14853MB
[2022-11-07 03:40:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][250/1251]	eta 0:07:54 lr 0.000298	time 0.4614 (0.4743)	loss 3.5320 (3.1523)	grad_norm 1.7358 (1.8647)	mem 14853MB
[2022-11-07 03:41:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][300/1251]	eta 0:07:29 lr 0.000298	time 0.4540 (0.4727)	loss 2.9126 (3.1734)	grad_norm 1.8765 (1.8742)	mem 14853MB
[2022-11-07 03:41:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][350/1251]	eta 0:07:05 lr 0.000298	time 0.4616 (0.4718)	loss 3.7909 (3.1988)	grad_norm 1.8246 (1.8810)	mem 14853MB
[2022-11-07 03:42:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][400/1251]	eta 0:06:40 lr 0.000297	time 0.4679 (0.4711)	loss 3.8326 (3.2057)	grad_norm 1.6949 (1.8787)	mem 14853MB
[2022-11-07 03:42:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][450/1251]	eta 0:06:16 lr 0.000297	time 0.4682 (0.4706)	loss 3.7161 (3.1935)	grad_norm 1.9939 (1.8779)	mem 14853MB
[2022-11-07 03:42:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][500/1251]	eta 0:05:53 lr 0.000297	time 0.4658 (0.4702)	loss 3.8236 (3.1916)	grad_norm 1.9341 (1.8804)	mem 14853MB
[2022-11-07 03:43:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][550/1251]	eta 0:05:29 lr 0.000297	time 0.4735 (0.4701)	loss 3.3529 (3.2006)	grad_norm 1.7438 (1.8855)	mem 14853MB
[2022-11-07 03:43:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][600/1251]	eta 0:05:05 lr 0.000297	time 0.4529 (0.4698)	loss 3.4803 (3.2027)	grad_norm 2.0524 (1.8885)	mem 14853MB
[2022-11-07 03:43:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][650/1251]	eta 0:04:42 lr 0.000296	time 0.4598 (0.4695)	loss 3.4540 (3.1977)	grad_norm 1.7472 (1.8900)	mem 14853MB
[2022-11-07 03:44:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][700/1251]	eta 0:04:18 lr 0.000296	time 0.4646 (0.4693)	loss 2.4713 (3.1980)	grad_norm 2.0310 (1.8872)	mem 14853MB
[2022-11-07 03:44:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][750/1251]	eta 0:03:55 lr 0.000296	time 0.4618 (0.4692)	loss 3.5904 (3.1998)	grad_norm 1.7388 (1.8858)	mem 14853MB
[2022-11-07 03:45:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][800/1251]	eta 0:03:31 lr 0.000296	time 0.4653 (0.4691)	loss 2.9624 (3.2003)	grad_norm 2.2361 (1.8849)	mem 14853MB
[2022-11-07 03:45:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][850/1251]	eta 0:03:08 lr 0.000296	time 0.4789 (0.4690)	loss 3.1118 (3.2004)	grad_norm 1.7710 (1.8865)	mem 14853MB
[2022-11-07 03:45:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][900/1251]	eta 0:02:44 lr 0.000296	time 0.5404 (0.4689)	loss 3.4201 (3.1965)	grad_norm 1.8008 (1.8882)	mem 14853MB
[2022-11-07 03:46:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][950/1251]	eta 0:02:21 lr 0.000295	time 0.4661 (0.4687)	loss 3.0029 (3.1882)	grad_norm 2.0351 (1.8870)	mem 14853MB
[2022-11-07 03:46:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][1000/1251]	eta 0:01:57 lr 0.000295	time 0.4536 (0.4686)	loss 3.3944 (3.1831)	grad_norm 2.0684 (1.8858)	mem 14853MB
[2022-11-07 03:47:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][1050/1251]	eta 0:01:34 lr 0.000295	time 0.4573 (0.4685)	loss 3.2660 (3.1836)	grad_norm 1.7534 (1.8872)	mem 14853MB
[2022-11-07 03:47:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][1100/1251]	eta 0:01:10 lr 0.000295	time 0.4631 (0.4685)	loss 2.6039 (3.1861)	grad_norm 1.9064 (1.8867)	mem 14853MB
[2022-11-07 03:47:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][1150/1251]	eta 0:00:47 lr 0.000295	time 0.4581 (0.4684)	loss 2.9614 (3.1855)	grad_norm 1.8029 (1.8861)	mem 14853MB
[2022-11-07 03:48:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][1200/1251]	eta 0:00:23 lr 0.000294	time 0.4626 (0.4683)	loss 1.9154 (3.1852)	grad_norm 1.7626 (1.8867)	mem 14853MB
[2022-11-07 03:48:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [191/300][1250/1251]	eta 0:00:00 lr 0.000294	time 0.4573 (0.4681)	loss 3.6352 (3.1854)	grad_norm 1.9154 (1.8866)	mem 14853MB
[2022-11-07 03:48:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 191 training takes 0:09:45
[2022-11-07 03:48:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_191.pth saving......
[2022-11-07 03:48:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_191.pth saved !!!
[2022-11-07 03:48:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.616 (1.616)	Loss 0.8822 (0.8822)	Acc@1 78.613 (78.613)	Acc@5 94.531 (94.531)	Mem 14853MB
[2022-11-07 03:48:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.518 Acc@5 95.086
[2022-11-07 03:48:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.5%
[2022-11-07 03:48:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.624 (1.624)	Loss 0.7618 (0.7618)	Acc@1 80.957 (80.957)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 03:48:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.202 Acc@5 95.774
[2022-11-07 03:48:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.2%
[2022-11-07 03:48:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.20% at 191 epoch
[2022-11-07 03:48:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][0/1251]	eta 0:42:51 lr 0.000294	time 2.0556 (2.0556)	loss 2.1421 (2.1421)	grad_norm 2.0394 (2.0394)	mem 14853MB
[2022-11-07 03:49:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][50/1251]	eta 0:10:07 lr 0.000294	time 0.4626 (0.5059)	loss 3.3880 (3.1507)	grad_norm 1.9753 (1.9154)	mem 14853MB
[2022-11-07 03:49:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][100/1251]	eta 0:09:19 lr 0.000294	time 0.4636 (0.4861)	loss 3.9443 (3.2241)	grad_norm 1.9512 (1.9093)	mem 14853MB
[2022-11-07 03:50:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][150/1251]	eta 0:08:48 lr 0.000294	time 0.4683 (0.4796)	loss 2.0534 (3.1661)	grad_norm 1.7475 (1.9095)	mem 14853MB
[2022-11-07 03:50:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][200/1251]	eta 0:08:20 lr 0.000293	time 0.4651 (0.4761)	loss 3.4930 (3.1691)	grad_norm 1.7414 (1.9159)	mem 14853MB
[2022-11-07 03:50:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][250/1251]	eta 0:07:54 lr 0.000293	time 0.4545 (0.4742)	loss 3.2313 (3.1837)	grad_norm 1.9501 (1.9151)	mem 14853MB
[2022-11-07 03:51:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][300/1251]	eta 0:07:29 lr 0.000293	time 0.4751 (0.4732)	loss 2.0439 (3.1622)	grad_norm 1.6521 (1.9131)	mem 14853MB
[2022-11-07 03:51:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][350/1251]	eta 0:07:05 lr 0.000293	time 0.4628 (0.4724)	loss 3.0998 (3.1591)	grad_norm 2.1930 (1.9183)	mem 14853MB
[2022-11-07 03:52:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][400/1251]	eta 0:06:41 lr 0.000293	time 0.4573 (0.4715)	loss 3.7331 (3.1674)	grad_norm 1.8933 (1.9161)	mem 14853MB
[2022-11-07 03:52:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][450/1251]	eta 0:06:17 lr 0.000293	time 0.4683 (0.4708)	loss 3.5804 (3.1610)	grad_norm 1.9807 (1.9212)	mem 14853MB
[2022-11-07 03:52:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][500/1251]	eta 0:05:53 lr 0.000292	time 0.4666 (0.4703)	loss 3.3264 (3.1678)	grad_norm 1.8497 (1.9170)	mem 14853MB
[2022-11-07 03:53:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][550/1251]	eta 0:05:29 lr 0.000292	time 0.4621 (0.4700)	loss 3.5911 (3.1703)	grad_norm 2.0970 (1.9167)	mem 14853MB
[2022-11-07 03:53:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][600/1251]	eta 0:05:05 lr 0.000292	time 0.4596 (0.4696)	loss 3.6045 (3.1704)	grad_norm 1.9648 (1.9175)	mem 14853MB
[2022-11-07 03:54:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][650/1251]	eta 0:04:41 lr 0.000292	time 0.4710 (0.4692)	loss 1.9449 (3.1634)	grad_norm 2.1979 (1.9166)	mem 14853MB
[2022-11-07 03:54:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][700/1251]	eta 0:04:18 lr 0.000292	time 0.4618 (0.4691)	loss 2.7125 (3.1645)	grad_norm 1.8906 (1.9160)	mem 14853MB
[2022-11-07 03:54:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][750/1251]	eta 0:03:55 lr 0.000291	time 0.4612 (0.4691)	loss 3.2738 (3.1635)	grad_norm 1.6535 (1.9121)	mem 14853MB
[2022-11-07 03:55:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][800/1251]	eta 0:03:31 lr 0.000291	time 0.4638 (0.4691)	loss 2.9353 (3.1683)	grad_norm 1.7642 (1.9135)	mem 14853MB
[2022-11-07 03:55:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][850/1251]	eta 0:03:08 lr 0.000291	time 0.4633 (0.4690)	loss 3.2522 (3.1798)	grad_norm 2.0144 (1.9113)	mem 14853MB
[2022-11-07 03:55:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][900/1251]	eta 0:02:44 lr 0.000291	time 0.4727 (0.4688)	loss 3.2223 (3.1804)	grad_norm 1.7090 (1.9116)	mem 14853MB
[2022-11-07 03:56:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][950/1251]	eta 0:02:21 lr 0.000291	time 0.4646 (0.4686)	loss 2.8830 (3.1830)	grad_norm 2.0191 (1.9083)	mem 14853MB
[2022-11-07 03:56:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][1000/1251]	eta 0:01:57 lr 0.000290	time 0.4665 (0.4686)	loss 3.1231 (3.1797)	grad_norm 1.7077 (1.9089)	mem 14853MB
[2022-11-07 03:57:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][1050/1251]	eta 0:01:34 lr 0.000290	time 0.4665 (0.4687)	loss 3.3983 (3.1811)	grad_norm 1.7178 (1.9064)	mem 14853MB
[2022-11-07 03:57:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][1100/1251]	eta 0:01:10 lr 0.000290	time 0.4733 (0.4686)	loss 3.3433 (3.1762)	grad_norm 1.8421 (1.9064)	mem 14853MB
[2022-11-07 03:57:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][1150/1251]	eta 0:00:47 lr 0.000290	time 0.4669 (0.4685)	loss 4.1141 (3.1794)	grad_norm 2.4107 (1.9073)	mem 14853MB
[2022-11-07 03:58:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][1200/1251]	eta 0:00:23 lr 0.000290	time 0.5432 (0.4684)	loss 2.7094 (3.1811)	grad_norm 1.8993 (1.9072)	mem 14853MB
[2022-11-07 03:58:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [192/300][1250/1251]	eta 0:00:00 lr 0.000290	time 0.4576 (0.4682)	loss 3.4610 (3.1826)	grad_norm 1.8250 (1.9064)	mem 14853MB
[2022-11-07 03:58:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 192 training takes 0:09:45
[2022-11-07 03:58:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_192.pth saving......
[2022-11-07 03:58:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_192.pth saved !!!
[2022-11-07 03:58:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.600 (1.600)	Loss 0.8712 (0.8712)	Acc@1 78.418 (78.418)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 03:58:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.804 Acc@5 95.168
[2022-11-07 03:58:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.8%
[2022-11-07 03:58:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.557 (1.557)	Loss 0.7950 (0.7950)	Acc@1 81.152 (81.152)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 03:59:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.234 Acc@5 95.778
[2022-11-07 03:59:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.2%
[2022-11-07 03:59:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.23% at 192 epoch
[2022-11-07 03:59:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][0/1251]	eta 0:41:55 lr 0.000290	time 2.0109 (2.0109)	loss 3.2509 (3.2509)	grad_norm 1.8090 (1.8090)	mem 14853MB
[2022-11-07 03:59:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][50/1251]	eta 0:09:59 lr 0.000289	time 0.4643 (0.4992)	loss 3.5418 (3.1874)	grad_norm 1.8021 (1.9070)	mem 14853MB
[2022-11-07 03:59:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][100/1251]	eta 0:09:16 lr 0.000289	time 0.4562 (0.4836)	loss 2.7536 (3.1832)	grad_norm 2.3920 (1.8931)	mem 14853MB
[2022-11-07 04:00:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][150/1251]	eta 0:08:45 lr 0.000289	time 0.4538 (0.4775)	loss 3.7201 (3.1829)	grad_norm 1.7892 (1.9089)	mem 14853MB
[2022-11-07 04:00:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][200/1251]	eta 0:08:20 lr 0.000289	time 0.4643 (0.4761)	loss 2.3818 (3.1846)	grad_norm 1.8837 (1.8968)	mem 14853MB
[2022-11-07 04:00:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][250/1251]	eta 0:07:54 lr 0.000289	time 0.4698 (0.4744)	loss 2.1427 (3.1651)	grad_norm 1.8977 (inf)	mem 14853MB
[2022-11-07 04:01:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][300/1251]	eta 0:07:29 lr 0.000288	time 0.4744 (0.4728)	loss 2.0923 (3.1581)	grad_norm 1.8567 (inf)	mem 14853MB
[2022-11-07 04:01:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][350/1251]	eta 0:07:05 lr 0.000288	time 0.4590 (0.4720)	loss 3.4485 (3.1438)	grad_norm 1.9553 (inf)	mem 14853MB
[2022-11-07 04:02:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][400/1251]	eta 0:06:40 lr 0.000288	time 0.4662 (0.4710)	loss 3.3586 (3.1420)	grad_norm 1.7827 (inf)	mem 14853MB
[2022-11-07 04:02:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][450/1251]	eta 0:06:16 lr 0.000288	time 0.4625 (0.4705)	loss 3.2489 (3.1391)	grad_norm 1.8364 (inf)	mem 14853MB
[2022-11-07 04:02:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][500/1251]	eta 0:05:53 lr 0.000288	time 0.4616 (0.4702)	loss 2.8203 (3.1386)	grad_norm 1.9318 (inf)	mem 14853MB
[2022-11-07 04:03:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][550/1251]	eta 0:05:29 lr 0.000288	time 0.4697 (0.4699)	loss 2.9275 (3.1402)	grad_norm 2.0107 (inf)	mem 14853MB
[2022-11-07 04:03:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][600/1251]	eta 0:05:05 lr 0.000287	time 0.4659 (0.4696)	loss 3.8398 (3.1467)	grad_norm 1.8978 (inf)	mem 14853MB
[2022-11-07 04:04:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][650/1251]	eta 0:04:42 lr 0.000287	time 0.4715 (0.4693)	loss 2.2935 (3.1433)	grad_norm 1.7516 (inf)	mem 14853MB
[2022-11-07 04:04:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][700/1251]	eta 0:04:18 lr 0.000287	time 0.4622 (0.4692)	loss 3.6382 (3.1529)	grad_norm 1.7791 (inf)	mem 14853MB
[2022-11-07 04:04:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][750/1251]	eta 0:03:54 lr 0.000287	time 0.4521 (0.4689)	loss 3.5652 (3.1525)	grad_norm 1.9796 (inf)	mem 14853MB
[2022-11-07 04:05:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][800/1251]	eta 0:03:31 lr 0.000287	time 0.4634 (0.4688)	loss 3.4044 (3.1547)	grad_norm 1.5927 (inf)	mem 14853MB
[2022-11-07 04:05:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][850/1251]	eta 0:03:07 lr 0.000286	time 0.4703 (0.4686)	loss 3.4635 (3.1558)	grad_norm 1.9936 (inf)	mem 14853MB
[2022-11-07 04:06:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][900/1251]	eta 0:02:44 lr 0.000286	time 0.4685 (0.4683)	loss 3.5742 (3.1550)	grad_norm 2.0909 (inf)	mem 14853MB
[2022-11-07 04:06:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][950/1251]	eta 0:02:20 lr 0.000286	time 0.4640 (0.4682)	loss 3.5956 (3.1577)	grad_norm 1.8322 (inf)	mem 14853MB
[2022-11-07 04:06:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][1000/1251]	eta 0:01:57 lr 0.000286	time 0.4697 (0.4682)	loss 3.6019 (3.1574)	grad_norm 1.7898 (inf)	mem 14853MB
[2022-11-07 04:07:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][1050/1251]	eta 0:01:34 lr 0.000286	time 0.4675 (0.4681)	loss 2.8406 (3.1591)	grad_norm 1.9905 (inf)	mem 14853MB
[2022-11-07 04:07:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][1100/1251]	eta 0:01:10 lr 0.000285	time 0.4596 (0.4681)	loss 2.3050 (3.1531)	grad_norm 1.8148 (inf)	mem 14853MB
[2022-11-07 04:07:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][1150/1251]	eta 0:00:47 lr 0.000285	time 0.4720 (0.4679)	loss 3.2247 (3.1530)	grad_norm 1.9664 (inf)	mem 14853MB
[2022-11-07 04:08:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][1200/1251]	eta 0:00:23 lr 0.000285	time 0.4643 (0.4680)	loss 2.5783 (3.1462)	grad_norm 1.8129 (inf)	mem 14853MB
[2022-11-07 04:08:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [193/300][1250/1251]	eta 0:00:00 lr 0.000285	time 0.4569 (0.4679)	loss 3.2475 (3.1538)	grad_norm 1.9577 (inf)	mem 14853MB
[2022-11-07 04:08:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 193 training takes 0:09:45
[2022-11-07 04:08:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_193.pth saving......
[2022-11-07 04:08:46 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_193.pth saved !!!
[2022-11-07 04:08:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.552 (1.552)	Loss 0.9222 (0.9222)	Acc@1 80.078 (80.078)	Acc@5 95.117 (95.117)	Mem 14853MB
[2022-11-07 04:08:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.814 Acc@5 95.228
[2022-11-07 04:08:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.8%
[2022-11-07 04:08:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.661 (1.661)	Loss 0.8199 (0.8199)	Acc@1 79.883 (79.883)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-07 04:09:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.222 Acc@5 95.772
[2022-11-07 04:09:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.2%
[2022-11-07 04:09:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.23% at 192 epoch
[2022-11-07 04:09:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][0/1251]	eta 0:41:23 lr 0.000285	time 1.9851 (1.9851)	loss 2.9830 (2.9830)	grad_norm 2.0841 (2.0841)	mem 14853MB
[2022-11-07 04:09:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][50/1251]	eta 0:09:57 lr 0.000285	time 0.4594 (0.4974)	loss 3.0203 (3.1768)	grad_norm 1.9023 (1.9188)	mem 14853MB
[2022-11-07 04:09:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][100/1251]	eta 0:09:15 lr 0.000285	time 0.4707 (0.4829)	loss 3.7424 (3.2167)	grad_norm 1.9698 (1.9296)	mem 14853MB
[2022-11-07 04:10:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][150/1251]	eta 0:08:46 lr 0.000284	time 0.4655 (0.4781)	loss 3.8119 (3.1911)	grad_norm 1.9058 (1.9177)	mem 14853MB
[2022-11-07 04:10:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][200/1251]	eta 0:08:19 lr 0.000284	time 0.4586 (0.4754)	loss 3.6476 (3.1612)	grad_norm 2.0487 (1.9179)	mem 14853MB
[2022-11-07 04:11:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][250/1251]	eta 0:07:54 lr 0.000284	time 0.4590 (0.4738)	loss 3.8447 (3.1529)	grad_norm 1.8432 (1.9129)	mem 14853MB
[2022-11-07 04:11:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][300/1251]	eta 0:07:29 lr 0.000284	time 0.4602 (0.4727)	loss 3.1397 (3.1591)	grad_norm 1.8654 (1.9168)	mem 14853MB
[2022-11-07 04:11:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][350/1251]	eta 0:07:05 lr 0.000284	time 0.4526 (0.4720)	loss 3.1347 (3.1643)	grad_norm 2.1715 (1.9123)	mem 14853MB
[2022-11-07 04:12:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][400/1251]	eta 0:06:41 lr 0.000283	time 0.4615 (0.4713)	loss 3.3494 (3.1625)	grad_norm 1.8373 (1.9088)	mem 14853MB
[2022-11-07 04:12:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][450/1251]	eta 0:06:16 lr 0.000283	time 0.4578 (0.4705)	loss 3.0706 (3.1625)	grad_norm 1.7081 (1.9093)	mem 14853MB
[2022-11-07 04:12:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][500/1251]	eta 0:05:53 lr 0.000283	time 0.4618 (0.4701)	loss 2.9486 (3.1539)	grad_norm 1.8427 (1.9155)	mem 14853MB
[2022-11-07 04:13:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][550/1251]	eta 0:05:29 lr 0.000283	time 0.4591 (0.4698)	loss 3.1599 (3.1561)	grad_norm 1.8164 (1.9141)	mem 14853MB
[2022-11-07 04:13:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][600/1251]	eta 0:05:05 lr 0.000283	time 0.4812 (0.4697)	loss 3.3688 (3.1660)	grad_norm 1.7664 (1.9224)	mem 14853MB
[2022-11-07 04:14:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][650/1251]	eta 0:04:42 lr 0.000282	time 0.4678 (0.4696)	loss 3.6214 (3.1618)	grad_norm 1.6696 (1.9241)	mem 14853MB
[2022-11-07 04:14:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][700/1251]	eta 0:04:18 lr 0.000282	time 0.4707 (0.4693)	loss 3.3942 (3.1530)	grad_norm 1.7640 (1.9216)	mem 14853MB
[2022-11-07 04:14:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][750/1251]	eta 0:03:54 lr 0.000282	time 0.4655 (0.4690)	loss 3.6455 (3.1632)	grad_norm 2.0537 (1.9212)	mem 14853MB
[2022-11-07 04:15:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][800/1251]	eta 0:03:31 lr 0.000282	time 0.4524 (0.4689)	loss 3.4716 (3.1635)	grad_norm 1.7931 (1.9193)	mem 14853MB
[2022-11-07 04:15:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][850/1251]	eta 0:03:07 lr 0.000282	time 0.4656 (0.4688)	loss 2.5046 (3.1589)	grad_norm 1.8025 (1.9213)	mem 14853MB
[2022-11-07 04:16:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][900/1251]	eta 0:02:44 lr 0.000282	time 0.4574 (0.4687)	loss 3.7071 (3.1485)	grad_norm 1.8827 (1.9174)	mem 14853MB
[2022-11-07 04:16:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][950/1251]	eta 0:02:21 lr 0.000281	time 0.4675 (0.4686)	loss 2.2153 (3.1513)	grad_norm 2.3343 (1.9174)	mem 14853MB
[2022-11-07 04:16:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][1000/1251]	eta 0:01:57 lr 0.000281	time 0.4634 (0.4684)	loss 2.0237 (3.1538)	grad_norm 1.9182 (1.9166)	mem 14853MB
[2022-11-07 04:17:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][1050/1251]	eta 0:01:34 lr 0.000281	time 0.4622 (0.4683)	loss 3.3015 (3.1557)	grad_norm 1.7201 (1.9139)	mem 14853MB
[2022-11-07 04:17:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][1100/1251]	eta 0:01:10 lr 0.000281	time 0.4708 (0.4682)	loss 3.3312 (3.1499)	grad_norm 2.0056 (1.9131)	mem 14853MB
[2022-11-07 04:18:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][1150/1251]	eta 0:00:47 lr 0.000281	time 0.4793 (0.4683)	loss 2.3511 (3.1485)	grad_norm 1.6845 (1.9109)	mem 14853MB
[2022-11-07 04:18:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][1200/1251]	eta 0:00:23 lr 0.000280	time 0.4601 (0.4682)	loss 2.2158 (3.1509)	grad_norm 2.0758 (inf)	mem 14853MB
[2022-11-07 04:18:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [194/300][1250/1251]	eta 0:00:00 lr 0.000280	time 0.4586 (0.4680)	loss 3.6026 (3.1469)	grad_norm 1.7192 (inf)	mem 14853MB
[2022-11-07 04:18:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 194 training takes 0:09:45
[2022-11-07 04:18:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_194.pth saving......
[2022-11-07 04:18:50 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_194.pth saved !!!
[2022-11-07 04:18:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.432 (1.432)	Loss 0.8579 (0.8579)	Acc@1 79.492 (79.492)	Acc@5 95.020 (95.020)	Mem 14853MB
[2022-11-07 04:18:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.784 Acc@5 95.152
[2022-11-07 04:18:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.8%
[2022-11-07 04:19:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.611 (1.611)	Loss 0.7405 (0.7405)	Acc@1 82.422 (82.422)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 04:19:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.254 Acc@5 95.802
[2022-11-07 04:19:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.3%
[2022-11-07 04:19:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.25% at 194 epoch
[2022-11-07 04:19:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][0/1251]	eta 0:42:10 lr 0.000280	time 2.0225 (2.0225)	loss 3.6072 (3.6072)	grad_norm 1.8305 (1.8305)	mem 14853MB
[2022-11-07 04:19:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][50/1251]	eta 0:09:58 lr 0.000280	time 0.4664 (0.4985)	loss 2.3324 (3.1137)	grad_norm 1.9623 (1.9504)	mem 14853MB
[2022-11-07 04:19:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][100/1251]	eta 0:09:17 lr 0.000280	time 0.5494 (0.4843)	loss 3.6435 (3.1222)	grad_norm 2.1766 (1.9129)	mem 14853MB
[2022-11-07 04:20:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][150/1251]	eta 0:08:46 lr 0.000280	time 0.4695 (0.4783)	loss 3.7016 (3.1654)	grad_norm 2.1249 (1.8987)	mem 14853MB
[2022-11-07 04:20:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][200/1251]	eta 0:08:19 lr 0.000280	time 0.4610 (0.4751)	loss 3.6235 (3.1491)	grad_norm 1.6394 (1.9100)	mem 14853MB
[2022-11-07 04:21:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][250/1251]	eta 0:07:53 lr 0.000279	time 0.4742 (0.4732)	loss 3.7402 (3.1638)	grad_norm 1.7704 (1.9128)	mem 14853MB
[2022-11-07 04:21:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][300/1251]	eta 0:07:28 lr 0.000279	time 0.4627 (0.4720)	loss 2.0230 (3.1618)	grad_norm 1.8157 (1.9146)	mem 14853MB
[2022-11-07 04:21:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][350/1251]	eta 0:07:04 lr 0.000279	time 0.4672 (0.4709)	loss 2.1431 (3.1592)	grad_norm 2.1125 (1.9112)	mem 14853MB
[2022-11-07 04:22:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][400/1251]	eta 0:06:40 lr 0.000279	time 0.4640 (0.4701)	loss 3.5800 (3.1598)	grad_norm 1.7998 (1.9120)	mem 14853MB
[2022-11-07 04:22:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][450/1251]	eta 0:06:16 lr 0.000279	time 0.4693 (0.4696)	loss 3.2644 (3.1533)	grad_norm 1.7427 (1.9128)	mem 14853MB
[2022-11-07 04:23:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][500/1251]	eta 0:05:52 lr 0.000278	time 0.4603 (0.4692)	loss 3.2061 (3.1590)	grad_norm 1.9652 (1.9144)	mem 14853MB
[2022-11-07 04:23:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][550/1251]	eta 0:05:28 lr 0.000278	time 0.4620 (0.4690)	loss 2.0814 (3.1604)	grad_norm 1.9492 (1.9146)	mem 14853MB
[2022-11-07 04:23:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][600/1251]	eta 0:05:05 lr 0.000278	time 0.4566 (0.4689)	loss 3.4642 (3.1571)	grad_norm 2.3257 (1.9151)	mem 14853MB
[2022-11-07 04:24:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][650/1251]	eta 0:04:41 lr 0.000278	time 0.4590 (0.4686)	loss 3.4100 (3.1630)	grad_norm 2.0433 (1.9160)	mem 14853MB
[2022-11-07 04:24:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][700/1251]	eta 0:04:18 lr 0.000278	time 0.4642 (0.4684)	loss 2.6442 (3.1669)	grad_norm 1.9549 (1.9195)	mem 14853MB
[2022-11-07 04:24:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][750/1251]	eta 0:03:54 lr 0.000278	time 0.4682 (0.4684)	loss 3.6231 (3.1744)	grad_norm 1.6554 (1.9223)	mem 14853MB
[2022-11-07 04:25:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][800/1251]	eta 0:03:31 lr 0.000277	time 0.4671 (0.4682)	loss 2.8088 (3.1712)	grad_norm 1.9020 (1.9228)	mem 14853MB
[2022-11-07 04:25:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][850/1251]	eta 0:03:07 lr 0.000277	time 0.4643 (0.4682)	loss 1.8965 (3.1677)	grad_norm 2.3854 (1.9246)	mem 14853MB
[2022-11-07 04:26:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][900/1251]	eta 0:02:44 lr 0.000277	time 0.4608 (0.4680)	loss 2.5321 (3.1614)	grad_norm 1.6715 (1.9235)	mem 14853MB
[2022-11-07 04:26:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][950/1251]	eta 0:02:20 lr 0.000277	time 0.4652 (0.4679)	loss 3.2179 (3.1681)	grad_norm 2.2394 (1.9266)	mem 14853MB
[2022-11-07 04:26:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][1000/1251]	eta 0:01:57 lr 0.000277	time 0.4646 (0.4678)	loss 3.6200 (3.1719)	grad_norm 1.9237 (1.9285)	mem 14853MB
[2022-11-07 04:27:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][1050/1251]	eta 0:01:34 lr 0.000276	time 0.4671 (0.4678)	loss 2.8980 (3.1761)	grad_norm 1.7815 (1.9278)	mem 14853MB
[2022-11-07 04:27:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][1100/1251]	eta 0:01:10 lr 0.000276	time 0.4755 (0.4678)	loss 2.4343 (3.1735)	grad_norm 1.9772 (1.9262)	mem 14853MB
[2022-11-07 04:28:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][1150/1251]	eta 0:00:47 lr 0.000276	time 0.4711 (0.4678)	loss 3.6985 (3.1707)	grad_norm 2.0622 (1.9250)	mem 14853MB
[2022-11-07 04:28:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][1200/1251]	eta 0:00:23 lr 0.000276	time 0.4594 (0.4677)	loss 2.8158 (3.1672)	grad_norm 2.1526 (1.9255)	mem 14853MB
[2022-11-07 04:28:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [195/300][1250/1251]	eta 0:00:00 lr 0.000276	time 0.4570 (0.4676)	loss 3.5982 (3.1665)	grad_norm 2.0984 (1.9263)	mem 14853MB
[2022-11-07 04:28:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 195 training takes 0:09:45
[2022-11-07 04:28:52 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_195.pth saving......
[2022-11-07 04:28:53 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_195.pth saved !!!
[2022-11-07 04:28:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.489 (1.489)	Loss 0.9191 (0.9191)	Acc@1 78.320 (78.320)	Acc@5 94.531 (94.531)	Mem 14853MB
[2022-11-07 04:29:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.066 Acc@5 95.128
[2022-11-07 04:29:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.1%
[2022-11-07 04:29:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.513 (1.513)	Loss 0.8430 (0.8430)	Acc@1 78.613 (78.613)	Acc@5 96.680 (96.680)	Mem 14853MB
[2022-11-07 04:29:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.266 Acc@5 95.814
[2022-11-07 04:29:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.3%
[2022-11-07 04:29:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.27% at 195 epoch
[2022-11-07 04:29:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][0/1251]	eta 0:39:58 lr 0.000276	time 1.9170 (1.9170)	loss 3.2567 (3.2567)	grad_norm 1.7862 (1.7862)	mem 14853MB
[2022-11-07 04:29:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][50/1251]	eta 0:09:56 lr 0.000275	time 0.4658 (0.4970)	loss 3.0003 (3.1034)	grad_norm 1.8936 (1.9070)	mem 14853MB
[2022-11-07 04:29:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][100/1251]	eta 0:09:17 lr 0.000275	time 0.4625 (0.4841)	loss 2.5585 (3.1414)	grad_norm 1.9261 (1.9135)	mem 14853MB
[2022-11-07 04:30:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][150/1251]	eta 0:08:47 lr 0.000275	time 0.4557 (0.4795)	loss 3.5685 (3.1433)	grad_norm 1.9221 (1.9204)	mem 14853MB
[2022-11-07 04:30:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][200/1251]	eta 0:08:20 lr 0.000275	time 0.4641 (0.4758)	loss 2.0287 (3.1072)	grad_norm 1.8453 (1.9333)	mem 14853MB
[2022-11-07 04:31:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][250/1251]	eta 0:07:54 lr 0.000275	time 0.4680 (0.4742)	loss 3.4301 (3.0899)	grad_norm 1.8999 (1.9324)	mem 14853MB
[2022-11-07 04:31:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][300/1251]	eta 0:07:29 lr 0.000275	time 0.4713 (0.4728)	loss 3.2808 (3.0796)	grad_norm 1.8956 (1.9258)	mem 14853MB
[2022-11-07 04:31:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][350/1251]	eta 0:07:05 lr 0.000274	time 0.4891 (0.4717)	loss 3.4340 (3.0891)	grad_norm 1.9394 (1.9248)	mem 14853MB
[2022-11-07 04:32:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][400/1251]	eta 0:06:40 lr 0.000274	time 0.4556 (0.4708)	loss 3.2186 (3.0939)	grad_norm 2.0336 (1.9231)	mem 14853MB
[2022-11-07 04:32:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][450/1251]	eta 0:06:16 lr 0.000274	time 0.4621 (0.4702)	loss 3.6163 (3.1029)	grad_norm 1.9463 (1.9292)	mem 14853MB
[2022-11-07 04:33:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][500/1251]	eta 0:05:52 lr 0.000274	time 0.4619 (0.4697)	loss 3.5521 (3.1162)	grad_norm 1.9707 (1.9283)	mem 14853MB
[2022-11-07 04:33:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][550/1251]	eta 0:05:29 lr 0.000274	time 0.4564 (0.4696)	loss 2.3417 (3.1231)	grad_norm 1.7177 (1.9329)	mem 14853MB
[2022-11-07 04:33:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][600/1251]	eta 0:05:05 lr 0.000273	time 0.4627 (0.4696)	loss 3.2835 (3.1267)	grad_norm 2.1503 (1.9369)	mem 14853MB
[2022-11-07 04:34:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][650/1251]	eta 0:04:42 lr 0.000273	time 0.4571 (0.4694)	loss 2.1049 (3.1366)	grad_norm 1.8202 (1.9402)	mem 14853MB
[2022-11-07 04:34:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][700/1251]	eta 0:04:18 lr 0.000273	time 0.4688 (0.4691)	loss 3.6053 (3.1460)	grad_norm 1.9066 (1.9393)	mem 14853MB
[2022-11-07 04:35:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][750/1251]	eta 0:03:54 lr 0.000273	time 0.4537 (0.4690)	loss 3.6739 (3.1442)	grad_norm 1.8607 (1.9393)	mem 14853MB
[2022-11-07 04:35:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][800/1251]	eta 0:03:31 lr 0.000273	time 0.4584 (0.4688)	loss 3.4460 (3.1470)	grad_norm 1.8278 (inf)	mem 14853MB
[2022-11-07 04:35:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][850/1251]	eta 0:03:08 lr 0.000273	time 0.4575 (0.4690)	loss 3.2675 (3.1432)	grad_norm 2.1791 (inf)	mem 14853MB
[2022-11-07 04:36:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][900/1251]	eta 0:02:44 lr 0.000272	time 0.4755 (0.4688)	loss 2.5100 (3.1411)	grad_norm 2.1895 (inf)	mem 14853MB
[2022-11-07 04:36:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][950/1251]	eta 0:02:21 lr 0.000272	time 0.4720 (0.4687)	loss 3.9356 (3.1419)	grad_norm 1.9413 (inf)	mem 14853MB
[2022-11-07 04:36:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][1000/1251]	eta 0:01:57 lr 0.000272	time 0.4676 (0.4686)	loss 2.6621 (3.1457)	grad_norm 1.7050 (inf)	mem 14853MB
[2022-11-07 04:37:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][1050/1251]	eta 0:01:34 lr 0.000272	time 0.4651 (0.4685)	loss 3.5067 (3.1507)	grad_norm 1.6925 (inf)	mem 14853MB
[2022-11-07 04:37:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][1100/1251]	eta 0:01:10 lr 0.000272	time 0.4602 (0.4685)	loss 2.6942 (3.1502)	grad_norm 1.7679 (inf)	mem 14853MB
[2022-11-07 04:38:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][1150/1251]	eta 0:00:47 lr 0.000271	time 0.4645 (0.4685)	loss 2.5205 (3.1529)	grad_norm 2.0640 (inf)	mem 14853MB
[2022-11-07 04:38:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][1200/1251]	eta 0:00:23 lr 0.000271	time 0.4667 (0.4684)	loss 3.5703 (3.1551)	grad_norm 1.9989 (inf)	mem 14853MB
[2022-11-07 04:38:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [196/300][1250/1251]	eta 0:00:00 lr 0.000271	time 0.4579 (0.4682)	loss 2.3558 (3.1546)	grad_norm 2.2401 (inf)	mem 14853MB
[2022-11-07 04:38:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 196 training takes 0:09:45
[2022-11-07 04:38:56 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_196.pth saving......
[2022-11-07 04:38:57 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_196.pth saved !!!
[2022-11-07 04:38:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.512 (1.512)	Loss 0.8836 (0.8836)	Acc@1 79.395 (79.395)	Acc@5 95.117 (95.117)	Mem 14853MB
[2022-11-07 04:39:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.812 Acc@5 95.148
[2022-11-07 04:39:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.8%
[2022-11-07 04:39:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.628 (1.628)	Loss 0.7655 (0.7655)	Acc@1 81.641 (81.641)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 04:39:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.274 Acc@5 95.830
[2022-11-07 04:39:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.3%
[2022-11-07 04:39:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.27% at 196 epoch
[2022-11-07 04:39:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][0/1251]	eta 0:40:24 lr 0.000271	time 1.9383 (1.9383)	loss 3.2762 (3.2762)	grad_norm 1.7908 (1.7908)	mem 14853MB
[2022-11-07 04:39:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][50/1251]	eta 0:09:58 lr 0.000271	time 0.5518 (0.4985)	loss 3.1689 (3.0812)	grad_norm 1.5703 (1.9570)	mem 14853MB
[2022-11-07 04:40:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][100/1251]	eta 0:09:18 lr 0.000271	time 0.4655 (0.4857)	loss 3.6566 (3.1186)	grad_norm 2.0870 (1.9354)	mem 14853MB
[2022-11-07 04:40:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][150/1251]	eta 0:08:48 lr 0.000271	time 0.4650 (0.4801)	loss 3.0139 (3.1274)	grad_norm 1.9006 (1.9363)	mem 14853MB
[2022-11-07 04:40:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][200/1251]	eta 0:08:20 lr 0.000270	time 0.4658 (0.4765)	loss 3.2706 (3.1472)	grad_norm 1.9087 (1.9594)	mem 14853MB
[2022-11-07 04:41:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][250/1251]	eta 0:07:54 lr 0.000270	time 0.4745 (0.4740)	loss 3.7782 (3.1549)	grad_norm 1.7960 (1.9518)	mem 14853MB
[2022-11-07 04:41:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][300/1251]	eta 0:07:29 lr 0.000270	time 0.4686 (0.4726)	loss 3.2926 (3.1573)	grad_norm 1.8540 (1.9481)	mem 14853MB
[2022-11-07 04:41:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][350/1251]	eta 0:07:04 lr 0.000270	time 0.4680 (0.4713)	loss 3.0809 (3.1566)	grad_norm 1.7793 (1.9504)	mem 14853MB
[2022-11-07 04:42:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][400/1251]	eta 0:06:40 lr 0.000270	time 0.4571 (0.4708)	loss 2.6703 (3.1479)	grad_norm 1.6795 (1.9482)	mem 14853MB
[2022-11-07 04:42:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][450/1251]	eta 0:06:16 lr 0.000269	time 0.4653 (0.4706)	loss 2.7647 (3.1469)	grad_norm 2.2425 (1.9522)	mem 14853MB
[2022-11-07 04:43:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][500/1251]	eta 0:05:53 lr 0.000269	time 0.4680 (0.4701)	loss 3.1077 (3.1483)	grad_norm 1.8897 (1.9540)	mem 14853MB
[2022-11-07 04:43:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][550/1251]	eta 0:05:29 lr 0.000269	time 0.4726 (0.4697)	loss 2.3124 (3.1386)	grad_norm 2.0859 (1.9510)	mem 14853MB
[2022-11-07 04:43:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][600/1251]	eta 0:05:05 lr 0.000269	time 0.4607 (0.4695)	loss 3.6018 (3.1390)	grad_norm 1.8301 (1.9518)	mem 14853MB
[2022-11-07 04:44:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][650/1251]	eta 0:04:41 lr 0.000269	time 0.4725 (0.4692)	loss 3.3380 (3.1427)	grad_norm 2.1132 (1.9519)	mem 14853MB
[2022-11-07 04:44:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][700/1251]	eta 0:04:18 lr 0.000269	time 0.4613 (0.4688)	loss 3.2316 (3.1474)	grad_norm 2.0459 (1.9489)	mem 14853MB
[2022-11-07 04:45:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][750/1251]	eta 0:03:54 lr 0.000268	time 0.4607 (0.4687)	loss 3.6430 (3.1523)	grad_norm 1.8023 (1.9475)	mem 14853MB
[2022-11-07 04:45:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][800/1251]	eta 0:03:31 lr 0.000268	time 0.5476 (0.4687)	loss 2.9500 (3.1523)	grad_norm 1.9691 (1.9494)	mem 14853MB
[2022-11-07 04:45:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][850/1251]	eta 0:03:07 lr 0.000268	time 0.4668 (0.4687)	loss 2.4194 (3.1516)	grad_norm 1.6972 (1.9469)	mem 14853MB
[2022-11-07 04:46:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][900/1251]	eta 0:02:44 lr 0.000268	time 0.4647 (0.4686)	loss 3.3071 (3.1460)	grad_norm 1.8853 (1.9482)	mem 14853MB
[2022-11-07 04:46:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][950/1251]	eta 0:02:20 lr 0.000268	time 0.4620 (0.4684)	loss 1.9971 (3.1446)	grad_norm 2.1292 (1.9486)	mem 14853MB
[2022-11-07 04:47:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][1000/1251]	eta 0:01:57 lr 0.000267	time 0.4653 (0.4683)	loss 3.6067 (3.1525)	grad_norm 1.6598 (1.9490)	mem 14853MB
[2022-11-07 04:47:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][1050/1251]	eta 0:01:34 lr 0.000267	time 0.4519 (0.4683)	loss 3.4124 (3.1516)	grad_norm 1.9709 (1.9508)	mem 14853MB
[2022-11-07 04:47:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][1100/1251]	eta 0:01:10 lr 0.000267	time 0.4584 (0.4683)	loss 3.5074 (3.1522)	grad_norm 1.8744 (1.9545)	mem 14853MB
[2022-11-07 04:48:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][1150/1251]	eta 0:00:47 lr 0.000267	time 0.4794 (0.4683)	loss 3.3771 (3.1529)	grad_norm 2.0181 (1.9526)	mem 14853MB
[2022-11-07 04:48:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][1200/1251]	eta 0:00:23 lr 0.000267	time 0.4629 (0.4682)	loss 3.2797 (3.1541)	grad_norm 1.8687 (1.9522)	mem 14853MB
[2022-11-07 04:48:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [197/300][1250/1251]	eta 0:00:00 lr 0.000267	time 0.4576 (0.4679)	loss 3.0963 (3.1543)	grad_norm 2.3245 (1.9557)	mem 14853MB
[2022-11-07 04:48:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 197 training takes 0:09:45
[2022-11-07 04:48:59 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_197.pth saving......
[2022-11-07 04:49:00 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_197.pth saved !!!
[2022-11-07 04:49:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.527 (1.527)	Loss 0.8411 (0.8411)	Acc@1 80.859 (80.859)	Acc@5 95.215 (95.215)	Mem 14853MB
[2022-11-07 04:49:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.994 Acc@5 95.204
[2022-11-07 04:49:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.0%
[2022-11-07 04:49:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.576 (1.576)	Loss 0.7633 (0.7633)	Acc@1 81.738 (81.738)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 04:49:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.282 Acc@5 95.830
[2022-11-07 04:49:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.3%
[2022-11-07 04:49:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.28% at 197 epoch
[2022-11-07 04:49:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][0/1251]	eta 0:41:06 lr 0.000267	time 1.9715 (1.9715)	loss 3.1989 (3.1989)	grad_norm 1.9818 (1.9818)	mem 14853MB
[2022-11-07 04:49:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][50/1251]	eta 0:10:03 lr 0.000266	time 0.4655 (0.5024)	loss 3.3073 (3.2704)	grad_norm 1.9533 (1.9371)	mem 14853MB
[2022-11-07 04:50:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][100/1251]	eta 0:09:19 lr 0.000266	time 0.4637 (0.4860)	loss 3.2724 (3.1794)	grad_norm 1.7450 (1.9310)	mem 14853MB
[2022-11-07 04:50:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][150/1251]	eta 0:08:47 lr 0.000266	time 0.4672 (0.4793)	loss 3.5156 (3.1753)	grad_norm 1.9899 (1.9484)	mem 14853MB
[2022-11-07 04:50:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][200/1251]	eta 0:08:19 lr 0.000266	time 0.4618 (0.4757)	loss 3.6544 (3.1558)	grad_norm 2.0829 (1.9515)	mem 14853MB
[2022-11-07 04:51:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][250/1251]	eta 0:07:54 lr 0.000266	time 0.4639 (0.4742)	loss 3.0293 (3.1512)	grad_norm 1.9258 (1.9449)	mem 14853MB
[2022-11-07 04:51:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][300/1251]	eta 0:07:30 lr 0.000265	time 0.4648 (0.4732)	loss 3.0501 (3.1358)	grad_norm 1.9299 (1.9579)	mem 14853MB
[2022-11-07 04:52:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][350/1251]	eta 0:07:05 lr 0.000265	time 0.4741 (0.4720)	loss 3.3325 (3.1276)	grad_norm 1.7984 (1.9594)	mem 14853MB
[2022-11-07 04:52:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][400/1251]	eta 0:06:40 lr 0.000265	time 0.4688 (0.4712)	loss 3.4078 (3.1409)	grad_norm 1.8805 (1.9581)	mem 14853MB
[2022-11-07 04:52:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][450/1251]	eta 0:06:16 lr 0.000265	time 0.4702 (0.4705)	loss 3.5427 (3.1374)	grad_norm 1.8776 (1.9624)	mem 14853MB
[2022-11-07 04:53:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][500/1251]	eta 0:05:52 lr 0.000265	time 0.4579 (0.4700)	loss 3.3674 (3.1358)	grad_norm 1.9558 (1.9583)	mem 14853MB
[2022-11-07 04:53:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][550/1251]	eta 0:05:29 lr 0.000265	time 0.4629 (0.4699)	loss 2.4313 (3.1486)	grad_norm 1.8617 (1.9560)	mem 14853MB
[2022-11-07 04:53:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][600/1251]	eta 0:05:05 lr 0.000264	time 0.4627 (0.4697)	loss 2.8820 (3.1530)	grad_norm 1.7176 (1.9546)	mem 14853MB
[2022-11-07 04:54:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][650/1251]	eta 0:04:42 lr 0.000264	time 0.4624 (0.4693)	loss 3.2883 (3.1471)	grad_norm 1.9363 (nan)	mem 14853MB
[2022-11-07 04:54:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][700/1251]	eta 0:04:18 lr 0.000264	time 0.5487 (0.4691)	loss 2.9803 (3.1441)	grad_norm 1.9814 (nan)	mem 14853MB
[2022-11-07 04:55:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][750/1251]	eta 0:03:54 lr 0.000264	time 0.4647 (0.4688)	loss 2.7491 (3.1511)	grad_norm 2.0929 (nan)	mem 14853MB
[2022-11-07 04:55:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][800/1251]	eta 0:03:31 lr 0.000264	time 0.4630 (0.4689)	loss 2.3506 (3.1472)	grad_norm 1.8041 (nan)	mem 14853MB
[2022-11-07 04:55:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][850/1251]	eta 0:03:08 lr 0.000263	time 0.4731 (0.4689)	loss 2.7606 (3.1444)	grad_norm 1.9568 (nan)	mem 14853MB
[2022-11-07 04:56:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][900/1251]	eta 0:02:44 lr 0.000263	time 0.4724 (0.4687)	loss 2.5041 (3.1411)	grad_norm 1.7442 (nan)	mem 14853MB
[2022-11-07 04:56:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][950/1251]	eta 0:02:21 lr 0.000263	time 0.4767 (0.4685)	loss 2.7177 (3.1400)	grad_norm 1.8105 (nan)	mem 14853MB
[2022-11-07 04:57:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][1000/1251]	eta 0:01:57 lr 0.000263	time 0.4624 (0.4683)	loss 3.0554 (3.1382)	grad_norm 1.8961 (nan)	mem 14853MB
[2022-11-07 04:57:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][1050/1251]	eta 0:01:34 lr 0.000263	time 0.4755 (0.4686)	loss 3.1431 (3.1338)	grad_norm 1.9297 (nan)	mem 14853MB
[2022-11-07 04:57:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][1100/1251]	eta 0:01:10 lr 0.000263	time 0.4667 (0.4684)	loss 3.5206 (3.1369)	grad_norm 1.8690 (nan)	mem 14853MB
[2022-11-07 04:58:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][1150/1251]	eta 0:00:47 lr 0.000262	time 0.4632 (0.4684)	loss 2.6114 (3.1395)	grad_norm 1.7957 (nan)	mem 14853MB
[2022-11-07 04:58:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][1200/1251]	eta 0:00:23 lr 0.000262	time 0.4598 (0.4682)	loss 3.0538 (3.1432)	grad_norm 2.1578 (nan)	mem 14853MB
[2022-11-07 04:59:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [198/300][1250/1251]	eta 0:00:00 lr 0.000262	time 0.4571 (0.4680)	loss 2.9967 (3.1434)	grad_norm 1.7337 (nan)	mem 14853MB
[2022-11-07 04:59:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 198 training takes 0:09:45
[2022-11-07 04:59:03 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_198.pth saving......
[2022-11-07 04:59:04 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_198.pth saved !!!
[2022-11-07 04:59:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.580 (1.580)	Loss 0.8750 (0.8750)	Acc@1 80.859 (80.859)	Acc@5 94.922 (94.922)	Mem 14853MB
[2022-11-07 04:59:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.926 Acc@5 95.236
[2022-11-07 04:59:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.9%
[2022-11-07 04:59:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.562 (1.562)	Loss 0.8143 (0.8143)	Acc@1 81.348 (81.348)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 04:59:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.308 Acc@5 95.832
[2022-11-07 04:59:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.3%
[2022-11-07 04:59:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.31% at 198 epoch
[2022-11-07 04:59:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][0/1251]	eta 0:41:48 lr 0.000262	time 2.0053 (2.0053)	loss 2.3627 (2.3627)	grad_norm 1.9055 (1.9055)	mem 14853MB
[2022-11-07 04:59:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][50/1251]	eta 0:10:05 lr 0.000262	time 0.4589 (0.5041)	loss 2.1860 (3.1297)	grad_norm 1.8385 (1.9707)	mem 14853MB
[2022-11-07 05:00:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][100/1251]	eta 0:09:19 lr 0.000262	time 0.4555 (0.4865)	loss 3.6575 (3.0997)	grad_norm 1.7587 (1.9869)	mem 14853MB
[2022-11-07 05:00:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][150/1251]	eta 0:08:47 lr 0.000261	time 0.4643 (0.4794)	loss 3.1699 (3.1348)	grad_norm 1.9422 (1.9907)	mem 14853MB
[2022-11-07 05:00:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][200/1251]	eta 0:08:20 lr 0.000261	time 0.4584 (0.4764)	loss 2.0865 (3.1206)	grad_norm 1.8418 (1.9950)	mem 14853MB
[2022-11-07 05:01:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][250/1251]	eta 0:07:54 lr 0.000261	time 0.4642 (0.4742)	loss 3.4251 (3.1313)	grad_norm 2.0511 (1.9921)	mem 14853MB
[2022-11-07 05:01:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][300/1251]	eta 0:07:29 lr 0.000261	time 0.4708 (0.4728)	loss 2.5681 (3.1282)	grad_norm 1.8347 (1.9829)	mem 14853MB
[2022-11-07 05:02:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][350/1251]	eta 0:07:05 lr 0.000261	time 0.4605 (0.4719)	loss 2.7418 (3.1195)	grad_norm 2.0067 (1.9731)	mem 14853MB
[2022-11-07 05:02:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][400/1251]	eta 0:06:40 lr 0.000261	time 0.4579 (0.4710)	loss 2.7832 (3.1244)	grad_norm 2.1972 (1.9702)	mem 14853MB
[2022-11-07 05:02:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][450/1251]	eta 0:06:16 lr 0.000260	time 0.4543 (0.4703)	loss 3.3197 (3.1337)	grad_norm 2.0320 (1.9692)	mem 14853MB
[2022-11-07 05:03:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][500/1251]	eta 0:05:52 lr 0.000260	time 0.4570 (0.4699)	loss 2.4649 (3.1198)	grad_norm 1.9330 (1.9717)	mem 14853MB
[2022-11-07 05:03:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][550/1251]	eta 0:05:29 lr 0.000260	time 0.4566 (0.4698)	loss 2.0407 (3.1171)	grad_norm 1.9836 (1.9738)	mem 14853MB
[2022-11-07 05:04:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][600/1251]	eta 0:05:05 lr 0.000260	time 0.4710 (0.4694)	loss 3.4585 (3.1236)	grad_norm 1.8433 (1.9686)	mem 14853MB
[2022-11-07 05:04:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][650/1251]	eta 0:04:42 lr 0.000260	time 0.4607 (0.4692)	loss 3.3569 (3.1255)	grad_norm 1.7777 (1.9703)	mem 14853MB
[2022-11-07 05:04:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][700/1251]	eta 0:04:18 lr 0.000259	time 0.4600 (0.4690)	loss 2.9487 (3.1248)	grad_norm 1.9430 (1.9720)	mem 14853MB
[2022-11-07 05:05:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][750/1251]	eta 0:03:54 lr 0.000259	time 0.4737 (0.4689)	loss 3.3238 (3.1320)	grad_norm 1.7918 (1.9703)	mem 14853MB
[2022-11-07 05:05:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][800/1251]	eta 0:03:31 lr 0.000259	time 0.4584 (0.4689)	loss 3.4788 (3.1323)	grad_norm 2.2098 (1.9736)	mem 14853MB
[2022-11-07 05:06:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][850/1251]	eta 0:03:07 lr 0.000259	time 0.4678 (0.4687)	loss 3.5151 (3.1399)	grad_norm 1.8288 (1.9720)	mem 14853MB
[2022-11-07 05:06:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][900/1251]	eta 0:02:44 lr 0.000259	time 0.5550 (0.4686)	loss 2.5547 (3.1421)	grad_norm 2.2176 (1.9716)	mem 14853MB
[2022-11-07 05:06:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][950/1251]	eta 0:02:20 lr 0.000259	time 0.4748 (0.4684)	loss 2.9912 (3.1418)	grad_norm 2.2897 (1.9714)	mem 14853MB
[2022-11-07 05:07:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][1000/1251]	eta 0:01:57 lr 0.000258	time 0.4735 (0.4682)	loss 3.5057 (3.1396)	grad_norm 1.8517 (1.9728)	mem 14853MB
[2022-11-07 05:07:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][1050/1251]	eta 0:01:34 lr 0.000258	time 0.5392 (0.4683)	loss 3.6618 (3.1343)	grad_norm 1.8144 (1.9737)	mem 14853MB
[2022-11-07 05:07:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][1100/1251]	eta 0:01:10 lr 0.000258	time 0.4613 (0.4682)	loss 3.3235 (3.1332)	grad_norm 1.7453 (1.9723)	mem 14853MB
[2022-11-07 05:08:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][1150/1251]	eta 0:00:47 lr 0.000258	time 0.4573 (0.4681)	loss 2.2143 (3.1266)	grad_norm 1.7475 (1.9734)	mem 14853MB
[2022-11-07 05:08:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][1200/1251]	eta 0:00:23 lr 0.000258	time 0.4571 (0.4679)	loss 3.2384 (3.1236)	grad_norm 1.8954 (1.9715)	mem 14853MB
[2022-11-07 05:09:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [199/300][1250/1251]	eta 0:00:00 lr 0.000258	time 0.4560 (0.4678)	loss 3.2649 (3.1257)	grad_norm 2.2175 (1.9744)	mem 14853MB
[2022-11-07 05:09:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 199 training takes 0:09:45
[2022-11-07 05:09:07 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_199.pth saving......
[2022-11-07 05:09:07 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_199.pth saved !!!
[2022-11-07 05:09:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.605 (1.605)	Loss 0.7522 (0.7522)	Acc@1 83.398 (83.398)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 05:09:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.976 Acc@5 95.198
[2022-11-07 05:09:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.0%
[2022-11-07 05:09:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.636 (1.636)	Loss 0.7135 (0.7135)	Acc@1 81.738 (81.738)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 05:09:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.318 Acc@5 95.822
[2022-11-07 05:09:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.3%
[2022-11-07 05:09:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.32% at 199 epoch
[2022-11-07 05:09:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][0/1251]	eta 0:39:50 lr 0.000258	time 1.9106 (1.9106)	loss 2.9913 (2.9913)	grad_norm 1.9111 (1.9111)	mem 14853MB
[2022-11-07 05:09:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][50/1251]	eta 0:10:00 lr 0.000257	time 0.4577 (0.4998)	loss 2.1290 (3.1009)	grad_norm 2.6351 (inf)	mem 14853MB
[2022-11-07 05:10:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][100/1251]	eta 0:09:17 lr 0.000257	time 0.4649 (0.4846)	loss 3.4380 (3.1082)	grad_norm 2.1991 (inf)	mem 14853MB
[2022-11-07 05:10:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][150/1251]	eta 0:08:47 lr 0.000257	time 0.4694 (0.4791)	loss 2.3001 (3.1158)	grad_norm 2.0906 (inf)	mem 14853MB
[2022-11-07 05:11:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][200/1251]	eta 0:08:19 lr 0.000257	time 0.4543 (0.4753)	loss 3.1004 (3.1150)	grad_norm 1.9545 (inf)	mem 14853MB
[2022-11-07 05:11:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][250/1251]	eta 0:07:53 lr 0.000257	time 0.4652 (0.4735)	loss 3.5651 (3.0986)	grad_norm 1.9550 (inf)	mem 14853MB
[2022-11-07 05:11:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][300/1251]	eta 0:07:29 lr 0.000256	time 0.4623 (0.4723)	loss 2.9470 (3.1139)	grad_norm 1.8804 (inf)	mem 14853MB
[2022-11-07 05:12:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][350/1251]	eta 0:07:04 lr 0.000256	time 0.4777 (0.4712)	loss 3.1234 (3.1058)	grad_norm 1.7927 (inf)	mem 14853MB
[2022-11-07 05:12:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][400/1251]	eta 0:06:40 lr 0.000256	time 0.4606 (0.4704)	loss 3.0936 (3.1168)	grad_norm 2.1194 (inf)	mem 14853MB
[2022-11-07 05:12:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][450/1251]	eta 0:06:16 lr 0.000256	time 0.4763 (0.4701)	loss 3.4771 (3.1170)	grad_norm 1.9280 (inf)	mem 14853MB
[2022-11-07 05:13:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][500/1251]	eta 0:05:52 lr 0.000256	time 0.4631 (0.4696)	loss 3.6716 (3.1147)	grad_norm 1.8782 (inf)	mem 14853MB
[2022-11-07 05:13:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][550/1251]	eta 0:05:29 lr 0.000256	time 0.4620 (0.4697)	loss 2.2104 (3.1136)	grad_norm 1.7336 (inf)	mem 14853MB
[2022-11-07 05:14:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][600/1251]	eta 0:05:05 lr 0.000255	time 0.4550 (0.4695)	loss 3.2665 (3.1199)	grad_norm 1.9238 (inf)	mem 14853MB
[2022-11-07 05:14:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][650/1251]	eta 0:04:41 lr 0.000255	time 0.4644 (0.4692)	loss 2.4138 (3.1206)	grad_norm 1.8681 (inf)	mem 14853MB
[2022-11-07 05:14:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][700/1251]	eta 0:04:18 lr 0.000255	time 0.4624 (0.4688)	loss 2.8140 (3.1228)	grad_norm 1.9996 (inf)	mem 14853MB
[2022-11-07 05:15:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][750/1251]	eta 0:03:54 lr 0.000255	time 0.4748 (0.4688)	loss 3.8101 (3.1280)	grad_norm 2.1820 (inf)	mem 14853MB
[2022-11-07 05:15:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][800/1251]	eta 0:03:31 lr 0.000255	time 0.4629 (0.4688)	loss 3.3000 (3.1292)	grad_norm 1.9302 (inf)	mem 14853MB
[2022-11-07 05:16:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][850/1251]	eta 0:03:07 lr 0.000254	time 0.4641 (0.4687)	loss 2.4264 (3.1327)	grad_norm 1.8693 (inf)	mem 14853MB
[2022-11-07 05:16:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][900/1251]	eta 0:02:44 lr 0.000254	time 0.4650 (0.4685)	loss 3.3229 (3.1306)	grad_norm 1.9230 (inf)	mem 14853MB
[2022-11-07 05:16:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][950/1251]	eta 0:02:20 lr 0.000254	time 0.4669 (0.4684)	loss 2.9468 (3.1320)	grad_norm 1.9403 (inf)	mem 14853MB
[2022-11-07 05:17:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][1000/1251]	eta 0:01:57 lr 0.000254	time 0.4722 (0.4683)	loss 2.4327 (3.1331)	grad_norm 1.9875 (inf)	mem 14853MB
[2022-11-07 05:17:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][1050/1251]	eta 0:01:34 lr 0.000254	time 0.4643 (0.4685)	loss 3.3503 (3.1306)	grad_norm 1.9601 (inf)	mem 14853MB
[2022-11-07 05:18:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][1100/1251]	eta 0:01:10 lr 0.000254	time 0.4665 (0.4684)	loss 3.3115 (3.1278)	grad_norm 1.8613 (inf)	mem 14853MB
[2022-11-07 05:18:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][1150/1251]	eta 0:00:47 lr 0.000253	time 0.4548 (0.4683)	loss 2.6116 (3.1279)	grad_norm 1.7530 (inf)	mem 14853MB
[2022-11-07 05:18:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][1200/1251]	eta 0:00:23 lr 0.000253	time 0.4702 (0.4681)	loss 3.4583 (3.1339)	grad_norm 1.8336 (inf)	mem 14853MB
[2022-11-07 05:19:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [200/300][1250/1251]	eta 0:00:00 lr 0.000253	time 0.4568 (0.4679)	loss 3.8213 (3.1397)	grad_norm 2.1429 (inf)	mem 14853MB
[2022-11-07 05:19:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 200 training takes 0:09:45
[2022-11-07 05:19:10 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_200.pth saving......
[2022-11-07 05:19:11 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_200.pth saved !!!
[2022-11-07 05:19:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.592 (1.592)	Loss 0.8666 (0.8666)	Acc@1 79.883 (79.883)	Acc@5 94.922 (94.922)	Mem 14853MB
[2022-11-07 05:19:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.098 Acc@5 95.306
[2022-11-07 05:19:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.1%
[2022-11-07 05:19:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.757 (1.757)	Loss 0.7750 (0.7750)	Acc@1 81.250 (81.250)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-07 05:19:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.326 Acc@5 95.834
[2022-11-07 05:19:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.3%
[2022-11-07 05:19:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.33% at 200 epoch
[2022-11-07 05:19:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][0/1251]	eta 0:41:03 lr 0.000253	time 1.9693 (1.9693)	loss 2.4749 (2.4749)	grad_norm 2.0064 (2.0064)	mem 14853MB
[2022-11-07 05:19:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][50/1251]	eta 0:10:00 lr 0.000253	time 0.4722 (0.5003)	loss 3.3119 (3.1021)	grad_norm 2.4400 (2.0360)	mem 14853MB
[2022-11-07 05:20:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][100/1251]	eta 0:09:18 lr 0.000253	time 0.4735 (0.4856)	loss 3.0974 (3.1153)	grad_norm 2.6831 (2.0360)	mem 14853MB
[2022-11-07 05:20:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][150/1251]	eta 0:08:47 lr 0.000252	time 0.4721 (0.4790)	loss 3.7867 (3.1752)	grad_norm 2.2778 (2.0244)	mem 14853MB
[2022-11-07 05:21:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][200/1251]	eta 0:08:19 lr 0.000252	time 0.4659 (0.4755)	loss 3.5871 (3.1834)	grad_norm 1.7765 (2.0161)	mem 14853MB
[2022-11-07 05:21:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][250/1251]	eta 0:07:53 lr 0.000252	time 0.4763 (0.4733)	loss 3.3389 (3.1802)	grad_norm 2.1391 (2.0071)	mem 14853MB
[2022-11-07 05:21:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][300/1251]	eta 0:07:29 lr 0.000252	time 0.4645 (0.4722)	loss 3.4747 (3.1452)	grad_norm 1.8884 (1.9976)	mem 14853MB
[2022-11-07 05:22:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][350/1251]	eta 0:07:04 lr 0.000252	time 0.4595 (0.4712)	loss 3.4069 (3.1454)	grad_norm 1.9615 (2.0042)	mem 14853MB
[2022-11-07 05:22:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][400/1251]	eta 0:06:40 lr 0.000252	time 0.4731 (0.4704)	loss 3.4902 (3.1499)	grad_norm 2.2195 (2.0030)	mem 14853MB
[2022-11-07 05:23:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][450/1251]	eta 0:06:16 lr 0.000251	time 0.4663 (0.4699)	loss 3.6584 (3.1607)	grad_norm 2.3656 (2.0044)	mem 14853MB
[2022-11-07 05:23:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][500/1251]	eta 0:05:52 lr 0.000251	time 0.4657 (0.4694)	loss 3.4736 (3.1623)	grad_norm 1.8393 (2.0011)	mem 14853MB
[2022-11-07 05:23:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][550/1251]	eta 0:05:29 lr 0.000251	time 0.4688 (0.4695)	loss 2.8453 (3.1602)	grad_norm 2.1906 (1.9984)	mem 14853MB
[2022-11-07 05:24:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][600/1251]	eta 0:05:05 lr 0.000251	time 0.4675 (0.4693)	loss 3.5932 (3.1529)	grad_norm 2.2929 (1.9952)	mem 14853MB
[2022-11-07 05:24:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][650/1251]	eta 0:04:41 lr 0.000251	time 0.4766 (0.4691)	loss 3.8572 (3.1501)	grad_norm 2.1250 (1.9946)	mem 14853MB
[2022-11-07 05:24:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][700/1251]	eta 0:04:18 lr 0.000251	time 0.4606 (0.4688)	loss 3.3930 (3.1491)	grad_norm 2.0402 (1.9947)	mem 14853MB
[2022-11-07 05:25:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][750/1251]	eta 0:03:54 lr 0.000250	time 0.4707 (0.4686)	loss 3.2603 (3.1485)	grad_norm 2.3213 (1.9931)	mem 14853MB
[2022-11-07 05:25:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][800/1251]	eta 0:03:31 lr 0.000250	time 0.4597 (0.4686)	loss 3.6844 (3.1543)	grad_norm 2.1743 (1.9930)	mem 14853MB
[2022-11-07 05:26:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][850/1251]	eta 0:03:07 lr 0.000250	time 0.4640 (0.4684)	loss 2.9749 (3.1552)	grad_norm 1.9762 (1.9962)	mem 14853MB
[2022-11-07 05:26:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][900/1251]	eta 0:02:44 lr 0.000250	time 0.4749 (0.4683)	loss 3.2086 (3.1546)	grad_norm 2.0745 (1.9938)	mem 14853MB
[2022-11-07 05:26:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][950/1251]	eta 0:02:20 lr 0.000250	time 0.4640 (0.4682)	loss 3.1226 (3.1576)	grad_norm 2.0210 (1.9926)	mem 14853MB
[2022-11-07 05:27:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][1000/1251]	eta 0:01:57 lr 0.000249	time 0.4643 (0.4681)	loss 2.9410 (3.1534)	grad_norm 2.2111 (1.9942)	mem 14853MB
[2022-11-07 05:27:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][1050/1251]	eta 0:01:34 lr 0.000249	time 0.4645 (0.4681)	loss 3.2269 (3.1486)	grad_norm 1.9691 (1.9947)	mem 14853MB
[2022-11-07 05:28:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][1100/1251]	eta 0:01:10 lr 0.000249	time 0.4614 (0.4680)	loss 3.6893 (3.1476)	grad_norm 2.2032 (1.9959)	mem 14853MB
[2022-11-07 05:28:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][1150/1251]	eta 0:00:47 lr 0.000249	time 0.4634 (0.4679)	loss 2.4465 (3.1476)	grad_norm 1.7075 (1.9967)	mem 14853MB
[2022-11-07 05:28:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][1200/1251]	eta 0:00:23 lr 0.000249	time 0.4613 (0.4678)	loss 3.2574 (3.1466)	grad_norm 1.9086 (1.9964)	mem 14853MB
[2022-11-07 05:29:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [201/300][1250/1251]	eta 0:00:00 lr 0.000249	time 0.4590 (0.4676)	loss 3.3900 (3.1477)	grad_norm 1.8814 (1.9975)	mem 14853MB
[2022-11-07 05:29:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 201 training takes 0:09:45
[2022-11-07 05:29:14 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_201.pth saving......
[2022-11-07 05:29:14 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_201.pth saved !!!
[2022-11-07 05:29:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.652 (1.652)	Loss 0.8172 (0.8172)	Acc@1 79.395 (79.395)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 05:29:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.004 Acc@5 95.268
[2022-11-07 05:29:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.0%
[2022-11-07 05:29:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.543 (1.543)	Loss 0.8323 (0.8323)	Acc@1 80.371 (80.371)	Acc@5 95.312 (95.312)	Mem 14853MB
[2022-11-07 05:29:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.396 Acc@5 95.844
[2022-11-07 05:29:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.4%
[2022-11-07 05:29:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.40% at 201 epoch
[2022-11-07 05:29:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][0/1251]	eta 0:42:30 lr 0.000249	time 2.0386 (2.0386)	loss 3.7083 (3.7083)	grad_norm 2.1656 (2.1656)	mem 14853MB
[2022-11-07 05:29:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][50/1251]	eta 0:10:01 lr 0.000248	time 0.4634 (0.5008)	loss 3.3013 (3.1836)	grad_norm 1.7737 (2.0381)	mem 14853MB
[2022-11-07 05:30:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][100/1251]	eta 0:09:20 lr 0.000248	time 0.4763 (0.4867)	loss 3.6557 (3.2346)	grad_norm 2.0199 (2.0096)	mem 14853MB
[2022-11-07 05:30:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][150/1251]	eta 0:08:48 lr 0.000248	time 0.4579 (0.4796)	loss 3.2678 (3.1544)	grad_norm 2.1282 (2.0082)	mem 14853MB
[2022-11-07 05:31:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][200/1251]	eta 0:08:20 lr 0.000248	time 0.4640 (0.4762)	loss 3.3370 (3.1632)	grad_norm 1.9731 (2.0079)	mem 14853MB
[2022-11-07 05:31:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][250/1251]	eta 0:07:54 lr 0.000248	time 0.4655 (0.4742)	loss 2.9575 (3.1546)	grad_norm 1.8979 (1.9962)	mem 14853MB
[2022-11-07 05:31:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][300/1251]	eta 0:07:29 lr 0.000248	time 0.4632 (0.4730)	loss 3.3872 (3.1502)	grad_norm 1.8778 (1.9976)	mem 14853MB
[2022-11-07 05:32:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][350/1251]	eta 0:07:05 lr 0.000247	time 0.4640 (0.4721)	loss 2.7784 (3.1317)	grad_norm 2.2901 (1.9958)	mem 14853MB
[2022-11-07 05:32:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][400/1251]	eta 0:06:41 lr 0.000247	time 0.4505 (0.4715)	loss 3.6343 (3.1360)	grad_norm 1.7644 (1.9894)	mem 14853MB
[2022-11-07 05:33:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][450/1251]	eta 0:06:17 lr 0.000247	time 0.4733 (0.4708)	loss 3.1678 (3.1479)	grad_norm 1.8601 (1.9923)	mem 14853MB
[2022-11-07 05:33:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][500/1251]	eta 0:05:53 lr 0.000247	time 0.4753 (0.4702)	loss 3.3968 (3.1491)	grad_norm 2.1254 (1.9898)	mem 14853MB
[2022-11-07 05:33:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][550/1251]	eta 0:05:29 lr 0.000247	time 0.4681 (0.4700)	loss 3.1904 (3.1499)	grad_norm 2.3427 (1.9919)	mem 14853MB
[2022-11-07 05:34:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][600/1251]	eta 0:05:05 lr 0.000246	time 0.4665 (0.4697)	loss 3.3623 (3.1534)	grad_norm 1.8745 (1.9909)	mem 14853MB
[2022-11-07 05:34:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][650/1251]	eta 0:04:42 lr 0.000246	time 0.4745 (0.4696)	loss 2.3556 (3.1492)	grad_norm 2.4503 (1.9930)	mem 14853MB
[2022-11-07 05:35:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][700/1251]	eta 0:04:18 lr 0.000246	time 0.4761 (0.4692)	loss 3.4570 (3.1503)	grad_norm 1.8997 (1.9921)	mem 14853MB
[2022-11-07 05:35:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][750/1251]	eta 0:03:54 lr 0.000246	time 0.4700 (0.4689)	loss 3.5232 (3.1436)	grad_norm 1.9889 (1.9923)	mem 14853MB
[2022-11-07 05:35:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][800/1251]	eta 0:03:31 lr 0.000246	time 0.4527 (0.4688)	loss 3.5757 (3.1400)	grad_norm 2.0354 (1.9956)	mem 14853MB
[2022-11-07 05:36:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][850/1251]	eta 0:03:07 lr 0.000246	time 0.4583 (0.4688)	loss 3.2336 (3.1367)	grad_norm 2.3676 (1.9982)	mem 14853MB
[2022-11-07 05:36:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][900/1251]	eta 0:02:44 lr 0.000245	time 0.4702 (0.4687)	loss 3.0122 (3.1324)	grad_norm 1.9506 (2.0000)	mem 14853MB
[2022-11-07 05:36:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][950/1251]	eta 0:02:21 lr 0.000245	time 0.5007 (0.4685)	loss 3.3390 (3.1288)	grad_norm 2.3970 (2.0022)	mem 14853MB
[2022-11-07 05:37:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][1000/1251]	eta 0:01:57 lr 0.000245	time 0.4726 (0.4684)	loss 3.5501 (3.1292)	grad_norm 2.6276 (2.0051)	mem 14853MB
[2022-11-07 05:37:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][1050/1251]	eta 0:01:34 lr 0.000245	time 0.4664 (0.4684)	loss 3.5673 (3.1301)	grad_norm 1.9294 (2.0077)	mem 14853MB
[2022-11-07 05:38:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][1100/1251]	eta 0:01:10 lr 0.000245	time 0.4676 (0.4684)	loss 3.2739 (3.1330)	grad_norm 2.0033 (2.0062)	mem 14853MB
[2022-11-07 05:38:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][1150/1251]	eta 0:00:47 lr 0.000245	time 0.4736 (0.4682)	loss 2.4274 (3.1345)	grad_norm 1.9335 (2.0067)	mem 14853MB
[2022-11-07 05:38:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][1200/1251]	eta 0:00:23 lr 0.000244	time 0.4649 (0.4681)	loss 3.0419 (3.1371)	grad_norm 1.9987 (2.0052)	mem 14853MB
[2022-11-07 05:39:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [202/300][1250/1251]	eta 0:00:00 lr 0.000244	time 0.4572 (0.4680)	loss 3.5584 (3.1362)	grad_norm 1.8353 (2.0049)	mem 14853MB
[2022-11-07 05:39:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 202 training takes 0:09:45
[2022-11-07 05:39:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_202.pth saving......
[2022-11-07 05:39:18 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_202.pth saved !!!
[2022-11-07 05:39:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.543 (1.543)	Loss 0.7701 (0.7701)	Acc@1 82.910 (82.910)	Acc@5 96.680 (96.680)	Mem 14853MB
[2022-11-07 05:39:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.102 Acc@5 95.336
[2022-11-07 05:39:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.1%
[2022-11-07 05:39:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.661 (1.661)	Loss 0.7823 (0.7823)	Acc@1 82.129 (82.129)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-07 05:39:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.404 Acc@5 95.858
[2022-11-07 05:39:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.4%
[2022-11-07 05:39:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.40% at 202 epoch
[2022-11-07 05:39:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][0/1251]	eta 0:44:02 lr 0.000244	time 2.1122 (2.1122)	loss 2.5812 (2.5812)	grad_norm 1.9733 (1.9733)	mem 14853MB
[2022-11-07 05:40:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][50/1251]	eta 0:10:02 lr 0.000244	time 0.4532 (0.5013)	loss 3.4375 (3.1742)	grad_norm 1.9083 (1.9827)	mem 14853MB
[2022-11-07 05:40:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][100/1251]	eta 0:09:17 lr 0.000244	time 0.4626 (0.4847)	loss 3.2413 (3.1695)	grad_norm 1.9599 (1.9878)	mem 14853MB
[2022-11-07 05:40:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][150/1251]	eta 0:08:47 lr 0.000244	time 0.4772 (0.4791)	loss 3.5171 (3.1811)	grad_norm 1.9605 (2.0002)	mem 14853MB
[2022-11-07 05:41:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][200/1251]	eta 0:08:20 lr 0.000243	time 0.4747 (0.4764)	loss 3.6662 (3.1853)	grad_norm 2.0946 (1.9990)	mem 14853MB
[2022-11-07 05:41:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][250/1251]	eta 0:07:55 lr 0.000243	time 0.4697 (0.4747)	loss 2.2342 (3.1737)	grad_norm 2.0859 (1.9910)	mem 14853MB
[2022-11-07 05:41:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][300/1251]	eta 0:07:30 lr 0.000243	time 0.4616 (0.4734)	loss 3.4398 (3.1480)	grad_norm 1.9482 (1.9871)	mem 14853MB
[2022-11-07 05:42:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][350/1251]	eta 0:07:05 lr 0.000243	time 0.4695 (0.4722)	loss 3.2157 (3.1351)	grad_norm 2.0765 (1.9833)	mem 14853MB
[2022-11-07 05:42:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][400/1251]	eta 0:06:41 lr 0.000243	time 0.4668 (0.4713)	loss 2.9244 (3.1305)	grad_norm 1.7621 (nan)	mem 14853MB
[2022-11-07 05:43:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][450/1251]	eta 0:06:16 lr 0.000243	time 0.4600 (0.4706)	loss 3.4009 (3.1343)	grad_norm 1.8576 (nan)	mem 14853MB
[2022-11-07 05:43:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][500/1251]	eta 0:05:53 lr 0.000242	time 0.4655 (0.4701)	loss 2.0356 (3.1226)	grad_norm 2.1042 (nan)	mem 14853MB
[2022-11-07 05:43:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][550/1251]	eta 0:05:29 lr 0.000242	time 0.4639 (0.4702)	loss 3.4134 (3.1272)	grad_norm 2.2829 (nan)	mem 14853MB
[2022-11-07 05:44:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][600/1251]	eta 0:05:05 lr 0.000242	time 0.4670 (0.4697)	loss 3.9018 (3.1380)	grad_norm 1.9898 (nan)	mem 14853MB
[2022-11-07 05:44:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][650/1251]	eta 0:04:42 lr 0.000242	time 0.4632 (0.4694)	loss 3.6540 (3.1367)	grad_norm 2.0381 (nan)	mem 14853MB
[2022-11-07 05:45:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][700/1251]	eta 0:04:18 lr 0.000242	time 0.4663 (0.4693)	loss 2.7496 (3.1374)	grad_norm 2.1508 (nan)	mem 14853MB
[2022-11-07 05:45:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][750/1251]	eta 0:03:55 lr 0.000242	time 0.4648 (0.4691)	loss 2.7781 (3.1412)	grad_norm 2.4265 (nan)	mem 14853MB
[2022-11-07 05:45:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][800/1251]	eta 0:03:31 lr 0.000241	time 0.4752 (0.4692)	loss 2.7809 (3.1467)	grad_norm 2.0441 (nan)	mem 14853MB
[2022-11-07 05:46:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][850/1251]	eta 0:03:08 lr 0.000241	time 0.4579 (0.4689)	loss 3.0880 (3.1395)	grad_norm 1.8688 (nan)	mem 14853MB
[2022-11-07 05:46:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][900/1251]	eta 0:02:44 lr 0.000241	time 0.4621 (0.4687)	loss 2.8642 (3.1458)	grad_norm 1.8140 (nan)	mem 14853MB
[2022-11-07 05:47:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][950/1251]	eta 0:02:21 lr 0.000241	time 0.4627 (0.4687)	loss 2.7097 (3.1498)	grad_norm 1.9939 (nan)	mem 14853MB
[2022-11-07 05:47:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][1000/1251]	eta 0:01:57 lr 0.000241	time 0.4646 (0.4685)	loss 2.2672 (3.1504)	grad_norm 1.8542 (nan)	mem 14853MB
[2022-11-07 05:47:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][1050/1251]	eta 0:01:34 lr 0.000240	time 0.4746 (0.4686)	loss 3.3699 (3.1452)	grad_norm 1.9728 (nan)	mem 14853MB
[2022-11-07 05:48:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][1100/1251]	eta 0:01:10 lr 0.000240	time 0.4690 (0.4685)	loss 2.5677 (3.1482)	grad_norm 1.8281 (nan)	mem 14853MB
[2022-11-07 05:48:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][1150/1251]	eta 0:00:47 lr 0.000240	time 0.4672 (0.4684)	loss 2.5828 (3.1478)	grad_norm 2.0443 (nan)	mem 14853MB
[2022-11-07 05:48:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][1200/1251]	eta 0:00:23 lr 0.000240	time 0.4744 (0.4683)	loss 3.3981 (3.1510)	grad_norm 1.9184 (nan)	mem 14853MB
[2022-11-07 05:49:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [203/300][1250/1251]	eta 0:00:00 lr 0.000240	time 0.4593 (0.4682)	loss 2.6684 (3.1498)	grad_norm 1.7863 (nan)	mem 14853MB
[2022-11-07 05:49:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 203 training takes 0:09:45
[2022-11-07 05:49:21 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_203.pth saving......
[2022-11-07 05:49:22 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_203.pth saved !!!
[2022-11-07 05:49:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.399 (1.399)	Loss 0.7770 (0.7770)	Acc@1 81.152 (81.152)	Acc@5 96.582 (96.582)	Mem 14853MB
[2022-11-07 05:49:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.018 Acc@5 95.366
[2022-11-07 05:49:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.0%
[2022-11-07 05:49:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.725 (1.725)	Loss 0.7559 (0.7559)	Acc@1 82.227 (82.227)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 05:49:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.436 Acc@5 95.886
[2022-11-07 05:49:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.4%
[2022-11-07 05:49:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.44% at 203 epoch
[2022-11-07 05:49:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][0/1251]	eta 0:41:14 lr 0.000240	time 1.9783 (1.9783)	loss 2.4619 (2.4619)	grad_norm 2.1218 (2.1218)	mem 14853MB
[2022-11-07 05:50:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][50/1251]	eta 0:09:59 lr 0.000240	time 0.4642 (0.4993)	loss 3.4271 (3.1297)	grad_norm 1.9960 (2.0579)	mem 14853MB
[2022-11-07 05:50:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][100/1251]	eta 0:09:17 lr 0.000239	time 0.4672 (0.4846)	loss 3.2731 (3.1757)	grad_norm 1.9412 (2.0214)	mem 14853MB
[2022-11-07 05:50:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][150/1251]	eta 0:08:46 lr 0.000239	time 0.4552 (0.4786)	loss 3.1793 (3.1450)	grad_norm 2.1899 (2.0311)	mem 14853MB
[2022-11-07 05:51:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][200/1251]	eta 0:08:19 lr 0.000239	time 0.4663 (0.4753)	loss 3.6330 (3.1374)	grad_norm 2.0728 (2.0435)	mem 14853MB
[2022-11-07 05:51:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][250/1251]	eta 0:07:53 lr 0.000239	time 0.4668 (0.4733)	loss 3.3887 (3.1597)	grad_norm 1.8704 (2.0412)	mem 14853MB
[2022-11-07 05:52:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][300/1251]	eta 0:07:28 lr 0.000239	time 0.4558 (0.4720)	loss 1.9315 (3.1435)	grad_norm 1.8052 (2.0355)	mem 14853MB
[2022-11-07 05:52:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][350/1251]	eta 0:07:04 lr 0.000239	time 0.4861 (0.4714)	loss 2.6866 (3.1505)	grad_norm 1.8355 (2.0334)	mem 14853MB
[2022-11-07 05:52:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][400/1251]	eta 0:06:40 lr 0.000238	time 0.4618 (0.4707)	loss 2.8782 (3.1579)	grad_norm 2.2970 (2.0380)	mem 14853MB
[2022-11-07 05:53:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][450/1251]	eta 0:06:16 lr 0.000238	time 0.4611 (0.4701)	loss 3.6218 (3.1442)	grad_norm 1.8397 (2.0384)	mem 14853MB
[2022-11-07 05:53:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][500/1251]	eta 0:05:52 lr 0.000238	time 0.4630 (0.4696)	loss 3.1727 (3.1471)	grad_norm 1.8989 (2.0391)	mem 14853MB
[2022-11-07 05:53:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][550/1251]	eta 0:05:29 lr 0.000238	time 0.4719 (0.4694)	loss 3.6923 (3.1491)	grad_norm 2.0207 (2.0298)	mem 14853MB
[2022-11-07 05:54:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][600/1251]	eta 0:05:05 lr 0.000238	time 0.4622 (0.4692)	loss 2.4448 (3.1562)	grad_norm 2.3064 (2.0295)	mem 14853MB
[2022-11-07 05:54:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][650/1251]	eta 0:04:41 lr 0.000237	time 0.4616 (0.4692)	loss 3.5653 (3.1490)	grad_norm 1.9137 (2.0279)	mem 14853MB
[2022-11-07 05:55:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][700/1251]	eta 0:04:18 lr 0.000237	time 0.4636 (0.4689)	loss 3.3019 (3.1558)	grad_norm 2.0595 (2.0278)	mem 14853MB
[2022-11-07 05:55:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][750/1251]	eta 0:03:54 lr 0.000237	time 0.4603 (0.4686)	loss 2.8100 (3.1571)	grad_norm 2.0363 (2.0299)	mem 14853MB
[2022-11-07 05:55:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][800/1251]	eta 0:03:31 lr 0.000237	time 0.4551 (0.4685)	loss 3.2271 (3.1551)	grad_norm 2.0344 (2.0302)	mem 14853MB
[2022-11-07 05:56:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][850/1251]	eta 0:03:07 lr 0.000237	time 0.4690 (0.4683)	loss 2.2306 (3.1561)	grad_norm 1.8675 (2.0311)	mem 14853MB
[2022-11-07 05:56:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][900/1251]	eta 0:02:44 lr 0.000237	time 0.4710 (0.4682)	loss 2.3714 (3.1480)	grad_norm 1.7108 (2.0297)	mem 14853MB
[2022-11-07 05:57:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][950/1251]	eta 0:02:20 lr 0.000236	time 0.4624 (0.4681)	loss 3.0948 (3.1487)	grad_norm 2.0677 (2.0351)	mem 14853MB
[2022-11-07 05:57:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][1000/1251]	eta 0:01:57 lr 0.000236	time 0.4546 (0.4680)	loss 2.6036 (3.1440)	grad_norm 2.3472 (2.0332)	mem 14853MB
[2022-11-07 05:57:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][1050/1251]	eta 0:01:34 lr 0.000236	time 0.4615 (0.4679)	loss 3.1423 (3.1499)	grad_norm 1.7948 (2.0328)	mem 14853MB
[2022-11-07 05:58:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][1100/1251]	eta 0:01:10 lr 0.000236	time 0.4588 (0.4678)	loss 2.9013 (3.1513)	grad_norm 2.1202 (2.0320)	mem 14853MB
[2022-11-07 05:58:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][1150/1251]	eta 0:00:47 lr 0.000236	time 0.4694 (0.4679)	loss 3.8804 (3.1498)	grad_norm 2.5223 (2.0337)	mem 14853MB
[2022-11-07 05:59:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][1200/1251]	eta 0:00:23 lr 0.000236	time 0.4749 (0.4677)	loss 2.7560 (3.1494)	grad_norm 1.8007 (2.0352)	mem 14853MB
[2022-11-07 05:59:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [204/300][1250/1251]	eta 0:00:00 lr 0.000235	time 0.4569 (0.4676)	loss 2.8514 (3.1501)	grad_norm 1.9942 (2.0332)	mem 14853MB
[2022-11-07 05:59:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 204 training takes 0:09:45
[2022-11-07 05:59:24 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_204.pth saving......
[2022-11-07 05:59:25 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_204.pth saved !!!
[2022-11-07 05:59:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.475 (1.475)	Loss 0.8462 (0.8462)	Acc@1 82.129 (82.129)	Acc@5 95.117 (95.117)	Mem 14853MB
[2022-11-07 05:59:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.362 Acc@5 95.386
[2022-11-07 05:59:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.4%
[2022-11-07 05:59:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.692 (1.692)	Loss 0.7325 (0.7325)	Acc@1 80.273 (80.273)	Acc@5 96.582 (96.582)	Mem 14853MB
[2022-11-07 05:59:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.466 Acc@5 95.900
[2022-11-07 05:59:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.5%
[2022-11-07 05:59:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.47% at 204 epoch
[2022-11-07 05:59:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][0/1251]	eta 0:40:18 lr 0.000235	time 1.9335 (1.9335)	loss 3.3533 (3.3533)	grad_norm 1.9586 (1.9586)	mem 14853MB
[2022-11-07 06:00:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][50/1251]	eta 0:09:57 lr 0.000235	time 0.4764 (0.4979)	loss 3.3186 (3.1354)	grad_norm 1.9747 (2.0043)	mem 14853MB
[2022-11-07 06:00:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][100/1251]	eta 0:09:16 lr 0.000235	time 0.4643 (0.4836)	loss 3.1530 (3.2121)	grad_norm 1.9659 (2.0444)	mem 14853MB
[2022-11-07 06:00:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][150/1251]	eta 0:08:47 lr 0.000235	time 0.4624 (0.4795)	loss 3.3324 (3.1562)	grad_norm 1.7715 (2.0726)	mem 14853MB
[2022-11-07 06:01:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][200/1251]	eta 0:08:20 lr 0.000235	time 0.4612 (0.4762)	loss 3.8051 (3.1580)	grad_norm 2.2823 (2.0615)	mem 14853MB
[2022-11-07 06:01:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][250/1251]	eta 0:07:54 lr 0.000235	time 0.4613 (0.4742)	loss 3.3879 (3.1163)	grad_norm 2.1947 (2.0504)	mem 14853MB
[2022-11-07 06:02:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][300/1251]	eta 0:07:29 lr 0.000234	time 0.4580 (0.4728)	loss 3.3790 (3.1202)	grad_norm 2.2337 (2.0423)	mem 14853MB
[2022-11-07 06:02:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][350/1251]	eta 0:07:04 lr 0.000234	time 0.4631 (0.4717)	loss 2.9665 (3.1137)	grad_norm 1.9100 (2.0368)	mem 14853MB
[2022-11-07 06:02:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][400/1251]	eta 0:06:40 lr 0.000234	time 0.4657 (0.4711)	loss 3.2844 (3.1062)	grad_norm 2.2820 (2.0423)	mem 14853MB
[2022-11-07 06:03:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][450/1251]	eta 0:06:16 lr 0.000234	time 0.4654 (0.4705)	loss 3.4011 (3.1218)	grad_norm 2.1704 (2.0416)	mem 14853MB
[2022-11-07 06:03:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][500/1251]	eta 0:05:52 lr 0.000234	time 0.4570 (0.4700)	loss 3.4103 (3.1219)	grad_norm 1.9132 (2.0369)	mem 14853MB
[2022-11-07 06:04:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][550/1251]	eta 0:05:29 lr 0.000233	time 0.4647 (0.4697)	loss 3.1552 (3.1259)	grad_norm 1.9804 (2.0324)	mem 14853MB
[2022-11-07 06:04:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][600/1251]	eta 0:05:05 lr 0.000233	time 0.4629 (0.4696)	loss 3.5074 (3.1288)	grad_norm 1.8476 (2.0317)	mem 14853MB
[2022-11-07 06:04:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][650/1251]	eta 0:04:42 lr 0.000233	time 0.4750 (0.4693)	loss 3.5896 (3.1207)	grad_norm 2.1360 (2.0336)	mem 14853MB
[2022-11-07 06:05:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][700/1251]	eta 0:04:18 lr 0.000233	time 0.4612 (0.4693)	loss 3.0926 (3.1133)	grad_norm 1.9165 (2.0284)	mem 14853MB
[2022-11-07 06:05:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][750/1251]	eta 0:03:55 lr 0.000233	time 0.4693 (0.4691)	loss 3.3379 (3.1130)	grad_norm 2.0720 (2.0297)	mem 14853MB
[2022-11-07 06:05:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][800/1251]	eta 0:03:31 lr 0.000233	time 0.4620 (0.4690)	loss 3.1902 (3.1129)	grad_norm 2.0715 (2.0313)	mem 14853MB
[2022-11-07 06:06:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][850/1251]	eta 0:03:08 lr 0.000232	time 0.4591 (0.4690)	loss 2.3094 (3.1148)	grad_norm 2.0221 (2.0334)	mem 14853MB
[2022-11-07 06:06:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][900/1251]	eta 0:02:44 lr 0.000232	time 0.5355 (0.4688)	loss 3.2961 (3.1108)	grad_norm 1.8511 (2.0316)	mem 14853MB
[2022-11-07 06:07:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][950/1251]	eta 0:02:21 lr 0.000232	time 0.4633 (0.4686)	loss 3.2183 (3.1112)	grad_norm 1.9151 (2.0308)	mem 14853MB
[2022-11-07 06:07:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][1000/1251]	eta 0:01:57 lr 0.000232	time 0.5357 (0.4686)	loss 2.3849 (3.1122)	grad_norm 2.2378 (2.0313)	mem 14853MB
[2022-11-07 06:07:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][1050/1251]	eta 0:01:34 lr 0.000232	time 0.4572 (0.4686)	loss 3.5791 (3.1122)	grad_norm 1.9768 (inf)	mem 14853MB
[2022-11-07 06:08:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][1100/1251]	eta 0:01:10 lr 0.000232	time 0.4651 (0.4686)	loss 2.2473 (3.1145)	grad_norm 2.2825 (inf)	mem 14853MB
[2022-11-07 06:08:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][1150/1251]	eta 0:00:47 lr 0.000231	time 0.4739 (0.4685)	loss 3.4619 (3.1123)	grad_norm 2.1267 (inf)	mem 14853MB
[2022-11-07 06:09:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][1200/1251]	eta 0:00:23 lr 0.000231	time 0.4703 (0.4684)	loss 3.5679 (3.1126)	grad_norm 1.9331 (inf)	mem 14853MB
[2022-11-07 06:09:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [205/300][1250/1251]	eta 0:00:00 lr 0.000231	time 0.4566 (0.4684)	loss 1.9405 (3.1138)	grad_norm 2.1594 (inf)	mem 14853MB
[2022-11-07 06:09:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 205 training takes 0:09:46
[2022-11-07 06:09:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_205.pth saving......
[2022-11-07 06:09:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_205.pth saved !!!
[2022-11-07 06:09:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.531 (1.531)	Loss 0.8333 (0.8333)	Acc@1 80.371 (80.371)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 06:09:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.176 Acc@5 95.432
[2022-11-07 06:09:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.2%
[2022-11-07 06:09:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.631 (1.631)	Loss 0.7466 (0.7466)	Acc@1 82.129 (82.129)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 06:09:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.454 Acc@5 95.908
[2022-11-07 06:09:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.5%
[2022-11-07 06:09:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.47% at 204 epoch
[2022-11-07 06:09:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][0/1251]	eta 0:40:17 lr 0.000231	time 1.9323 (1.9323)	loss 3.3556 (3.3556)	grad_norm 1.9977 (1.9977)	mem 14853MB
[2022-11-07 06:10:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][50/1251]	eta 0:10:01 lr 0.000231	time 0.4652 (0.5007)	loss 3.6504 (3.0637)	grad_norm 1.9612 (2.0653)	mem 14853MB
[2022-11-07 06:10:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][100/1251]	eta 0:09:20 lr 0.000231	time 0.4722 (0.4868)	loss 2.3658 (3.0858)	grad_norm 1.8817 (2.0487)	mem 14853MB
[2022-11-07 06:10:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][150/1251]	eta 0:08:48 lr 0.000231	time 0.4630 (0.4803)	loss 3.4626 (3.0954)	grad_norm 2.0248 (2.0512)	mem 14853MB
[2022-11-07 06:11:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][200/1251]	eta 0:08:21 lr 0.000230	time 0.4572 (0.4771)	loss 3.6867 (3.1142)	grad_norm 2.3697 (2.0434)	mem 14853MB
[2022-11-07 06:11:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][250/1251]	eta 0:07:54 lr 0.000230	time 0.4694 (0.4745)	loss 3.6303 (3.1257)	grad_norm 1.9967 (2.0370)	mem 14853MB
[2022-11-07 06:12:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][300/1251]	eta 0:07:29 lr 0.000230	time 0.4525 (0.4729)	loss 1.7565 (3.1233)	grad_norm 1.9761 (2.0439)	mem 14853MB
[2022-11-07 06:12:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][350/1251]	eta 0:07:05 lr 0.000230	time 0.4671 (0.4719)	loss 3.2035 (3.1137)	grad_norm 2.0589 (2.0397)	mem 14853MB
[2022-11-07 06:12:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][400/1251]	eta 0:06:40 lr 0.000230	time 0.4628 (0.4711)	loss 3.4832 (3.1141)	grad_norm 2.0588 (2.0443)	mem 14853MB
[2022-11-07 06:13:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][450/1251]	eta 0:06:16 lr 0.000230	time 0.4670 (0.4705)	loss 2.8619 (3.0971)	grad_norm 1.7936 (2.0388)	mem 14853MB
[2022-11-07 06:13:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][500/1251]	eta 0:05:52 lr 0.000229	time 0.4679 (0.4700)	loss 1.8808 (3.0895)	grad_norm 1.8118 (2.0377)	mem 14853MB
[2022-11-07 06:14:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][550/1251]	eta 0:05:29 lr 0.000229	time 0.4547 (0.4698)	loss 3.5866 (3.0911)	grad_norm 2.2444 (2.0406)	mem 14853MB
[2022-11-07 06:14:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][600/1251]	eta 0:05:05 lr 0.000229	time 0.4681 (0.4696)	loss 3.2444 (3.0902)	grad_norm 2.4523 (2.0479)	mem 14853MB
[2022-11-07 06:14:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][650/1251]	eta 0:04:42 lr 0.000229	time 0.4635 (0.4693)	loss 2.1974 (3.0923)	grad_norm 2.2729 (2.0455)	mem 14853MB
[2022-11-07 06:15:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][700/1251]	eta 0:04:18 lr 0.000229	time 0.4613 (0.4691)	loss 3.4086 (3.0956)	grad_norm 2.3649 (2.0430)	mem 14853MB
[2022-11-07 06:15:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][750/1251]	eta 0:03:54 lr 0.000228	time 0.4755 (0.4690)	loss 2.3186 (3.0851)	grad_norm 2.1230 (2.0430)	mem 14853MB
[2022-11-07 06:16:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][800/1251]	eta 0:03:31 lr 0.000228	time 0.4627 (0.4690)	loss 3.2156 (3.0865)	grad_norm 1.8519 (2.0439)	mem 14853MB
[2022-11-07 06:16:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][850/1251]	eta 0:03:08 lr 0.000228	time 0.4680 (0.4690)	loss 3.7851 (3.0904)	grad_norm 2.4159 (2.0449)	mem 14853MB
[2022-11-07 06:16:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][900/1251]	eta 0:02:44 lr 0.000228	time 0.5328 (0.4689)	loss 2.9148 (3.0947)	grad_norm 2.2634 (2.0458)	mem 14853MB
[2022-11-07 06:17:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][950/1251]	eta 0:02:21 lr 0.000228	time 0.4570 (0.4687)	loss 3.0470 (3.0957)	grad_norm 2.0480 (2.0436)	mem 14853MB
[2022-11-07 06:17:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][1000/1251]	eta 0:01:57 lr 0.000228	time 0.4631 (0.4685)	loss 3.2118 (3.0935)	grad_norm 2.0239 (2.0444)	mem 14853MB
[2022-11-07 06:17:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][1050/1251]	eta 0:01:34 lr 0.000227	time 0.4753 (0.4688)	loss 3.2786 (3.0995)	grad_norm 2.3949 (2.0446)	mem 14853MB
[2022-11-07 06:18:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][1100/1251]	eta 0:01:10 lr 0.000227	time 0.4698 (0.4686)	loss 3.2449 (3.1039)	grad_norm 2.0582 (2.0421)	mem 14853MB
[2022-11-07 06:18:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][1150/1251]	eta 0:00:47 lr 0.000227	time 0.4756 (0.4686)	loss 3.2939 (3.1013)	grad_norm 1.8126 (2.0441)	mem 14853MB
[2022-11-07 06:19:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][1200/1251]	eta 0:00:23 lr 0.000227	time 0.4550 (0.4684)	loss 3.4812 (3.1069)	grad_norm 1.9079 (2.0452)	mem 14853MB
[2022-11-07 06:19:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [206/300][1250/1251]	eta 0:00:00 lr 0.000227	time 0.4568 (0.4682)	loss 2.3800 (3.1084)	grad_norm 1.9301 (2.0439)	mem 14853MB
[2022-11-07 06:19:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 206 training takes 0:09:45
[2022-11-07 06:19:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_206.pth saving......
[2022-11-07 06:19:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_206.pth saved !!!
[2022-11-07 06:19:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.731 (1.731)	Loss 0.8502 (0.8502)	Acc@1 81.152 (81.152)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 06:19:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.558 Acc@5 95.402
[2022-11-07 06:19:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.6%
[2022-11-07 06:19:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.721 (1.721)	Loss 0.7107 (0.7107)	Acc@1 83.203 (83.203)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 06:19:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.478 Acc@5 95.906
[2022-11-07 06:19:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.5%
[2022-11-07 06:19:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.48% at 206 epoch
[2022-11-07 06:19:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][0/1251]	eta 0:46:17 lr 0.000227	time 2.2201 (2.2201)	loss 2.9827 (2.9827)	grad_norm 1.9856 (1.9856)	mem 14853MB
[2022-11-07 06:20:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][50/1251]	eta 0:10:08 lr 0.000227	time 0.4722 (0.5066)	loss 2.1025 (3.0890)	grad_norm 1.8885 (2.1162)	mem 14853MB
[2022-11-07 06:20:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][100/1251]	eta 0:09:22 lr 0.000226	time 0.4745 (0.4890)	loss 3.0804 (3.0187)	grad_norm 2.3734 (2.0572)	mem 14853MB
[2022-11-07 06:21:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][150/1251]	eta 0:08:49 lr 0.000226	time 0.4547 (0.4811)	loss 2.5793 (3.0459)	grad_norm 2.0049 (2.0534)	mem 14853MB
[2022-11-07 06:21:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][200/1251]	eta 0:08:21 lr 0.000226	time 0.4647 (0.4773)	loss 3.2749 (3.0831)	grad_norm 2.2320 (2.0471)	mem 14853MB
[2022-11-07 06:21:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][250/1251]	eta 0:07:55 lr 0.000226	time 0.4634 (0.4754)	loss 2.5953 (3.0922)	grad_norm 2.3131 (2.0587)	mem 14853MB
[2022-11-07 06:22:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][300/1251]	eta 0:07:30 lr 0.000226	time 0.4646 (0.4735)	loss 3.2602 (3.1062)	grad_norm 2.3955 (2.0758)	mem 14853MB
[2022-11-07 06:22:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][350/1251]	eta 0:07:05 lr 0.000226	time 0.4794 (0.4726)	loss 2.7149 (3.0872)	grad_norm 2.2351 (2.0766)	mem 14853MB
[2022-11-07 06:23:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][400/1251]	eta 0:06:41 lr 0.000225	time 0.4616 (0.4717)	loss 2.5589 (3.1035)	grad_norm 1.8782 (2.0766)	mem 14853MB
[2022-11-07 06:23:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][450/1251]	eta 0:06:17 lr 0.000225	time 0.4623 (0.4711)	loss 3.1052 (3.1129)	grad_norm 1.9928 (2.0777)	mem 14853MB
[2022-11-07 06:23:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][500/1251]	eta 0:05:53 lr 0.000225	time 0.4758 (0.4707)	loss 3.4012 (3.1142)	grad_norm 2.3516 (2.0737)	mem 14853MB
[2022-11-07 06:24:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][550/1251]	eta 0:05:29 lr 0.000225	time 0.4585 (0.4705)	loss 3.8401 (3.1100)	grad_norm 2.3728 (2.0739)	mem 14853MB
[2022-11-07 06:24:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][600/1251]	eta 0:05:06 lr 0.000225	time 0.4570 (0.4704)	loss 3.1397 (3.1014)	grad_norm 1.9572 (nan)	mem 14853MB
[2022-11-07 06:24:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][650/1251]	eta 0:04:42 lr 0.000225	time 0.4661 (0.4699)	loss 3.2945 (3.1035)	grad_norm 2.1171 (nan)	mem 14853MB
[2022-11-07 06:25:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][700/1251]	eta 0:04:18 lr 0.000224	time 0.4641 (0.4696)	loss 3.9332 (3.0992)	grad_norm 2.2262 (nan)	mem 14853MB
[2022-11-07 06:25:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][750/1251]	eta 0:03:55 lr 0.000224	time 0.4582 (0.4694)	loss 3.4608 (3.0942)	grad_norm 2.2485 (nan)	mem 14853MB
[2022-11-07 06:26:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][800/1251]	eta 0:03:31 lr 0.000224	time 0.4540 (0.4695)	loss 3.5122 (3.0987)	grad_norm 2.2443 (nan)	mem 14853MB
[2022-11-07 06:26:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][850/1251]	eta 0:03:08 lr 0.000224	time 0.4544 (0.4694)	loss 3.0617 (3.0992)	grad_norm 2.0599 (nan)	mem 14853MB
[2022-11-07 06:26:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][900/1251]	eta 0:02:44 lr 0.000224	time 0.4742 (0.4692)	loss 3.0664 (3.0949)	grad_norm 1.9565 (nan)	mem 14853MB
[2022-11-07 06:27:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][950/1251]	eta 0:02:21 lr 0.000224	time 0.4764 (0.4690)	loss 3.7411 (3.0994)	grad_norm 2.0574 (nan)	mem 14853MB
[2022-11-07 06:27:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][1000/1251]	eta 0:01:57 lr 0.000223	time 0.4701 (0.4688)	loss 3.3768 (3.1032)	grad_norm 1.8291 (nan)	mem 14853MB
[2022-11-07 06:28:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][1050/1251]	eta 0:01:34 lr 0.000223	time 0.4619 (0.4688)	loss 3.3628 (3.1040)	grad_norm 2.5425 (nan)	mem 14853MB
[2022-11-07 06:28:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][1100/1251]	eta 0:01:10 lr 0.000223	time 0.4643 (0.4688)	loss 3.8151 (3.1103)	grad_norm 2.4813 (nan)	mem 14853MB
[2022-11-07 06:28:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][1150/1251]	eta 0:00:47 lr 0.000223	time 0.4657 (0.4686)	loss 3.4408 (3.1060)	grad_norm 2.2345 (nan)	mem 14853MB
[2022-11-07 06:29:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][1200/1251]	eta 0:00:23 lr 0.000223	time 0.4593 (0.4685)	loss 2.5779 (3.1063)	grad_norm 2.1581 (nan)	mem 14853MB
[2022-11-07 06:29:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [207/300][1250/1251]	eta 0:00:00 lr 0.000223	time 0.4575 (0.4683)	loss 3.0688 (3.1063)	grad_norm 2.0969 (nan)	mem 14853MB
[2022-11-07 06:29:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 207 training takes 0:09:45
[2022-11-07 06:29:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_207.pth saving......
[2022-11-07 06:29:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_207.pth saved !!!
[2022-11-07 06:29:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.532 (1.532)	Loss 0.8537 (0.8537)	Acc@1 80.273 (80.273)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 06:29:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.334 Acc@5 95.356
[2022-11-07 06:29:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.3%
[2022-11-07 06:29:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.580 (1.580)	Loss 0.7493 (0.7493)	Acc@1 82.812 (82.812)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 06:29:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.474 Acc@5 95.916
[2022-11-07 06:29:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.5%
[2022-11-07 06:29:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.48% at 206 epoch
[2022-11-07 06:29:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][0/1251]	eta 0:40:34 lr 0.000222	time 1.9461 (1.9461)	loss 3.7438 (3.7438)	grad_norm 2.4989 (2.4989)	mem 14853MB
[2022-11-07 06:30:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][50/1251]	eta 0:10:03 lr 0.000222	time 0.4748 (0.5022)	loss 3.4331 (3.0733)	grad_norm 2.2106 (2.1112)	mem 14853MB
[2022-11-07 06:30:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][100/1251]	eta 0:09:19 lr 0.000222	time 0.4565 (0.4857)	loss 3.2879 (3.0986)	grad_norm 1.9723 (2.0802)	mem 14853MB
[2022-11-07 06:31:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][150/1251]	eta 0:08:47 lr 0.000222	time 0.4620 (0.4787)	loss 3.4410 (3.0833)	grad_norm 1.9674 (2.0696)	mem 14853MB
[2022-11-07 06:31:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][200/1251]	eta 0:08:20 lr 0.000222	time 0.4678 (0.4757)	loss 2.2965 (3.0957)	grad_norm 1.8172 (2.0806)	mem 14853MB
[2022-11-07 06:31:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][250/1251]	eta 0:07:54 lr 0.000222	time 0.4663 (0.4739)	loss 2.8838 (3.0726)	grad_norm 2.0485 (2.0661)	mem 14853MB
[2022-11-07 06:32:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][300/1251]	eta 0:07:29 lr 0.000221	time 0.4642 (0.4725)	loss 2.3707 (3.0843)	grad_norm 1.9414 (2.0635)	mem 14853MB
[2022-11-07 06:32:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][350/1251]	eta 0:07:04 lr 0.000221	time 0.4608 (0.4715)	loss 3.3444 (3.0937)	grad_norm 1.8472 (2.0616)	mem 14853MB
[2022-11-07 06:33:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][400/1251]	eta 0:06:40 lr 0.000221	time 0.4688 (0.4707)	loss 2.6577 (3.0814)	grad_norm 1.8832 (2.0635)	mem 14853MB
[2022-11-07 06:33:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][450/1251]	eta 0:06:16 lr 0.000221	time 0.4514 (0.4702)	loss 3.1055 (3.0789)	grad_norm 1.9076 (2.0600)	mem 14853MB
[2022-11-07 06:33:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][500/1251]	eta 0:05:52 lr 0.000221	time 0.4524 (0.4696)	loss 3.2675 (3.0829)	grad_norm 1.8777 (2.0620)	mem 14853MB
[2022-11-07 06:34:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][550/1251]	eta 0:05:29 lr 0.000221	time 0.4647 (0.4695)	loss 3.1362 (3.0805)	grad_norm 2.0909 (2.0600)	mem 14853MB
[2022-11-07 06:34:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][600/1251]	eta 0:05:05 lr 0.000220	time 0.4651 (0.4694)	loss 3.5427 (3.0885)	grad_norm 1.8989 (2.0609)	mem 14853MB
[2022-11-07 06:35:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][650/1251]	eta 0:04:41 lr 0.000220	time 0.4750 (0.4690)	loss 3.6886 (3.0862)	grad_norm 1.7757 (2.0590)	mem 14853MB
[2022-11-07 06:35:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][700/1251]	eta 0:04:18 lr 0.000220	time 0.4768 (0.4688)	loss 3.6364 (3.0898)	grad_norm 2.0930 (2.0588)	mem 14853MB
[2022-11-07 06:35:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][750/1251]	eta 0:03:54 lr 0.000220	time 0.4742 (0.4686)	loss 3.5476 (3.0822)	grad_norm 2.2236 (2.0615)	mem 14853MB
[2022-11-07 06:36:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][800/1251]	eta 0:03:31 lr 0.000220	time 0.4642 (0.4688)	loss 2.6003 (3.0834)	grad_norm 2.5154 (2.0618)	mem 14853MB
[2022-11-07 06:36:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][850/1251]	eta 0:03:07 lr 0.000220	time 0.4696 (0.4687)	loss 3.2232 (3.0842)	grad_norm 2.0470 (2.0643)	mem 14853MB
[2022-11-07 06:36:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][900/1251]	eta 0:02:44 lr 0.000219	time 0.4815 (0.4685)	loss 3.5389 (3.0804)	grad_norm 1.9830 (2.0658)	mem 14853MB
[2022-11-07 06:37:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][950/1251]	eta 0:02:20 lr 0.000219	time 0.4536 (0.4684)	loss 3.3255 (3.0825)	grad_norm 2.2709 (2.0673)	mem 14853MB
[2022-11-07 06:37:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][1000/1251]	eta 0:01:57 lr 0.000219	time 0.4658 (0.4683)	loss 3.5588 (3.0817)	grad_norm 2.3251 (2.0671)	mem 14853MB
[2022-11-07 06:38:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][1050/1251]	eta 0:01:34 lr 0.000219	time 0.4690 (0.4684)	loss 3.1714 (3.0818)	grad_norm 2.1542 (2.0657)	mem 14853MB
[2022-11-07 06:38:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][1100/1251]	eta 0:01:10 lr 0.000219	time 0.4657 (0.4684)	loss 2.0870 (3.0817)	grad_norm 2.1752 (2.0648)	mem 14853MB
[2022-11-07 06:38:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][1150/1251]	eta 0:00:47 lr 0.000219	time 0.4748 (0.4683)	loss 2.7402 (3.0824)	grad_norm 2.2282 (2.0650)	mem 14853MB
[2022-11-07 06:39:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][1200/1251]	eta 0:00:23 lr 0.000218	time 0.4663 (0.4682)	loss 3.0662 (3.0848)	grad_norm 2.0826 (2.0680)	mem 14853MB
[2022-11-07 06:39:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [208/300][1250/1251]	eta 0:00:00 lr 0.000218	time 0.4586 (0.4681)	loss 3.0060 (3.0884)	grad_norm 2.4137 (2.0668)	mem 14853MB
[2022-11-07 06:39:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 208 training takes 0:09:45
[2022-11-07 06:39:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_208.pth saving......
[2022-11-07 06:39:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_208.pth saved !!!
[2022-11-07 06:39:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.565 (1.565)	Loss 0.7643 (0.7643)	Acc@1 81.641 (81.641)	Acc@5 96.484 (96.484)	Mem 14853MB
[2022-11-07 06:39:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.350 Acc@5 95.402
[2022-11-07 06:39:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.4%
[2022-11-07 06:39:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.688 (1.688)	Loss 0.7242 (0.7242)	Acc@1 82.324 (82.324)	Acc@5 96.582 (96.582)	Mem 14853MB
[2022-11-07 06:39:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.536 Acc@5 95.914
[2022-11-07 06:39:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.5%
[2022-11-07 06:39:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.54% at 208 epoch
[2022-11-07 06:40:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][0/1251]	eta 0:41:11 lr 0.000218	time 1.9753 (1.9753)	loss 2.9214 (2.9214)	grad_norm 1.9612 (1.9612)	mem 14853MB
[2022-11-07 06:40:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][50/1251]	eta 0:10:03 lr 0.000218	time 0.4796 (0.5024)	loss 3.2989 (3.1196)	grad_norm 1.9562 (2.0697)	mem 14853MB
[2022-11-07 06:40:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][100/1251]	eta 0:09:18 lr 0.000218	time 0.4784 (0.4853)	loss 2.5620 (3.1472)	grad_norm 2.0947 (2.0442)	mem 14853MB
[2022-11-07 06:41:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][150/1251]	eta 0:08:48 lr 0.000218	time 0.4603 (0.4801)	loss 3.2012 (3.1351)	grad_norm 2.2582 (inf)	mem 14853MB
[2022-11-07 06:41:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][200/1251]	eta 0:08:20 lr 0.000218	time 0.4772 (0.4765)	loss 2.4629 (3.1071)	grad_norm 1.9981 (inf)	mem 14853MB
[2022-11-07 06:41:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][250/1251]	eta 0:07:54 lr 0.000217	time 0.4644 (0.4740)	loss 2.7254 (3.0881)	grad_norm 1.8748 (inf)	mem 14853MB
[2022-11-07 06:42:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][300/1251]	eta 0:07:29 lr 0.000217	time 0.4515 (0.4728)	loss 2.9369 (3.0854)	grad_norm 2.1087 (inf)	mem 14853MB
[2022-11-07 06:42:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][350/1251]	eta 0:07:05 lr 0.000217	time 0.4521 (0.4719)	loss 3.4199 (3.0886)	grad_norm 1.8394 (inf)	mem 14853MB
[2022-11-07 06:43:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][400/1251]	eta 0:06:41 lr 0.000217	time 0.4642 (0.4713)	loss 3.5919 (3.0788)	grad_norm 1.9696 (inf)	mem 14853MB
[2022-11-07 06:43:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][450/1251]	eta 0:06:16 lr 0.000217	time 0.4623 (0.4706)	loss 3.4609 (3.0848)	grad_norm 2.6128 (inf)	mem 14853MB
[2022-11-07 06:43:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][500/1251]	eta 0:05:52 lr 0.000217	time 0.4534 (0.4700)	loss 3.5466 (3.0857)	grad_norm 2.1904 (inf)	mem 14853MB
[2022-11-07 06:44:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][550/1251]	eta 0:05:29 lr 0.000216	time 0.4670 (0.4701)	loss 3.4714 (3.0839)	grad_norm 2.2533 (inf)	mem 14853MB
[2022-11-07 06:44:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][600/1251]	eta 0:05:05 lr 0.000216	time 0.4639 (0.4697)	loss 2.0639 (3.0752)	grad_norm 1.7238 (inf)	mem 14853MB
[2022-11-07 06:45:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][650/1251]	eta 0:04:42 lr 0.000216	time 0.4615 (0.4695)	loss 3.5181 (3.0820)	grad_norm 2.1300 (inf)	mem 14853MB
[2022-11-07 06:45:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][700/1251]	eta 0:04:18 lr 0.000216	time 0.4582 (0.4692)	loss 2.7063 (3.0803)	grad_norm 2.0221 (inf)	mem 14853MB
[2022-11-07 06:45:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][750/1251]	eta 0:03:54 lr 0.000216	time 0.4610 (0.4689)	loss 2.5028 (3.0890)	grad_norm 1.9789 (inf)	mem 14853MB
[2022-11-07 06:46:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][800/1251]	eta 0:03:31 lr 0.000216	time 0.4669 (0.4690)	loss 2.9715 (3.0901)	grad_norm 2.3677 (inf)	mem 14853MB
[2022-11-07 06:46:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][850/1251]	eta 0:03:08 lr 0.000215	time 0.4630 (0.4689)	loss 2.5573 (3.0914)	grad_norm 1.9876 (inf)	mem 14853MB
[2022-11-07 06:47:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][900/1251]	eta 0:02:44 lr 0.000215	time 0.4599 (0.4689)	loss 3.8410 (3.0881)	grad_norm 1.9581 (inf)	mem 14853MB
[2022-11-07 06:47:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][950/1251]	eta 0:02:21 lr 0.000215	time 0.4719 (0.4687)	loss 3.2784 (3.0911)	grad_norm 2.0868 (inf)	mem 14853MB
[2022-11-07 06:47:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][1000/1251]	eta 0:01:57 lr 0.000215	time 0.4655 (0.4685)	loss 3.7408 (3.0974)	grad_norm 2.7994 (inf)	mem 14853MB
[2022-11-07 06:48:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][1050/1251]	eta 0:01:34 lr 0.000215	time 0.4601 (0.4685)	loss 3.1704 (3.1000)	grad_norm 2.1989 (inf)	mem 14853MB
[2022-11-07 06:48:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][1100/1251]	eta 0:01:10 lr 0.000215	time 0.4753 (0.4684)	loss 3.1050 (3.1023)	grad_norm 1.9792 (inf)	mem 14853MB
[2022-11-07 06:48:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][1150/1251]	eta 0:00:47 lr 0.000214	time 0.4765 (0.4684)	loss 2.9162 (3.0999)	grad_norm 2.0000 (inf)	mem 14853MB
[2022-11-07 06:49:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][1200/1251]	eta 0:00:23 lr 0.000214	time 0.4645 (0.4684)	loss 3.4162 (3.1008)	grad_norm 2.0073 (inf)	mem 14853MB
[2022-11-07 06:49:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [209/300][1250/1251]	eta 0:00:00 lr 0.000214	time 0.4578 (0.4681)	loss 2.6446 (3.0978)	grad_norm 2.1033 (inf)	mem 14853MB
[2022-11-07 06:49:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 209 training takes 0:09:45
[2022-11-07 06:49:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_209.pth saving......
[2022-11-07 06:49:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_209.pth saved !!!
[2022-11-07 06:49:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.543 (1.543)	Loss 0.7803 (0.7803)	Acc@1 81.543 (81.543)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 06:49:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.402 Acc@5 95.446
[2022-11-07 06:49:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.4%
[2022-11-07 06:49:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.690 (1.690)	Loss 0.7795 (0.7795)	Acc@1 81.348 (81.348)	Acc@5 96.387 (96.387)	Mem 14853MB
[2022-11-07 06:50:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.528 Acc@5 95.920
[2022-11-07 06:50:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.5%
[2022-11-07 06:50:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.54% at 208 epoch
[2022-11-07 06:50:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][0/1251]	eta 0:41:32 lr 0.000214	time 1.9927 (1.9927)	loss 3.8958 (3.8958)	grad_norm 1.9697 (1.9697)	mem 14853MB
[2022-11-07 06:50:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][50/1251]	eta 0:10:02 lr 0.000214	time 0.4647 (0.5021)	loss 2.5975 (3.0518)	grad_norm 2.0820 (2.0403)	mem 14853MB
[2022-11-07 06:50:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][100/1251]	eta 0:09:18 lr 0.000214	time 0.4664 (0.4856)	loss 2.0941 (3.0269)	grad_norm 1.9007 (2.0627)	mem 14853MB
[2022-11-07 06:51:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][150/1251]	eta 0:08:48 lr 0.000214	time 0.4665 (0.4801)	loss 3.3687 (3.0276)	grad_norm 1.9122 (2.0870)	mem 14853MB
[2022-11-07 06:51:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][200/1251]	eta 0:08:21 lr 0.000213	time 0.4666 (0.4768)	loss 3.2870 (3.0531)	grad_norm 2.0678 (2.0859)	mem 14853MB
[2022-11-07 06:52:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][250/1251]	eta 0:07:55 lr 0.000213	time 0.4588 (0.4748)	loss 3.1615 (3.0838)	grad_norm 2.1373 (2.0917)	mem 14853MB
[2022-11-07 06:52:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][300/1251]	eta 0:07:30 lr 0.000213	time 0.4725 (0.4732)	loss 3.4287 (3.0781)	grad_norm 1.8533 (2.0849)	mem 14853MB
[2022-11-07 06:52:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][350/1251]	eta 0:07:05 lr 0.000213	time 0.4693 (0.4722)	loss 3.2056 (3.0931)	grad_norm 2.3045 (2.0853)	mem 14853MB
[2022-11-07 06:53:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][400/1251]	eta 0:06:41 lr 0.000213	time 0.4629 (0.4714)	loss 3.4589 (3.0937)	grad_norm 2.0113 (2.0890)	mem 14853MB
[2022-11-07 06:53:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][450/1251]	eta 0:06:17 lr 0.000213	time 0.4656 (0.4707)	loss 2.9506 (3.0812)	grad_norm 1.8849 (2.0871)	mem 14853MB
[2022-11-07 06:53:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][500/1251]	eta 0:05:53 lr 0.000212	time 0.4754 (0.4703)	loss 3.2673 (3.0765)	grad_norm 2.1358 (2.0876)	mem 14853MB
[2022-11-07 06:54:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][550/1251]	eta 0:05:29 lr 0.000212	time 0.4646 (0.4701)	loss 2.8674 (3.0789)	grad_norm 2.0013 (2.0868)	mem 14853MB
[2022-11-07 06:54:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][600/1251]	eta 0:05:05 lr 0.000212	time 0.4753 (0.4698)	loss 3.3278 (3.0793)	grad_norm 1.9329 (2.0882)	mem 14853MB
[2022-11-07 06:55:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][650/1251]	eta 0:04:42 lr 0.000212	time 0.4609 (0.4694)	loss 2.9692 (3.0927)	grad_norm 2.0186 (2.0882)	mem 14853MB
[2022-11-07 06:55:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][700/1251]	eta 0:04:18 lr 0.000212	time 0.5483 (0.4692)	loss 3.5231 (3.0957)	grad_norm 2.8086 (2.0901)	mem 14853MB
[2022-11-07 06:55:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][750/1251]	eta 0:03:55 lr 0.000212	time 0.4590 (0.4692)	loss 3.7557 (3.0970)	grad_norm 1.8405 (2.0926)	mem 14853MB
[2022-11-07 06:56:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][800/1251]	eta 0:03:31 lr 0.000211	time 0.4668 (0.4692)	loss 2.7070 (3.0946)	grad_norm 2.1583 (2.0918)	mem 14853MB
[2022-11-07 06:56:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][850/1251]	eta 0:03:08 lr 0.000211	time 0.4660 (0.4690)	loss 3.0460 (3.0993)	grad_norm 2.1452 (2.0939)	mem 14853MB
[2022-11-07 06:57:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][900/1251]	eta 0:02:44 lr 0.000211	time 0.4694 (0.4689)	loss 3.3111 (3.1050)	grad_norm 1.8282 (2.0971)	mem 14853MB
[2022-11-07 06:57:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][950/1251]	eta 0:02:21 lr 0.000211	time 0.4684 (0.4687)	loss 3.1659 (3.1022)	grad_norm 1.8836 (2.0970)	mem 14853MB
[2022-11-07 06:57:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][1000/1251]	eta 0:01:57 lr 0.000211	time 0.4589 (0.4687)	loss 3.1297 (3.1023)	grad_norm 2.0826 (2.0966)	mem 14853MB
[2022-11-07 06:58:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][1050/1251]	eta 0:01:34 lr 0.000211	time 0.4624 (0.4687)	loss 3.3437 (3.1029)	grad_norm 1.9665 (2.0949)	mem 14853MB
[2022-11-07 06:58:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][1100/1251]	eta 0:01:10 lr 0.000210	time 0.4609 (0.4686)	loss 3.1813 (3.0979)	grad_norm 2.1689 (2.0961)	mem 14853MB
[2022-11-07 06:59:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][1150/1251]	eta 0:00:47 lr 0.000210	time 0.4591 (0.4684)	loss 3.2557 (3.1002)	grad_norm 2.1479 (2.0968)	mem 14853MB
[2022-11-07 06:59:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][1200/1251]	eta 0:00:23 lr 0.000210	time 0.5453 (0.4683)	loss 2.8038 (3.0972)	grad_norm 1.9320 (2.0982)	mem 14853MB
[2022-11-07 06:59:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [210/300][1250/1251]	eta 0:00:00 lr 0.000210	time 0.4571 (0.4681)	loss 3.2911 (3.0931)	grad_norm 1.8454 (2.0988)	mem 14853MB
[2022-11-07 06:59:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 210 training takes 0:09:45
[2022-11-07 06:59:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_210.pth saving......
[2022-11-07 06:59:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_210.pth saved !!!
[2022-11-07 06:59:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.464 (1.464)	Loss 0.8512 (0.8512)	Acc@1 80.469 (80.469)	Acc@5 95.215 (95.215)	Mem 14853MB
[2022-11-07 06:59:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.376 Acc@5 95.432
[2022-11-07 06:59:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.4%
[2022-11-07 06:59:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.586 (1.586)	Loss 0.7215 (0.7215)	Acc@1 82.031 (82.031)	Acc@5 96.582 (96.582)	Mem 14853MB
[2022-11-07 07:00:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.554 Acc@5 95.948
[2022-11-07 07:00:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.6%
[2022-11-07 07:00:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.55% at 210 epoch
[2022-11-07 07:00:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][0/1251]	eta 0:41:18 lr 0.000210	time 1.9815 (1.9815)	loss 2.8916 (2.8916)	grad_norm 2.1350 (2.1350)	mem 14853MB
[2022-11-07 07:00:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][50/1251]	eta 0:10:02 lr 0.000210	time 0.4689 (0.5016)	loss 2.8939 (3.0118)	grad_norm 2.0096 (2.1179)	mem 14853MB
[2022-11-07 07:00:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][100/1251]	eta 0:09:17 lr 0.000210	time 0.4702 (0.4842)	loss 2.1587 (3.0145)	grad_norm 2.2042 (nan)	mem 14853MB
[2022-11-07 07:01:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][150/1251]	eta 0:08:46 lr 0.000209	time 0.4645 (0.4783)	loss 3.3424 (3.0314)	grad_norm 2.0665 (nan)	mem 14853MB
[2022-11-07 07:01:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][200/1251]	eta 0:08:18 lr 0.000209	time 0.4587 (0.4747)	loss 2.5165 (2.9981)	grad_norm 1.9557 (nan)	mem 14853MB
[2022-11-07 07:02:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][250/1251]	eta 0:07:53 lr 0.000209	time 0.4705 (0.4730)	loss 3.2788 (3.0305)	grad_norm 2.1959 (nan)	mem 14853MB
[2022-11-07 07:02:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][300/1251]	eta 0:07:28 lr 0.000209	time 0.4531 (0.4715)	loss 3.6067 (3.0391)	grad_norm 1.9475 (nan)	mem 14853MB
[2022-11-07 07:02:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][350/1251]	eta 0:07:03 lr 0.000209	time 0.4633 (0.4705)	loss 3.4080 (3.0466)	grad_norm 2.0152 (nan)	mem 14853MB
[2022-11-07 07:03:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][400/1251]	eta 0:06:39 lr 0.000209	time 0.4589 (0.4699)	loss 3.2014 (3.0433)	grad_norm 2.2635 (nan)	mem 14853MB
[2022-11-07 07:03:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][450/1251]	eta 0:06:15 lr 0.000208	time 0.4690 (0.4693)	loss 2.3024 (3.0614)	grad_norm 1.9071 (nan)	mem 14853MB
[2022-11-07 07:04:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][500/1251]	eta 0:05:52 lr 0.000208	time 0.4652 (0.4689)	loss 3.4548 (3.0523)	grad_norm 2.5045 (nan)	mem 14853MB
[2022-11-07 07:04:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][550/1251]	eta 0:05:28 lr 0.000208	time 0.5392 (0.4691)	loss 2.6045 (3.0611)	grad_norm 2.2315 (nan)	mem 14853MB
[2022-11-07 07:04:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][600/1251]	eta 0:05:05 lr 0.000208	time 0.4533 (0.4688)	loss 2.6643 (3.0586)	grad_norm 2.1336 (nan)	mem 14853MB
[2022-11-07 07:05:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][650/1251]	eta 0:04:41 lr 0.000208	time 0.4663 (0.4686)	loss 3.4093 (3.0611)	grad_norm 1.7625 (nan)	mem 14853MB
[2022-11-07 07:05:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][700/1251]	eta 0:04:18 lr 0.000208	time 0.4678 (0.4683)	loss 3.0018 (3.0614)	grad_norm 2.0102 (nan)	mem 14853MB
[2022-11-07 07:05:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][750/1251]	eta 0:03:54 lr 0.000207	time 0.4745 (0.4681)	loss 3.2304 (3.0673)	grad_norm 1.9906 (nan)	mem 14853MB
[2022-11-07 07:06:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][800/1251]	eta 0:03:31 lr 0.000207	time 0.4660 (0.4682)	loss 2.5462 (3.0650)	grad_norm 1.9935 (nan)	mem 14853MB
[2022-11-07 07:06:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][850/1251]	eta 0:03:07 lr 0.000207	time 0.4589 (0.4680)	loss 3.4281 (3.0731)	grad_norm 3.0672 (nan)	mem 14853MB
[2022-11-07 07:07:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][900/1251]	eta 0:02:44 lr 0.000207	time 0.4609 (0.4680)	loss 2.2858 (3.0724)	grad_norm 2.1020 (nan)	mem 14853MB
[2022-11-07 07:07:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][950/1251]	eta 0:02:20 lr 0.000207	time 0.4583 (0.4678)	loss 2.9783 (3.0699)	grad_norm 2.4533 (nan)	mem 14853MB
[2022-11-07 07:07:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][1000/1251]	eta 0:01:57 lr 0.000207	time 0.4637 (0.4677)	loss 2.9557 (3.0723)	grad_norm 2.0117 (nan)	mem 14853MB
[2022-11-07 07:08:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][1050/1251]	eta 0:01:34 lr 0.000206	time 0.5333 (0.4678)	loss 3.0903 (3.0726)	grad_norm 1.9095 (nan)	mem 14853MB
[2022-11-07 07:08:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][1100/1251]	eta 0:01:10 lr 0.000206	time 0.4691 (0.4677)	loss 3.6265 (3.0685)	grad_norm 2.0232 (nan)	mem 14853MB
[2022-11-07 07:09:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][1150/1251]	eta 0:00:47 lr 0.000206	time 0.4608 (0.4677)	loss 2.3094 (3.0665)	grad_norm 2.3187 (nan)	mem 14853MB
[2022-11-07 07:09:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][1200/1251]	eta 0:00:23 lr 0.000206	time 0.4736 (0.4675)	loss 1.9262 (3.0625)	grad_norm 1.9353 (nan)	mem 14853MB
[2022-11-07 07:09:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [211/300][1250/1251]	eta 0:00:00 lr 0.000206	time 0.4569 (0.4673)	loss 3.0892 (3.0650)	grad_norm 1.8935 (nan)	mem 14853MB
[2022-11-07 07:09:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 211 training takes 0:09:44
[2022-11-07 07:09:51 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_211.pth saving......
[2022-11-07 07:09:52 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_211.pth saved !!!
[2022-11-07 07:09:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.677 (1.677)	Loss 0.8371 (0.8371)	Acc@1 79.785 (79.785)	Acc@5 95.117 (95.117)	Mem 14853MB
[2022-11-07 07:10:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.428 Acc@5 95.414
[2022-11-07 07:10:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.4%
[2022-11-07 07:10:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.550 (1.550)	Loss 0.7946 (0.7946)	Acc@1 81.543 (81.543)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 07:10:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.624 Acc@5 95.952
[2022-11-07 07:10:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.6%
[2022-11-07 07:10:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.62% at 211 epoch
[2022-11-07 07:10:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][0/1251]	eta 0:42:14 lr 0.000206	time 2.0262 (2.0262)	loss 2.0558 (2.0558)	grad_norm 1.8544 (1.8544)	mem 14853MB
[2022-11-07 07:10:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][50/1251]	eta 0:10:01 lr 0.000206	time 0.4596 (0.5007)	loss 3.0771 (2.9997)	grad_norm 2.3777 (2.1097)	mem 14853MB
[2022-11-07 07:10:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][100/1251]	eta 0:09:18 lr 0.000205	time 0.4655 (0.4856)	loss 2.4152 (3.0348)	grad_norm 2.0162 (2.1489)	mem 14853MB
[2022-11-07 07:11:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][150/1251]	eta 0:08:47 lr 0.000205	time 0.4682 (0.4791)	loss 3.3252 (3.0700)	grad_norm 2.0826 (2.1304)	mem 14853MB
[2022-11-07 07:11:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][200/1251]	eta 0:08:19 lr 0.000205	time 0.4581 (0.4755)	loss 3.1456 (3.0851)	grad_norm 2.1055 (2.1125)	mem 14853MB
[2022-11-07 07:12:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][250/1251]	eta 0:07:53 lr 0.000205	time 0.4562 (0.4733)	loss 3.2332 (3.0706)	grad_norm 2.1628 (2.1145)	mem 14853MB
[2022-11-07 07:12:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][300/1251]	eta 0:07:28 lr 0.000205	time 0.4663 (0.4721)	loss 2.2423 (3.0506)	grad_norm 2.0881 (2.1176)	mem 14853MB
[2022-11-07 07:12:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][350/1251]	eta 0:07:04 lr 0.000205	time 0.4627 (0.4710)	loss 3.1614 (3.0399)	grad_norm 1.8331 (2.1151)	mem 14853MB
[2022-11-07 07:13:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][400/1251]	eta 0:06:40 lr 0.000204	time 0.4710 (0.4704)	loss 3.5484 (3.0368)	grad_norm 2.6702 (2.1081)	mem 14853MB
[2022-11-07 07:13:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][450/1251]	eta 0:06:16 lr 0.000204	time 0.4716 (0.4698)	loss 3.6412 (3.0375)	grad_norm 2.3404 (2.1040)	mem 14853MB
[2022-11-07 07:14:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][500/1251]	eta 0:05:52 lr 0.000204	time 0.4676 (0.4695)	loss 3.4778 (3.0354)	grad_norm 2.0708 (2.1053)	mem 14853MB
[2022-11-07 07:14:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][550/1251]	eta 0:05:29 lr 0.000204	time 0.4662 (0.4695)	loss 3.0653 (3.0394)	grad_norm 2.1941 (2.1059)	mem 14853MB
[2022-11-07 07:14:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][600/1251]	eta 0:05:05 lr 0.000204	time 0.4634 (0.4692)	loss 2.9896 (3.0424)	grad_norm 2.2010 (2.1081)	mem 14853MB
[2022-11-07 07:15:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][650/1251]	eta 0:04:41 lr 0.000204	time 0.4600 (0.4690)	loss 3.4264 (3.0475)	grad_norm 1.8249 (2.1102)	mem 14853MB
[2022-11-07 07:15:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][700/1251]	eta 0:04:18 lr 0.000203	time 0.4618 (0.4687)	loss 3.0328 (3.0494)	grad_norm 2.2920 (2.1143)	mem 14853MB
[2022-11-07 07:16:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][750/1251]	eta 0:03:54 lr 0.000203	time 0.4611 (0.4685)	loss 3.3996 (3.0500)	grad_norm 2.3884 (2.1165)	mem 14853MB
[2022-11-07 07:16:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][800/1251]	eta 0:03:31 lr 0.000203	time 0.4680 (0.4686)	loss 3.2797 (3.0552)	grad_norm 2.2618 (2.1146)	mem 14853MB
[2022-11-07 07:16:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][850/1251]	eta 0:03:07 lr 0.000203	time 0.4629 (0.4685)	loss 3.0688 (3.0514)	grad_norm 2.1282 (2.1128)	mem 14853MB
[2022-11-07 07:17:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][900/1251]	eta 0:02:44 lr 0.000203	time 0.4528 (0.4683)	loss 3.3893 (3.0546)	grad_norm 2.1695 (2.1122)	mem 14853MB
[2022-11-07 07:17:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][950/1251]	eta 0:02:20 lr 0.000203	time 0.4658 (0.4681)	loss 2.4129 (3.0559)	grad_norm 2.0344 (2.1102)	mem 14853MB
[2022-11-07 07:17:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][1000/1251]	eta 0:01:57 lr 0.000202	time 0.4572 (0.4682)	loss 3.7119 (3.0568)	grad_norm 2.2799 (2.1120)	mem 14853MB
[2022-11-07 07:18:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][1050/1251]	eta 0:01:34 lr 0.000202	time 0.4586 (0.4681)	loss 2.7067 (3.0541)	grad_norm 2.2094 (nan)	mem 14853MB
[2022-11-07 07:18:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][1100/1251]	eta 0:01:10 lr 0.000202	time 0.4623 (0.4681)	loss 2.6227 (3.0548)	grad_norm 2.2134 (nan)	mem 14853MB
[2022-11-07 07:19:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][1150/1251]	eta 0:00:47 lr 0.000202	time 0.4650 (0.4680)	loss 3.3435 (3.0553)	grad_norm 2.1508 (nan)	mem 14853MB
[2022-11-07 07:19:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][1200/1251]	eta 0:00:23 lr 0.000202	time 0.4746 (0.4679)	loss 3.0933 (3.0582)	grad_norm 2.0231 (nan)	mem 14853MB
[2022-11-07 07:19:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [212/300][1250/1251]	eta 0:00:00 lr 0.000202	time 0.4569 (0.4677)	loss 2.6376 (3.0590)	grad_norm 2.0095 (nan)	mem 14853MB
[2022-11-07 07:19:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 212 training takes 0:09:45
[2022-11-07 07:19:55 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_212.pth saving......
[2022-11-07 07:19:55 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_212.pth saved !!!
[2022-11-07 07:19:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.585 (1.585)	Loss 0.8333 (0.8333)	Acc@1 80.762 (80.762)	Acc@5 95.312 (95.312)	Mem 14853MB
[2022-11-07 07:20:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.432 Acc@5 95.490
[2022-11-07 07:20:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.4%
[2022-11-07 07:20:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.617 (1.617)	Loss 0.8309 (0.8309)	Acc@1 80.078 (80.078)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 07:20:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.650 Acc@5 95.934
[2022-11-07 07:20:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.7%
[2022-11-07 07:20:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.65% at 212 epoch
[2022-11-07 07:20:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][0/1251]	eta 0:40:25 lr 0.000202	time 1.9390 (1.9390)	loss 3.4945 (3.4945)	grad_norm 2.1141 (2.1141)	mem 14853MB
[2022-11-07 07:20:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][50/1251]	eta 0:10:04 lr 0.000201	time 0.4637 (0.5030)	loss 2.7186 (3.0132)	grad_norm 2.0028 (2.0996)	mem 14853MB
[2022-11-07 07:21:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][100/1251]	eta 0:09:19 lr 0.000201	time 0.4806 (0.4862)	loss 3.5021 (3.0179)	grad_norm 2.6095 (2.1240)	mem 14853MB
[2022-11-07 07:21:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][150/1251]	eta 0:08:47 lr 0.000201	time 0.4640 (0.4794)	loss 2.5243 (3.0400)	grad_norm 1.8317 (2.1093)	mem 14853MB
[2022-11-07 07:21:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][200/1251]	eta 0:08:20 lr 0.000201	time 0.4707 (0.4758)	loss 3.2404 (3.0511)	grad_norm 2.3904 (2.1080)	mem 14853MB
[2022-11-07 07:22:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][250/1251]	eta 0:07:54 lr 0.000201	time 0.4603 (0.4740)	loss 3.5068 (3.0523)	grad_norm 1.7594 (2.1175)	mem 14853MB
[2022-11-07 07:22:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][300/1251]	eta 0:07:29 lr 0.000201	time 0.4660 (0.4725)	loss 3.2704 (3.0692)	grad_norm 2.3111 (2.1186)	mem 14853MB
[2022-11-07 07:22:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][350/1251]	eta 0:07:04 lr 0.000200	time 0.4627 (0.4714)	loss 2.9740 (3.0620)	grad_norm 2.5808 (2.1250)	mem 14853MB
[2022-11-07 07:23:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][400/1251]	eta 0:06:40 lr 0.000200	time 0.4628 (0.4706)	loss 2.2915 (3.0641)	grad_norm 2.1674 (2.1327)	mem 14853MB
[2022-11-07 07:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][450/1251]	eta 0:06:16 lr 0.000200	time 0.4649 (0.4701)	loss 2.2268 (3.0627)	grad_norm 1.7475 (2.1359)	mem 14853MB
[2022-11-07 07:24:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][500/1251]	eta 0:05:52 lr 0.000200	time 0.4685 (0.4697)	loss 2.2006 (3.0463)	grad_norm 2.2557 (2.1337)	mem 14853MB
[2022-11-07 07:24:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][550/1251]	eta 0:05:29 lr 0.000200	time 0.4544 (0.4700)	loss 3.5310 (3.0462)	grad_norm 2.0669 (2.1337)	mem 14853MB
[2022-11-07 07:24:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][600/1251]	eta 0:05:05 lr 0.000200	time 0.4639 (0.4697)	loss 3.3731 (3.0544)	grad_norm 2.3032 (2.1390)	mem 14853MB
[2022-11-07 07:25:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][650/1251]	eta 0:04:42 lr 0.000199	time 0.4675 (0.4693)	loss 2.8152 (3.0608)	grad_norm 2.0445 (2.1355)	mem 14853MB
[2022-11-07 07:25:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][700/1251]	eta 0:04:18 lr 0.000199	time 0.4636 (0.4691)	loss 2.5864 (3.0659)	grad_norm 2.2509 (2.1360)	mem 14853MB
[2022-11-07 07:26:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][750/1251]	eta 0:03:54 lr 0.000199	time 0.5483 (0.4690)	loss 3.4915 (3.0636)	grad_norm 2.0642 (2.1334)	mem 14853MB
[2022-11-07 07:26:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][800/1251]	eta 0:03:31 lr 0.000199	time 0.4619 (0.4690)	loss 3.5886 (3.0600)	grad_norm 1.9411 (2.1312)	mem 14853MB
[2022-11-07 07:26:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][850/1251]	eta 0:03:08 lr 0.000199	time 0.4667 (0.4690)	loss 2.9860 (3.0630)	grad_norm 2.1730 (2.1343)	mem 14853MB
[2022-11-07 07:27:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][900/1251]	eta 0:02:44 lr 0.000199	time 0.4711 (0.4688)	loss 3.7101 (3.0610)	grad_norm 2.2318 (2.1340)	mem 14853MB
[2022-11-07 07:27:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][950/1251]	eta 0:02:21 lr 0.000199	time 0.4737 (0.4686)	loss 2.9315 (3.0637)	grad_norm 2.4504 (2.1313)	mem 14853MB
[2022-11-07 07:28:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][1000/1251]	eta 0:01:57 lr 0.000198	time 0.4740 (0.4685)	loss 2.9480 (3.0626)	grad_norm 2.6059 (2.1343)	mem 14853MB
[2022-11-07 07:28:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][1050/1251]	eta 0:01:34 lr 0.000198	time 0.4572 (0.4685)	loss 3.2516 (3.0635)	grad_norm 2.0212 (2.1341)	mem 14853MB
[2022-11-07 07:28:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][1100/1251]	eta 0:01:10 lr 0.000198	time 0.4605 (0.4684)	loss 3.5650 (3.0678)	grad_norm 2.0528 (2.1335)	mem 14853MB
[2022-11-07 07:29:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][1150/1251]	eta 0:00:47 lr 0.000198	time 0.4580 (0.4683)	loss 2.7298 (3.0651)	grad_norm 2.1337 (2.1319)	mem 14853MB
[2022-11-07 07:29:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][1200/1251]	eta 0:00:23 lr 0.000198	time 0.4572 (0.4682)	loss 3.7848 (3.0662)	grad_norm 2.1761 (2.1327)	mem 14853MB
[2022-11-07 07:29:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [213/300][1250/1251]	eta 0:00:00 lr 0.000198	time 0.4568 (0.4680)	loss 2.2596 (3.0729)	grad_norm 2.3545 (2.1324)	mem 14853MB
[2022-11-07 07:29:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 213 training takes 0:09:45
[2022-11-07 07:29:58 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_213.pth saving......
[2022-11-07 07:29:59 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_213.pth saved !!!
[2022-11-07 07:30:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.661 (1.661)	Loss 0.8145 (0.8145)	Acc@1 80.859 (80.859)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 07:30:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.438 Acc@5 95.414
[2022-11-07 07:30:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.4%
[2022-11-07 07:30:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.587 (1.587)	Loss 0.7834 (0.7834)	Acc@1 81.934 (81.934)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 07:30:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.680 Acc@5 95.954
[2022-11-07 07:30:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.7%
[2022-11-07 07:30:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.68% at 213 epoch
[2022-11-07 07:30:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][0/1251]	eta 0:39:37 lr 0.000198	time 1.9004 (1.9004)	loss 3.5358 (3.5358)	grad_norm 2.1253 (2.1253)	mem 14853MB
[2022-11-07 07:30:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][50/1251]	eta 0:10:01 lr 0.000197	time 0.5495 (0.5008)	loss 3.0037 (3.1185)	grad_norm 1.9497 (2.1268)	mem 14853MB
[2022-11-07 07:31:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][100/1251]	eta 0:09:19 lr 0.000197	time 0.4712 (0.4859)	loss 3.8156 (3.0870)	grad_norm 2.1654 (2.1133)	mem 14853MB
[2022-11-07 07:31:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][150/1251]	eta 0:08:47 lr 0.000197	time 0.4816 (0.4792)	loss 3.2741 (3.0759)	grad_norm 1.9408 (2.1186)	mem 14853MB
[2022-11-07 07:31:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][200/1251]	eta 0:08:19 lr 0.000197	time 0.4672 (0.4757)	loss 3.1984 (3.0744)	grad_norm 2.1945 (2.1098)	mem 14853MB
[2022-11-07 07:32:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][250/1251]	eta 0:07:54 lr 0.000197	time 0.4686 (0.4736)	loss 2.0528 (3.0831)	grad_norm 2.3053 (2.1168)	mem 14853MB
[2022-11-07 07:32:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][300/1251]	eta 0:07:28 lr 0.000197	time 0.4650 (0.4721)	loss 3.2582 (3.0951)	grad_norm 2.4407 (2.1215)	mem 14853MB
[2022-11-07 07:33:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][350/1251]	eta 0:07:04 lr 0.000196	time 0.4695 (0.4711)	loss 3.2986 (3.0739)	grad_norm 2.3824 (2.1326)	mem 14853MB
[2022-11-07 07:33:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][400/1251]	eta 0:06:40 lr 0.000196	time 0.4719 (0.4705)	loss 3.3202 (3.0833)	grad_norm 2.1593 (2.1375)	mem 14853MB
[2022-11-07 07:33:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][450/1251]	eta 0:06:16 lr 0.000196	time 0.4646 (0.4699)	loss 3.2595 (3.0825)	grad_norm 2.3250 (2.1383)	mem 14853MB
[2022-11-07 07:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][500/1251]	eta 0:05:52 lr 0.000196	time 0.4676 (0.4697)	loss 2.8408 (3.0802)	grad_norm 2.2191 (2.1398)	mem 14853MB
[2022-11-07 07:34:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][550/1251]	eta 0:05:29 lr 0.000196	time 0.5405 (0.4697)	loss 3.5671 (3.0885)	grad_norm 2.0299 (2.1433)	mem 14853MB
[2022-11-07 07:34:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][600/1251]	eta 0:05:05 lr 0.000196	time 0.4755 (0.4696)	loss 3.0318 (3.0986)	grad_norm 2.2457 (inf)	mem 14853MB
[2022-11-07 07:35:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][650/1251]	eta 0:04:42 lr 0.000195	time 0.4650 (0.4692)	loss 3.2877 (3.0937)	grad_norm 2.1667 (inf)	mem 14853MB
[2022-11-07 07:35:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][700/1251]	eta 0:04:18 lr 0.000195	time 0.4759 (0.4691)	loss 3.5643 (3.0843)	grad_norm 2.0273 (inf)	mem 14853MB
[2022-11-07 07:36:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][750/1251]	eta 0:03:54 lr 0.000195	time 0.4634 (0.4690)	loss 2.6648 (3.0876)	grad_norm 2.1810 (inf)	mem 14853MB
[2022-11-07 07:36:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][800/1251]	eta 0:03:31 lr 0.000195	time 0.4602 (0.4691)	loss 2.9308 (3.0891)	grad_norm 1.9607 (inf)	mem 14853MB
[2022-11-07 07:36:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][850/1251]	eta 0:03:07 lr 0.000195	time 0.4662 (0.4688)	loss 3.2184 (3.0904)	grad_norm 2.1387 (inf)	mem 14853MB
[2022-11-07 07:37:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][900/1251]	eta 0:02:44 lr 0.000195	time 0.4722 (0.4687)	loss 3.7934 (3.0951)	grad_norm 2.8531 (inf)	mem 14853MB
[2022-11-07 07:37:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][950/1251]	eta 0:02:20 lr 0.000194	time 0.4589 (0.4684)	loss 3.3841 (3.0947)	grad_norm 1.9370 (inf)	mem 14853MB
[2022-11-07 07:38:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][1000/1251]	eta 0:01:57 lr 0.000194	time 0.4624 (0.4685)	loss 2.7837 (3.0931)	grad_norm 1.9289 (inf)	mem 14853MB
[2022-11-07 07:38:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][1050/1251]	eta 0:01:34 lr 0.000194	time 0.5692 (0.4686)	loss 3.1427 (3.0979)	grad_norm 2.4779 (inf)	mem 14853MB
[2022-11-07 07:38:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][1100/1251]	eta 0:01:10 lr 0.000194	time 0.4629 (0.4686)	loss 2.3295 (3.0950)	grad_norm 1.9400 (inf)	mem 14853MB
[2022-11-07 07:39:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][1150/1251]	eta 0:00:47 lr 0.000194	time 0.4647 (0.4684)	loss 3.6920 (3.0967)	grad_norm 2.2142 (inf)	mem 14853MB
[2022-11-07 07:39:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][1200/1251]	eta 0:00:23 lr 0.000194	time 0.4598 (0.4682)	loss 3.2337 (3.0949)	grad_norm 2.0842 (inf)	mem 14853MB
[2022-11-07 07:40:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [214/300][1250/1251]	eta 0:00:00 lr 0.000193	time 0.4582 (0.4680)	loss 2.5871 (3.0952)	grad_norm 2.4286 (inf)	mem 14853MB
[2022-11-07 07:40:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 214 training takes 0:09:45
[2022-11-07 07:40:02 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_214.pth saving......
[2022-11-07 07:40:03 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_214.pth saved !!!
[2022-11-07 07:40:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.674 (1.674)	Loss 0.7781 (0.7781)	Acc@1 82.910 (82.910)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 07:40:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.602 Acc@5 95.446
[2022-11-07 07:40:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.6%
[2022-11-07 07:40:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.565 (1.565)	Loss 0.7580 (0.7580)	Acc@1 81.738 (81.738)	Acc@5 96.289 (96.289)	Mem 14853MB
[2022-11-07 07:40:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.698 Acc@5 95.944
[2022-11-07 07:40:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.7%
[2022-11-07 07:40:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.70% at 214 epoch
[2022-11-07 07:40:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][0/1251]	eta 0:40:40 lr 0.000193	time 1.9511 (1.9511)	loss 2.3304 (2.3304)	grad_norm 2.2043 (2.2043)	mem 14853MB
[2022-11-07 07:40:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][50/1251]	eta 0:10:01 lr 0.000193	time 0.4626 (0.5012)	loss 3.1018 (3.2118)	grad_norm 1.8642 (2.2127)	mem 14853MB
[2022-11-07 07:41:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][100/1251]	eta 0:09:20 lr 0.000193	time 0.4626 (0.4869)	loss 3.3580 (3.0935)	grad_norm 2.3220 (2.1489)	mem 14853MB
[2022-11-07 07:41:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][150/1251]	eta 0:08:48 lr 0.000193	time 0.4825 (0.4799)	loss 2.6377 (3.0772)	grad_norm 1.9557 (2.1472)	mem 14853MB
[2022-11-07 07:41:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][200/1251]	eta 0:08:20 lr 0.000193	time 0.4546 (0.4762)	loss 3.6927 (3.0712)	grad_norm 2.3213 (2.1480)	mem 14853MB
[2022-11-07 07:42:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][250/1251]	eta 0:07:54 lr 0.000193	time 0.4675 (0.4744)	loss 3.1644 (3.0571)	grad_norm 1.8648 (2.1449)	mem 14853MB
[2022-11-07 07:42:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][300/1251]	eta 0:07:29 lr 0.000193	time 0.4699 (0.4729)	loss 3.4160 (3.0684)	grad_norm 2.1363 (2.1429)	mem 14853MB
[2022-11-07 07:43:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][350/1251]	eta 0:07:05 lr 0.000192	time 0.4579 (0.4717)	loss 1.9758 (3.0695)	grad_norm 2.0263 (2.1532)	mem 14853MB
[2022-11-07 07:43:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][400/1251]	eta 0:06:40 lr 0.000192	time 0.4637 (0.4709)	loss 3.0949 (3.0835)	grad_norm 2.2189 (2.1476)	mem 14853MB
[2022-11-07 07:43:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][450/1251]	eta 0:06:16 lr 0.000192	time 0.4660 (0.4704)	loss 3.3183 (3.0913)	grad_norm 1.9976 (2.1437)	mem 14853MB
[2022-11-07 07:44:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][500/1251]	eta 0:05:53 lr 0.000192	time 0.4564 (0.4702)	loss 3.2162 (3.0865)	grad_norm 2.0539 (2.1419)	mem 14853MB
[2022-11-07 07:44:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][550/1251]	eta 0:05:29 lr 0.000192	time 0.4568 (0.4701)	loss 2.3916 (3.0817)	grad_norm 2.2589 (2.1487)	mem 14853MB
[2022-11-07 07:45:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][600/1251]	eta 0:05:05 lr 0.000192	time 0.4667 (0.4698)	loss 2.1171 (3.0854)	grad_norm 2.2711 (2.1535)	mem 14853MB
[2022-11-07 07:45:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][650/1251]	eta 0:04:42 lr 0.000191	time 0.4849 (0.4694)	loss 3.7910 (3.0886)	grad_norm 2.0827 (2.1545)	mem 14853MB
[2022-11-07 07:45:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][700/1251]	eta 0:04:18 lr 0.000191	time 0.4703 (0.4692)	loss 2.1319 (3.0816)	grad_norm 2.3695 (2.1520)	mem 14853MB
[2022-11-07 07:46:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][750/1251]	eta 0:03:55 lr 0.000191	time 0.4569 (0.4691)	loss 3.4872 (3.0849)	grad_norm 2.0809 (2.1507)	mem 14853MB
[2022-11-07 07:46:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][800/1251]	eta 0:03:31 lr 0.000191	time 0.4668 (0.4691)	loss 3.4031 (3.0801)	grad_norm 1.9610 (2.1483)	mem 14853MB
[2022-11-07 07:46:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][850/1251]	eta 0:03:08 lr 0.000191	time 0.4680 (0.4690)	loss 2.9946 (3.0782)	grad_norm 2.3760 (2.1516)	mem 14853MB
[2022-11-07 07:47:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][900/1251]	eta 0:02:44 lr 0.000191	time 0.4685 (0.4688)	loss 3.2493 (3.0812)	grad_norm 2.0675 (2.1489)	mem 14853MB
[2022-11-07 07:47:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][950/1251]	eta 0:02:21 lr 0.000190	time 0.4791 (0.4687)	loss 3.7057 (3.0791)	grad_norm 2.4910 (2.1479)	mem 14853MB
[2022-11-07 07:48:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][1000/1251]	eta 0:01:57 lr 0.000190	time 0.4713 (0.4686)	loss 3.4496 (3.0772)	grad_norm 2.1938 (2.1446)	mem 14853MB
[2022-11-07 07:48:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][1050/1251]	eta 0:01:34 lr 0.000190	time 0.4590 (0.4686)	loss 3.1284 (3.0809)	grad_norm 2.3221 (2.1471)	mem 14853MB
[2022-11-07 07:48:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][1100/1251]	eta 0:01:10 lr 0.000190	time 0.4623 (0.4685)	loss 2.0175 (3.0803)	grad_norm 2.4436 (2.1494)	mem 14853MB
[2022-11-07 07:49:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][1150/1251]	eta 0:00:47 lr 0.000190	time 0.4611 (0.4684)	loss 3.2211 (3.0807)	grad_norm 2.0936 (2.1478)	mem 14853MB
[2022-11-07 07:49:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][1200/1251]	eta 0:00:23 lr 0.000190	time 0.4671 (0.4683)	loss 3.6984 (3.0776)	grad_norm 2.3012 (2.1484)	mem 14853MB
[2022-11-07 07:50:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [215/300][1250/1251]	eta 0:00:00 lr 0.000189	time 0.4557 (0.4681)	loss 2.7363 (3.0775)	grad_norm 1.9209 (2.1470)	mem 14853MB
[2022-11-07 07:50:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 215 training takes 0:09:45
[2022-11-07 07:50:06 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_215.pth saving......
[2022-11-07 07:50:07 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_215.pth saved !!!
[2022-11-07 07:50:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.567 (1.567)	Loss 0.8396 (0.8396)	Acc@1 80.859 (80.859)	Acc@5 95.215 (95.215)	Mem 14853MB
[2022-11-07 07:50:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.542 Acc@5 95.442
[2022-11-07 07:50:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.5%
[2022-11-07 07:50:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.636 (1.636)	Loss 0.7666 (0.7666)	Acc@1 81.348 (81.348)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 07:50:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.750 Acc@5 95.964
[2022-11-07 07:50:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.8%
[2022-11-07 07:50:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.75% at 215 epoch
[2022-11-07 07:50:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][0/1251]	eta 0:40:10 lr 0.000189	time 1.9266 (1.9266)	loss 3.0976 (3.0976)	grad_norm 1.9557 (1.9557)	mem 14853MB
[2022-11-07 07:50:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][50/1251]	eta 0:10:02 lr 0.000189	time 0.4586 (0.5016)	loss 3.3213 (3.0934)	grad_norm 2.0098 (2.1191)	mem 14853MB
[2022-11-07 07:51:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][100/1251]	eta 0:09:18 lr 0.000189	time 0.4637 (0.4853)	loss 3.5220 (3.0726)	grad_norm 2.6727 (2.1561)	mem 14853MB
[2022-11-07 07:51:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][150/1251]	eta 0:08:47 lr 0.000189	time 0.4581 (0.4792)	loss 2.9627 (3.0240)	grad_norm 1.9455 (2.1423)	mem 14853MB
[2022-11-07 07:52:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][200/1251]	eta 0:08:19 lr 0.000189	time 0.4799 (0.4757)	loss 3.0919 (3.0518)	grad_norm 2.3222 (2.1383)	mem 14853MB
[2022-11-07 07:52:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][250/1251]	eta 0:07:53 lr 0.000189	time 0.4676 (0.4735)	loss 3.2483 (3.0292)	grad_norm 2.2193 (2.1381)	mem 14853MB
[2022-11-07 07:52:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][300/1251]	eta 0:07:28 lr 0.000189	time 0.4620 (0.4720)	loss 3.4315 (3.0404)	grad_norm 1.8930 (inf)	mem 14853MB
[2022-11-07 07:53:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][350/1251]	eta 0:07:04 lr 0.000188	time 0.4848 (0.4713)	loss 2.5391 (3.0329)	grad_norm 2.1869 (inf)	mem 14853MB
[2022-11-07 07:53:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][400/1251]	eta 0:06:40 lr 0.000188	time 0.4674 (0.4705)	loss 3.3449 (3.0451)	grad_norm 2.0401 (inf)	mem 14853MB
[2022-11-07 07:53:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][450/1251]	eta 0:06:16 lr 0.000188	time 0.4659 (0.4700)	loss 3.5454 (3.0398)	grad_norm 2.1916 (inf)	mem 14853MB
[2022-11-07 07:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][500/1251]	eta 0:05:52 lr 0.000188	time 0.4680 (0.4695)	loss 2.9642 (3.0583)	grad_norm 1.8293 (inf)	mem 14853MB
[2022-11-07 07:54:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][550/1251]	eta 0:05:29 lr 0.000188	time 0.4697 (0.4696)	loss 3.1845 (3.0468)	grad_norm 2.2870 (inf)	mem 14853MB
[2022-11-07 07:55:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][600/1251]	eta 0:05:05 lr 0.000188	time 0.4624 (0.4693)	loss 2.8578 (3.0339)	grad_norm 1.9916 (inf)	mem 14853MB
[2022-11-07 07:55:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][650/1251]	eta 0:04:41 lr 0.000187	time 0.4598 (0.4691)	loss 3.3472 (3.0334)	grad_norm 2.2998 (inf)	mem 14853MB
[2022-11-07 07:55:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][700/1251]	eta 0:04:18 lr 0.000187	time 0.4615 (0.4690)	loss 3.1660 (3.0322)	grad_norm 2.1459 (inf)	mem 14853MB
[2022-11-07 07:56:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][750/1251]	eta 0:03:54 lr 0.000187	time 0.4641 (0.4689)	loss 3.2066 (3.0298)	grad_norm 2.1416 (inf)	mem 14853MB
[2022-11-07 07:56:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][800/1251]	eta 0:03:31 lr 0.000187	time 0.4753 (0.4689)	loss 2.9616 (3.0286)	grad_norm 2.6606 (inf)	mem 14853MB
[2022-11-07 07:57:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][850/1251]	eta 0:03:07 lr 0.000187	time 0.4604 (0.4687)	loss 2.3226 (3.0264)	grad_norm 2.1934 (inf)	mem 14853MB
[2022-11-07 07:57:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][900/1251]	eta 0:02:44 lr 0.000187	time 0.4615 (0.4684)	loss 3.1643 (3.0348)	grad_norm 2.3394 (inf)	mem 14853MB
[2022-11-07 07:57:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][950/1251]	eta 0:02:20 lr 0.000186	time 0.4672 (0.4683)	loss 3.4782 (3.0409)	grad_norm 2.3144 (inf)	mem 14853MB
[2022-11-07 07:58:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][1000/1251]	eta 0:01:57 lr 0.000186	time 0.4629 (0.4683)	loss 2.9063 (3.0383)	grad_norm 1.9559 (inf)	mem 14853MB
[2022-11-07 07:58:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][1050/1251]	eta 0:01:34 lr 0.000186	time 0.4628 (0.4683)	loss 3.5859 (3.0418)	grad_norm 2.3143 (inf)	mem 14853MB
[2022-11-07 07:59:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][1100/1251]	eta 0:01:10 lr 0.000186	time 0.4751 (0.4683)	loss 3.5254 (3.0504)	grad_norm 2.0422 (inf)	mem 14853MB
[2022-11-07 07:59:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][1150/1251]	eta 0:00:47 lr 0.000186	time 0.4600 (0.4682)	loss 2.5992 (3.0516)	grad_norm 2.2268 (inf)	mem 14853MB
[2022-11-07 07:59:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][1200/1251]	eta 0:00:23 lr 0.000186	time 0.4564 (0.4681)	loss 3.5257 (3.0520)	grad_norm 2.2004 (inf)	mem 14853MB
[2022-11-07 08:00:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [216/300][1250/1251]	eta 0:00:00 lr 0.000186	time 0.4572 (0.4679)	loss 3.2892 (3.0557)	grad_norm 2.0493 (inf)	mem 14853MB
[2022-11-07 08:00:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 216 training takes 0:09:45
[2022-11-07 08:00:10 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_216.pth saving......
[2022-11-07 08:00:11 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_216.pth saved !!!
[2022-11-07 08:00:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.676 (1.676)	Loss 0.7103 (0.7103)	Acc@1 83.203 (83.203)	Acc@5 97.070 (97.070)	Mem 14853MB
[2022-11-07 08:00:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.662 Acc@5 95.482
[2022-11-07 08:00:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.7%
[2022-11-07 08:00:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.600 (1.600)	Loss 0.8436 (0.8436)	Acc@1 80.273 (80.273)	Acc@5 94.531 (94.531)	Mem 14853MB
[2022-11-07 08:00:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.736 Acc@5 95.944
[2022-11-07 08:00:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.7%
[2022-11-07 08:00:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.75% at 215 epoch
[2022-11-07 08:00:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][0/1251]	eta 0:40:36 lr 0.000185	time 1.9477 (1.9477)	loss 2.8931 (2.8931)	grad_norm 2.2021 (2.2021)	mem 14853MB
[2022-11-07 08:00:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][50/1251]	eta 0:09:58 lr 0.000185	time 0.4629 (0.4985)	loss 3.1508 (3.0137)	grad_norm 2.0432 (2.1440)	mem 14853MB
[2022-11-07 08:01:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][100/1251]	eta 0:09:17 lr 0.000185	time 0.4647 (0.4843)	loss 2.9944 (3.0502)	grad_norm 1.9730 (2.1545)	mem 14853MB
[2022-11-07 08:01:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][150/1251]	eta 0:08:46 lr 0.000185	time 0.4803 (0.4783)	loss 2.4167 (3.0391)	grad_norm 2.3121 (2.1640)	mem 14853MB
[2022-11-07 08:02:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][200/1251]	eta 0:08:19 lr 0.000185	time 0.4673 (0.4749)	loss 3.3654 (3.0471)	grad_norm 2.3513 (2.1703)	mem 14853MB
[2022-11-07 08:02:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][250/1251]	eta 0:07:53 lr 0.000185	time 0.4740 (0.4729)	loss 3.1082 (3.0378)	grad_norm 1.9216 (2.1844)	mem 14853MB
[2022-11-07 08:02:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][300/1251]	eta 0:07:28 lr 0.000185	time 0.4559 (0.4721)	loss 2.9264 (3.0276)	grad_norm 1.9669 (2.1828)	mem 14853MB
[2022-11-07 08:03:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][350/1251]	eta 0:07:04 lr 0.000184	time 0.4619 (0.4711)	loss 3.3472 (3.0465)	grad_norm 2.0830 (2.1840)	mem 14853MB
[2022-11-07 08:03:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][400/1251]	eta 0:06:40 lr 0.000184	time 0.4716 (0.4703)	loss 3.5053 (3.0460)	grad_norm 1.8117 (2.1835)	mem 14853MB
[2022-11-07 08:04:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][450/1251]	eta 0:06:16 lr 0.000184	time 0.4622 (0.4698)	loss 2.7132 (3.0432)	grad_norm 2.2885 (2.1891)	mem 14853MB
[2022-11-07 08:04:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][500/1251]	eta 0:05:52 lr 0.000184	time 0.4738 (0.4694)	loss 2.6114 (3.0521)	grad_norm 2.3476 (2.1907)	mem 14853MB
[2022-11-07 08:04:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][550/1251]	eta 0:05:29 lr 0.000184	time 0.4690 (0.4695)	loss 3.0826 (3.0531)	grad_norm 2.3435 (2.1902)	mem 14853MB
[2022-11-07 08:05:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][600/1251]	eta 0:05:05 lr 0.000184	time 0.4590 (0.4690)	loss 2.6935 (3.0579)	grad_norm 2.1778 (2.1936)	mem 14853MB
[2022-11-07 08:05:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][650/1251]	eta 0:04:41 lr 0.000183	time 0.4679 (0.4687)	loss 3.4192 (3.0603)	grad_norm 2.1751 (2.1945)	mem 14853MB
[2022-11-07 08:05:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][700/1251]	eta 0:04:18 lr 0.000183	time 0.4669 (0.4684)	loss 3.3779 (3.0565)	grad_norm 2.2439 (2.1898)	mem 14853MB
[2022-11-07 08:06:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][750/1251]	eta 0:03:54 lr 0.000183	time 0.4575 (0.4683)	loss 2.9078 (3.0523)	grad_norm 2.1156 (2.1882)	mem 14853MB
[2022-11-07 08:06:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][800/1251]	eta 0:03:31 lr 0.000183	time 0.4712 (0.4684)	loss 2.4534 (3.0553)	grad_norm 2.2209 (2.1847)	mem 14853MB
[2022-11-07 08:07:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][850/1251]	eta 0:03:07 lr 0.000183	time 0.4654 (0.4683)	loss 3.7468 (3.0496)	grad_norm 2.3380 (2.1857)	mem 14853MB
[2022-11-07 08:07:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][900/1251]	eta 0:02:44 lr 0.000183	time 0.4746 (0.4681)	loss 3.1427 (3.0494)	grad_norm 2.2497 (2.1832)	mem 14853MB
[2022-11-07 08:07:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][950/1251]	eta 0:02:20 lr 0.000183	time 0.4665 (0.4680)	loss 2.8662 (3.0503)	grad_norm 2.1754 (2.1828)	mem 14853MB
[2022-11-07 08:08:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][1000/1251]	eta 0:01:57 lr 0.000182	time 0.4584 (0.4679)	loss 3.3472 (3.0477)	grad_norm 2.2206 (2.1839)	mem 14853MB
[2022-11-07 08:08:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][1050/1251]	eta 0:01:34 lr 0.000182	time 0.4611 (0.4681)	loss 2.7303 (3.0437)	grad_norm 2.3535 (2.1843)	mem 14853MB
[2022-11-07 08:09:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][1100/1251]	eta 0:01:10 lr 0.000182	time 0.4531 (0.4680)	loss 3.0044 (3.0369)	grad_norm 1.9244 (2.1865)	mem 14853MB
[2022-11-07 08:09:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][1150/1251]	eta 0:00:47 lr 0.000182	time 0.4644 (0.4679)	loss 2.9727 (3.0377)	grad_norm 2.1385 (2.1871)	mem 14853MB
[2022-11-07 08:09:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][1200/1251]	eta 0:00:23 lr 0.000182	time 0.4634 (0.4677)	loss 3.0798 (3.0372)	grad_norm 2.2521 (inf)	mem 14853MB
[2022-11-07 08:10:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [217/300][1250/1251]	eta 0:00:00 lr 0.000182	time 0.4563 (0.4676)	loss 3.2814 (3.0409)	grad_norm 2.2002 (inf)	mem 14853MB
[2022-11-07 08:10:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 217 training takes 0:09:45
[2022-11-07 08:10:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_217.pth saving......
[2022-11-07 08:10:14 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_217.pth saved !!!
[2022-11-07 08:10:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.547 (1.547)	Loss 0.8255 (0.8255)	Acc@1 79.883 (79.883)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 08:10:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.630 Acc@5 95.522
[2022-11-07 08:10:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.6%
[2022-11-07 08:10:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.637 (1.637)	Loss 0.7905 (0.7905)	Acc@1 81.055 (81.055)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 08:10:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.804 Acc@5 95.968
[2022-11-07 08:10:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.8%
[2022-11-07 08:10:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.80% at 217 epoch
[2022-11-07 08:10:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][0/1251]	eta 0:40:43 lr 0.000182	time 1.9529 (1.9529)	loss 3.1042 (3.1042)	grad_norm 2.0593 (2.0593)	mem 14853MB
[2022-11-07 08:10:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][50/1251]	eta 0:10:00 lr 0.000181	time 0.4573 (0.4999)	loss 2.6205 (3.1221)	grad_norm 2.1827 (2.2689)	mem 14853MB
[2022-11-07 08:11:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][100/1251]	eta 0:09:17 lr 0.000181	time 0.4769 (0.4844)	loss 2.0538 (3.0577)	grad_norm 1.9023 (2.2450)	mem 14853MB
[2022-11-07 08:11:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][150/1251]	eta 0:08:46 lr 0.000181	time 0.4700 (0.4783)	loss 3.4514 (3.0645)	grad_norm 1.9485 (2.2263)	mem 14853MB
[2022-11-07 08:12:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][200/1251]	eta 0:08:20 lr 0.000181	time 0.4633 (0.4760)	loss 3.0486 (3.0802)	grad_norm 1.9202 (2.2125)	mem 14853MB
[2022-11-07 08:12:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][250/1251]	eta 0:07:55 lr 0.000181	time 0.4598 (0.4750)	loss 3.5724 (3.0799)	grad_norm 2.2032 (2.2134)	mem 14853MB
[2022-11-07 08:12:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][300/1251]	eta 0:07:30 lr 0.000181	time 0.4610 (0.4734)	loss 3.3317 (3.0556)	grad_norm 2.1572 (2.1937)	mem 14853MB
[2022-11-07 08:13:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][350/1251]	eta 0:07:05 lr 0.000180	time 0.4722 (0.4723)	loss 2.7322 (3.0589)	grad_norm 2.0857 (inf)	mem 14853MB
[2022-11-07 08:13:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][400/1251]	eta 0:06:41 lr 0.000180	time 0.4589 (0.4715)	loss 3.3639 (3.0599)	grad_norm 2.1448 (inf)	mem 14853MB
[2022-11-07 08:14:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][450/1251]	eta 0:06:17 lr 0.000180	time 0.4751 (0.4709)	loss 3.4263 (3.0603)	grad_norm 2.3862 (inf)	mem 14853MB
[2022-11-07 08:14:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][500/1251]	eta 0:05:53 lr 0.000180	time 0.4701 (0.4706)	loss 2.8539 (3.0636)	grad_norm 2.3104 (inf)	mem 14853MB
[2022-11-07 08:14:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][550/1251]	eta 0:05:29 lr 0.000180	time 0.4695 (0.4704)	loss 2.8301 (3.0670)	grad_norm 2.1478 (inf)	mem 14853MB
[2022-11-07 08:15:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][600/1251]	eta 0:05:05 lr 0.000180	time 0.4624 (0.4700)	loss 3.6884 (3.0673)	grad_norm 2.4759 (inf)	mem 14853MB
[2022-11-07 08:15:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][650/1251]	eta 0:04:42 lr 0.000180	time 0.4593 (0.4696)	loss 3.4784 (3.0660)	grad_norm 2.6551 (inf)	mem 14853MB
[2022-11-07 08:16:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][700/1251]	eta 0:04:18 lr 0.000179	time 0.4579 (0.4695)	loss 3.1159 (3.0646)	grad_norm 2.0826 (inf)	mem 14853MB
[2022-11-07 08:16:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][750/1251]	eta 0:03:55 lr 0.000179	time 0.4672 (0.4693)	loss 2.1682 (3.0605)	grad_norm 1.9816 (inf)	mem 14853MB
[2022-11-07 08:16:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][800/1251]	eta 0:03:31 lr 0.000179	time 0.4622 (0.4691)	loss 3.2076 (3.0600)	grad_norm 2.3564 (inf)	mem 14853MB
[2022-11-07 08:17:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][850/1251]	eta 0:03:08 lr 0.000179	time 0.4786 (0.4691)	loss 3.0104 (3.0658)	grad_norm 2.1473 (inf)	mem 14853MB
[2022-11-07 08:17:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][900/1251]	eta 0:02:44 lr 0.000179	time 0.5332 (0.4690)	loss 3.2966 (3.0619)	grad_norm 2.2457 (inf)	mem 14853MB
[2022-11-07 08:17:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][950/1251]	eta 0:02:21 lr 0.000179	time 0.4752 (0.4689)	loss 2.0605 (3.0607)	grad_norm 2.3129 (inf)	mem 14853MB
[2022-11-07 08:18:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][1000/1251]	eta 0:01:57 lr 0.000178	time 0.5512 (0.4688)	loss 3.0180 (3.0589)	grad_norm 2.0881 (inf)	mem 14853MB
[2022-11-07 08:18:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][1050/1251]	eta 0:01:34 lr 0.000178	time 0.4709 (0.4687)	loss 3.2930 (3.0524)	grad_norm 2.2320 (inf)	mem 14853MB
[2022-11-07 08:19:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][1100/1251]	eta 0:01:10 lr 0.000178	time 0.4600 (0.4686)	loss 2.3839 (3.0571)	grad_norm 2.9472 (inf)	mem 14853MB
[2022-11-07 08:19:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][1150/1251]	eta 0:00:47 lr 0.000178	time 0.4528 (0.4685)	loss 3.3406 (3.0579)	grad_norm 2.2984 (inf)	mem 14853MB
[2022-11-07 08:19:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][1200/1251]	eta 0:00:23 lr 0.000178	time 0.4698 (0.4685)	loss 3.5322 (3.0602)	grad_norm 1.8825 (inf)	mem 14853MB
[2022-11-07 08:20:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [218/300][1250/1251]	eta 0:00:00 lr 0.000178	time 0.4593 (0.4683)	loss 3.0142 (3.0584)	grad_norm 2.1840 (inf)	mem 14853MB
[2022-11-07 08:20:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 218 training takes 0:09:46
[2022-11-07 08:20:18 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_218.pth saving......
[2022-11-07 08:20:18 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_218.pth saved !!!
[2022-11-07 08:20:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.534 (1.534)	Loss 0.8699 (0.8699)	Acc@1 79.004 (79.004)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-07 08:20:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.706 Acc@5 95.462
[2022-11-07 08:20:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.7%
[2022-11-07 08:20:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.665 (1.665)	Loss 0.7323 (0.7323)	Acc@1 83.398 (83.398)	Acc@5 96.484 (96.484)	Mem 14853MB
[2022-11-07 08:20:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.776 Acc@5 95.960
[2022-11-07 08:20:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.8%
[2022-11-07 08:20:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.80% at 217 epoch
[2022-11-07 08:20:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][0/1251]	eta 0:44:38 lr 0.000178	time 2.1407 (2.1407)	loss 2.9213 (2.9213)	grad_norm 1.9540 (1.9540)	mem 14853MB
[2022-11-07 08:21:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][50/1251]	eta 0:10:08 lr 0.000177	time 0.4547 (0.5063)	loss 3.1553 (3.0596)	grad_norm 2.3128 (2.2683)	mem 14853MB
[2022-11-07 08:21:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][100/1251]	eta 0:09:24 lr 0.000177	time 0.4602 (0.4900)	loss 2.2651 (3.0308)	grad_norm 2.2344 (2.2379)	mem 14853MB
[2022-11-07 08:21:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][150/1251]	eta 0:08:51 lr 0.000177	time 0.4691 (0.4823)	loss 2.4761 (3.0497)	grad_norm 2.3026 (2.2105)	mem 14853MB
[2022-11-07 08:22:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][200/1251]	eta 0:08:22 lr 0.000177	time 0.4693 (0.4782)	loss 3.4784 (3.0495)	grad_norm 2.6736 (2.2231)	mem 14853MB
[2022-11-07 08:22:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][250/1251]	eta 0:07:56 lr 0.000177	time 0.4657 (0.4757)	loss 3.5304 (3.0621)	grad_norm 2.9039 (2.2344)	mem 14853MB
[2022-11-07 08:22:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][300/1251]	eta 0:07:30 lr 0.000177	time 0.4615 (0.4739)	loss 2.6438 (3.0580)	grad_norm 1.9858 (2.2285)	mem 14853MB
[2022-11-07 08:23:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][350/1251]	eta 0:07:05 lr 0.000177	time 0.4724 (0.4727)	loss 3.5079 (3.0482)	grad_norm 1.8909 (2.2309)	mem 14853MB
[2022-11-07 08:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][400/1251]	eta 0:06:41 lr 0.000176	time 0.4591 (0.4721)	loss 3.5136 (3.0521)	grad_norm 2.2023 (2.2222)	mem 14853MB
[2022-11-07 08:24:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][450/1251]	eta 0:06:17 lr 0.000176	time 0.4600 (0.4714)	loss 2.2727 (3.0463)	grad_norm 2.0830 (2.2193)	mem 14853MB
[2022-11-07 08:24:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][500/1251]	eta 0:05:53 lr 0.000176	time 0.4640 (0.4710)	loss 1.9566 (3.0429)	grad_norm 2.2464 (2.2171)	mem 14853MB
[2022-11-07 08:24:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][550/1251]	eta 0:05:29 lr 0.000176	time 0.4664 (0.4706)	loss 2.5959 (3.0434)	grad_norm 1.9547 (2.2174)	mem 14853MB
[2022-11-07 08:25:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][600/1251]	eta 0:05:06 lr 0.000176	time 0.4623 (0.4703)	loss 3.1891 (3.0480)	grad_norm 1.9457 (2.2182)	mem 14853MB
[2022-11-07 08:25:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][650/1251]	eta 0:04:42 lr 0.000176	time 0.4652 (0.4700)	loss 3.4232 (3.0405)	grad_norm 2.5046 (2.2170)	mem 14853MB
[2022-11-07 08:26:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][700/1251]	eta 0:04:18 lr 0.000175	time 0.4750 (0.4697)	loss 3.0031 (3.0463)	grad_norm 2.1657 (2.2166)	mem 14853MB
[2022-11-07 08:26:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][750/1251]	eta 0:03:55 lr 0.000175	time 0.4772 (0.4697)	loss 3.7641 (3.0452)	grad_norm 2.9458 (2.2151)	mem 14853MB
[2022-11-07 08:26:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][800/1251]	eta 0:03:31 lr 0.000175	time 0.4648 (0.4695)	loss 2.8235 (3.0455)	grad_norm 1.8008 (2.2165)	mem 14853MB
[2022-11-07 08:27:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][850/1251]	eta 0:03:08 lr 0.000175	time 0.4622 (0.4694)	loss 3.4645 (3.0472)	grad_norm 2.2084 (2.2174)	mem 14853MB
[2022-11-07 08:27:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][900/1251]	eta 0:02:44 lr 0.000175	time 0.4615 (0.4693)	loss 3.2902 (3.0490)	grad_norm 1.9685 (2.2173)	mem 14853MB
[2022-11-07 08:28:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][950/1251]	eta 0:02:21 lr 0.000175	time 0.4641 (0.4691)	loss 3.7600 (3.0519)	grad_norm 2.1745 (2.2161)	mem 14853MB
[2022-11-07 08:28:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][1000/1251]	eta 0:01:57 lr 0.000175	time 0.4527 (0.4692)	loss 2.9331 (3.0487)	grad_norm 2.6937 (2.2192)	mem 14853MB
[2022-11-07 08:28:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][1050/1251]	eta 0:01:34 lr 0.000174	time 0.4635 (0.4690)	loss 3.1124 (3.0453)	grad_norm 2.2394 (2.2139)	mem 14853MB
[2022-11-07 08:29:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][1100/1251]	eta 0:01:10 lr 0.000174	time 0.4742 (0.4689)	loss 2.5910 (3.0475)	grad_norm 2.2862 (2.2147)	mem 14853MB
[2022-11-07 08:29:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][1150/1251]	eta 0:00:47 lr 0.000174	time 0.4661 (0.4689)	loss 3.3031 (3.0463)	grad_norm 2.0620 (2.2125)	mem 14853MB
[2022-11-07 08:29:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][1200/1251]	eta 0:00:23 lr 0.000174	time 0.4619 (0.4688)	loss 2.6605 (3.0411)	grad_norm 2.1806 (2.2161)	mem 14853MB
[2022-11-07 08:30:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [219/300][1250/1251]	eta 0:00:00 lr 0.000174	time 0.4573 (0.4686)	loss 3.0257 (3.0425)	grad_norm 2.4578 (2.2155)	mem 14853MB
[2022-11-07 08:30:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 219 training takes 0:09:46
[2022-11-07 08:30:22 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_219.pth saving......
[2022-11-07 08:30:23 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_219.pth saved !!!
[2022-11-07 08:30:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.637 (1.637)	Loss 0.8793 (0.8793)	Acc@1 79.688 (79.688)	Acc@5 94.922 (94.922)	Mem 14853MB
[2022-11-07 08:30:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.560 Acc@5 95.536
[2022-11-07 08:30:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.6%
[2022-11-07 08:30:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.652 (1.652)	Loss 0.7397 (0.7397)	Acc@1 82.324 (82.324)	Acc@5 96.484 (96.484)	Mem 14853MB
[2022-11-07 08:30:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.800 Acc@5 95.988
[2022-11-07 08:30:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.8%
[2022-11-07 08:30:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.80% at 217 epoch
[2022-11-07 08:30:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][0/1251]	eta 0:44:44 lr 0.000174	time 2.1456 (2.1456)	loss 3.1829 (3.1829)	grad_norm 2.4918 (2.4918)	mem 14853MB
[2022-11-07 08:31:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][50/1251]	eta 0:10:09 lr 0.000174	time 0.4631 (0.5074)	loss 3.5800 (3.0397)	grad_norm 2.4477 (2.2577)	mem 14853MB
[2022-11-07 08:31:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][100/1251]	eta 0:09:23 lr 0.000173	time 0.4529 (0.4894)	loss 3.3793 (3.0459)	grad_norm 2.2471 (2.2180)	mem 14853MB
[2022-11-07 08:31:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][150/1251]	eta 0:08:51 lr 0.000173	time 0.4525 (0.4824)	loss 3.5797 (3.0613)	grad_norm 2.2477 (2.2148)	mem 14853MB
[2022-11-07 08:32:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][200/1251]	eta 0:08:23 lr 0.000173	time 0.4856 (0.4791)	loss 3.1524 (3.0664)	grad_norm 2.0092 (2.2240)	mem 14853MB
[2022-11-07 08:32:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][250/1251]	eta 0:07:57 lr 0.000173	time 0.4604 (0.4772)	loss 3.0959 (3.0709)	grad_norm 2.0366 (2.2235)	mem 14853MB
[2022-11-07 08:33:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][300/1251]	eta 0:07:32 lr 0.000173	time 0.4804 (0.4755)	loss 2.8883 (3.0642)	grad_norm 2.1864 (2.2231)	mem 14853MB
[2022-11-07 08:33:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][350/1251]	eta 0:07:07 lr 0.000173	time 0.4710 (0.4744)	loss 3.2387 (3.0525)	grad_norm 2.1913 (2.2192)	mem 14853MB
[2022-11-07 08:33:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][400/1251]	eta 0:06:42 lr 0.000173	time 0.4664 (0.4734)	loss 2.2028 (3.0532)	grad_norm 2.6971 (2.2206)	mem 14853MB
[2022-11-07 08:34:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][450/1251]	eta 0:06:18 lr 0.000172	time 0.4667 (0.4727)	loss 2.3247 (3.0439)	grad_norm 1.9774 (2.2198)	mem 14853MB
[2022-11-07 08:34:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][500/1251]	eta 0:05:54 lr 0.000172	time 0.4629 (0.4723)	loss 3.1287 (3.0470)	grad_norm 2.0559 (2.2225)	mem 14853MB
[2022-11-07 08:35:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][550/1251]	eta 0:05:31 lr 0.000172	time 0.4899 (0.4723)	loss 2.8645 (3.0450)	grad_norm 1.9603 (2.2242)	mem 14853MB
[2022-11-07 08:35:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][600/1251]	eta 0:05:07 lr 0.000172	time 0.4700 (0.4722)	loss 2.8497 (3.0408)	grad_norm 1.8856 (2.2211)	mem 14853MB
[2022-11-07 08:35:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][650/1251]	eta 0:04:43 lr 0.000172	time 0.4662 (0.4718)	loss 2.9807 (3.0468)	grad_norm 2.4086 (2.2259)	mem 14853MB
[2022-11-07 08:36:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][700/1251]	eta 0:04:19 lr 0.000172	time 0.4677 (0.4714)	loss 3.4863 (3.0506)	grad_norm 2.3844 (2.2233)	mem 14853MB
[2022-11-07 08:36:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][750/1251]	eta 0:03:56 lr 0.000171	time 0.4603 (0.4713)	loss 3.4905 (3.0511)	grad_norm 2.2917 (2.2209)	mem 14853MB
[2022-11-07 08:36:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][800/1251]	eta 0:03:32 lr 0.000171	time 0.4785 (0.4712)	loss 2.7130 (3.0489)	grad_norm 2.0790 (2.2189)	mem 14853MB
[2022-11-07 08:37:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][850/1251]	eta 0:03:08 lr 0.000171	time 0.4694 (0.4711)	loss 3.2435 (3.0489)	grad_norm 2.1734 (2.2197)	mem 14853MB
[2022-11-07 08:37:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][900/1251]	eta 0:02:45 lr 0.000171	time 0.5444 (0.4709)	loss 2.5670 (3.0502)	grad_norm 2.4376 (2.2227)	mem 14853MB
[2022-11-07 08:38:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][950/1251]	eta 0:02:21 lr 0.000171	time 0.4658 (0.4707)	loss 2.9364 (3.0519)	grad_norm 2.4644 (2.2246)	mem 14853MB
[2022-11-07 08:38:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][1000/1251]	eta 0:01:58 lr 0.000171	time 0.4555 (0.4706)	loss 2.2271 (3.0570)	grad_norm 2.6009 (2.2250)	mem 14853MB
[2022-11-07 08:38:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][1050/1251]	eta 0:01:34 lr 0.000171	time 0.4557 (0.4705)	loss 2.5443 (3.0543)	grad_norm 2.0764 (2.2232)	mem 14853MB
[2022-11-07 08:39:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][1100/1251]	eta 0:01:11 lr 0.000170	time 0.4638 (0.4705)	loss 2.5504 (3.0443)	grad_norm 2.6413 (2.2264)	mem 14853MB
[2022-11-07 08:39:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][1150/1251]	eta 0:00:47 lr 0.000170	time 0.4591 (0.4704)	loss 1.9936 (3.0449)	grad_norm 2.1984 (2.2255)	mem 14853MB
[2022-11-07 08:40:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][1200/1251]	eta 0:00:23 lr 0.000170	time 0.4702 (0.4702)	loss 3.3758 (3.0467)	grad_norm 2.1645 (2.2251)	mem 14853MB
[2022-11-07 08:40:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [220/300][1250/1251]	eta 0:00:00 lr 0.000170	time 0.4566 (0.4700)	loss 3.4468 (3.0492)	grad_norm 2.1333 (2.2268)	mem 14853MB
[2022-11-07 08:40:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 220 training takes 0:09:48
[2022-11-07 08:40:28 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_220.pth saving......
[2022-11-07 08:40:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_220.pth saved !!!
[2022-11-07 08:40:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.628 (1.628)	Loss 0.8377 (0.8377)	Acc@1 80.762 (80.762)	Acc@5 95.117 (95.117)	Mem 14853MB
[2022-11-07 08:40:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.778 Acc@5 95.496
[2022-11-07 08:40:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.8%
[2022-11-07 08:40:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.750 (1.750)	Loss 0.7244 (0.7244)	Acc@1 82.227 (82.227)	Acc@5 96.680 (96.680)	Mem 14853MB
[2022-11-07 08:40:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.798 Acc@5 95.976
[2022-11-07 08:40:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.8%
[2022-11-07 08:40:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.80% at 217 epoch
[2022-11-07 08:40:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][0/1251]	eta 0:40:40 lr 0.000170	time 1.9512 (1.9512)	loss 2.5050 (2.5050)	grad_norm 1.8568 (1.8568)	mem 14853MB
[2022-11-07 08:41:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][50/1251]	eta 0:10:02 lr 0.000170	time 0.4756 (0.5016)	loss 2.4430 (3.0914)	grad_norm 2.0995 (2.2075)	mem 14853MB
[2022-11-07 08:41:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][100/1251]	eta 0:09:19 lr 0.000170	time 0.4686 (0.4865)	loss 1.7320 (3.0038)	grad_norm 2.1649 (2.1962)	mem 14853MB
[2022-11-07 08:41:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][150/1251]	eta 0:08:47 lr 0.000169	time 0.4616 (0.4795)	loss 3.3231 (3.0027)	grad_norm 2.4825 (2.1976)	mem 14853MB
[2022-11-07 08:42:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][200/1251]	eta 0:08:20 lr 0.000169	time 0.4571 (0.4760)	loss 2.6211 (3.0225)	grad_norm 2.2725 (2.2147)	mem 14853MB
[2022-11-07 08:42:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][250/1251]	eta 0:07:54 lr 0.000169	time 0.4646 (0.4741)	loss 3.1470 (3.0312)	grad_norm 2.2178 (2.2280)	mem 14853MB
[2022-11-07 08:43:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][300/1251]	eta 0:07:29 lr 0.000169	time 0.4605 (0.4729)	loss 3.2942 (3.0327)	grad_norm 2.2410 (2.2151)	mem 14853MB
[2022-11-07 08:43:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][350/1251]	eta 0:07:05 lr 0.000169	time 0.4817 (0.4720)	loss 2.7802 (3.0214)	grad_norm 2.1525 (2.2214)	mem 14853MB
[2022-11-07 08:43:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][400/1251]	eta 0:06:41 lr 0.000169	time 0.4541 (0.4715)	loss 3.2362 (3.0235)	grad_norm 2.2245 (2.2232)	mem 14853MB
[2022-11-07 08:44:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][450/1251]	eta 0:06:17 lr 0.000169	time 0.4676 (0.4707)	loss 2.7417 (3.0161)	grad_norm 1.9600 (2.2283)	mem 14853MB
[2022-11-07 08:44:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][500/1251]	eta 0:05:53 lr 0.000168	time 0.4617 (0.4703)	loss 3.5164 (3.0197)	grad_norm 1.9790 (2.2330)	mem 14853MB
[2022-11-07 08:45:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][550/1251]	eta 0:05:29 lr 0.000168	time 0.4568 (0.4700)	loss 2.8668 (3.0227)	grad_norm 2.3258 (2.2342)	mem 14853MB
[2022-11-07 08:45:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][600/1251]	eta 0:05:05 lr 0.000168	time 0.4736 (0.4697)	loss 3.3677 (3.0272)	grad_norm 2.3459 (2.2362)	mem 14853MB
[2022-11-07 08:45:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][650/1251]	eta 0:04:42 lr 0.000168	time 0.4574 (0.4693)	loss 3.1666 (3.0320)	grad_norm 2.3774 (inf)	mem 14853MB
[2022-11-07 08:46:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][700/1251]	eta 0:04:18 lr 0.000168	time 0.4660 (0.4690)	loss 3.1995 (3.0341)	grad_norm 2.1330 (inf)	mem 14853MB
[2022-11-07 08:46:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][750/1251]	eta 0:03:54 lr 0.000168	time 0.4566 (0.4690)	loss 3.4496 (3.0323)	grad_norm 2.2832 (inf)	mem 14853MB
[2022-11-07 08:47:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][800/1251]	eta 0:03:31 lr 0.000168	time 0.4690 (0.4689)	loss 3.0646 (3.0318)	grad_norm 2.2807 (inf)	mem 14853MB
[2022-11-07 08:47:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][850/1251]	eta 0:03:08 lr 0.000167	time 0.4673 (0.4689)	loss 2.8006 (3.0239)	grad_norm 2.1227 (inf)	mem 14853MB
[2022-11-07 08:47:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][900/1251]	eta 0:02:44 lr 0.000167	time 0.4580 (0.4687)	loss 3.0684 (3.0230)	grad_norm 2.6001 (inf)	mem 14853MB
[2022-11-07 08:48:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][950/1251]	eta 0:02:21 lr 0.000167	time 0.4657 (0.4686)	loss 3.1045 (3.0219)	grad_norm 2.2861 (inf)	mem 14853MB
[2022-11-07 08:48:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][1000/1251]	eta 0:01:57 lr 0.000167	time 0.4634 (0.4684)	loss 3.8730 (3.0277)	grad_norm 2.1173 (inf)	mem 14853MB
[2022-11-07 08:48:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][1050/1251]	eta 0:01:34 lr 0.000167	time 0.4680 (0.4685)	loss 2.9764 (3.0269)	grad_norm 2.1579 (inf)	mem 14853MB
[2022-11-07 08:49:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][1100/1251]	eta 0:01:10 lr 0.000167	time 0.4613 (0.4685)	loss 2.2157 (3.0258)	grad_norm 2.0815 (inf)	mem 14853MB
[2022-11-07 08:49:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][1150/1251]	eta 0:00:47 lr 0.000166	time 0.4644 (0.4684)	loss 2.6164 (3.0267)	grad_norm 1.9175 (inf)	mem 14853MB
[2022-11-07 08:50:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][1200/1251]	eta 0:00:23 lr 0.000166	time 0.4598 (0.4682)	loss 2.9565 (3.0254)	grad_norm 1.9275 (inf)	mem 14853MB
[2022-11-07 08:50:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [221/300][1250/1251]	eta 0:00:00 lr 0.000166	time 0.4564 (0.4680)	loss 2.9678 (3.0258)	grad_norm 2.1662 (inf)	mem 14853MB
[2022-11-07 08:50:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 221 training takes 0:09:45
[2022-11-07 08:50:32 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_221.pth saving......
[2022-11-07 08:50:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_221.pth saved !!!
[2022-11-07 08:50:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.748 (1.748)	Loss 0.8000 (0.8000)	Acc@1 82.324 (82.324)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 08:50:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.712 Acc@5 95.596
[2022-11-07 08:50:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.7%
[2022-11-07 08:50:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.668 (1.668)	Loss 0.8229 (0.8229)	Acc@1 81.250 (81.250)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 08:50:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.824 Acc@5 95.972
[2022-11-07 08:50:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.8%
[2022-11-07 08:50:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.82% at 221 epoch
[2022-11-07 08:50:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][0/1251]	eta 0:40:55 lr 0.000166	time 1.9627 (1.9627)	loss 3.5436 (3.5436)	grad_norm 2.4675 (2.4675)	mem 14853MB
[2022-11-07 08:51:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][50/1251]	eta 0:10:02 lr 0.000166	time 0.4751 (0.5019)	loss 3.0935 (3.0851)	grad_norm 2.2184 (2.1425)	mem 14853MB
[2022-11-07 08:51:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][100/1251]	eta 0:09:17 lr 0.000166	time 0.4555 (0.4846)	loss 3.3474 (2.9761)	grad_norm 2.0708 (2.1645)	mem 14853MB
[2022-11-07 08:52:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][150/1251]	eta 0:08:46 lr 0.000166	time 0.4665 (0.4786)	loss 3.1890 (2.9742)	grad_norm 2.3303 (2.1755)	mem 14853MB
[2022-11-07 08:52:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][200/1251]	eta 0:08:19 lr 0.000166	time 0.4753 (0.4755)	loss 3.1585 (2.9700)	grad_norm 2.2694 (2.1765)	mem 14853MB
[2022-11-07 08:52:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][250/1251]	eta 0:07:54 lr 0.000165	time 0.4610 (0.4739)	loss 3.3352 (2.9922)	grad_norm 2.4499 (inf)	mem 14853MB
[2022-11-07 08:53:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][300/1251]	eta 0:07:29 lr 0.000165	time 0.4661 (0.4725)	loss 2.1746 (2.9989)	grad_norm 2.1271 (inf)	mem 14853MB
[2022-11-07 08:53:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][350/1251]	eta 0:07:04 lr 0.000165	time 0.4642 (0.4713)	loss 2.7500 (2.9842)	grad_norm 2.0900 (inf)	mem 14853MB
[2022-11-07 08:54:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][400/1251]	eta 0:06:40 lr 0.000165	time 0.4720 (0.4707)	loss 3.2899 (2.9917)	grad_norm 1.9618 (inf)	mem 14853MB
[2022-11-07 08:54:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][450/1251]	eta 0:06:16 lr 0.000165	time 0.4609 (0.4701)	loss 2.4293 (2.9930)	grad_norm 2.2805 (inf)	mem 14853MB
[2022-11-07 08:54:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][500/1251]	eta 0:05:52 lr 0.000165	time 0.4714 (0.4698)	loss 2.1308 (2.9865)	grad_norm 2.2897 (inf)	mem 14853MB
[2022-11-07 08:55:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][550/1251]	eta 0:05:29 lr 0.000164	time 0.4583 (0.4698)	loss 2.5286 (2.9816)	grad_norm 2.1295 (inf)	mem 14853MB
[2022-11-07 08:55:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][600/1251]	eta 0:05:05 lr 0.000164	time 0.4568 (0.4693)	loss 3.5040 (2.9902)	grad_norm 3.0701 (inf)	mem 14853MB
[2022-11-07 08:55:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][650/1251]	eta 0:04:41 lr 0.000164	time 0.4627 (0.4689)	loss 2.8037 (2.9891)	grad_norm 2.3154 (inf)	mem 14853MB
[2022-11-07 08:56:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][700/1251]	eta 0:04:18 lr 0.000164	time 0.4674 (0.4688)	loss 3.3320 (2.9891)	grad_norm 2.1481 (inf)	mem 14853MB
[2022-11-07 08:56:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][750/1251]	eta 0:03:54 lr 0.000164	time 0.4645 (0.4687)	loss 3.2376 (2.9854)	grad_norm 2.1809 (inf)	mem 14853MB
[2022-11-07 08:57:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][800/1251]	eta 0:03:31 lr 0.000164	time 0.4739 (0.4687)	loss 3.7920 (2.9845)	grad_norm 2.6608 (inf)	mem 14853MB
[2022-11-07 08:57:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][850/1251]	eta 0:03:07 lr 0.000164	time 0.4683 (0.4685)	loss 2.8675 (2.9899)	grad_norm 2.3461 (inf)	mem 14853MB
[2022-11-07 08:57:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][900/1251]	eta 0:02:44 lr 0.000163	time 0.5420 (0.4684)	loss 2.3893 (2.9924)	grad_norm 2.2908 (inf)	mem 14853MB
[2022-11-07 08:58:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][950/1251]	eta 0:02:20 lr 0.000163	time 0.4606 (0.4682)	loss 3.0834 (2.9945)	grad_norm 2.7293 (inf)	mem 14853MB
[2022-11-07 08:58:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][1000/1251]	eta 0:01:57 lr 0.000163	time 0.4675 (0.4681)	loss 3.6681 (2.9984)	grad_norm 2.2081 (inf)	mem 14853MB
[2022-11-07 08:59:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][1050/1251]	eta 0:01:34 lr 0.000163	time 0.4623 (0.4681)	loss 3.1284 (2.9961)	grad_norm 1.9522 (inf)	mem 14853MB
[2022-11-07 08:59:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][1100/1251]	eta 0:01:10 lr 0.000163	time 0.5514 (0.4681)	loss 2.7563 (2.9958)	grad_norm 2.6298 (inf)	mem 14853MB
[2022-11-07 08:59:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][1150/1251]	eta 0:00:47 lr 0.000163	time 0.4565 (0.4680)	loss 3.2628 (3.0010)	grad_norm 2.3500 (inf)	mem 14853MB
[2022-11-07 09:00:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][1200/1251]	eta 0:00:23 lr 0.000163	time 0.4688 (0.4679)	loss 3.5363 (2.9996)	grad_norm 2.5651 (inf)	mem 14853MB
[2022-11-07 09:00:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [222/300][1250/1251]	eta 0:00:00 lr 0.000162	time 0.4569 (0.4678)	loss 3.0802 (3.0007)	grad_norm 2.1312 (inf)	mem 14853MB
[2022-11-07 09:00:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 222 training takes 0:09:45
[2022-11-07 09:00:36 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_222.pth saving......
[2022-11-07 09:00:37 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_222.pth saved !!!
[2022-11-07 09:00:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.568 (1.568)	Loss 0.8656 (0.8656)	Acc@1 79.688 (79.688)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 09:00:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.788 Acc@5 95.560
[2022-11-07 09:00:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.8%
[2022-11-07 09:00:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.690 (1.690)	Loss 0.7507 (0.7507)	Acc@1 82.129 (82.129)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 09:00:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.860 Acc@5 95.966
[2022-11-07 09:00:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.9%
[2022-11-07 09:00:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.86% at 222 epoch
[2022-11-07 09:00:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][0/1251]	eta 0:40:59 lr 0.000162	time 1.9657 (1.9657)	loss 3.3302 (3.3302)	grad_norm 2.0166 (2.0166)	mem 14853MB
[2022-11-07 09:01:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][50/1251]	eta 0:10:04 lr 0.000162	time 0.4627 (0.5036)	loss 3.4690 (3.0014)	grad_norm 2.2743 (2.2565)	mem 14853MB
[2022-11-07 09:01:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][100/1251]	eta 0:09:19 lr 0.000162	time 0.4593 (0.4862)	loss 2.4500 (3.0399)	grad_norm 2.0688 (2.2639)	mem 14853MB
[2022-11-07 09:02:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][150/1251]	eta 0:08:48 lr 0.000162	time 0.5409 (0.4799)	loss 2.8578 (2.9996)	grad_norm 2.1864 (2.2477)	mem 14853MB
[2022-11-07 09:02:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][200/1251]	eta 0:08:20 lr 0.000162	time 0.4741 (0.4760)	loss 3.2739 (3.0169)	grad_norm 1.9577 (2.2543)	mem 14853MB
[2022-11-07 09:02:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][250/1251]	eta 0:07:54 lr 0.000162	time 0.4640 (0.4739)	loss 3.1833 (3.0228)	grad_norm 2.1977 (2.2752)	mem 14853MB
[2022-11-07 09:03:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][300/1251]	eta 0:07:29 lr 0.000161	time 0.4631 (0.4724)	loss 3.1856 (3.0039)	grad_norm 2.1835 (2.2727)	mem 14853MB
[2022-11-07 09:03:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][350/1251]	eta 0:07:05 lr 0.000161	time 0.4585 (0.4718)	loss 2.6266 (3.0206)	grad_norm 2.1498 (2.2802)	mem 14853MB
[2022-11-07 09:04:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][400/1251]	eta 0:06:40 lr 0.000161	time 0.4571 (0.4709)	loss 3.5548 (3.0309)	grad_norm 2.3297 (2.2702)	mem 14853MB
[2022-11-07 09:04:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][450/1251]	eta 0:06:16 lr 0.000161	time 0.4798 (0.4703)	loss 2.1263 (3.0415)	grad_norm 2.2458 (2.2643)	mem 14853MB
[2022-11-07 09:04:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][500/1251]	eta 0:05:53 lr 0.000161	time 0.4727 (0.4701)	loss 3.1291 (3.0429)	grad_norm 2.3265 (2.2643)	mem 14853MB
[2022-11-07 09:05:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][550/1251]	eta 0:05:29 lr 0.000161	time 0.4676 (0.4698)	loss 3.4420 (3.0380)	grad_norm 2.4334 (2.2651)	mem 14853MB
[2022-11-07 09:05:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][600/1251]	eta 0:05:05 lr 0.000161	time 0.4603 (0.4694)	loss 2.9326 (3.0416)	grad_norm 1.9893 (2.2584)	mem 14853MB
[2022-11-07 09:06:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][650/1251]	eta 0:04:42 lr 0.000160	time 0.4588 (0.4692)	loss 3.5189 (3.0514)	grad_norm 2.5326 (2.2636)	mem 14853MB
[2022-11-07 09:06:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][700/1251]	eta 0:04:18 lr 0.000160	time 0.4572 (0.4690)	loss 3.4137 (3.0527)	grad_norm 2.3074 (2.2606)	mem 14853MB
[2022-11-07 09:06:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][750/1251]	eta 0:03:54 lr 0.000160	time 0.4621 (0.4687)	loss 3.2841 (3.0497)	grad_norm 2.2704 (2.2581)	mem 14853MB
[2022-11-07 09:07:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][800/1251]	eta 0:03:31 lr 0.000160	time 0.4640 (0.4686)	loss 3.1145 (3.0441)	grad_norm 2.2038 (2.2555)	mem 14853MB
[2022-11-07 09:07:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][850/1251]	eta 0:03:07 lr 0.000160	time 0.4643 (0.4685)	loss 2.9180 (3.0394)	grad_norm 2.1030 (2.2574)	mem 14853MB
[2022-11-07 09:07:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][900/1251]	eta 0:02:44 lr 0.000160	time 0.4608 (0.4685)	loss 3.4167 (3.0369)	grad_norm 2.6185 (2.2570)	mem 14853MB
[2022-11-07 09:08:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][950/1251]	eta 0:02:20 lr 0.000160	time 0.4637 (0.4683)	loss 2.4430 (3.0346)	grad_norm 2.2550 (2.2541)	mem 14853MB
[2022-11-07 09:08:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][1000/1251]	eta 0:01:57 lr 0.000159	time 0.5477 (0.4683)	loss 3.6815 (3.0397)	grad_norm 2.2719 (2.2558)	mem 14853MB
[2022-11-07 09:09:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][1050/1251]	eta 0:01:34 lr 0.000159	time 0.4655 (0.4682)	loss 2.0564 (3.0407)	grad_norm 2.2045 (2.2559)	mem 14853MB
[2022-11-07 09:09:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][1100/1251]	eta 0:01:10 lr 0.000159	time 0.4676 (0.4680)	loss 2.0977 (3.0412)	grad_norm 2.3810 (2.2580)	mem 14853MB
[2022-11-07 09:09:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][1150/1251]	eta 0:00:47 lr 0.000159	time 0.4672 (0.4680)	loss 2.9485 (3.0377)	grad_norm 2.1389 (2.2582)	mem 14853MB
[2022-11-07 09:10:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][1200/1251]	eta 0:00:23 lr 0.000159	time 0.4777 (0.4680)	loss 2.0584 (3.0344)	grad_norm 2.1950 (2.2599)	mem 14853MB
[2022-11-07 09:10:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [223/300][1250/1251]	eta 0:00:00 lr 0.000159	time 0.4568 (0.4678)	loss 2.5489 (3.0366)	grad_norm 2.0940 (2.2605)	mem 14853MB
[2022-11-07 09:10:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 223 training takes 0:09:45
[2022-11-07 09:10:40 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_223.pth saving......
[2022-11-07 09:10:41 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_223.pth saved !!!
[2022-11-07 09:10:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.682 (1.682)	Loss 0.7792 (0.7792)	Acc@1 82.129 (82.129)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 09:10:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.968 Acc@5 95.520
[2022-11-07 09:10:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.0%
[2022-11-07 09:10:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.577 (1.577)	Loss 0.7674 (0.7674)	Acc@1 82.031 (82.031)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 09:10:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.868 Acc@5 95.986
[2022-11-07 09:10:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.9%
[2022-11-07 09:10:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.87% at 223 epoch
[2022-11-07 09:11:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][0/1251]	eta 0:41:37 lr 0.000159	time 1.9965 (1.9965)	loss 3.2573 (3.2573)	grad_norm 2.0945 (2.0945)	mem 14853MB
[2022-11-07 09:11:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][50/1251]	eta 0:10:04 lr 0.000159	time 0.4739 (0.5029)	loss 2.7149 (3.1080)	grad_norm 2.2098 (2.3289)	mem 14853MB
[2022-11-07 09:11:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][100/1251]	eta 0:09:19 lr 0.000158	time 0.4605 (0.4863)	loss 3.4066 (3.0978)	grad_norm 1.9045 (2.2797)	mem 14853MB
[2022-11-07 09:12:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][150/1251]	eta 0:08:48 lr 0.000158	time 0.4649 (0.4799)	loss 1.9883 (3.0550)	grad_norm 2.2130 (2.2945)	mem 14853MB
[2022-11-07 09:12:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][200/1251]	eta 0:08:20 lr 0.000158	time 0.4657 (0.4761)	loss 3.1260 (3.0082)	grad_norm 2.0747 (2.2892)	mem 14853MB
[2022-11-07 09:12:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][250/1251]	eta 0:07:54 lr 0.000158	time 0.4605 (0.4741)	loss 3.1516 (3.0135)	grad_norm 2.2766 (2.2785)	mem 14853MB
[2022-11-07 09:13:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][300/1251]	eta 0:07:29 lr 0.000158	time 0.4650 (0.4724)	loss 3.5207 (3.0231)	grad_norm 2.7387 (2.2796)	mem 14853MB
[2022-11-07 09:13:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][350/1251]	eta 0:07:04 lr 0.000158	time 0.4529 (0.4713)	loss 2.2814 (3.0328)	grad_norm 2.0769 (2.2741)	mem 14853MB
[2022-11-07 09:14:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][400/1251]	eta 0:06:40 lr 0.000157	time 0.4671 (0.4707)	loss 2.9470 (3.0346)	grad_norm 2.1663 (2.2821)	mem 14853MB
[2022-11-07 09:14:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][450/1251]	eta 0:06:16 lr 0.000157	time 0.4593 (0.4701)	loss 3.6805 (3.0363)	grad_norm 2.1577 (2.2752)	mem 14853MB
[2022-11-07 09:14:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][500/1251]	eta 0:05:52 lr 0.000157	time 0.4754 (0.4699)	loss 2.4735 (3.0391)	grad_norm 2.1924 (2.2745)	mem 14853MB
[2022-11-07 09:15:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][550/1251]	eta 0:05:29 lr 0.000157	time 0.4606 (0.4699)	loss 2.0778 (3.0410)	grad_norm 2.2538 (2.2729)	mem 14853MB
[2022-11-07 09:15:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][600/1251]	eta 0:05:05 lr 0.000157	time 0.4615 (0.4695)	loss 3.7825 (3.0404)	grad_norm 2.4095 (2.2749)	mem 14853MB
[2022-11-07 09:16:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][650/1251]	eta 0:04:42 lr 0.000157	time 0.4544 (0.4693)	loss 3.0440 (3.0296)	grad_norm 2.5176 (2.2714)	mem 14853MB
[2022-11-07 09:16:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][700/1251]	eta 0:04:18 lr 0.000157	time 0.4682 (0.4691)	loss 3.0445 (3.0363)	grad_norm 2.4778 (2.2678)	mem 14853MB
[2022-11-07 09:16:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][750/1251]	eta 0:03:54 lr 0.000156	time 0.4509 (0.4690)	loss 3.0796 (3.0340)	grad_norm 2.4301 (2.2705)	mem 14853MB
[2022-11-07 09:17:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][800/1251]	eta 0:03:31 lr 0.000156	time 0.4624 (0.4690)	loss 2.5602 (3.0259)	grad_norm 2.7823 (2.2760)	mem 14853MB
[2022-11-07 09:17:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][850/1251]	eta 0:03:08 lr 0.000156	time 0.4717 (0.4688)	loss 3.5874 (3.0285)	grad_norm 2.3018 (2.2797)	mem 14853MB
[2022-11-07 09:18:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][900/1251]	eta 0:02:44 lr 0.000156	time 0.4663 (0.4687)	loss 3.4498 (3.0347)	grad_norm 2.0452 (2.2839)	mem 14853MB
[2022-11-07 09:18:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][950/1251]	eta 0:02:21 lr 0.000156	time 0.4653 (0.4686)	loss 3.6123 (3.0348)	grad_norm 2.1823 (2.2817)	mem 14853MB
[2022-11-07 09:18:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][1000/1251]	eta 0:01:57 lr 0.000156	time 0.5311 (0.4685)	loss 3.4388 (3.0379)	grad_norm 2.0861 (2.2838)	mem 14853MB
[2022-11-07 09:19:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][1050/1251]	eta 0:01:34 lr 0.000156	time 0.4552 (0.4686)	loss 3.4390 (3.0383)	grad_norm 2.4642 (2.2831)	mem 14853MB
[2022-11-07 09:19:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][1100/1251]	eta 0:01:10 lr 0.000155	time 0.4615 (0.4684)	loss 3.0235 (3.0350)	grad_norm 2.4430 (2.2837)	mem 14853MB
[2022-11-07 09:19:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][1150/1251]	eta 0:00:47 lr 0.000155	time 0.4673 (0.4683)	loss 3.0381 (3.0334)	grad_norm 2.5135 (2.2829)	mem 14853MB
[2022-11-07 09:20:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][1200/1251]	eta 0:00:23 lr 0.000155	time 0.4498 (0.4682)	loss 3.3540 (3.0329)	grad_norm 2.0489 (2.2789)	mem 14853MB
[2022-11-07 09:20:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [224/300][1250/1251]	eta 0:00:00 lr 0.000155	time 0.4582 (0.4680)	loss 2.5542 (3.0322)	grad_norm 2.3599 (2.2802)	mem 14853MB
[2022-11-07 09:20:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 224 training takes 0:09:45
[2022-11-07 09:20:44 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_224.pth saving......
[2022-11-07 09:20:44 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_224.pth saved !!!
[2022-11-07 09:20:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.705 (1.705)	Loss 0.8159 (0.8159)	Acc@1 80.762 (80.762)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 09:20:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.924 Acc@5 95.618
[2022-11-07 09:20:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.9%
[2022-11-07 09:20:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.649 (1.649)	Loss 0.7328 (0.7328)	Acc@1 83.398 (83.398)	Acc@5 96.191 (96.191)	Mem 14853MB
[2022-11-07 09:21:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.898 Acc@5 95.968
[2022-11-07 09:21:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.9%
[2022-11-07 09:21:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.90% at 224 epoch
[2022-11-07 09:21:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][0/1251]	eta 0:41:15 lr 0.000155	time 1.9787 (1.9787)	loss 2.0934 (2.0934)	grad_norm 2.1754 (2.1754)	mem 14853MB
[2022-11-07 09:21:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][50/1251]	eta 0:10:03 lr 0.000155	time 0.4702 (0.5024)	loss 2.6113 (2.7913)	grad_norm 2.2739 (2.2351)	mem 14853MB
[2022-11-07 09:21:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][100/1251]	eta 0:09:17 lr 0.000155	time 0.4738 (0.4847)	loss 3.0696 (2.8941)	grad_norm 2.2170 (2.2900)	mem 14853MB
[2022-11-07 09:22:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][150/1251]	eta 0:08:47 lr 0.000155	time 0.4708 (0.4789)	loss 2.8249 (2.9060)	grad_norm 2.1294 (2.3062)	mem 14853MB
[2022-11-07 09:22:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][200/1251]	eta 0:08:19 lr 0.000154	time 0.4758 (0.4755)	loss 3.1454 (2.9081)	grad_norm 2.1170 (2.2852)	mem 14853MB
[2022-11-07 09:23:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][250/1251]	eta 0:07:54 lr 0.000154	time 0.4642 (0.4737)	loss 3.0614 (2.9332)	grad_norm 2.8056 (2.2779)	mem 14853MB
[2022-11-07 09:23:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][300/1251]	eta 0:07:29 lr 0.000154	time 0.4701 (0.4724)	loss 2.2585 (2.9302)	grad_norm 2.0865 (2.2793)	mem 14853MB
[2022-11-07 09:23:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][350/1251]	eta 0:07:04 lr 0.000154	time 0.4647 (0.4713)	loss 3.2674 (2.9142)	grad_norm 2.3785 (2.2766)	mem 14853MB
[2022-11-07 09:24:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][400/1251]	eta 0:06:40 lr 0.000154	time 0.4634 (0.4706)	loss 3.1159 (2.9185)	grad_norm 2.7898 (2.2764)	mem 14853MB
[2022-11-07 09:24:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][450/1251]	eta 0:06:16 lr 0.000154	time 0.4553 (0.4700)	loss 3.5274 (2.9353)	grad_norm 2.1646 (2.2756)	mem 14853MB
[2022-11-07 09:24:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][500/1251]	eta 0:05:52 lr 0.000154	time 0.4578 (0.4695)	loss 3.2418 (2.9417)	grad_norm 2.0454 (2.2757)	mem 14853MB
[2022-11-07 09:25:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][550/1251]	eta 0:05:29 lr 0.000153	time 0.4627 (0.4697)	loss 2.9175 (2.9442)	grad_norm 2.1003 (inf)	mem 14853MB
[2022-11-07 09:25:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][600/1251]	eta 0:05:05 lr 0.000153	time 0.4750 (0.4694)	loss 2.3646 (2.9463)	grad_norm 2.2493 (inf)	mem 14853MB
[2022-11-07 09:26:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][650/1251]	eta 0:04:41 lr 0.000153	time 0.4624 (0.4690)	loss 2.7033 (2.9522)	grad_norm 2.3052 (inf)	mem 14853MB
[2022-11-07 09:26:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][700/1251]	eta 0:04:18 lr 0.000153	time 0.4660 (0.4687)	loss 2.1982 (2.9581)	grad_norm 2.2161 (inf)	mem 14853MB
[2022-11-07 09:26:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][750/1251]	eta 0:03:54 lr 0.000153	time 0.4624 (0.4685)	loss 2.4863 (2.9662)	grad_norm 2.4221 (inf)	mem 14853MB
[2022-11-07 09:27:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][800/1251]	eta 0:03:31 lr 0.000153	time 0.4680 (0.4686)	loss 3.3246 (2.9669)	grad_norm 2.3537 (inf)	mem 14853MB
[2022-11-07 09:27:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][850/1251]	eta 0:03:07 lr 0.000153	time 0.4583 (0.4685)	loss 3.3307 (2.9771)	grad_norm 1.8323 (inf)	mem 14853MB
[2022-11-07 09:28:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][900/1251]	eta 0:02:44 lr 0.000152	time 0.4709 (0.4684)	loss 2.2745 (2.9742)	grad_norm 2.3081 (inf)	mem 14853MB
[2022-11-07 09:28:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][950/1251]	eta 0:02:20 lr 0.000152	time 0.4560 (0.4682)	loss 3.0853 (2.9813)	grad_norm 2.3014 (inf)	mem 14853MB
[2022-11-07 09:28:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][1000/1251]	eta 0:01:57 lr 0.000152	time 0.4641 (0.4680)	loss 2.9448 (2.9834)	grad_norm 2.3870 (inf)	mem 14853MB
[2022-11-07 09:29:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][1050/1251]	eta 0:01:34 lr 0.000152	time 0.4583 (0.4682)	loss 2.9757 (2.9894)	grad_norm 2.1138 (inf)	mem 14853MB
[2022-11-07 09:29:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][1100/1251]	eta 0:01:10 lr 0.000152	time 0.4650 (0.4681)	loss 3.1323 (2.9943)	grad_norm 2.0776 (inf)	mem 14853MB
[2022-11-07 09:30:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][1150/1251]	eta 0:00:47 lr 0.000152	time 0.4641 (0.4680)	loss 3.4765 (2.9917)	grad_norm 2.4932 (inf)	mem 14853MB
[2022-11-07 09:30:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][1200/1251]	eta 0:00:23 lr 0.000151	time 0.4538 (0.4679)	loss 3.5791 (2.9941)	grad_norm 2.1733 (inf)	mem 14853MB
[2022-11-07 09:30:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [225/300][1250/1251]	eta 0:00:00 lr 0.000151	time 0.4574 (0.4677)	loss 3.3382 (2.9959)	grad_norm 2.1580 (inf)	mem 14853MB
[2022-11-07 09:30:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 225 training takes 0:09:45
[2022-11-07 09:30:47 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_225.pth saving......
[2022-11-07 09:30:48 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_225.pth saved !!!
[2022-11-07 09:30:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.486 (1.486)	Loss 0.7774 (0.7774)	Acc@1 81.836 (81.836)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 09:30:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.096 Acc@5 95.706
[2022-11-07 09:30:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.1%
[2022-11-07 09:30:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.566 (1.566)	Loss 0.7946 (0.7946)	Acc@1 81.348 (81.348)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 09:31:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.908 Acc@5 95.980
[2022-11-07 09:31:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.9%
[2022-11-07 09:31:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.91% at 225 epoch
[2022-11-07 09:31:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][0/1251]	eta 0:40:46 lr 0.000151	time 1.9554 (1.9554)	loss 3.3000 (3.3000)	grad_norm 1.9954 (1.9954)	mem 14853MB
[2022-11-07 09:31:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][50/1251]	eta 0:10:05 lr 0.000151	time 0.4704 (0.5041)	loss 3.1504 (2.9806)	grad_norm 2.0683 (2.2550)	mem 14853MB
[2022-11-07 09:31:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][100/1251]	eta 0:09:20 lr 0.000151	time 0.4628 (0.4867)	loss 2.2444 (2.9690)	grad_norm 2.1489 (2.2581)	mem 14853MB
[2022-11-07 09:32:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][150/1251]	eta 0:08:48 lr 0.000151	time 0.4643 (0.4799)	loss 2.1654 (2.9856)	grad_norm 2.4695 (2.2776)	mem 14853MB
[2022-11-07 09:32:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][200/1251]	eta 0:08:21 lr 0.000151	time 0.4718 (0.4769)	loss 3.3641 (3.0079)	grad_norm 2.3264 (2.2808)	mem 14853MB
[2022-11-07 09:33:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][250/1251]	eta 0:07:55 lr 0.000151	time 0.4593 (0.4746)	loss 3.3091 (2.9870)	grad_norm 2.4391 (2.2814)	mem 14853MB
[2022-11-07 09:33:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][300/1251]	eta 0:07:29 lr 0.000150	time 0.4562 (0.4730)	loss 3.2130 (2.9929)	grad_norm 2.4022 (2.2942)	mem 14853MB
[2022-11-07 09:33:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][350/1251]	eta 0:07:05 lr 0.000150	time 0.4635 (0.4723)	loss 3.5720 (3.0005)	grad_norm 2.5616 (2.2956)	mem 14853MB
[2022-11-07 09:34:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][400/1251]	eta 0:06:41 lr 0.000150	time 0.4529 (0.4716)	loss 3.3291 (2.9981)	grad_norm 2.1302 (2.3009)	mem 14853MB
[2022-11-07 09:34:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][450/1251]	eta 0:06:17 lr 0.000150	time 0.4606 (0.4709)	loss 2.7657 (3.0011)	grad_norm 2.1424 (2.3011)	mem 14853MB
[2022-11-07 09:35:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][500/1251]	eta 0:05:53 lr 0.000150	time 0.4617 (0.4706)	loss 3.0994 (3.0031)	grad_norm 2.0677 (2.3037)	mem 14853MB
[2022-11-07 09:35:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][550/1251]	eta 0:05:29 lr 0.000150	time 0.4648 (0.4702)	loss 2.8945 (2.9899)	grad_norm 2.3699 (2.3068)	mem 14853MB
[2022-11-07 09:35:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][600/1251]	eta 0:05:06 lr 0.000150	time 0.4643 (0.4701)	loss 3.2342 (2.9938)	grad_norm 2.3578 (2.3077)	mem 14853MB
[2022-11-07 09:36:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][650/1251]	eta 0:04:42 lr 0.000149	time 0.4849 (0.4697)	loss 3.1380 (3.0028)	grad_norm 2.2829 (2.3079)	mem 14853MB
[2022-11-07 09:36:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][700/1251]	eta 0:04:18 lr 0.000149	time 0.4768 (0.4695)	loss 3.0915 (3.0037)	grad_norm 1.8592 (2.3081)	mem 14853MB
[2022-11-07 09:36:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][750/1251]	eta 0:03:55 lr 0.000149	time 0.4668 (0.4692)	loss 3.4315 (3.0054)	grad_norm 2.3612 (2.3078)	mem 14853MB
[2022-11-07 09:37:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][800/1251]	eta 0:03:31 lr 0.000149	time 0.4632 (0.4692)	loss 3.6515 (3.0117)	grad_norm 2.4682 (2.3102)	mem 14853MB
[2022-11-07 09:37:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][850/1251]	eta 0:03:08 lr 0.000149	time 0.4603 (0.4691)	loss 3.4677 (3.0189)	grad_norm 1.9661 (2.3089)	mem 14853MB
[2022-11-07 09:38:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][900/1251]	eta 0:02:44 lr 0.000149	time 0.4676 (0.4688)	loss 2.7130 (3.0250)	grad_norm 1.9871 (2.3092)	mem 14853MB
[2022-11-07 09:38:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][950/1251]	eta 0:02:21 lr 0.000149	time 0.4640 (0.4687)	loss 3.1040 (3.0241)	grad_norm 1.9279 (2.3068)	mem 14853MB
[2022-11-07 09:38:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][1000/1251]	eta 0:01:57 lr 0.000148	time 0.4651 (0.4687)	loss 2.5610 (3.0226)	grad_norm 2.6428 (2.3118)	mem 14853MB
[2022-11-07 09:39:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][1050/1251]	eta 0:01:34 lr 0.000148	time 0.4654 (0.4686)	loss 3.4890 (3.0226)	grad_norm 2.5552 (2.3190)	mem 14853MB
[2022-11-07 09:39:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][1100/1251]	eta 0:01:10 lr 0.000148	time 0.4710 (0.4687)	loss 3.6678 (3.0235)	grad_norm 2.1360 (2.3192)	mem 14853MB
[2022-11-07 09:40:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][1150/1251]	eta 0:00:47 lr 0.000148	time 0.4664 (0.4686)	loss 3.4651 (3.0239)	grad_norm 2.0416 (2.3165)	mem 14853MB
[2022-11-07 09:40:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][1200/1251]	eta 0:00:23 lr 0.000148	time 0.4713 (0.4685)	loss 3.5421 (3.0248)	grad_norm 2.2700 (2.3149)	mem 14853MB
[2022-11-07 09:40:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [226/300][1250/1251]	eta 0:00:00 lr 0.000148	time 0.4562 (0.4683)	loss 3.0359 (3.0265)	grad_norm 2.2415 (2.3115)	mem 14853MB
[2022-11-07 09:40:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 226 training takes 0:09:45
[2022-11-07 09:40:51 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_226.pth saving......
[2022-11-07 09:40:52 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_226.pth saved !!!
[2022-11-07 09:40:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.709 (1.709)	Loss 0.8207 (0.8207)	Acc@1 82.031 (82.031)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 09:41:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.984 Acc@5 95.578
[2022-11-07 09:41:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.0%
[2022-11-07 09:41:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.607 (1.607)	Loss 0.8765 (0.8765)	Acc@1 80.078 (80.078)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 09:41:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.920 Acc@5 95.980
[2022-11-07 09:41:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.9%
[2022-11-07 09:41:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.92% at 226 epoch
[2022-11-07 09:41:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][0/1251]	eta 0:43:04 lr 0.000148	time 2.0659 (2.0659)	loss 2.1378 (2.1378)	grad_norm 2.2825 (2.2825)	mem 14853MB
[2022-11-07 09:41:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][50/1251]	eta 0:09:59 lr 0.000148	time 0.4654 (0.4991)	loss 3.5678 (2.9877)	grad_norm 2.3346 (2.2472)	mem 14853MB
[2022-11-07 09:41:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][100/1251]	eta 0:09:17 lr 0.000147	time 0.4747 (0.4847)	loss 2.7885 (3.0279)	grad_norm 2.5352 (2.2750)	mem 14853MB
[2022-11-07 09:42:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][150/1251]	eta 0:08:46 lr 0.000147	time 0.4754 (0.4780)	loss 3.4266 (3.0320)	grad_norm 2.2711 (inf)	mem 14853MB
[2022-11-07 09:42:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][200/1251]	eta 0:08:19 lr 0.000147	time 0.4690 (0.4748)	loss 2.9457 (3.0403)	grad_norm 2.2274 (inf)	mem 14853MB
[2022-11-07 09:43:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][250/1251]	eta 0:07:54 lr 0.000147	time 0.4585 (0.4737)	loss 2.3948 (3.0201)	grad_norm 2.4866 (inf)	mem 14853MB
[2022-11-07 09:43:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][300/1251]	eta 0:07:29 lr 0.000147	time 0.4668 (0.4724)	loss 2.8361 (3.0209)	grad_norm 2.3314 (inf)	mem 14853MB
[2022-11-07 09:43:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][350/1251]	eta 0:07:04 lr 0.000147	time 0.4547 (0.4716)	loss 3.5499 (3.0187)	grad_norm 2.5654 (inf)	mem 14853MB
[2022-11-07 09:44:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][400/1251]	eta 0:06:40 lr 0.000147	time 0.4664 (0.4708)	loss 2.5497 (3.0030)	grad_norm 2.4542 (inf)	mem 14853MB
[2022-11-07 09:44:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][450/1251]	eta 0:06:16 lr 0.000146	time 0.4708 (0.4702)	loss 2.9403 (3.0116)	grad_norm 2.2699 (inf)	mem 14853MB
[2022-11-07 09:45:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][500/1251]	eta 0:05:52 lr 0.000146	time 0.4673 (0.4699)	loss 3.3498 (3.0051)	grad_norm 2.5579 (inf)	mem 14853MB
[2022-11-07 09:45:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][550/1251]	eta 0:05:29 lr 0.000146	time 0.4625 (0.4698)	loss 2.0709 (3.0035)	grad_norm 2.2109 (inf)	mem 14853MB
[2022-11-07 09:45:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][600/1251]	eta 0:05:05 lr 0.000146	time 0.4661 (0.4695)	loss 3.8063 (3.0022)	grad_norm 2.0562 (inf)	mem 14853MB
[2022-11-07 09:46:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][650/1251]	eta 0:04:42 lr 0.000146	time 0.4697 (0.4692)	loss 3.1113 (3.0064)	grad_norm 2.4616 (inf)	mem 14853MB
[2022-11-07 09:46:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][700/1251]	eta 0:04:18 lr 0.000146	time 0.4700 (0.4690)	loss 3.3208 (3.0081)	grad_norm 2.3841 (inf)	mem 14853MB
[2022-11-07 09:47:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][750/1251]	eta 0:03:54 lr 0.000146	time 0.4714 (0.4689)	loss 3.1930 (3.0087)	grad_norm 2.6365 (inf)	mem 14853MB
[2022-11-07 09:47:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][800/1251]	eta 0:03:31 lr 0.000145	time 0.5445 (0.4688)	loss 3.2085 (3.0147)	grad_norm 2.1054 (inf)	mem 14853MB
[2022-11-07 09:47:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][850/1251]	eta 0:03:07 lr 0.000145	time 0.4644 (0.4688)	loss 2.2449 (3.0145)	grad_norm 1.9819 (inf)	mem 14853MB
[2022-11-07 09:48:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][900/1251]	eta 0:02:44 lr 0.000145	time 0.4743 (0.4685)	loss 3.2037 (3.0143)	grad_norm 2.1115 (inf)	mem 14853MB
[2022-11-07 09:48:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][950/1251]	eta 0:02:20 lr 0.000145	time 0.4679 (0.4684)	loss 2.1987 (3.0072)	grad_norm 2.2724 (inf)	mem 14853MB
[2022-11-07 09:48:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][1000/1251]	eta 0:01:57 lr 0.000145	time 0.4656 (0.4684)	loss 3.2499 (3.0059)	grad_norm 2.2136 (inf)	mem 14853MB
[2022-11-07 09:49:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][1050/1251]	eta 0:01:34 lr 0.000145	time 0.4568 (0.4684)	loss 3.3245 (3.0106)	grad_norm 2.4606 (inf)	mem 14853MB
[2022-11-07 09:49:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][1100/1251]	eta 0:01:10 lr 0.000145	time 0.4766 (0.4684)	loss 2.8546 (3.0088)	grad_norm 2.1764 (inf)	mem 14853MB
[2022-11-07 09:50:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][1150/1251]	eta 0:00:47 lr 0.000144	time 0.4617 (0.4682)	loss 3.3778 (3.0061)	grad_norm 2.2696 (inf)	mem 14853MB
[2022-11-07 09:50:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][1200/1251]	eta 0:00:23 lr 0.000144	time 0.4715 (0.4681)	loss 2.2089 (3.0078)	grad_norm 2.1092 (inf)	mem 14853MB
[2022-11-07 09:50:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [227/300][1250/1251]	eta 0:00:00 lr 0.000144	time 0.4575 (0.4679)	loss 2.0011 (3.0024)	grad_norm 1.9367 (inf)	mem 14853MB
[2022-11-07 09:50:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 227 training takes 0:09:45
[2022-11-07 09:50:55 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_227.pth saving......
[2022-11-07 09:50:56 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_227.pth saved !!!
[2022-11-07 09:50:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.498 (1.498)	Loss 0.8857 (0.8857)	Acc@1 80.078 (80.078)	Acc@5 94.824 (94.824)	Mem 14853MB
[2022-11-07 09:51:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.090 Acc@5 95.598
[2022-11-07 09:51:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.1%
[2022-11-07 09:51:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.768 (1.768)	Loss 0.7520 (0.7520)	Acc@1 82.031 (82.031)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 09:51:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.914 Acc@5 95.974
[2022-11-07 09:51:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.9%
[2022-11-07 09:51:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.92% at 226 epoch
[2022-11-07 09:51:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][0/1251]	eta 0:44:07 lr 0.000144	time 2.1163 (2.1163)	loss 3.7861 (3.7861)	grad_norm 2.2920 (2.2920)	mem 14853MB
[2022-11-07 09:51:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][50/1251]	eta 0:10:06 lr 0.000144	time 0.4665 (0.5048)	loss 2.1052 (3.0957)	grad_norm 2.4267 (2.2614)	mem 14853MB
[2022-11-07 09:52:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][100/1251]	eta 0:09:19 lr 0.000144	time 0.4630 (0.4859)	loss 3.3129 (3.1004)	grad_norm 2.5282 (2.2991)	mem 14853MB
[2022-11-07 09:52:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][150/1251]	eta 0:08:47 lr 0.000144	time 0.4552 (0.4795)	loss 1.8072 (3.0366)	grad_norm 2.1528 (2.3045)	mem 14853MB
[2022-11-07 09:52:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][200/1251]	eta 0:08:20 lr 0.000144	time 0.4657 (0.4764)	loss 2.2564 (3.0310)	grad_norm 2.0667 (2.3300)	mem 14853MB
[2022-11-07 09:53:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][250/1251]	eta 0:07:54 lr 0.000143	time 0.4686 (0.4743)	loss 2.5278 (3.0462)	grad_norm 2.4276 (2.3229)	mem 14853MB
[2022-11-07 09:53:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][300/1251]	eta 0:07:29 lr 0.000143	time 0.4671 (0.4727)	loss 3.4051 (3.0507)	grad_norm 2.1491 (2.3269)	mem 14853MB
[2022-11-07 09:53:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][350/1251]	eta 0:07:05 lr 0.000143	time 0.4620 (0.4720)	loss 2.9838 (3.0405)	grad_norm 2.3260 (2.3265)	mem 14853MB
[2022-11-07 09:54:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][400/1251]	eta 0:06:41 lr 0.000143	time 0.4644 (0.4713)	loss 3.6172 (3.0333)	grad_norm 2.3958 (2.3228)	mem 14853MB
[2022-11-07 09:54:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][450/1251]	eta 0:06:16 lr 0.000143	time 0.4683 (0.4706)	loss 3.2943 (3.0308)	grad_norm 2.2915 (2.3214)	mem 14853MB
[2022-11-07 09:55:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][500/1251]	eta 0:05:53 lr 0.000143	time 0.4673 (0.4701)	loss 2.9748 (3.0342)	grad_norm 2.1148 (2.3237)	mem 14853MB
[2022-11-07 09:55:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][550/1251]	eta 0:05:29 lr 0.000143	time 0.4620 (0.4698)	loss 2.8921 (3.0325)	grad_norm 2.1516 (2.3253)	mem 14853MB
[2022-11-07 09:55:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][600/1251]	eta 0:05:05 lr 0.000142	time 0.4743 (0.4696)	loss 3.2416 (3.0315)	grad_norm 2.2462 (inf)	mem 14853MB
[2022-11-07 09:56:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][650/1251]	eta 0:04:42 lr 0.000142	time 0.4626 (0.4693)	loss 3.1761 (3.0330)	grad_norm 2.0450 (inf)	mem 14853MB
[2022-11-07 09:56:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][700/1251]	eta 0:04:18 lr 0.000142	time 0.4664 (0.4692)	loss 3.5953 (3.0329)	grad_norm 2.1648 (inf)	mem 14853MB
[2022-11-07 09:57:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][750/1251]	eta 0:03:55 lr 0.000142	time 0.4660 (0.4691)	loss 2.5116 (3.0333)	grad_norm 2.2207 (inf)	mem 14853MB
[2022-11-07 09:57:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][800/1251]	eta 0:03:31 lr 0.000142	time 0.4644 (0.4689)	loss 3.4233 (3.0224)	grad_norm 2.0848 (inf)	mem 14853MB
[2022-11-07 09:57:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][850/1251]	eta 0:03:07 lr 0.000142	time 0.4780 (0.4688)	loss 3.0839 (3.0247)	grad_norm 2.5090 (inf)	mem 14853MB
[2022-11-07 09:58:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][900/1251]	eta 0:02:44 lr 0.000142	time 0.4622 (0.4687)	loss 1.9740 (3.0224)	grad_norm 2.3583 (inf)	mem 14853MB
[2022-11-07 09:58:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][950/1251]	eta 0:02:21 lr 0.000141	time 0.4724 (0.4687)	loss 2.8568 (3.0201)	grad_norm 2.1905 (inf)	mem 14853MB
[2022-11-07 09:59:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][1000/1251]	eta 0:01:57 lr 0.000141	time 0.4597 (0.4686)	loss 1.8526 (3.0263)	grad_norm 2.1776 (inf)	mem 14853MB
[2022-11-07 09:59:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][1050/1251]	eta 0:01:34 lr 0.000141	time 0.4681 (0.4685)	loss 3.7177 (3.0292)	grad_norm 2.4499 (inf)	mem 14853MB
[2022-11-07 09:59:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][1100/1251]	eta 0:01:10 lr 0.000141	time 0.4687 (0.4684)	loss 3.4012 (3.0327)	grad_norm 2.0813 (inf)	mem 14853MB
[2022-11-07 10:00:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][1150/1251]	eta 0:00:47 lr 0.000141	time 0.4732 (0.4685)	loss 3.3878 (3.0295)	grad_norm 2.5623 (inf)	mem 14853MB
[2022-11-07 10:00:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][1200/1251]	eta 0:00:23 lr 0.000141	time 0.4613 (0.4684)	loss 3.5014 (3.0308)	grad_norm 2.2380 (inf)	mem 14853MB
[2022-11-07 10:00:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [228/300][1250/1251]	eta 0:00:00 lr 0.000141	time 0.4580 (0.4682)	loss 3.2512 (3.0250)	grad_norm 2.3547 (inf)	mem 14853MB
[2022-11-07 10:00:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 228 training takes 0:09:45
[2022-11-07 10:00:59 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_228.pth saving......
[2022-11-07 10:01:00 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_228.pth saved !!!
[2022-11-07 10:01:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.557 (1.557)	Loss 0.8424 (0.8424)	Acc@1 79.785 (79.785)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 10:01:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.030 Acc@5 95.614
[2022-11-07 10:01:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.0%
[2022-11-07 10:01:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.610 (1.610)	Loss 0.7837 (0.7837)	Acc@1 80.664 (80.664)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 10:01:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.928 Acc@5 95.992
[2022-11-07 10:01:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.9%
[2022-11-07 10:01:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.93% at 228 epoch
[2022-11-07 10:01:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][0/1251]	eta 0:41:34 lr 0.000141	time 1.9938 (1.9938)	loss 2.2318 (2.2318)	grad_norm 2.2367 (2.2367)	mem 14853MB
[2022-11-07 10:01:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][50/1251]	eta 0:10:00 lr 0.000140	time 0.4650 (0.5003)	loss 2.7042 (2.9396)	grad_norm 2.2221 (2.3652)	mem 14853MB
[2022-11-07 10:02:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][100/1251]	eta 0:09:18 lr 0.000140	time 0.4620 (0.4853)	loss 2.8530 (2.9601)	grad_norm 2.1909 (2.3322)	mem 14853MB
[2022-11-07 10:02:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][150/1251]	eta 0:08:49 lr 0.000140	time 0.4587 (0.4805)	loss 3.1110 (2.9401)	grad_norm 2.2055 (2.3187)	mem 14853MB
[2022-11-07 10:02:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][200/1251]	eta 0:08:20 lr 0.000140	time 0.4529 (0.4766)	loss 2.9806 (2.9789)	grad_norm 2.3661 (2.3292)	mem 14853MB
[2022-11-07 10:03:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][250/1251]	eta 0:07:55 lr 0.000140	time 0.4628 (0.4746)	loss 3.3362 (2.9901)	grad_norm 2.1441 (2.3338)	mem 14853MB
[2022-11-07 10:03:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][300/1251]	eta 0:07:29 lr 0.000140	time 0.4608 (0.4730)	loss 3.4054 (2.9979)	grad_norm 2.3396 (2.3395)	mem 14853MB
[2022-11-07 10:04:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][350/1251]	eta 0:07:05 lr 0.000140	time 0.4634 (0.4720)	loss 3.4017 (2.9973)	grad_norm 2.7285 (2.3364)	mem 14853MB
[2022-11-07 10:04:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][400/1251]	eta 0:06:41 lr 0.000140	time 0.4601 (0.4715)	loss 2.6038 (2.9812)	grad_norm 2.6015 (2.3314)	mem 14853MB
[2022-11-07 10:04:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][450/1251]	eta 0:06:17 lr 0.000139	time 0.4645 (0.4707)	loss 1.9642 (2.9643)	grad_norm 2.3449 (2.3315)	mem 14853MB
[2022-11-07 10:05:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][500/1251]	eta 0:05:53 lr 0.000139	time 0.4570 (0.4702)	loss 2.9441 (2.9751)	grad_norm 2.3077 (2.3367)	mem 14853MB
[2022-11-07 10:05:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][550/1251]	eta 0:05:29 lr 0.000139	time 0.4658 (0.4699)	loss 2.7547 (2.9887)	grad_norm 2.6680 (2.3429)	mem 14853MB
[2022-11-07 10:05:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][600/1251]	eta 0:05:05 lr 0.000139	time 0.4700 (0.4696)	loss 3.2555 (2.9865)	grad_norm 2.0299 (2.3414)	mem 14853MB
[2022-11-07 10:06:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][650/1251]	eta 0:04:42 lr 0.000139	time 0.4753 (0.4694)	loss 3.7576 (2.9920)	grad_norm 3.4207 (2.3421)	mem 14853MB
[2022-11-07 10:06:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][700/1251]	eta 0:04:18 lr 0.000139	time 0.4630 (0.4693)	loss 3.3821 (2.9919)	grad_norm 1.9899 (2.3409)	mem 14853MB
[2022-11-07 10:07:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][750/1251]	eta 0:03:55 lr 0.000139	time 0.5333 (0.4692)	loss 2.0957 (2.9859)	grad_norm 2.2261 (2.3404)	mem 14853MB
[2022-11-07 10:07:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][800/1251]	eta 0:03:31 lr 0.000138	time 0.4528 (0.4689)	loss 2.7355 (2.9732)	grad_norm 2.5202 (2.3422)	mem 14853MB
[2022-11-07 10:07:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][850/1251]	eta 0:03:08 lr 0.000138	time 0.4584 (0.4688)	loss 3.4555 (2.9762)	grad_norm 2.4859 (2.3429)	mem 14853MB
[2022-11-07 10:08:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][900/1251]	eta 0:02:44 lr 0.000138	time 0.4680 (0.4686)	loss 3.2368 (2.9780)	grad_norm 2.3034 (2.3412)	mem 14853MB
[2022-11-07 10:08:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][950/1251]	eta 0:02:21 lr 0.000138	time 0.4600 (0.4686)	loss 3.2814 (2.9747)	grad_norm 2.2215 (2.3445)	mem 14853MB
[2022-11-07 10:09:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][1000/1251]	eta 0:01:57 lr 0.000138	time 0.4552 (0.4685)	loss 2.8998 (2.9753)	grad_norm 2.1719 (2.3459)	mem 14853MB
[2022-11-07 10:09:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][1050/1251]	eta 0:01:34 lr 0.000138	time 0.4632 (0.4683)	loss 3.1928 (2.9772)	grad_norm 2.9138 (2.3457)	mem 14853MB
[2022-11-07 10:09:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][1100/1251]	eta 0:01:10 lr 0.000138	time 0.4643 (0.4683)	loss 3.1136 (2.9821)	grad_norm 2.2579 (2.3488)	mem 14853MB
[2022-11-07 10:10:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][1150/1251]	eta 0:00:47 lr 0.000137	time 0.4673 (0.4682)	loss 1.9804 (2.9815)	grad_norm 2.0973 (2.3468)	mem 14853MB
[2022-11-07 10:10:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][1200/1251]	eta 0:00:23 lr 0.000137	time 0.4560 (0.4682)	loss 3.0659 (2.9891)	grad_norm 2.2884 (2.3477)	mem 14853MB
[2022-11-07 10:11:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [229/300][1250/1251]	eta 0:00:00 lr 0.000137	time 0.5379 (0.4681)	loss 2.3401 (2.9889)	grad_norm 2.6975 (2.3479)	mem 14853MB
[2022-11-07 10:11:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 229 training takes 0:09:45
[2022-11-07 10:11:03 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_229.pth saving......
[2022-11-07 10:11:03 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_229.pth saved !!!
[2022-11-07 10:11:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.595 (1.595)	Loss 0.8483 (0.8483)	Acc@1 80.273 (80.273)	Acc@5 96.191 (96.191)	Mem 14853MB
[2022-11-07 10:11:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.076 Acc@5 95.626
[2022-11-07 10:11:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.1%
[2022-11-07 10:11:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.588 (1.588)	Loss 0.7809 (0.7809)	Acc@1 82.715 (82.715)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-07 10:11:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.998 Acc@5 95.986
[2022-11-07 10:11:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.0%
[2022-11-07 10:11:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.00% at 229 epoch
[2022-11-07 10:11:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][0/1251]	eta 0:42:32 lr 0.000137	time 2.0400 (2.0400)	loss 3.5952 (3.5952)	grad_norm 2.1356 (2.1356)	mem 14853MB
[2022-11-07 10:11:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][50/1251]	eta 0:10:03 lr 0.000137	time 0.4715 (0.5021)	loss 3.7675 (2.9444)	grad_norm 2.4492 (2.3189)	mem 14853MB
[2022-11-07 10:12:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][100/1251]	eta 0:09:18 lr 0.000137	time 0.4707 (0.4850)	loss 2.5439 (2.9312)	grad_norm 2.2065 (2.3360)	mem 14853MB
[2022-11-07 10:12:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][150/1251]	eta 0:08:48 lr 0.000137	time 0.4533 (0.4798)	loss 3.0645 (2.9340)	grad_norm 2.5348 (2.3279)	mem 14853MB
[2022-11-07 10:12:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][200/1251]	eta 0:08:20 lr 0.000137	time 0.4650 (0.4759)	loss 3.5471 (2.9297)	grad_norm 2.5594 (2.3327)	mem 14853MB
[2022-11-07 10:13:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][250/1251]	eta 0:07:54 lr 0.000136	time 0.4643 (0.4741)	loss 3.1734 (2.9212)	grad_norm 2.9989 (2.3542)	mem 14853MB
[2022-11-07 10:13:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][300/1251]	eta 0:07:29 lr 0.000136	time 0.4647 (0.4731)	loss 3.1505 (2.9211)	grad_norm 2.3931 (2.3669)	mem 14853MB
[2022-11-07 10:14:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][350/1251]	eta 0:07:05 lr 0.000136	time 0.4685 (0.4718)	loss 1.9631 (2.9250)	grad_norm 2.4046 (2.3653)	mem 14853MB
[2022-11-07 10:14:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][400/1251]	eta 0:06:40 lr 0.000136	time 0.4701 (0.4709)	loss 3.2271 (2.9275)	grad_norm 2.1880 (2.3663)	mem 14853MB
[2022-11-07 10:14:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][450/1251]	eta 0:06:16 lr 0.000136	time 0.4591 (0.4703)	loss 2.2547 (2.9203)	grad_norm 2.2574 (2.3666)	mem 14853MB
[2022-11-07 10:15:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][500/1251]	eta 0:05:53 lr 0.000136	time 0.4686 (0.4702)	loss 3.3941 (2.9307)	grad_norm 2.7901 (2.3722)	mem 14853MB
[2022-11-07 10:15:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][550/1251]	eta 0:05:29 lr 0.000136	time 0.4611 (0.4698)	loss 3.2562 (2.9396)	grad_norm 2.1779 (2.3773)	mem 14853MB
[2022-11-07 10:16:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][600/1251]	eta 0:05:05 lr 0.000135	time 0.4711 (0.4696)	loss 2.8214 (2.9478)	grad_norm 2.5374 (2.3720)	mem 14853MB
[2022-11-07 10:16:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][650/1251]	eta 0:04:42 lr 0.000135	time 0.4610 (0.4693)	loss 2.4585 (2.9486)	grad_norm 2.4471 (2.3726)	mem 14853MB
[2022-11-07 10:16:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][700/1251]	eta 0:04:18 lr 0.000135	time 0.4665 (0.4690)	loss 3.3361 (2.9439)	grad_norm 2.2331 (2.3738)	mem 14853MB
[2022-11-07 10:17:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][750/1251]	eta 0:03:54 lr 0.000135	time 0.4711 (0.4689)	loss 3.3283 (2.9537)	grad_norm 2.3117 (2.3796)	mem 14853MB
[2022-11-07 10:17:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][800/1251]	eta 0:03:31 lr 0.000135	time 0.4621 (0.4687)	loss 2.5404 (2.9486)	grad_norm 2.2156 (2.3775)	mem 14853MB
[2022-11-07 10:17:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][850/1251]	eta 0:03:07 lr 0.000135	time 0.4575 (0.4685)	loss 2.6332 (2.9505)	grad_norm 2.3437 (2.3730)	mem 14853MB
[2022-11-07 10:18:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][900/1251]	eta 0:02:44 lr 0.000135	time 0.4678 (0.4683)	loss 3.4440 (2.9495)	grad_norm 2.3891 (2.3671)	mem 14853MB
[2022-11-07 10:18:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][950/1251]	eta 0:02:20 lr 0.000135	time 0.4729 (0.4681)	loss 3.3253 (2.9540)	grad_norm 2.2086 (2.3639)	mem 14853MB
[2022-11-07 10:19:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][1000/1251]	eta 0:01:57 lr 0.000134	time 0.4691 (0.4680)	loss 3.0950 (2.9609)	grad_norm 2.1722 (2.3616)	mem 14853MB
[2022-11-07 10:19:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][1050/1251]	eta 0:01:34 lr 0.000134	time 0.4659 (0.4680)	loss 2.5896 (2.9617)	grad_norm 2.3614 (2.3619)	mem 14853MB
[2022-11-07 10:19:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][1100/1251]	eta 0:01:10 lr 0.000134	time 0.4679 (0.4681)	loss 3.0991 (2.9625)	grad_norm 2.0535 (2.3632)	mem 14853MB
[2022-11-07 10:20:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][1150/1251]	eta 0:00:47 lr 0.000134	time 0.4631 (0.4680)	loss 3.3963 (2.9607)	grad_norm 2.1216 (2.3625)	mem 14853MB
[2022-11-07 10:20:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][1200/1251]	eta 0:00:23 lr 0.000134	time 0.4724 (0.4678)	loss 3.4132 (2.9639)	grad_norm 2.3445 (2.3665)	mem 14853MB
[2022-11-07 10:21:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [230/300][1250/1251]	eta 0:00:00 lr 0.000134	time 0.4592 (0.4677)	loss 2.9954 (2.9698)	grad_norm 2.3006 (2.3641)	mem 14853MB
[2022-11-07 10:21:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 230 training takes 0:09:45
[2022-11-07 10:21:06 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_230.pth saving......
[2022-11-07 10:21:06 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_230.pth saved !!!
[2022-11-07 10:21:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.551 (1.551)	Loss 0.7954 (0.7954)	Acc@1 81.836 (81.836)	Acc@5 94.629 (94.629)	Mem 14853MB
[2022-11-07 10:21:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.074 Acc@5 95.640
[2022-11-07 10:21:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.1%
[2022-11-07 10:21:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.639 (1.639)	Loss 0.8269 (0.8269)	Acc@1 80.957 (80.957)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 10:21:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.014 Acc@5 95.974
[2022-11-07 10:21:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.0%
[2022-11-07 10:21:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.01% at 230 epoch
[2022-11-07 10:21:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][0/1251]	eta 0:40:29 lr 0.000134	time 1.9419 (1.9419)	loss 2.3291 (2.3291)	grad_norm 2.2965 (2.2965)	mem 14853MB
[2022-11-07 10:21:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][50/1251]	eta 0:09:58 lr 0.000134	time 0.4704 (0.4985)	loss 3.3588 (3.0160)	grad_norm 2.4638 (2.3102)	mem 14853MB
[2022-11-07 10:22:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][100/1251]	eta 0:09:15 lr 0.000133	time 0.4630 (0.4827)	loss 3.5919 (3.0683)	grad_norm 2.3299 (2.3380)	mem 14853MB
[2022-11-07 10:22:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][150/1251]	eta 0:08:45 lr 0.000133	time 0.4526 (0.4773)	loss 3.8652 (3.0441)	grad_norm 2.3370 (2.3458)	mem 14853MB
[2022-11-07 10:23:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][200/1251]	eta 0:08:19 lr 0.000133	time 0.4672 (0.4748)	loss 2.6795 (3.0410)	grad_norm 2.3385 (2.3501)	mem 14853MB
[2022-11-07 10:23:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][250/1251]	eta 0:07:53 lr 0.000133	time 0.4655 (0.4727)	loss 2.5830 (3.0131)	grad_norm 2.8718 (inf)	mem 14853MB
[2022-11-07 10:23:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][300/1251]	eta 0:07:28 lr 0.000133	time 0.5713 (0.4720)	loss 3.1887 (2.9979)	grad_norm 2.2018 (inf)	mem 14853MB
[2022-11-07 10:24:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][350/1251]	eta 0:07:04 lr 0.000133	time 0.4713 (0.4711)	loss 3.2047 (2.9864)	grad_norm 2.5278 (inf)	mem 14853MB
[2022-11-07 10:24:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][400/1251]	eta 0:06:40 lr 0.000133	time 0.4657 (0.4701)	loss 3.2625 (3.0005)	grad_norm 2.2880 (inf)	mem 14853MB
[2022-11-07 10:24:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][450/1251]	eta 0:06:16 lr 0.000132	time 0.4617 (0.4698)	loss 3.1581 (3.0031)	grad_norm 2.6012 (inf)	mem 14853MB
[2022-11-07 10:25:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][500/1251]	eta 0:05:52 lr 0.000132	time 0.4703 (0.4695)	loss 3.7983 (3.0034)	grad_norm 2.6634 (inf)	mem 14853MB
[2022-11-07 10:25:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][550/1251]	eta 0:05:28 lr 0.000132	time 0.4732 (0.4693)	loss 2.9745 (3.0093)	grad_norm 3.0351 (inf)	mem 14853MB
[2022-11-07 10:26:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][600/1251]	eta 0:05:05 lr 0.000132	time 0.4586 (0.4690)	loss 3.4658 (3.0029)	grad_norm 2.3504 (inf)	mem 14853MB
[2022-11-07 10:26:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][650/1251]	eta 0:04:41 lr 0.000132	time 0.4615 (0.4689)	loss 3.4574 (3.0080)	grad_norm 2.4651 (inf)	mem 14853MB
[2022-11-07 10:26:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][700/1251]	eta 0:04:18 lr 0.000132	time 0.4610 (0.4688)	loss 2.6167 (3.0007)	grad_norm 2.5587 (inf)	mem 14853MB
[2022-11-07 10:27:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][750/1251]	eta 0:03:54 lr 0.000132	time 0.4624 (0.4687)	loss 3.5538 (3.0048)	grad_norm 2.7632 (inf)	mem 14853MB
[2022-11-07 10:27:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][800/1251]	eta 0:03:31 lr 0.000132	time 0.5361 (0.4686)	loss 3.2536 (3.0058)	grad_norm 2.1405 (inf)	mem 14853MB
[2022-11-07 10:28:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][850/1251]	eta 0:03:07 lr 0.000131	time 0.4570 (0.4684)	loss 2.9464 (3.0040)	grad_norm 2.3721 (inf)	mem 14853MB
[2022-11-07 10:28:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][900/1251]	eta 0:02:44 lr 0.000131	time 0.5340 (0.4683)	loss 3.1679 (2.9976)	grad_norm 2.2109 (inf)	mem 14853MB
[2022-11-07 10:28:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][950/1251]	eta 0:02:20 lr 0.000131	time 0.4689 (0.4683)	loss 3.2378 (2.9952)	grad_norm 2.1324 (inf)	mem 14853MB
[2022-11-07 10:29:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][1000/1251]	eta 0:01:57 lr 0.000131	time 0.4615 (0.4681)	loss 3.1317 (2.9920)	grad_norm 2.2367 (inf)	mem 14853MB
[2022-11-07 10:29:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][1050/1251]	eta 0:01:34 lr 0.000131	time 0.4767 (0.4681)	loss 3.5541 (2.9867)	grad_norm 2.3465 (inf)	mem 14853MB
[2022-11-07 10:29:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][1100/1251]	eta 0:01:10 lr 0.000131	time 0.4633 (0.4680)	loss 2.2100 (2.9863)	grad_norm 2.1896 (inf)	mem 14853MB
[2022-11-07 10:30:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][1150/1251]	eta 0:00:47 lr 0.000131	time 0.4646 (0.4679)	loss 3.4777 (2.9889)	grad_norm 2.4547 (inf)	mem 14853MB
[2022-11-07 10:30:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][1200/1251]	eta 0:00:23 lr 0.000130	time 0.4633 (0.4677)	loss 3.1233 (2.9918)	grad_norm 2.4480 (inf)	mem 14853MB
[2022-11-07 10:31:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [231/300][1250/1251]	eta 0:00:00 lr 0.000130	time 0.4570 (0.4676)	loss 3.4462 (2.9944)	grad_norm 2.3389 (inf)	mem 14853MB
[2022-11-07 10:31:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 231 training takes 0:09:45
[2022-11-07 10:31:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_231.pth saving......
[2022-11-07 10:31:10 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_231.pth saved !!!
[2022-11-07 10:31:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.523 (1.523)	Loss 0.7654 (0.7654)	Acc@1 81.836 (81.836)	Acc@5 96.289 (96.289)	Mem 14853MB
[2022-11-07 10:31:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.082 Acc@5 95.648
[2022-11-07 10:31:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.1%
[2022-11-07 10:31:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.600 (1.600)	Loss 0.7586 (0.7586)	Acc@1 82.324 (82.324)	Acc@5 96.387 (96.387)	Mem 14853MB
[2022-11-07 10:31:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.030 Acc@5 95.960
[2022-11-07 10:31:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.0%
[2022-11-07 10:31:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.03% at 231 epoch
[2022-11-07 10:31:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][0/1251]	eta 0:39:58 lr 0.000130	time 1.9175 (1.9175)	loss 3.1271 (3.1271)	grad_norm 2.2322 (2.2322)	mem 14853MB
[2022-11-07 10:31:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][50/1251]	eta 0:10:01 lr 0.000130	time 0.4641 (0.5006)	loss 3.1975 (3.0841)	grad_norm 2.1517 (2.3417)	mem 14853MB
[2022-11-07 10:32:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][100/1251]	eta 0:09:17 lr 0.000130	time 0.4575 (0.4843)	loss 2.9400 (3.0474)	grad_norm 2.3998 (2.3543)	mem 14853MB
[2022-11-07 10:32:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][150/1251]	eta 0:08:46 lr 0.000130	time 0.4681 (0.4785)	loss 2.5185 (3.0000)	grad_norm 2.2257 (2.3749)	mem 14853MB
[2022-11-07 10:33:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][200/1251]	eta 0:08:19 lr 0.000130	time 0.4588 (0.4751)	loss 3.2657 (2.9829)	grad_norm 2.4035 (2.3760)	mem 14853MB
[2022-11-07 10:33:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][250/1251]	eta 0:07:53 lr 0.000130	time 0.4766 (0.4733)	loss 3.2490 (2.9863)	grad_norm 2.4424 (2.3875)	mem 14853MB
[2022-11-07 10:33:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][300/1251]	eta 0:07:28 lr 0.000129	time 0.4616 (0.4721)	loss 1.9880 (2.9843)	grad_norm 2.3294 (2.3750)	mem 14853MB
[2022-11-07 10:34:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][350/1251]	eta 0:07:04 lr 0.000129	time 0.4644 (0.4711)	loss 1.7977 (2.9757)	grad_norm 2.1649 (2.3667)	mem 14853MB
[2022-11-07 10:34:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][400/1251]	eta 0:06:40 lr 0.000129	time 0.4570 (0.4704)	loss 2.7524 (2.9822)	grad_norm 2.3667 (2.3644)	mem 14853MB
[2022-11-07 10:34:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][450/1251]	eta 0:06:16 lr 0.000129	time 0.4604 (0.4698)	loss 2.9233 (2.9802)	grad_norm 2.2563 (2.3644)	mem 14853MB
[2022-11-07 10:35:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][500/1251]	eta 0:05:52 lr 0.000129	time 0.4721 (0.4695)	loss 3.1545 (2.9770)	grad_norm 2.1887 (2.3670)	mem 14853MB
[2022-11-07 10:35:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][550/1251]	eta 0:05:28 lr 0.000129	time 0.4667 (0.4693)	loss 3.4446 (2.9827)	grad_norm 2.5084 (2.3743)	mem 14853MB
[2022-11-07 10:36:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][600/1251]	eta 0:05:05 lr 0.000129	time 0.4763 (0.4691)	loss 2.9690 (2.9743)	grad_norm 2.4189 (2.3730)	mem 14853MB
[2022-11-07 10:36:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][650/1251]	eta 0:04:41 lr 0.000129	time 0.4676 (0.4688)	loss 3.0870 (2.9771)	grad_norm 2.4418 (2.3694)	mem 14853MB
[2022-11-07 10:36:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][700/1251]	eta 0:04:18 lr 0.000128	time 0.4593 (0.4686)	loss 3.1242 (2.9832)	grad_norm 2.3541 (2.3758)	mem 14853MB
[2022-11-07 10:37:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][750/1251]	eta 0:03:54 lr 0.000128	time 0.4684 (0.4688)	loss 3.5575 (2.9804)	grad_norm 2.4973 (2.3758)	mem 14853MB
[2022-11-07 10:37:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][800/1251]	eta 0:03:31 lr 0.000128	time 0.4633 (0.4687)	loss 3.1442 (2.9774)	grad_norm 1.9891 (2.3768)	mem 14853MB
[2022-11-07 10:38:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][850/1251]	eta 0:03:07 lr 0.000128	time 0.4575 (0.4687)	loss 3.0336 (2.9731)	grad_norm 2.5891 (2.3783)	mem 14853MB
[2022-11-07 10:38:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][900/1251]	eta 0:02:44 lr 0.000128	time 0.4604 (0.4685)	loss 2.3908 (2.9659)	grad_norm 2.5092 (2.3781)	mem 14853MB
[2022-11-07 10:38:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][950/1251]	eta 0:02:20 lr 0.000128	time 0.4666 (0.4683)	loss 3.2855 (2.9725)	grad_norm 2.6746 (2.3829)	mem 14853MB
[2022-11-07 10:39:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][1000/1251]	eta 0:01:57 lr 0.000128	time 0.4647 (0.4683)	loss 3.3485 (2.9741)	grad_norm 2.3531 (2.3835)	mem 14853MB
[2022-11-07 10:39:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][1050/1251]	eta 0:01:34 lr 0.000127	time 0.4512 (0.4684)	loss 3.1931 (2.9759)	grad_norm 2.2318 (2.3803)	mem 14853MB
[2022-11-07 10:40:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][1100/1251]	eta 0:01:10 lr 0.000127	time 0.4650 (0.4683)	loss 3.0410 (2.9738)	grad_norm 2.3241 (2.3837)	mem 14853MB
[2022-11-07 10:40:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][1150/1251]	eta 0:00:47 lr 0.000127	time 0.4667 (0.4682)	loss 3.2106 (2.9739)	grad_norm 2.5413 (2.3858)	mem 14853MB
[2022-11-07 10:40:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][1200/1251]	eta 0:00:23 lr 0.000127	time 0.4511 (0.4681)	loss 3.2628 (2.9700)	grad_norm 2.2254 (2.3865)	mem 14853MB
[2022-11-07 10:41:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [232/300][1250/1251]	eta 0:00:00 lr 0.000127	time 0.4582 (0.4679)	loss 3.2868 (2.9743)	grad_norm 2.2598 (2.3860)	mem 14853MB
[2022-11-07 10:41:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 232 training takes 0:09:45
[2022-11-07 10:41:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_232.pth saving......
[2022-11-07 10:41:14 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_232.pth saved !!!
[2022-11-07 10:41:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.559 (1.559)	Loss 0.8171 (0.8171)	Acc@1 80.859 (80.859)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 10:41:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.138 Acc@5 95.640
[2022-11-07 10:41:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.1%
[2022-11-07 10:41:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.677 (1.677)	Loss 0.8106 (0.8106)	Acc@1 81.152 (81.152)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 10:41:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.066 Acc@5 95.988
[2022-11-07 10:41:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.1%
[2022-11-07 10:41:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.07% at 232 epoch
[2022-11-07 10:41:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][0/1251]	eta 0:42:45 lr 0.000127	time 2.0507 (2.0507)	loss 2.9102 (2.9102)	grad_norm 2.2866 (2.2866)	mem 14853MB
[2022-11-07 10:41:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][50/1251]	eta 0:10:04 lr 0.000127	time 0.4655 (0.5030)	loss 3.6167 (3.1197)	grad_norm 2.7637 (2.4112)	mem 14853MB
[2022-11-07 10:42:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][100/1251]	eta 0:09:20 lr 0.000127	time 0.4675 (0.4871)	loss 1.9701 (3.0522)	grad_norm 2.5807 (2.4065)	mem 14853MB
[2022-11-07 10:42:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][150/1251]	eta 0:08:49 lr 0.000127	time 0.4639 (0.4807)	loss 2.5313 (3.0730)	grad_norm 2.4959 (2.3782)	mem 14853MB
[2022-11-07 10:43:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][200/1251]	eta 0:08:21 lr 0.000126	time 0.4612 (0.4767)	loss 3.4182 (3.0632)	grad_norm 2.6636 (2.3848)	mem 14853MB
[2022-11-07 10:43:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][250/1251]	eta 0:07:54 lr 0.000126	time 0.4642 (0.4743)	loss 2.8611 (3.0537)	grad_norm 2.6679 (2.3973)	mem 14853MB
[2022-11-07 10:43:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][300/1251]	eta 0:07:29 lr 0.000126	time 0.4651 (0.4730)	loss 3.5168 (3.0399)	grad_norm 2.2304 (2.3923)	mem 14853MB
[2022-11-07 10:44:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][350/1251]	eta 0:07:05 lr 0.000126	time 0.4654 (0.4720)	loss 3.4881 (3.0370)	grad_norm 2.5726 (2.3954)	mem 14853MB
[2022-11-07 10:44:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][400/1251]	eta 0:06:41 lr 0.000126	time 0.4628 (0.4713)	loss 3.2877 (3.0443)	grad_norm 2.2722 (2.3997)	mem 14853MB
[2022-11-07 10:45:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][450/1251]	eta 0:06:16 lr 0.000126	time 0.4680 (0.4705)	loss 3.7835 (3.0443)	grad_norm 2.3021 (2.4044)	mem 14853MB
[2022-11-07 10:45:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][500/1251]	eta 0:05:52 lr 0.000126	time 0.4548 (0.4698)	loss 3.0375 (3.0451)	grad_norm 2.4944 (2.4006)	mem 14853MB
[2022-11-07 10:45:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][550/1251]	eta 0:05:29 lr 0.000125	time 0.4652 (0.4698)	loss 3.5391 (3.0492)	grad_norm 2.3720 (2.4021)	mem 14853MB
[2022-11-07 10:46:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][600/1251]	eta 0:05:05 lr 0.000125	time 0.4674 (0.4698)	loss 3.0221 (3.0368)	grad_norm 2.8984 (2.4023)	mem 14853MB
[2022-11-07 10:46:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][650/1251]	eta 0:04:42 lr 0.000125	time 0.4640 (0.4695)	loss 2.9794 (3.0286)	grad_norm 2.1972 (2.4016)	mem 14853MB
[2022-11-07 10:47:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][700/1251]	eta 0:04:18 lr 0.000125	time 0.4629 (0.4691)	loss 3.1943 (3.0256)	grad_norm 2.2993 (2.4067)	mem 14853MB
[2022-11-07 10:47:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][750/1251]	eta 0:03:54 lr 0.000125	time 0.5341 (0.4689)	loss 2.5570 (3.0236)	grad_norm 2.6226 (2.4032)	mem 14853MB
[2022-11-07 10:47:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][800/1251]	eta 0:03:31 lr 0.000125	time 0.4571 (0.4687)	loss 3.0939 (3.0163)	grad_norm 2.3227 (2.4027)	mem 14853MB
[2022-11-07 10:48:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][850/1251]	eta 0:03:07 lr 0.000125	time 0.4630 (0.4688)	loss 3.0420 (3.0200)	grad_norm 2.7791 (2.4021)	mem 14853MB
[2022-11-07 10:48:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][900/1251]	eta 0:02:44 lr 0.000125	time 0.4632 (0.4687)	loss 3.4598 (3.0159)	grad_norm 2.1921 (2.4030)	mem 14853MB
[2022-11-07 10:48:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][950/1251]	eta 0:02:21 lr 0.000124	time 0.4658 (0.4686)	loss 2.4191 (3.0165)	grad_norm 2.3988 (2.4009)	mem 14853MB
[2022-11-07 10:49:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][1000/1251]	eta 0:01:57 lr 0.000124	time 0.4752 (0.4684)	loss 2.2385 (3.0194)	grad_norm 2.3695 (2.3981)	mem 14853MB
[2022-11-07 10:49:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][1050/1251]	eta 0:01:34 lr 0.000124	time 0.4595 (0.4684)	loss 2.1334 (3.0229)	grad_norm 2.8363 (2.3975)	mem 14853MB
[2022-11-07 10:50:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][1100/1251]	eta 0:01:10 lr 0.000124	time 0.4584 (0.4685)	loss 3.1976 (3.0231)	grad_norm 2.3873 (2.4006)	mem 14853MB
[2022-11-07 10:50:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][1150/1251]	eta 0:00:47 lr 0.000124	time 0.4708 (0.4684)	loss 2.3262 (3.0232)	grad_norm 2.0876 (2.4057)	mem 14853MB
[2022-11-07 10:50:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][1200/1251]	eta 0:00:23 lr 0.000124	time 0.4725 (0.4682)	loss 3.3057 (3.0257)	grad_norm 2.4962 (2.4077)	mem 14853MB
[2022-11-07 10:51:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [233/300][1250/1251]	eta 0:00:00 lr 0.000124	time 0.4590 (0.4681)	loss 3.3542 (3.0277)	grad_norm 2.5821 (2.4050)	mem 14853MB
[2022-11-07 10:51:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 233 training takes 0:09:45
[2022-11-07 10:51:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_233.pth saving......
[2022-11-07 10:51:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_233.pth saved !!!
[2022-11-07 10:51:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.608 (1.608)	Loss 0.8379 (0.8379)	Acc@1 80.273 (80.273)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 10:51:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.176 Acc@5 95.636
[2022-11-07 10:51:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.2%
[2022-11-07 10:51:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.666 (1.666)	Loss 0.8054 (0.8054)	Acc@1 81.055 (81.055)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 10:51:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.092 Acc@5 96.024
[2022-11-07 10:51:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.1%
[2022-11-07 10:51:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.09% at 233 epoch
[2022-11-07 10:51:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][0/1251]	eta 0:42:44 lr 0.000124	time 2.0497 (2.0497)	loss 3.3434 (3.3434)	grad_norm 2.1356 (2.1356)	mem 14853MB
[2022-11-07 10:52:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][50/1251]	eta 0:10:01 lr 0.000123	time 0.4686 (0.5010)	loss 3.2839 (3.1682)	grad_norm 2.3098 (2.4106)	mem 14853MB
[2022-11-07 10:52:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][100/1251]	eta 0:09:19 lr 0.000123	time 0.4644 (0.4864)	loss 3.4672 (3.0875)	grad_norm 2.1079 (2.4337)	mem 14853MB
[2022-11-07 10:52:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][150/1251]	eta 0:08:48 lr 0.000123	time 0.4812 (0.4802)	loss 2.0986 (3.0324)	grad_norm 2.3006 (2.4100)	mem 14853MB
[2022-11-07 10:53:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][200/1251]	eta 0:08:21 lr 0.000123	time 0.4661 (0.4767)	loss 2.4047 (3.0023)	grad_norm 2.3844 (2.4043)	mem 14853MB
[2022-11-07 10:53:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][250/1251]	eta 0:07:54 lr 0.000123	time 0.4780 (0.4745)	loss 3.5181 (2.9888)	grad_norm 2.5088 (2.4023)	mem 14853MB
[2022-11-07 10:53:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][300/1251]	eta 0:07:29 lr 0.000123	time 0.4662 (0.4731)	loss 3.3475 (2.9810)	grad_norm 2.2404 (2.4149)	mem 14853MB
[2022-11-07 10:54:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][350/1251]	eta 0:07:05 lr 0.000123	time 0.4768 (0.4723)	loss 2.3102 (2.9866)	grad_norm 2.5330 (2.4146)	mem 14853MB
[2022-11-07 10:54:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][400/1251]	eta 0:06:41 lr 0.000123	time 0.4599 (0.4714)	loss 2.7470 (2.9846)	grad_norm 2.5024 (2.4190)	mem 14853MB
[2022-11-07 10:55:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][450/1251]	eta 0:06:17 lr 0.000122	time 0.4620 (0.4709)	loss 3.4427 (2.9793)	grad_norm 2.2991 (2.4178)	mem 14853MB
[2022-11-07 10:55:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][500/1251]	eta 0:05:53 lr 0.000122	time 0.4620 (0.4706)	loss 3.2476 (2.9776)	grad_norm 2.1648 (2.4162)	mem 14853MB
[2022-11-07 10:55:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][550/1251]	eta 0:05:29 lr 0.000122	time 0.4667 (0.4701)	loss 3.2468 (2.9842)	grad_norm 2.4693 (inf)	mem 14853MB
[2022-11-07 10:56:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][600/1251]	eta 0:05:05 lr 0.000122	time 0.4641 (0.4699)	loss 3.7014 (2.9843)	grad_norm 2.4690 (inf)	mem 14853MB
[2022-11-07 10:56:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][650/1251]	eta 0:04:42 lr 0.000122	time 0.4525 (0.4695)	loss 2.9990 (2.9791)	grad_norm 2.3125 (inf)	mem 14853MB
[2022-11-07 10:57:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][700/1251]	eta 0:04:18 lr 0.000122	time 0.4682 (0.4692)	loss 3.3787 (2.9821)	grad_norm 2.5818 (inf)	mem 14853MB
[2022-11-07 10:57:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][750/1251]	eta 0:03:54 lr 0.000122	time 0.4517 (0.4690)	loss 3.1192 (2.9825)	grad_norm 2.7092 (inf)	mem 14853MB
[2022-11-07 10:57:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][800/1251]	eta 0:03:31 lr 0.000121	time 0.4653 (0.4690)	loss 2.4939 (2.9879)	grad_norm 2.2354 (inf)	mem 14853MB
[2022-11-07 10:58:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][850/1251]	eta 0:03:08 lr 0.000121	time 0.4659 (0.4689)	loss 3.2051 (2.9875)	grad_norm 2.7722 (inf)	mem 14853MB
[2022-11-07 10:58:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][900/1251]	eta 0:02:44 lr 0.000121	time 0.4629 (0.4686)	loss 3.2388 (2.9929)	grad_norm 2.7773 (inf)	mem 14853MB
[2022-11-07 10:59:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][950/1251]	eta 0:02:21 lr 0.000121	time 0.4639 (0.4685)	loss 2.3020 (2.9930)	grad_norm 2.4363 (nan)	mem 14853MB
[2022-11-07 10:59:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][1000/1251]	eta 0:01:57 lr 0.000121	time 0.4567 (0.4684)	loss 3.2462 (2.9914)	grad_norm 2.1730 (nan)	mem 14853MB
[2022-11-07 10:59:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][1050/1251]	eta 0:01:34 lr 0.000121	time 0.4610 (0.4684)	loss 2.8875 (2.9903)	grad_norm 2.8351 (nan)	mem 14853MB
[2022-11-07 11:00:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][1100/1251]	eta 0:01:10 lr 0.000121	time 0.4623 (0.4684)	loss 3.4632 (2.9915)	grad_norm 2.2256 (nan)	mem 14853MB
[2022-11-07 11:00:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][1150/1251]	eta 0:00:47 lr 0.000121	time 0.4690 (0.4682)	loss 2.9834 (2.9882)	grad_norm 2.3621 (nan)	mem 14853MB
[2022-11-07 11:00:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][1200/1251]	eta 0:00:23 lr 0.000120	time 0.4633 (0.4681)	loss 3.6375 (2.9876)	grad_norm 2.0757 (nan)	mem 14853MB
[2022-11-07 11:01:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [234/300][1250/1251]	eta 0:00:00 lr 0.000120	time 0.4571 (0.4679)	loss 3.2729 (2.9860)	grad_norm 2.0357 (nan)	mem 14853MB
[2022-11-07 11:01:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 234 training takes 0:09:45
[2022-11-07 11:01:20 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_234.pth saving......
[2022-11-07 11:01:21 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_234.pth saved !!!
[2022-11-07 11:01:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.492 (1.492)	Loss 0.7978 (0.7978)	Acc@1 80.762 (80.762)	Acc@5 96.387 (96.387)	Mem 14853MB
[2022-11-07 11:01:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.224 Acc@5 95.662
[2022-11-07 11:01:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.2%
[2022-11-07 11:01:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.666 (1.666)	Loss 0.6876 (0.6876)	Acc@1 85.059 (85.059)	Acc@5 96.191 (96.191)	Mem 14853MB
[2022-11-07 11:01:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.058 Acc@5 96.012
[2022-11-07 11:01:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.1%
[2022-11-07 11:01:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.09% at 233 epoch
[2022-11-07 11:01:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][0/1251]	eta 0:42:01 lr 0.000120	time 2.0160 (2.0160)	loss 3.0676 (3.0676)	grad_norm 2.3096 (2.3096)	mem 14853MB
[2022-11-07 11:02:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][50/1251]	eta 0:10:01 lr 0.000120	time 0.4532 (0.5008)	loss 3.2695 (3.0025)	grad_norm 2.6184 (2.5050)	mem 14853MB
[2022-11-07 11:02:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][100/1251]	eta 0:09:18 lr 0.000120	time 0.4585 (0.4853)	loss 3.4467 (2.9509)	grad_norm 2.3841 (2.4679)	mem 14853MB
[2022-11-07 11:02:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][150/1251]	eta 0:08:47 lr 0.000120	time 0.4663 (0.4787)	loss 3.3512 (2.9443)	grad_norm 2.1359 (2.4366)	mem 14853MB
[2022-11-07 11:03:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][200/1251]	eta 0:08:19 lr 0.000120	time 0.4549 (0.4756)	loss 2.6314 (2.9722)	grad_norm 2.3693 (2.4146)	mem 14853MB
[2022-11-07 11:03:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][250/1251]	eta 0:07:54 lr 0.000120	time 0.4538 (0.4738)	loss 2.8214 (2.9633)	grad_norm 2.4719 (2.4059)	mem 14853MB
[2022-11-07 11:04:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][300/1251]	eta 0:07:29 lr 0.000120	time 0.4750 (0.4726)	loss 3.0837 (2.9793)	grad_norm 2.4955 (2.4043)	mem 14853MB
[2022-11-07 11:04:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][350/1251]	eta 0:07:04 lr 0.000119	time 0.4704 (0.4715)	loss 2.2051 (2.9775)	grad_norm 2.2034 (2.4118)	mem 14853MB
[2022-11-07 11:04:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][400/1251]	eta 0:06:40 lr 0.000119	time 0.4616 (0.4706)	loss 2.8976 (2.9740)	grad_norm 2.3663 (2.4102)	mem 14853MB
[2022-11-07 11:05:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][450/1251]	eta 0:06:16 lr 0.000119	time 0.4665 (0.4700)	loss 3.2912 (2.9774)	grad_norm 2.1478 (2.4135)	mem 14853MB
[2022-11-07 11:05:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][500/1251]	eta 0:05:52 lr 0.000119	time 0.4696 (0.4696)	loss 2.4683 (2.9719)	grad_norm 2.3282 (2.4179)	mem 14853MB
[2022-11-07 11:05:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][550/1251]	eta 0:05:29 lr 0.000119	time 0.4591 (0.4695)	loss 3.0951 (2.9790)	grad_norm 2.2467 (2.4193)	mem 14853MB
[2022-11-07 11:06:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][600/1251]	eta 0:05:05 lr 0.000119	time 0.4640 (0.4693)	loss 3.1053 (2.9799)	grad_norm 2.3972 (2.4220)	mem 14853MB
[2022-11-07 11:06:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][650/1251]	eta 0:04:41 lr 0.000119	time 0.4651 (0.4689)	loss 3.3583 (2.9840)	grad_norm 2.4398 (2.4205)	mem 14853MB
[2022-11-07 11:07:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][700/1251]	eta 0:04:18 lr 0.000118	time 0.5566 (0.4689)	loss 3.2612 (2.9795)	grad_norm 2.8018 (2.4215)	mem 14853MB
[2022-11-07 11:07:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][750/1251]	eta 0:03:54 lr 0.000118	time 0.4591 (0.4687)	loss 2.1411 (2.9747)	grad_norm 2.7167 (2.4230)	mem 14853MB
[2022-11-07 11:07:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][800/1251]	eta 0:03:31 lr 0.000118	time 0.4610 (0.4687)	loss 3.0953 (2.9759)	grad_norm 2.9378 (2.4265)	mem 14853MB
[2022-11-07 11:08:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][850/1251]	eta 0:03:07 lr 0.000118	time 0.4570 (0.4685)	loss 2.7005 (2.9806)	grad_norm 2.7218 (2.4318)	mem 14853MB
[2022-11-07 11:08:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][900/1251]	eta 0:02:44 lr 0.000118	time 0.4661 (0.4685)	loss 3.6557 (2.9806)	grad_norm 2.3732 (2.4321)	mem 14853MB
[2022-11-07 11:09:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][950/1251]	eta 0:02:20 lr 0.000118	time 0.4749 (0.4684)	loss 2.8841 (2.9759)	grad_norm 2.5379 (2.4333)	mem 14853MB
[2022-11-07 11:09:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][1000/1251]	eta 0:01:57 lr 0.000118	time 0.4606 (0.4683)	loss 2.7788 (2.9795)	grad_norm 2.3545 (2.4330)	mem 14853MB
[2022-11-07 11:09:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][1050/1251]	eta 0:01:34 lr 0.000118	time 0.4599 (0.4684)	loss 3.4633 (2.9847)	grad_norm 2.2728 (2.4312)	mem 14853MB
[2022-11-07 11:10:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][1100/1251]	eta 0:01:10 lr 0.000117	time 0.4548 (0.4682)	loss 3.1035 (2.9843)	grad_norm 2.9518 (2.4316)	mem 14853MB
[2022-11-07 11:10:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][1150/1251]	eta 0:00:47 lr 0.000117	time 0.4649 (0.4681)	loss 2.8955 (2.9814)	grad_norm 2.4411 (2.4353)	mem 14853MB
[2022-11-07 11:11:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][1200/1251]	eta 0:00:23 lr 0.000117	time 0.5461 (0.4682)	loss 2.2864 (2.9815)	grad_norm 2.4816 (2.4345)	mem 14853MB
[2022-11-07 11:11:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [235/300][1250/1251]	eta 0:00:00 lr 0.000117	time 0.4584 (0.4680)	loss 3.2618 (2.9748)	grad_norm 2.2815 (2.4318)	mem 14853MB
[2022-11-07 11:11:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 235 training takes 0:09:45
[2022-11-07 11:11:24 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_235.pth saving......
[2022-11-07 11:11:25 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_235.pth saved !!!
[2022-11-07 11:11:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.477 (1.477)	Loss 0.8339 (0.8339)	Acc@1 80.859 (80.859)	Acc@5 95.215 (95.215)	Mem 14853MB
[2022-11-07 11:11:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.354 Acc@5 95.688
[2022-11-07 11:11:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.4%
[2022-11-07 11:11:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.628 (1.628)	Loss 0.7760 (0.7760)	Acc@1 82.324 (82.324)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 11:11:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.058 Acc@5 96.016
[2022-11-07 11:11:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.1%
[2022-11-07 11:11:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.09% at 233 epoch
[2022-11-07 11:11:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][0/1251]	eta 0:39:18 lr 0.000117	time 1.8853 (1.8853)	loss 3.5824 (3.5824)	grad_norm 2.4241 (2.4241)	mem 14853MB
[2022-11-07 11:12:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][50/1251]	eta 0:10:03 lr 0.000117	time 0.4648 (0.5024)	loss 3.6341 (3.0231)	grad_norm 2.0792 (2.4445)	mem 14853MB
[2022-11-07 11:12:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][100/1251]	eta 0:09:18 lr 0.000117	time 0.4644 (0.4852)	loss 2.3797 (3.0075)	grad_norm 2.3993 (2.4530)	mem 14853MB
[2022-11-07 11:12:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][150/1251]	eta 0:08:47 lr 0.000117	time 0.4648 (0.4794)	loss 3.0296 (2.9993)	grad_norm 2.5939 (2.4530)	mem 14853MB
[2022-11-07 11:13:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][200/1251]	eta 0:08:20 lr 0.000117	time 0.4542 (0.4759)	loss 3.3238 (2.9786)	grad_norm 3.4652 (2.4589)	mem 14853MB
[2022-11-07 11:13:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][250/1251]	eta 0:07:54 lr 0.000116	time 0.4561 (0.4738)	loss 3.3624 (2.9579)	grad_norm 2.5856 (2.4518)	mem 14853MB
[2022-11-07 11:14:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][300/1251]	eta 0:07:29 lr 0.000116	time 0.4670 (0.4722)	loss 3.4488 (2.9641)	grad_norm 2.4236 (2.4426)	mem 14853MB
[2022-11-07 11:14:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][350/1251]	eta 0:07:04 lr 0.000116	time 0.4592 (0.4712)	loss 3.0552 (2.9732)	grad_norm 2.4403 (2.4494)	mem 14853MB
[2022-11-07 11:14:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][400/1251]	eta 0:06:40 lr 0.000116	time 0.4689 (0.4706)	loss 3.3858 (2.9716)	grad_norm 2.3170 (2.4438)	mem 14853MB
[2022-11-07 11:15:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][450/1251]	eta 0:06:16 lr 0.000116	time 0.4695 (0.4701)	loss 3.1940 (2.9762)	grad_norm 2.4718 (2.4393)	mem 14853MB
[2022-11-07 11:15:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][500/1251]	eta 0:05:52 lr 0.000116	time 0.4547 (0.4696)	loss 2.5929 (2.9700)	grad_norm 2.4802 (2.4532)	mem 14853MB
[2022-11-07 11:16:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][550/1251]	eta 0:05:29 lr 0.000116	time 0.4613 (0.4696)	loss 2.8512 (2.9753)	grad_norm 2.2498 (2.4517)	mem 14853MB
[2022-11-07 11:16:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][600/1251]	eta 0:05:05 lr 0.000116	time 0.4608 (0.4693)	loss 3.2616 (2.9713)	grad_norm 2.4190 (2.4516)	mem 14853MB
[2022-11-07 11:16:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][650/1251]	eta 0:04:41 lr 0.000115	time 0.4557 (0.4691)	loss 3.2810 (2.9703)	grad_norm 2.3061 (2.4465)	mem 14853MB
[2022-11-07 11:17:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][700/1251]	eta 0:04:18 lr 0.000115	time 0.4713 (0.4689)	loss 3.2236 (2.9639)	grad_norm 2.1925 (2.4391)	mem 14853MB
[2022-11-07 11:17:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][750/1251]	eta 0:03:54 lr 0.000115	time 0.4630 (0.4690)	loss 3.1310 (2.9675)	grad_norm 2.3693 (2.4380)	mem 14853MB
[2022-11-07 11:17:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][800/1251]	eta 0:03:31 lr 0.000115	time 0.4552 (0.4689)	loss 3.0920 (2.9641)	grad_norm 2.3280 (2.4371)	mem 14853MB
[2022-11-07 11:18:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][850/1251]	eta 0:03:07 lr 0.000115	time 0.4617 (0.4688)	loss 3.5589 (2.9632)	grad_norm 2.6050 (2.4389)	mem 14853MB
[2022-11-07 11:18:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][900/1251]	eta 0:02:44 lr 0.000115	time 0.4874 (0.4687)	loss 2.7202 (2.9601)	grad_norm 2.5513 (2.4460)	mem 14853MB
[2022-11-07 11:19:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][950/1251]	eta 0:02:21 lr 0.000115	time 0.4739 (0.4686)	loss 3.4210 (2.9615)	grad_norm 2.5128 (2.4461)	mem 14853MB
[2022-11-07 11:19:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][1000/1251]	eta 0:01:57 lr 0.000115	time 0.4553 (0.4685)	loss 3.7009 (2.9601)	grad_norm 2.6959 (2.4465)	mem 14853MB
[2022-11-07 11:19:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][1050/1251]	eta 0:01:34 lr 0.000114	time 0.4555 (0.4684)	loss 3.3716 (2.9622)	grad_norm 2.3448 (2.4476)	mem 14853MB
[2022-11-07 11:20:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][1100/1251]	eta 0:01:10 lr 0.000114	time 0.4661 (0.4683)	loss 2.7819 (2.9638)	grad_norm 2.7988 (2.4512)	mem 14853MB
[2022-11-07 11:20:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][1150/1251]	eta 0:00:47 lr 0.000114	time 0.4607 (0.4683)	loss 2.9431 (2.9638)	grad_norm 2.3434 (2.4514)	mem 14853MB
[2022-11-07 11:21:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][1200/1251]	eta 0:00:23 lr 0.000114	time 0.4625 (0.4682)	loss 3.3376 (2.9634)	grad_norm 2.1400 (2.4505)	mem 14853MB
[2022-11-07 11:21:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [236/300][1250/1251]	eta 0:00:00 lr 0.000114	time 0.4579 (0.4680)	loss 3.1921 (2.9668)	grad_norm 2.3135 (2.4515)	mem 14853MB
[2022-11-07 11:21:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 236 training takes 0:09:45
[2022-11-07 11:21:28 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_236.pth saving......
[2022-11-07 11:21:29 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_236.pth saved !!!
[2022-11-07 11:21:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.609 (1.609)	Loss 0.7818 (0.7818)	Acc@1 81.055 (81.055)	Acc@5 96.387 (96.387)	Mem 14853MB
[2022-11-07 11:21:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.258 Acc@5 95.748
[2022-11-07 11:21:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.3%
[2022-11-07 11:21:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.638 (1.638)	Loss 0.7310 (0.7310)	Acc@1 82.715 (82.715)	Acc@5 96.387 (96.387)	Mem 14853MB
[2022-11-07 11:21:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.072 Acc@5 96.016
[2022-11-07 11:21:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.1%
[2022-11-07 11:21:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.09% at 233 epoch
[2022-11-07 11:21:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][0/1251]	eta 0:41:52 lr 0.000114	time 2.0084 (2.0084)	loss 2.9460 (2.9460)	grad_norm 2.5155 (2.5155)	mem 14853MB
[2022-11-07 11:22:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][50/1251]	eta 0:10:02 lr 0.000114	time 0.4639 (0.5017)	loss 3.2160 (2.9584)	grad_norm 2.3852 (2.4959)	mem 14853MB
[2022-11-07 11:22:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][100/1251]	eta 0:09:18 lr 0.000114	time 0.4610 (0.4851)	loss 2.1352 (2.8990)	grad_norm 2.9193 (2.4585)	mem 14853MB
[2022-11-07 11:22:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][150/1251]	eta 0:08:47 lr 0.000113	time 0.4518 (0.4789)	loss 3.2206 (2.8769)	grad_norm 2.4533 (2.4702)	mem 14853MB
[2022-11-07 11:23:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][200/1251]	eta 0:08:19 lr 0.000113	time 0.4655 (0.4755)	loss 3.1208 (2.9213)	grad_norm 2.2627 (2.4782)	mem 14853MB
[2022-11-07 11:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][250/1251]	eta 0:07:54 lr 0.000113	time 0.4662 (0.4740)	loss 3.1941 (2.9288)	grad_norm 2.0435 (2.4710)	mem 14853MB
[2022-11-07 11:24:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][300/1251]	eta 0:07:29 lr 0.000113	time 0.4653 (0.4728)	loss 3.3229 (2.9338)	grad_norm 2.6077 (2.4692)	mem 14853MB
[2022-11-07 11:24:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][350/1251]	eta 0:07:04 lr 0.000113	time 0.4649 (0.4716)	loss 2.5371 (2.9357)	grad_norm 2.5983 (2.4708)	mem 14853MB
[2022-11-07 11:24:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][400/1251]	eta 0:06:40 lr 0.000113	time 0.4719 (0.4708)	loss 2.9390 (2.9461)	grad_norm 2.2772 (2.4641)	mem 14853MB
[2022-11-07 11:25:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][450/1251]	eta 0:06:16 lr 0.000113	time 0.4657 (0.4701)	loss 3.4658 (2.9509)	grad_norm 2.5071 (2.4659)	mem 14853MB
[2022-11-07 11:25:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][500/1251]	eta 0:05:52 lr 0.000113	time 0.4617 (0.4697)	loss 3.0402 (2.9471)	grad_norm 2.4383 (2.4731)	mem 14853MB
[2022-11-07 11:26:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][550/1251]	eta 0:05:29 lr 0.000112	time 0.4677 (0.4698)	loss 2.6238 (2.9454)	grad_norm 2.5244 (2.4748)	mem 14853MB
[2022-11-07 11:26:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][600/1251]	eta 0:05:05 lr 0.000112	time 0.4669 (0.4696)	loss 3.2265 (2.9568)	grad_norm 2.2733 (2.4711)	mem 14853MB
[2022-11-07 11:26:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][650/1251]	eta 0:04:42 lr 0.000112	time 0.4639 (0.4693)	loss 3.0487 (2.9530)	grad_norm 2.4047 (2.4709)	mem 14853MB
[2022-11-07 11:27:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][700/1251]	eta 0:04:18 lr 0.000112	time 0.5315 (0.4691)	loss 3.6797 (2.9530)	grad_norm 2.4473 (2.4716)	mem 14853MB
[2022-11-07 11:27:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][750/1251]	eta 0:03:54 lr 0.000112	time 0.4614 (0.4689)	loss 3.4237 (2.9508)	grad_norm 2.4856 (2.4680)	mem 14853MB
[2022-11-07 11:28:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][800/1251]	eta 0:03:31 lr 0.000112	time 0.4674 (0.4689)	loss 3.3555 (2.9538)	grad_norm 2.3436 (2.4623)	mem 14853MB
[2022-11-07 11:28:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][850/1251]	eta 0:03:08 lr 0.000112	time 0.4681 (0.4689)	loss 2.8452 (2.9536)	grad_norm 2.2075 (2.4597)	mem 14853MB
[2022-11-07 11:28:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][900/1251]	eta 0:02:44 lr 0.000112	time 0.4562 (0.4686)	loss 2.0906 (2.9570)	grad_norm 2.4321 (2.4580)	mem 14853MB
[2022-11-07 11:29:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][950/1251]	eta 0:02:21 lr 0.000111	time 0.4682 (0.4685)	loss 3.5388 (2.9594)	grad_norm 2.6347 (2.4591)	mem 14853MB
[2022-11-07 11:29:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][1000/1251]	eta 0:01:57 lr 0.000111	time 0.4632 (0.4684)	loss 2.9584 (2.9586)	grad_norm 2.4330 (2.4569)	mem 14853MB
[2022-11-07 11:29:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][1050/1251]	eta 0:01:34 lr 0.000111	time 0.4605 (0.4685)	loss 2.6629 (2.9578)	grad_norm 2.4033 (2.4583)	mem 14853MB
[2022-11-07 11:30:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][1100/1251]	eta 0:01:10 lr 0.000111	time 0.4709 (0.4685)	loss 2.7908 (2.9541)	grad_norm 2.6090 (2.4584)	mem 14853MB
[2022-11-07 11:30:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][1150/1251]	eta 0:00:47 lr 0.000111	time 0.4762 (0.4684)	loss 2.1820 (2.9545)	grad_norm 2.1277 (2.4560)	mem 14853MB
[2022-11-07 11:31:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][1200/1251]	eta 0:00:23 lr 0.000111	time 0.5486 (0.4683)	loss 3.2679 (2.9552)	grad_norm 2.2381 (inf)	mem 14853MB
[2022-11-07 11:31:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [237/300][1250/1251]	eta 0:00:00 lr 0.000111	time 0.4573 (0.4681)	loss 3.0507 (2.9585)	grad_norm 2.7299 (inf)	mem 14853MB
[2022-11-07 11:31:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 237 training takes 0:09:45
[2022-11-07 11:31:32 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_237.pth saving......
[2022-11-07 11:31:33 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_237.pth saved !!!
[2022-11-07 11:31:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.621 (1.621)	Loss 0.8208 (0.8208)	Acc@1 81.445 (81.445)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 11:31:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.298 Acc@5 95.706
[2022-11-07 11:31:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.3%
[2022-11-07 11:31:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.614 (1.614)	Loss 0.7225 (0.7225)	Acc@1 82.422 (82.422)	Acc@5 97.070 (97.070)	Mem 14853MB
[2022-11-07 11:31:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.086 Acc@5 96.020
[2022-11-07 11:31:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.1%
[2022-11-07 11:31:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.09% at 233 epoch
[2022-11-07 11:31:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][0/1251]	eta 0:40:59 lr 0.000111	time 1.9663 (1.9663)	loss 3.2213 (3.2213)	grad_norm 2.6664 (2.6664)	mem 14853MB
[2022-11-07 11:32:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][50/1251]	eta 0:10:01 lr 0.000111	time 0.4712 (0.5011)	loss 3.5623 (3.0530)	grad_norm 2.7035 (2.5483)	mem 14853MB
[2022-11-07 11:32:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][100/1251]	eta 0:09:19 lr 0.000110	time 0.4682 (0.4857)	loss 3.0415 (3.0216)	grad_norm 2.5810 (2.5513)	mem 14853MB
[2022-11-07 11:33:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][150/1251]	eta 0:08:48 lr 0.000110	time 0.4694 (0.4797)	loss 2.2288 (2.9849)	grad_norm 2.0866 (2.5282)	mem 14853MB
[2022-11-07 11:33:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][200/1251]	eta 0:08:20 lr 0.000110	time 0.4680 (0.4758)	loss 2.9417 (2.9887)	grad_norm 2.2144 (2.4947)	mem 14853MB
[2022-11-07 11:33:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][250/1251]	eta 0:07:54 lr 0.000110	time 0.4704 (0.4735)	loss 2.3870 (2.9902)	grad_norm 2.6006 (2.4820)	mem 14853MB
[2022-11-07 11:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][300/1251]	eta 0:07:29 lr 0.000110	time 0.4728 (0.4721)	loss 3.3777 (2.9848)	grad_norm 2.6919 (2.4696)	mem 14853MB
[2022-11-07 11:34:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][350/1251]	eta 0:07:04 lr 0.000110	time 0.4679 (0.4710)	loss 3.3033 (2.9876)	grad_norm 2.4097 (2.4719)	mem 14853MB
[2022-11-07 11:34:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][400/1251]	eta 0:06:40 lr 0.000110	time 0.4626 (0.4703)	loss 3.1334 (2.9935)	grad_norm 2.6063 (2.4731)	mem 14853MB
[2022-11-07 11:35:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][450/1251]	eta 0:06:16 lr 0.000110	time 0.4642 (0.4697)	loss 3.0081 (2.9812)	grad_norm 2.3936 (2.4703)	mem 14853MB
[2022-11-07 11:35:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][500/1251]	eta 0:05:52 lr 0.000109	time 0.4628 (0.4693)	loss 3.2119 (2.9673)	grad_norm 2.2330 (2.4681)	mem 14853MB
[2022-11-07 11:36:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][550/1251]	eta 0:05:29 lr 0.000109	time 0.4618 (0.4694)	loss 2.7360 (2.9735)	grad_norm 2.4008 (2.4704)	mem 14853MB
[2022-11-07 11:36:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][600/1251]	eta 0:05:05 lr 0.000109	time 0.4705 (0.4690)	loss 2.4365 (2.9699)	grad_norm 2.8824 (2.4667)	mem 14853MB
[2022-11-07 11:36:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][650/1251]	eta 0:04:41 lr 0.000109	time 0.4708 (0.4688)	loss 2.6089 (2.9746)	grad_norm 2.6545 (2.4711)	mem 14853MB
[2022-11-07 11:37:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][700/1251]	eta 0:04:18 lr 0.000109	time 0.4613 (0.4686)	loss 3.0834 (2.9659)	grad_norm 2.3000 (2.4695)	mem 14853MB
[2022-11-07 11:37:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][750/1251]	eta 0:03:54 lr 0.000109	time 0.4573 (0.4685)	loss 2.6984 (2.9688)	grad_norm 2.4702 (2.4710)	mem 14853MB
[2022-11-07 11:38:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][800/1251]	eta 0:03:31 lr 0.000109	time 0.4733 (0.4686)	loss 3.0756 (2.9727)	grad_norm 2.6789 (2.4678)	mem 14853MB
[2022-11-07 11:38:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][850/1251]	eta 0:03:07 lr 0.000109	time 0.4609 (0.4685)	loss 2.9872 (2.9721)	grad_norm 2.4079 (2.4667)	mem 14853MB
[2022-11-07 11:38:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][900/1251]	eta 0:02:44 lr 0.000108	time 0.4513 (0.4682)	loss 3.2068 (2.9726)	grad_norm 2.7190 (2.4686)	mem 14853MB
[2022-11-07 11:39:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][950/1251]	eta 0:02:20 lr 0.000108	time 0.4560 (0.4681)	loss 3.1076 (2.9756)	grad_norm 2.1223 (2.4683)	mem 14853MB
[2022-11-07 11:39:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][1000/1251]	eta 0:01:57 lr 0.000108	time 0.4660 (0.4681)	loss 3.2473 (2.9764)	grad_norm 2.2262 (2.4658)	mem 14853MB
[2022-11-07 11:40:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][1050/1251]	eta 0:01:34 lr 0.000108	time 0.4743 (0.4682)	loss 3.3718 (2.9782)	grad_norm 2.4857 (2.4661)	mem 14853MB
[2022-11-07 11:40:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][1100/1251]	eta 0:01:10 lr 0.000108	time 0.4581 (0.4681)	loss 3.1941 (2.9746)	grad_norm 2.4503 (2.4655)	mem 14853MB
[2022-11-07 11:40:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][1150/1251]	eta 0:00:47 lr 0.000108	time 0.4708 (0.4679)	loss 3.4751 (2.9742)	grad_norm 2.5312 (2.4710)	mem 14853MB
[2022-11-07 11:41:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][1200/1251]	eta 0:00:23 lr 0.000108	time 0.4698 (0.4679)	loss 3.1251 (2.9752)	grad_norm 2.3111 (2.4697)	mem 14853MB
[2022-11-07 11:41:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [238/300][1250/1251]	eta 0:00:00 lr 0.000108	time 0.4570 (0.4677)	loss 2.9338 (2.9723)	grad_norm 2.3044 (2.4698)	mem 14853MB
[2022-11-07 11:41:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 238 training takes 0:09:45
[2022-11-07 11:41:35 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_238.pth saving......
[2022-11-07 11:41:36 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_238.pth saved !!!
[2022-11-07 11:41:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.568 (1.568)	Loss 0.8005 (0.8005)	Acc@1 81.543 (81.543)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 11:41:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.184 Acc@5 95.722
[2022-11-07 11:41:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.2%
[2022-11-07 11:41:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.650 (1.650)	Loss 0.7905 (0.7905)	Acc@1 81.641 (81.641)	Acc@5 96.289 (96.289)	Mem 14853MB
[2022-11-07 11:41:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.118 Acc@5 96.032
[2022-11-07 11:41:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.1%
[2022-11-07 11:41:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.12% at 238 epoch
[2022-11-07 11:41:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][0/1251]	eta 0:40:07 lr 0.000108	time 1.9243 (1.9243)	loss 3.2289 (3.2289)	grad_norm 2.6396 (2.6396)	mem 14853MB
[2022-11-07 11:42:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][50/1251]	eta 0:10:01 lr 0.000107	time 0.4620 (0.5007)	loss 3.3171 (2.8878)	grad_norm 2.4286 (2.4439)	mem 14853MB
[2022-11-07 11:42:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][100/1251]	eta 0:09:17 lr 0.000107	time 0.4530 (0.4840)	loss 2.3311 (2.8822)	grad_norm 2.6878 (2.4432)	mem 14853MB
[2022-11-07 11:43:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][150/1251]	eta 0:08:46 lr 0.000107	time 0.4624 (0.4785)	loss 3.4698 (2.9284)	grad_norm 2.3402 (2.4472)	mem 14853MB
[2022-11-07 11:43:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][200/1251]	eta 0:08:19 lr 0.000107	time 0.4685 (0.4755)	loss 2.3053 (2.9582)	grad_norm 2.3075 (2.4629)	mem 14853MB
[2022-11-07 11:43:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][250/1251]	eta 0:07:54 lr 0.000107	time 0.4494 (0.4737)	loss 3.1904 (2.9798)	grad_norm 2.3888 (2.4815)	mem 14853MB
[2022-11-07 11:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][300/1251]	eta 0:07:29 lr 0.000107	time 0.4825 (0.4724)	loss 2.9908 (2.9801)	grad_norm 2.4375 (2.4734)	mem 14853MB
[2022-11-07 11:44:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][350/1251]	eta 0:07:04 lr 0.000107	time 0.4629 (0.4716)	loss 2.9049 (2.9794)	grad_norm 2.6347 (2.4717)	mem 14853MB
[2022-11-07 11:45:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][400/1251]	eta 0:06:40 lr 0.000107	time 0.4642 (0.4709)	loss 3.4863 (2.9866)	grad_norm 2.4017 (2.4716)	mem 14853MB
[2022-11-07 11:45:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][450/1251]	eta 0:06:16 lr 0.000106	time 0.4606 (0.4702)	loss 1.8888 (2.9791)	grad_norm 3.1005 (2.4708)	mem 14853MB
[2022-11-07 11:45:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][500/1251]	eta 0:05:52 lr 0.000106	time 0.4673 (0.4698)	loss 2.7884 (2.9793)	grad_norm 2.4735 (2.4750)	mem 14853MB
[2022-11-07 11:46:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][550/1251]	eta 0:05:29 lr 0.000106	time 0.4655 (0.4698)	loss 3.0087 (2.9728)	grad_norm 2.1354 (2.4754)	mem 14853MB
[2022-11-07 11:46:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][600/1251]	eta 0:05:05 lr 0.000106	time 0.4717 (0.4694)	loss 3.7642 (2.9801)	grad_norm 3.7586 (2.4890)	mem 14853MB
[2022-11-07 11:46:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][650/1251]	eta 0:04:41 lr 0.000106	time 0.4641 (0.4691)	loss 2.5388 (2.9770)	grad_norm 2.4496 (2.4848)	mem 14853MB
[2022-11-07 11:47:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][700/1251]	eta 0:04:18 lr 0.000106	time 0.4618 (0.4687)	loss 3.6492 (2.9750)	grad_norm 2.3300 (2.4839)	mem 14853MB
[2022-11-07 11:47:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][750/1251]	eta 0:03:54 lr 0.000106	time 0.4633 (0.4687)	loss 3.4350 (2.9748)	grad_norm 2.4948 (2.4808)	mem 14853MB
[2022-11-07 11:48:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][800/1251]	eta 0:03:31 lr 0.000106	time 0.4478 (0.4687)	loss 3.3685 (2.9757)	grad_norm inf (inf)	mem 14853MB
[2022-11-07 11:48:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][850/1251]	eta 0:03:07 lr 0.000106	time 0.4567 (0.4685)	loss 2.5205 (2.9760)	grad_norm 2.1990 (inf)	mem 14853MB
[2022-11-07 11:48:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][900/1251]	eta 0:02:44 lr 0.000105	time 0.4704 (0.4683)	loss 2.1043 (2.9705)	grad_norm 2.5832 (inf)	mem 14853MB
[2022-11-07 11:49:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][950/1251]	eta 0:02:20 lr 0.000105	time 0.4647 (0.4681)	loss 2.9187 (2.9657)	grad_norm 2.3769 (inf)	mem 14853MB
[2022-11-07 11:49:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][1000/1251]	eta 0:01:57 lr 0.000105	time 0.4601 (0.4679)	loss 3.1000 (2.9583)	grad_norm 2.6172 (inf)	mem 14853MB
[2022-11-07 11:50:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][1050/1251]	eta 0:01:34 lr 0.000105	time 0.4622 (0.4679)	loss 3.2330 (2.9585)	grad_norm 2.4686 (inf)	mem 14853MB
[2022-11-07 11:50:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][1100/1251]	eta 0:01:10 lr 0.000105	time 0.4768 (0.4678)	loss 3.1984 (2.9586)	grad_norm 2.3707 (inf)	mem 14853MB
[2022-11-07 11:50:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][1150/1251]	eta 0:00:47 lr 0.000105	time 0.4572 (0.4677)	loss 3.2334 (2.9646)	grad_norm 2.1810 (inf)	mem 14853MB
[2022-11-07 11:51:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][1200/1251]	eta 0:00:23 lr 0.000105	time 0.4666 (0.4676)	loss 3.4936 (2.9650)	grad_norm 2.5687 (inf)	mem 14853MB
[2022-11-07 11:51:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [239/300][1250/1251]	eta 0:00:00 lr 0.000105	time 0.4672 (0.4675)	loss 3.4806 (2.9680)	grad_norm 2.4480 (inf)	mem 14853MB
[2022-11-07 11:51:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 239 training takes 0:09:44
[2022-11-07 11:51:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_239.pth saving......
[2022-11-07 11:51:39 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_239.pth saved !!!
[2022-11-07 11:51:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.607 (1.607)	Loss 0.7358 (0.7358)	Acc@1 83.008 (83.008)	Acc@5 96.680 (96.680)	Mem 14853MB
[2022-11-07 11:51:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.354 Acc@5 95.750
[2022-11-07 11:51:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.4%
[2022-11-07 11:51:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.626 (1.626)	Loss 0.7367 (0.7367)	Acc@1 82.715 (82.715)	Acc@5 96.484 (96.484)	Mem 14853MB
[2022-11-07 11:51:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.158 Acc@5 96.036
[2022-11-07 11:51:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-07 11:51:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.16% at 239 epoch
[2022-11-07 11:51:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][0/1251]	eta 0:40:49 lr 0.000105	time 1.9583 (1.9583)	loss 2.3595 (2.3595)	grad_norm 2.3664 (2.3664)	mem 14853MB
[2022-11-07 11:52:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][50/1251]	eta 0:10:03 lr 0.000104	time 0.4575 (0.5029)	loss 2.9251 (2.9201)	grad_norm 2.8724 (2.5079)	mem 14853MB
[2022-11-07 11:52:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][100/1251]	eta 0:09:19 lr 0.000104	time 0.4697 (0.4860)	loss 3.4695 (2.9774)	grad_norm 2.5213 (2.5355)	mem 14853MB
[2022-11-07 11:53:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][150/1251]	eta 0:08:47 lr 0.000104	time 0.4647 (0.4790)	loss 2.6527 (2.9843)	grad_norm 3.0441 (2.5282)	mem 14853MB
[2022-11-07 11:53:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][200/1251]	eta 0:08:20 lr 0.000104	time 0.4619 (0.4760)	loss 3.3089 (2.9489)	grad_norm 1.9628 (2.5408)	mem 14853MB
[2022-11-07 11:53:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][250/1251]	eta 0:07:54 lr 0.000104	time 0.4687 (0.4744)	loss 3.0159 (2.9539)	grad_norm 2.4557 (2.5364)	mem 14853MB
[2022-11-07 11:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][300/1251]	eta 0:07:29 lr 0.000104	time 0.4655 (0.4731)	loss 2.8643 (2.9622)	grad_norm 2.2126 (2.5268)	mem 14853MB
[2022-11-07 11:54:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][350/1251]	eta 0:07:05 lr 0.000104	time 0.4686 (0.4724)	loss 3.2594 (2.9592)	grad_norm 2.3612 (2.5318)	mem 14853MB
[2022-11-07 11:55:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][400/1251]	eta 0:06:41 lr 0.000104	time 0.4583 (0.4715)	loss 3.7581 (2.9623)	grad_norm 2.5396 (2.5270)	mem 14853MB
[2022-11-07 11:55:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][450/1251]	eta 0:06:17 lr 0.000103	time 0.4594 (0.4707)	loss 2.2468 (2.9625)	grad_norm 2.7332 (2.5177)	mem 14853MB
[2022-11-07 11:55:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][500/1251]	eta 0:05:53 lr 0.000103	time 0.4621 (0.4702)	loss 2.2827 (2.9746)	grad_norm 2.7212 (2.5205)	mem 14853MB
[2022-11-07 11:56:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][550/1251]	eta 0:05:29 lr 0.000103	time 0.4648 (0.4699)	loss 3.3781 (2.9642)	grad_norm 2.7064 (2.5208)	mem 14853MB
[2022-11-07 11:56:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][600/1251]	eta 0:05:05 lr 0.000103	time 0.4611 (0.4696)	loss 2.7432 (2.9606)	grad_norm 2.5074 (2.5198)	mem 14853MB
[2022-11-07 11:57:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][650/1251]	eta 0:04:42 lr 0.000103	time 0.4686 (0.4694)	loss 2.8769 (2.9632)	grad_norm 3.0663 (2.5177)	mem 14853MB
[2022-11-07 11:57:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][700/1251]	eta 0:04:18 lr 0.000103	time 0.5432 (0.4692)	loss 2.3396 (2.9621)	grad_norm 2.5968 (2.5181)	mem 14853MB
[2022-11-07 11:57:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][750/1251]	eta 0:03:54 lr 0.000103	time 0.4633 (0.4690)	loss 3.0824 (2.9622)	grad_norm 2.3637 (2.5192)	mem 14853MB
[2022-11-07 11:58:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][800/1251]	eta 0:03:31 lr 0.000103	time 0.4636 (0.4689)	loss 2.1057 (2.9643)	grad_norm 2.6390 (2.5189)	mem 14853MB
[2022-11-07 11:58:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][850/1251]	eta 0:03:07 lr 0.000102	time 0.4628 (0.4688)	loss 3.5243 (2.9670)	grad_norm 2.3435 (2.5217)	mem 14853MB
[2022-11-07 11:58:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][900/1251]	eta 0:02:44 lr 0.000102	time 0.4641 (0.4686)	loss 3.2678 (2.9696)	grad_norm 2.5496 (2.5196)	mem 14853MB
[2022-11-07 11:59:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][950/1251]	eta 0:02:21 lr 0.000102	time 0.4596 (0.4686)	loss 3.0511 (2.9631)	grad_norm 2.5959 (2.5210)	mem 14853MB
[2022-11-07 11:59:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][1000/1251]	eta 0:01:57 lr 0.000102	time 0.4644 (0.4685)	loss 3.4385 (2.9661)	grad_norm 2.2936 (2.5176)	mem 14853MB
[2022-11-07 12:00:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][1050/1251]	eta 0:01:34 lr 0.000102	time 0.4824 (0.4685)	loss 2.0704 (2.9659)	grad_norm 2.8127 (2.5150)	mem 14853MB
[2022-11-07 12:00:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][1100/1251]	eta 0:01:10 lr 0.000102	time 0.4597 (0.4684)	loss 3.0504 (2.9599)	grad_norm 3.3275 (2.5142)	mem 14853MB
[2022-11-07 12:00:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][1150/1251]	eta 0:00:47 lr 0.000102	time 0.4681 (0.4683)	loss 1.8292 (2.9581)	grad_norm 2.7106 (2.5143)	mem 14853MB
[2022-11-07 12:01:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][1200/1251]	eta 0:00:23 lr 0.000102	time 0.5271 (0.4682)	loss 2.8319 (2.9586)	grad_norm 2.3255 (inf)	mem 14853MB
[2022-11-07 12:01:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [240/300][1250/1251]	eta 0:00:00 lr 0.000102	time 0.4565 (0.4682)	loss 2.3885 (2.9579)	grad_norm 2.2871 (inf)	mem 14853MB
[2022-11-07 12:01:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 240 training takes 0:09:45
[2022-11-07 12:01:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_240.pth saving......
[2022-11-07 12:01:43 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_240.pth saved !!!
[2022-11-07 12:01:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.608 (1.608)	Loss 0.7552 (0.7552)	Acc@1 83.105 (83.105)	Acc@5 96.289 (96.289)	Mem 14853MB
[2022-11-07 12:01:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.460 Acc@5 95.778
[2022-11-07 12:01:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.5%
[2022-11-07 12:01:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.687 (1.687)	Loss 0.7263 (0.7263)	Acc@1 82.422 (82.422)	Acc@5 97.266 (97.266)	Mem 14853MB
[2022-11-07 12:02:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.176 Acc@5 96.042
[2022-11-07 12:02:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-07 12:02:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.18% at 240 epoch
[2022-11-07 12:02:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][0/1251]	eta 0:40:48 lr 0.000102	time 1.9571 (1.9571)	loss 2.6302 (2.6302)	grad_norm 2.3276 (2.3276)	mem 14853MB
[2022-11-07 12:02:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][50/1251]	eta 0:09:58 lr 0.000101	time 0.4660 (0.4984)	loss 3.0343 (2.9075)	grad_norm 2.3702 (2.5591)	mem 14853MB
[2022-11-07 12:02:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][100/1251]	eta 0:09:17 lr 0.000101	time 0.4609 (0.4840)	loss 3.3973 (2.9446)	grad_norm 2.4220 (2.5753)	mem 14853MB
[2022-11-07 12:03:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][150/1251]	eta 0:08:47 lr 0.000101	time 0.4558 (0.4789)	loss 3.3627 (2.9622)	grad_norm 2.3980 (2.5557)	mem 14853MB
[2022-11-07 12:03:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][200/1251]	eta 0:08:20 lr 0.000101	time 0.4693 (0.4758)	loss 3.1772 (2.9634)	grad_norm 2.8879 (2.5389)	mem 14853MB
[2022-11-07 12:03:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][250/1251]	eta 0:07:54 lr 0.000101	time 0.4522 (0.4741)	loss 2.8627 (2.9764)	grad_norm 2.3531 (2.5374)	mem 14853MB
[2022-11-07 12:04:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][300/1251]	eta 0:07:29 lr 0.000101	time 0.4530 (0.4726)	loss 2.9343 (2.9693)	grad_norm 2.5133 (2.5434)	mem 14853MB
[2022-11-07 12:04:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][350/1251]	eta 0:07:04 lr 0.000101	time 0.4595 (0.4714)	loss 2.9342 (2.9604)	grad_norm 2.5931 (2.5483)	mem 14853MB
[2022-11-07 12:05:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][400/1251]	eta 0:06:40 lr 0.000101	time 0.4537 (0.4709)	loss 3.2722 (2.9642)	grad_norm 2.5745 (2.5444)	mem 14853MB
[2022-11-07 12:05:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][450/1251]	eta 0:06:16 lr 0.000100	time 0.4644 (0.4703)	loss 3.1335 (2.9714)	grad_norm 2.8778 (2.5409)	mem 14853MB
[2022-11-07 12:05:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][500/1251]	eta 0:05:52 lr 0.000100	time 0.4571 (0.4700)	loss 2.2980 (2.9713)	grad_norm 2.4340 (2.5401)	mem 14853MB
[2022-11-07 12:06:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][550/1251]	eta 0:05:29 lr 0.000100	time 0.4646 (0.4699)	loss 3.2408 (2.9607)	grad_norm 2.4338 (2.5434)	mem 14853MB
[2022-11-07 12:06:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][600/1251]	eta 0:05:05 lr 0.000100	time 0.4645 (0.4696)	loss 3.1198 (2.9690)	grad_norm 2.3064 (2.5399)	mem 14853MB
[2022-11-07 12:07:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][650/1251]	eta 0:04:42 lr 0.000100	time 0.4786 (0.4694)	loss 2.2197 (2.9657)	grad_norm 2.1695 (2.5366)	mem 14853MB
[2022-11-07 12:07:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][700/1251]	eta 0:04:18 lr 0.000100	time 0.4648 (0.4690)	loss 2.4684 (2.9595)	grad_norm 2.2143 (2.5409)	mem 14853MB
[2022-11-07 12:07:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][750/1251]	eta 0:03:54 lr 0.000100	time 0.4770 (0.4688)	loss 2.9619 (2.9647)	grad_norm 2.6947 (2.5398)	mem 14853MB
[2022-11-07 12:08:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][800/1251]	eta 0:03:31 lr 0.000100	time 0.4590 (0.4688)	loss 3.1857 (2.9660)	grad_norm 2.2527 (2.5344)	mem 14853MB
[2022-11-07 12:08:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][850/1251]	eta 0:03:07 lr 0.000099	time 0.4626 (0.4686)	loss 3.0863 (2.9618)	grad_norm 2.5095 (2.5417)	mem 14853MB
[2022-11-07 12:09:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][900/1251]	eta 0:02:44 lr 0.000099	time 0.4667 (0.4685)	loss 2.9340 (2.9693)	grad_norm 2.5415 (2.5413)	mem 14853MB
[2022-11-07 12:09:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][950/1251]	eta 0:02:20 lr 0.000099	time 0.4588 (0.4684)	loss 2.0478 (2.9696)	grad_norm 2.5088 (2.5399)	mem 14853MB
[2022-11-07 12:09:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][1000/1251]	eta 0:01:57 lr 0.000099	time 0.4676 (0.4682)	loss 2.1775 (2.9673)	grad_norm 2.8141 (2.5393)	mem 14853MB
[2022-11-07 12:10:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][1050/1251]	eta 0:01:34 lr 0.000099	time 0.4528 (0.4684)	loss 3.2915 (2.9663)	grad_norm 2.3382 (2.5377)	mem 14853MB
[2022-11-07 12:10:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][1100/1251]	eta 0:01:10 lr 0.000099	time 0.4570 (0.4683)	loss 3.1821 (2.9640)	grad_norm 2.9005 (2.5378)	mem 14853MB
[2022-11-07 12:10:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][1150/1251]	eta 0:00:47 lr 0.000099	time 0.4545 (0.4684)	loss 3.0906 (2.9624)	grad_norm 2.3194 (2.5355)	mem 14853MB
[2022-11-07 12:11:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][1200/1251]	eta 0:00:23 lr 0.000099	time 0.4637 (0.4682)	loss 3.3530 (2.9646)	grad_norm 2.3494 (2.5335)	mem 14853MB
[2022-11-07 12:11:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [241/300][1250/1251]	eta 0:00:00 lr 0.000099	time 0.4567 (0.4681)	loss 2.9156 (2.9669)	grad_norm 2.9319 (2.5342)	mem 14853MB
[2022-11-07 12:11:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 241 training takes 0:09:45
[2022-11-07 12:11:46 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_241.pth saving......
[2022-11-07 12:11:47 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_241.pth saved !!!
[2022-11-07 12:11:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.606 (1.606)	Loss 0.8413 (0.8413)	Acc@1 80.371 (80.371)	Acc@5 95.312 (95.312)	Mem 14853MB
[2022-11-07 12:11:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.456 Acc@5 95.690
[2022-11-07 12:11:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.5%
[2022-11-07 12:11:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.667 (1.667)	Loss 0.7079 (0.7079)	Acc@1 83.594 (83.594)	Acc@5 97.363 (97.363)	Mem 14853MB
[2022-11-07 12:12:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.196 Acc@5 96.054
[2022-11-07 12:12:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-07 12:12:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.20% at 241 epoch
[2022-11-07 12:12:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][0/1251]	eta 0:41:31 lr 0.000099	time 1.9914 (1.9914)	loss 3.2625 (3.2625)	grad_norm 2.2112 (2.2112)	mem 14853MB
[2022-11-07 12:12:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][50/1251]	eta 0:10:05 lr 0.000098	time 0.4736 (0.5040)	loss 2.1544 (2.8872)	grad_norm 2.4880 (2.5775)	mem 14853MB
[2022-11-07 12:12:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][100/1251]	eta 0:09:19 lr 0.000098	time 0.4678 (0.4860)	loss 3.3703 (2.9001)	grad_norm 2.5131 (2.5532)	mem 14853MB
[2022-11-07 12:13:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][150/1251]	eta 0:08:48 lr 0.000098	time 0.4614 (0.4799)	loss 2.9745 (2.9068)	grad_norm 2.3018 (2.5532)	mem 14853MB
[2022-11-07 12:13:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][200/1251]	eta 0:08:20 lr 0.000098	time 0.4627 (0.4764)	loss 3.1300 (2.9126)	grad_norm 2.3079 (2.5377)	mem 14853MB
[2022-11-07 12:14:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][250/1251]	eta 0:07:54 lr 0.000098	time 0.4736 (0.4745)	loss 3.5072 (2.9223)	grad_norm 2.5758 (2.5350)	mem 14853MB
[2022-11-07 12:14:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][300/1251]	eta 0:07:30 lr 0.000098	time 0.4573 (0.4734)	loss 3.1216 (2.9200)	grad_norm 2.2957 (2.5366)	mem 14853MB
[2022-11-07 12:14:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][350/1251]	eta 0:07:05 lr 0.000098	time 0.4571 (0.4725)	loss 2.9185 (2.9188)	grad_norm 2.6886 (2.5499)	mem 14853MB
[2022-11-07 12:15:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][400/1251]	eta 0:06:41 lr 0.000098	time 0.4559 (0.4715)	loss 3.5430 (2.9127)	grad_norm 2.5567 (2.5502)	mem 14853MB
[2022-11-07 12:15:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][450/1251]	eta 0:06:17 lr 0.000097	time 0.4576 (0.4711)	loss 2.7483 (2.9293)	grad_norm 2.4590 (2.5452)	mem 14853MB
[2022-11-07 12:16:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][500/1251]	eta 0:05:53 lr 0.000097	time 0.4640 (0.4707)	loss 3.1148 (2.9293)	grad_norm 2.5664 (2.5491)	mem 14853MB
[2022-11-07 12:16:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][550/1251]	eta 0:05:29 lr 0.000097	time 0.4740 (0.4705)	loss 2.0374 (2.9354)	grad_norm 2.5610 (2.5490)	mem 14853MB
[2022-11-07 12:16:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][600/1251]	eta 0:05:06 lr 0.000097	time 0.4646 (0.4704)	loss 3.1901 (2.9390)	grad_norm 2.5783 (2.5489)	mem 14853MB
[2022-11-07 12:17:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][650/1251]	eta 0:04:42 lr 0.000097	time 0.4725 (0.4703)	loss 2.8812 (2.9430)	grad_norm 2.4966 (2.5449)	mem 14853MB
[2022-11-07 12:17:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][700/1251]	eta 0:04:19 lr 0.000097	time 0.5360 (0.4701)	loss 3.3861 (2.9405)	grad_norm 2.5771 (2.5467)	mem 14853MB
[2022-11-07 12:17:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][750/1251]	eta 0:03:55 lr 0.000097	time 0.4660 (0.4699)	loss 3.3402 (2.9397)	grad_norm 2.6361 (2.5455)	mem 14853MB
[2022-11-07 12:18:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][800/1251]	eta 0:03:31 lr 0.000097	time 0.4539 (0.4698)	loss 3.3910 (2.9457)	grad_norm 2.6091 (2.5461)	mem 14853MB
[2022-11-07 12:18:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][850/1251]	eta 0:03:08 lr 0.000097	time 0.4640 (0.4698)	loss 3.6328 (2.9446)	grad_norm 2.7075 (2.5432)	mem 14853MB
[2022-11-07 12:19:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][900/1251]	eta 0:02:44 lr 0.000096	time 0.4659 (0.4696)	loss 2.2643 (2.9394)	grad_norm 2.1296 (2.5414)	mem 14853MB
[2022-11-07 12:19:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][950/1251]	eta 0:02:21 lr 0.000096	time 0.4604 (0.4695)	loss 3.1917 (2.9378)	grad_norm 2.3594 (2.5429)	mem 14853MB
[2022-11-07 12:19:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][1000/1251]	eta 0:01:57 lr 0.000096	time 0.4609 (0.4695)	loss 3.2912 (2.9413)	grad_norm 3.2032 (2.5434)	mem 14853MB
[2022-11-07 12:20:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][1050/1251]	eta 0:01:34 lr 0.000096	time 0.4653 (0.4694)	loss 2.5061 (2.9407)	grad_norm 2.6117 (2.5405)	mem 14853MB
[2022-11-07 12:20:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][1100/1251]	eta 0:01:10 lr 0.000096	time 0.4782 (0.4693)	loss 1.9824 (2.9363)	grad_norm 2.3291 (2.5421)	mem 14853MB
[2022-11-07 12:21:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][1150/1251]	eta 0:00:47 lr 0.000096	time 0.4727 (0.4693)	loss 2.2639 (2.9314)	grad_norm 2.5278 (2.5405)	mem 14853MB
[2022-11-07 12:21:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][1200/1251]	eta 0:00:23 lr 0.000096	time 0.5405 (0.4692)	loss 3.4594 (2.9290)	grad_norm 2.4364 (2.5378)	mem 14853MB
[2022-11-07 12:21:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [242/300][1250/1251]	eta 0:00:00 lr 0.000096	time 0.4585 (0.4690)	loss 2.9737 (2.9285)	grad_norm 2.4806 (2.5385)	mem 14853MB
[2022-11-07 12:21:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 242 training takes 0:09:46
[2022-11-07 12:21:51 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_242.pth saving......
[2022-11-07 12:21:52 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_242.pth saved !!!
[2022-11-07 12:21:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.555 (1.555)	Loss 0.7856 (0.7856)	Acc@1 81.445 (81.445)	Acc@5 96.289 (96.289)	Mem 14853MB
[2022-11-07 12:22:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.424 Acc@5 95.766
[2022-11-07 12:22:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.4%
[2022-11-07 12:22:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.623 (1.623)	Loss 0.7395 (0.7395)	Acc@1 82.031 (82.031)	Acc@5 96.387 (96.387)	Mem 14853MB
[2022-11-07 12:22:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.198 Acc@5 96.048
[2022-11-07 12:22:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-07 12:22:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.20% at 242 epoch
[2022-11-07 12:22:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][0/1251]	eta 0:41:21 lr 0.000096	time 1.9840 (1.9840)	loss 2.8967 (2.8967)	grad_norm 3.0932 (3.0932)	mem 14853MB
[2022-11-07 12:22:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][50/1251]	eta 0:10:02 lr 0.000095	time 0.4674 (0.5020)	loss 3.2771 (3.0545)	grad_norm 2.4971 (2.5243)	mem 14853MB
[2022-11-07 12:22:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][100/1251]	eta 0:09:19 lr 0.000095	time 0.4587 (0.4861)	loss 2.8528 (3.0053)	grad_norm 2.5201 (2.5574)	mem 14853MB
[2022-11-07 12:23:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][150/1251]	eta 0:08:48 lr 0.000095	time 0.4695 (0.4804)	loss 2.4713 (2.9787)	grad_norm 2.6985 (2.5484)	mem 14853MB
[2022-11-07 12:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][200/1251]	eta 0:08:21 lr 0.000095	time 0.4609 (0.4770)	loss 2.2100 (2.9653)	grad_norm 2.3008 (2.5281)	mem 14853MB
[2022-11-07 12:24:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][250/1251]	eta 0:07:55 lr 0.000095	time 0.4583 (0.4747)	loss 3.2125 (2.9626)	grad_norm 2.6607 (2.5400)	mem 14853MB
[2022-11-07 12:24:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][300/1251]	eta 0:07:30 lr 0.000095	time 0.4808 (0.4733)	loss 3.2924 (2.9590)	grad_norm 2.4997 (2.5472)	mem 14853MB
[2022-11-07 12:24:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][350/1251]	eta 0:07:05 lr 0.000095	time 0.4638 (0.4722)	loss 3.3159 (2.9579)	grad_norm 2.3076 (2.5463)	mem 14853MB
[2022-11-07 12:25:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][400/1251]	eta 0:06:41 lr 0.000095	time 0.4664 (0.4716)	loss 3.4107 (2.9704)	grad_norm 2.9705 (2.5562)	mem 14853MB
[2022-11-07 12:25:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][450/1251]	eta 0:06:17 lr 0.000095	time 0.4655 (0.4710)	loss 3.0532 (2.9586)	grad_norm 2.4030 (2.5549)	mem 14853MB
[2022-11-07 12:26:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][500/1251]	eta 0:05:53 lr 0.000094	time 0.4588 (0.4705)	loss 3.0379 (2.9581)	grad_norm 2.6526 (2.5604)	mem 14853MB
[2022-11-07 12:26:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][550/1251]	eta 0:05:29 lr 0.000094	time 0.4598 (0.4705)	loss 2.4408 (2.9603)	grad_norm 2.3418 (2.5620)	mem 14853MB
[2022-11-07 12:26:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][600/1251]	eta 0:05:06 lr 0.000094	time 0.4714 (0.4702)	loss 2.5910 (2.9562)	grad_norm 2.6594 (2.5663)	mem 14853MB
[2022-11-07 12:27:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][650/1251]	eta 0:04:42 lr 0.000094	time 0.4602 (0.4699)	loss 2.9377 (2.9545)	grad_norm 3.0400 (2.5688)	mem 14853MB
[2022-11-07 12:27:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][700/1251]	eta 0:04:18 lr 0.000094	time 0.4681 (0.4695)	loss 1.8800 (2.9521)	grad_norm 2.6310 (2.5659)	mem 14853MB
[2022-11-07 12:28:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][750/1251]	eta 0:03:55 lr 0.000094	time 0.4618 (0.4695)	loss 2.3444 (2.9434)	grad_norm 2.7353 (2.5767)	mem 14853MB
[2022-11-07 12:28:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][800/1251]	eta 0:03:31 lr 0.000094	time 0.4628 (0.4694)	loss 3.1800 (2.9441)	grad_norm 2.5753 (2.5736)	mem 14853MB
[2022-11-07 12:28:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][850/1251]	eta 0:03:08 lr 0.000094	time 0.4686 (0.4693)	loss 3.4694 (2.9437)	grad_norm 2.6248 (2.5710)	mem 14853MB
[2022-11-07 12:29:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][900/1251]	eta 0:02:44 lr 0.000094	time 0.4706 (0.4691)	loss 2.8314 (2.9400)	grad_norm 2.4186 (inf)	mem 14853MB
[2022-11-07 12:29:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][950/1251]	eta 0:02:21 lr 0.000093	time 0.4698 (0.4689)	loss 1.9878 (2.9454)	grad_norm 2.7286 (inf)	mem 14853MB
[2022-11-07 12:29:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][1000/1251]	eta 0:01:57 lr 0.000093	time 0.4650 (0.4687)	loss 3.3591 (2.9474)	grad_norm 2.8127 (inf)	mem 14853MB
[2022-11-07 12:30:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][1050/1251]	eta 0:01:34 lr 0.000093	time 0.4642 (0.4688)	loss 2.4819 (2.9525)	grad_norm 2.8687 (inf)	mem 14853MB
[2022-11-07 12:30:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][1100/1251]	eta 0:01:10 lr 0.000093	time 0.4703 (0.4688)	loss 2.9032 (2.9511)	grad_norm 2.2871 (inf)	mem 14853MB
[2022-11-07 12:31:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][1150/1251]	eta 0:00:47 lr 0.000093	time 0.4587 (0.4686)	loss 2.9347 (2.9490)	grad_norm 2.3948 (inf)	mem 14853MB
[2022-11-07 12:31:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][1200/1251]	eta 0:00:23 lr 0.000093	time 0.4641 (0.4685)	loss 2.6622 (2.9363)	grad_norm 2.2342 (inf)	mem 14853MB
[2022-11-07 12:31:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [243/300][1250/1251]	eta 0:00:00 lr 0.000093	time 0.4566 (0.4682)	loss 3.1802 (2.9347)	grad_norm 2.5147 (inf)	mem 14853MB
[2022-11-07 12:31:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 243 training takes 0:09:45
[2022-11-07 12:31:55 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_243.pth saving......
[2022-11-07 12:31:56 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_243.pth saved !!!
[2022-11-07 12:31:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.654 (1.654)	Loss 0.7843 (0.7843)	Acc@1 81.836 (81.836)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 12:32:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.528 Acc@5 95.716
[2022-11-07 12:32:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.5%
[2022-11-07 12:32:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.667 (1.667)	Loss 0.7800 (0.7800)	Acc@1 81.543 (81.543)	Acc@5 96.680 (96.680)	Mem 14853MB
[2022-11-07 12:32:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.204 Acc@5 96.042
[2022-11-07 12:32:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-07 12:32:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.20% at 243 epoch
[2022-11-07 12:32:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][0/1251]	eta 0:42:58 lr 0.000093	time 2.0614 (2.0614)	loss 2.4723 (2.4723)	grad_norm 2.7347 (2.7347)	mem 14853MB
[2022-11-07 12:32:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][50/1251]	eta 0:10:03 lr 0.000093	time 0.4797 (0.5028)	loss 3.3204 (2.9847)	grad_norm 2.3734 (2.6435)	mem 14853MB
[2022-11-07 12:33:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][100/1251]	eta 0:09:18 lr 0.000092	time 0.4606 (0.4852)	loss 2.5459 (2.9945)	grad_norm 2.8571 (2.5889)	mem 14853MB
[2022-11-07 12:33:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][150/1251]	eta 0:08:47 lr 0.000092	time 0.4530 (0.4788)	loss 2.1008 (2.9811)	grad_norm 2.4547 (2.5910)	mem 14853MB
[2022-11-07 12:33:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][200/1251]	eta 0:08:19 lr 0.000092	time 0.4696 (0.4756)	loss 1.9668 (2.9627)	grad_norm 2.6611 (2.5839)	mem 14853MB
[2022-11-07 12:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][250/1251]	eta 0:07:54 lr 0.000092	time 0.4542 (0.4741)	loss 2.4543 (2.9517)	grad_norm 2.2382 (2.5914)	mem 14853MB
[2022-11-07 12:34:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][300/1251]	eta 0:07:29 lr 0.000092	time 0.4662 (0.4728)	loss 3.3301 (2.9509)	grad_norm 2.2408 (2.5895)	mem 14853MB
[2022-11-07 12:34:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][350/1251]	eta 0:07:05 lr 0.000092	time 0.4665 (0.4718)	loss 3.0275 (2.9466)	grad_norm 2.7405 (2.5799)	mem 14853MB
[2022-11-07 12:35:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][400/1251]	eta 0:06:40 lr 0.000092	time 0.4693 (0.4709)	loss 2.9696 (2.9400)	grad_norm 2.4156 (2.5885)	mem 14853MB
[2022-11-07 12:35:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][450/1251]	eta 0:06:16 lr 0.000092	time 0.4525 (0.4703)	loss 2.1318 (2.9477)	grad_norm 2.2881 (2.5781)	mem 14853MB
[2022-11-07 12:36:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][500/1251]	eta 0:05:52 lr 0.000092	time 0.4637 (0.4698)	loss 2.5788 (2.9442)	grad_norm 2.3210 (2.5764)	mem 14853MB
[2022-11-07 12:36:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][550/1251]	eta 0:05:29 lr 0.000091	time 0.4602 (0.4697)	loss 3.4012 (2.9434)	grad_norm 2.3707 (2.5752)	mem 14853MB
[2022-11-07 12:36:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][600/1251]	eta 0:05:05 lr 0.000091	time 0.5575 (0.4699)	loss 3.4226 (2.9434)	grad_norm 2.8066 (2.5753)	mem 14853MB
[2022-11-07 12:37:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][650/1251]	eta 0:04:42 lr 0.000091	time 0.4757 (0.4695)	loss 3.5523 (2.9349)	grad_norm 3.4941 (2.5829)	mem 14853MB
[2022-11-07 12:37:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][700/1251]	eta 0:04:18 lr 0.000091	time 0.5436 (0.4694)	loss 3.1078 (2.9329)	grad_norm 2.4606 (2.5788)	mem 14853MB
[2022-11-07 12:38:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][750/1251]	eta 0:03:55 lr 0.000091	time 0.4599 (0.4691)	loss 2.6383 (2.9263)	grad_norm 2.1358 (2.5753)	mem 14853MB
[2022-11-07 12:38:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][800/1251]	eta 0:03:31 lr 0.000091	time 0.4627 (0.4691)	loss 2.6694 (2.9290)	grad_norm 2.4551 (2.5721)	mem 14853MB
[2022-11-07 12:38:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][850/1251]	eta 0:03:08 lr 0.000091	time 0.4555 (0.4689)	loss 3.1859 (2.9309)	grad_norm 2.5347 (2.5722)	mem 14853MB
[2022-11-07 12:39:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][900/1251]	eta 0:02:44 lr 0.000091	time 0.4824 (0.4688)	loss 2.9969 (2.9314)	grad_norm 2.5437 (2.5756)	mem 14853MB
[2022-11-07 12:39:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][950/1251]	eta 0:02:21 lr 0.000091	time 0.4687 (0.4689)	loss 3.2601 (2.9356)	grad_norm 2.6885 (2.5771)	mem 14853MB
[2022-11-07 12:40:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][1000/1251]	eta 0:01:57 lr 0.000090	time 0.4629 (0.4687)	loss 3.2457 (2.9378)	grad_norm 2.5065 (2.5800)	mem 14853MB
[2022-11-07 12:40:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][1050/1251]	eta 0:01:34 lr 0.000090	time 0.4594 (0.4687)	loss 2.1350 (2.9343)	grad_norm 3.1178 (2.5820)	mem 14853MB
[2022-11-07 12:40:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][1100/1251]	eta 0:01:10 lr 0.000090	time 0.4641 (0.4686)	loss 3.3063 (2.9354)	grad_norm 2.8678 (2.5822)	mem 14853MB
[2022-11-07 12:41:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][1150/1251]	eta 0:00:47 lr 0.000090	time 0.4711 (0.4685)	loss 3.0875 (2.9380)	grad_norm 2.9965 (2.5783)	mem 14853MB
[2022-11-07 12:41:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][1200/1251]	eta 0:00:23 lr 0.000090	time 0.5414 (0.4685)	loss 2.9901 (2.9400)	grad_norm 2.5427 (2.5795)	mem 14853MB
[2022-11-07 12:41:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [244/300][1250/1251]	eta 0:00:00 lr 0.000090	time 0.4567 (0.4683)	loss 3.1897 (2.9424)	grad_norm 2.3534 (2.5814)	mem 14853MB
[2022-11-07 12:42:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 244 training takes 0:09:46
[2022-11-07 12:42:00 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_244.pth saving......
[2022-11-07 12:42:00 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_244.pth saved !!!
[2022-11-07 12:42:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.755 (1.755)	Loss 0.7576 (0.7576)	Acc@1 82.324 (82.324)	Acc@5 96.289 (96.289)	Mem 14853MB
[2022-11-07 12:42:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.498 Acc@5 95.752
[2022-11-07 12:42:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.5%
[2022-11-07 12:42:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.555 (1.555)	Loss 0.7074 (0.7074)	Acc@1 83.594 (83.594)	Acc@5 96.582 (96.582)	Mem 14853MB
[2022-11-07 12:42:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.206 Acc@5 96.042
[2022-11-07 12:42:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-07 12:42:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.21% at 244 epoch
[2022-11-07 12:42:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][0/1251]	eta 0:41:36 lr 0.000090	time 1.9957 (1.9957)	loss 2.9234 (2.9234)	grad_norm 2.8175 (2.8175)	mem 14853MB
[2022-11-07 12:42:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][50/1251]	eta 0:09:58 lr 0.000090	time 0.4646 (0.4980)	loss 3.2243 (3.0581)	grad_norm 2.4905 (2.6695)	mem 14853MB
[2022-11-07 12:43:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][100/1251]	eta 0:09:19 lr 0.000090	time 0.4591 (0.4861)	loss 2.7628 (2.9988)	grad_norm 2.5529 (2.6448)	mem 14853MB
[2022-11-07 12:43:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][150/1251]	eta 0:08:48 lr 0.000090	time 0.4557 (0.4799)	loss 3.0712 (2.9413)	grad_norm 2.3843 (2.6260)	mem 14853MB
[2022-11-07 12:43:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][200/1251]	eta 0:08:20 lr 0.000089	time 0.4689 (0.4762)	loss 3.3711 (2.9379)	grad_norm 2.4981 (2.6094)	mem 14853MB
[2022-11-07 12:44:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][250/1251]	eta 0:07:54 lr 0.000089	time 0.4678 (0.4739)	loss 2.8026 (2.9172)	grad_norm 2.6876 (2.6074)	mem 14853MB
[2022-11-07 12:44:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][300/1251]	eta 0:07:29 lr 0.000089	time 0.4575 (0.4725)	loss 3.3417 (2.9256)	grad_norm 2.6348 (2.6084)	mem 14853MB
[2022-11-07 12:45:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][350/1251]	eta 0:07:04 lr 0.000089	time 0.4674 (0.4714)	loss 2.8527 (2.9171)	grad_norm 2.7082 (2.6095)	mem 14853MB
[2022-11-07 12:45:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][400/1251]	eta 0:06:40 lr 0.000089	time 0.4758 (0.4707)	loss 3.5025 (2.9178)	grad_norm 2.2203 (2.5960)	mem 14853MB
[2022-11-07 12:45:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][450/1251]	eta 0:06:16 lr 0.000089	time 0.4640 (0.4704)	loss 2.4297 (2.9128)	grad_norm 2.8811 (2.5968)	mem 14853MB
[2022-11-07 12:46:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][500/1251]	eta 0:05:52 lr 0.000089	time 0.4695 (0.4699)	loss 3.3089 (2.9224)	grad_norm 2.5749 (2.5956)	mem 14853MB
[2022-11-07 12:46:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][550/1251]	eta 0:05:29 lr 0.000089	time 0.4615 (0.4699)	loss 3.2867 (2.9153)	grad_norm 2.0480 (2.5943)	mem 14853MB
[2022-11-07 12:47:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][600/1251]	eta 0:05:05 lr 0.000089	time 0.4642 (0.4699)	loss 2.8401 (2.9143)	grad_norm 2.8214 (2.5991)	mem 14853MB
[2022-11-07 12:47:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][650/1251]	eta 0:04:42 lr 0.000088	time 0.4624 (0.4697)	loss 3.0969 (2.9215)	grad_norm 2.8278 (2.5921)	mem 14853MB
[2022-11-07 12:47:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][700/1251]	eta 0:04:18 lr 0.000088	time 0.4528 (0.4695)	loss 3.3898 (2.9341)	grad_norm 2.6737 (2.5936)	mem 14853MB
[2022-11-07 12:48:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][750/1251]	eta 0:03:55 lr 0.000088	time 0.4660 (0.4692)	loss 3.1793 (2.9347)	grad_norm 2.3826 (2.5907)	mem 14853MB
[2022-11-07 12:48:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][800/1251]	eta 0:03:31 lr 0.000088	time 0.4654 (0.4693)	loss 2.3223 (2.9245)	grad_norm 2.8085 (2.5890)	mem 14853MB
[2022-11-07 12:48:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][850/1251]	eta 0:03:08 lr 0.000088	time 0.4508 (0.4691)	loss 2.6311 (2.9254)	grad_norm 2.7801 (2.5901)	mem 14853MB
[2022-11-07 12:49:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][900/1251]	eta 0:02:44 lr 0.000088	time 0.4659 (0.4688)	loss 2.3802 (2.9231)	grad_norm 2.5768 (2.5905)	mem 14853MB
[2022-11-07 12:49:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][950/1251]	eta 0:02:21 lr 0.000088	time 0.4587 (0.4687)	loss 3.3213 (2.9239)	grad_norm 2.6873 (2.5918)	mem 14853MB
[2022-11-07 12:50:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][1000/1251]	eta 0:01:57 lr 0.000088	time 0.4532 (0.4685)	loss 3.0764 (2.9217)	grad_norm 2.3203 (2.5945)	mem 14853MB
[2022-11-07 12:50:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][1050/1251]	eta 0:01:34 lr 0.000088	time 0.4641 (0.4686)	loss 3.3963 (2.9273)	grad_norm 2.3924 (2.6011)	mem 14853MB
[2022-11-07 12:50:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][1100/1251]	eta 0:01:10 lr 0.000087	time 0.4632 (0.4684)	loss 3.4221 (2.9240)	grad_norm 2.5130 (2.5968)	mem 14853MB
[2022-11-07 12:51:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][1150/1251]	eta 0:00:47 lr 0.000087	time 0.4717 (0.4684)	loss 3.4369 (2.9232)	grad_norm 2.4684 (2.5959)	mem 14853MB
[2022-11-07 12:51:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][1200/1251]	eta 0:00:23 lr 0.000087	time 0.4597 (0.4682)	loss 3.5077 (2.9254)	grad_norm 2.4836 (2.6017)	mem 14853MB
[2022-11-07 12:52:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [245/300][1250/1251]	eta 0:00:00 lr 0.000087	time 0.4569 (0.4680)	loss 2.3349 (2.9269)	grad_norm 2.6823 (2.6031)	mem 14853MB
[2022-11-07 12:52:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 245 training takes 0:09:45
[2022-11-07 12:52:03 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_245.pth saving......
[2022-11-07 12:52:04 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_245.pth saved !!!
[2022-11-07 12:52:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.537 (1.537)	Loss 0.7566 (0.7566)	Acc@1 83.984 (83.984)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 12:52:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.506 Acc@5 95.822
[2022-11-07 12:52:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.5%
[2022-11-07 12:52:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.648 (1.648)	Loss 0.7813 (0.7813)	Acc@1 81.836 (81.836)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 12:52:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.220 Acc@5 96.064
[2022-11-07 12:52:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-07 12:52:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.22% at 245 epoch
[2022-11-07 12:52:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][0/1251]	eta 0:41:02 lr 0.000087	time 1.9685 (1.9685)	loss 3.2737 (3.2737)	grad_norm 2.6699 (2.6699)	mem 14853MB
[2022-11-07 12:52:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][50/1251]	eta 0:10:00 lr 0.000087	time 0.4556 (0.5003)	loss 2.0383 (2.9227)	grad_norm 2.3729 (2.6779)	mem 14853MB
[2022-11-07 12:53:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][100/1251]	eta 0:09:17 lr 0.000087	time 0.4617 (0.4845)	loss 3.0985 (2.8902)	grad_norm 2.4506 (2.6487)	mem 14853MB
[2022-11-07 12:53:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][150/1251]	eta 0:08:46 lr 0.000087	time 0.4722 (0.4785)	loss 3.3408 (2.8817)	grad_norm 2.3451 (2.6073)	mem 14853MB
[2022-11-07 12:53:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][200/1251]	eta 0:08:19 lr 0.000087	time 0.4681 (0.4751)	loss 3.1953 (2.8938)	grad_norm 2.4560 (2.6164)	mem 14853MB
[2022-11-07 12:54:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][250/1251]	eta 0:07:54 lr 0.000087	time 0.4656 (0.4736)	loss 3.3780 (2.9157)	grad_norm 2.3903 (inf)	mem 14853MB
[2022-11-07 12:54:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][300/1251]	eta 0:07:29 lr 0.000086	time 0.4767 (0.4724)	loss 2.6807 (2.9150)	grad_norm 2.3981 (inf)	mem 14853MB
[2022-11-07 12:55:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][350/1251]	eta 0:07:04 lr 0.000086	time 0.4674 (0.4713)	loss 2.9885 (2.9267)	grad_norm 2.8629 (inf)	mem 14853MB
[2022-11-07 12:55:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][400/1251]	eta 0:06:40 lr 0.000086	time 0.4667 (0.4707)	loss 2.0240 (2.9265)	grad_norm 2.7376 (inf)	mem 14853MB
[2022-11-07 12:55:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][450/1251]	eta 0:06:16 lr 0.000086	time 0.4566 (0.4700)	loss 3.0842 (2.9380)	grad_norm 2.3302 (inf)	mem 14853MB
[2022-11-07 12:56:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][500/1251]	eta 0:05:52 lr 0.000086	time 0.4564 (0.4696)	loss 2.1989 (2.9511)	grad_norm 2.5994 (inf)	mem 14853MB
[2022-11-07 12:56:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][550/1251]	eta 0:05:29 lr 0.000086	time 0.4663 (0.4699)	loss 3.3932 (2.9390)	grad_norm 2.7800 (inf)	mem 14853MB
[2022-11-07 12:57:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][600/1251]	eta 0:05:05 lr 0.000086	time 0.4630 (0.4695)	loss 3.5775 (2.9254)	grad_norm 2.8476 (inf)	mem 14853MB
[2022-11-07 12:57:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][650/1251]	eta 0:04:42 lr 0.000086	time 0.4604 (0.4692)	loss 3.1080 (2.9314)	grad_norm 2.6840 (inf)	mem 14853MB
[2022-11-07 12:57:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][700/1251]	eta 0:04:18 lr 0.000086	time 0.4668 (0.4690)	loss 2.3029 (2.9174)	grad_norm 2.6779 (inf)	mem 14853MB
[2022-11-07 12:58:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][750/1251]	eta 0:03:54 lr 0.000085	time 0.4610 (0.4688)	loss 2.4243 (2.9126)	grad_norm 2.5313 (inf)	mem 14853MB
[2022-11-07 12:58:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][800/1251]	eta 0:03:31 lr 0.000085	time 0.4648 (0.4690)	loss 3.1064 (2.9164)	grad_norm 2.5380 (inf)	mem 14853MB
[2022-11-07 12:59:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][850/1251]	eta 0:03:07 lr 0.000085	time 0.4607 (0.4687)	loss 2.1628 (2.9147)	grad_norm 2.3609 (inf)	mem 14853MB
[2022-11-07 12:59:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][900/1251]	eta 0:02:44 lr 0.000085	time 0.4657 (0.4685)	loss 3.0828 (2.9190)	grad_norm 2.5778 (inf)	mem 14853MB
[2022-11-07 12:59:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][950/1251]	eta 0:02:21 lr 0.000085	time 0.4602 (0.4684)	loss 3.2122 (2.9233)	grad_norm 2.4321 (inf)	mem 14853MB
[2022-11-07 13:00:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][1000/1251]	eta 0:01:57 lr 0.000085	time 0.4695 (0.4683)	loss 3.1295 (2.9264)	grad_norm 3.1782 (inf)	mem 14853MB
[2022-11-07 13:00:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][1050/1251]	eta 0:01:34 lr 0.000085	time 0.4509 (0.4684)	loss 3.2468 (2.9259)	grad_norm 2.4398 (inf)	mem 14853MB
[2022-11-07 13:00:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][1100/1251]	eta 0:01:10 lr 0.000085	time 0.4607 (0.4683)	loss 2.6199 (2.9199)	grad_norm 2.6035 (inf)	mem 14853MB
[2022-11-07 13:01:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][1150/1251]	eta 0:00:47 lr 0.000085	time 0.4643 (0.4682)	loss 2.3746 (2.9233)	grad_norm 2.6800 (inf)	mem 14853MB
[2022-11-07 13:01:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][1200/1251]	eta 0:00:23 lr 0.000084	time 0.4646 (0.4682)	loss 2.9913 (2.9266)	grad_norm 2.5585 (inf)	mem 14853MB
[2022-11-07 13:02:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [246/300][1250/1251]	eta 0:00:00 lr 0.000084	time 0.4571 (0.4679)	loss 2.5424 (2.9262)	grad_norm 3.5973 (inf)	mem 14853MB
[2022-11-07 13:02:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 246 training takes 0:09:45
[2022-11-07 13:02:07 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_246.pth saving......
[2022-11-07 13:02:08 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_246.pth saved !!!
[2022-11-07 13:02:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.575 (1.575)	Loss 0.7836 (0.7836)	Acc@1 81.641 (81.641)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 13:02:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.556 Acc@5 95.850
[2022-11-07 13:02:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.6%
[2022-11-07 13:02:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.625 (1.625)	Loss 0.7497 (0.7497)	Acc@1 82.129 (82.129)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 13:02:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.170 Acc@5 96.088
[2022-11-07 13:02:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-07 13:02:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.22% at 245 epoch
[2022-11-07 13:02:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][0/1251]	eta 0:42:18 lr 0.000084	time 2.0295 (2.0295)	loss 2.6869 (2.6869)	grad_norm 2.4869 (2.4869)	mem 14853MB
[2022-11-07 13:02:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][50/1251]	eta 0:10:05 lr 0.000084	time 0.4727 (0.5042)	loss 3.0636 (2.9292)	grad_norm 2.2757 (2.5771)	mem 14853MB
[2022-11-07 13:03:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][100/1251]	eta 0:09:20 lr 0.000084	time 0.4658 (0.4867)	loss 3.2632 (2.9212)	grad_norm 2.4604 (2.6017)	mem 14853MB
[2022-11-07 13:03:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][150/1251]	eta 0:08:48 lr 0.000084	time 0.4691 (0.4801)	loss 3.0886 (2.9163)	grad_norm 3.0267 (2.6210)	mem 14853MB
[2022-11-07 13:04:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][200/1251]	eta 0:08:20 lr 0.000084	time 0.4752 (0.4763)	loss 2.6983 (2.9079)	grad_norm 2.3635 (2.6224)	mem 14853MB
[2022-11-07 13:04:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][250/1251]	eta 0:07:54 lr 0.000084	time 0.4814 (0.4741)	loss 3.4784 (2.9151)	grad_norm 2.5485 (2.6225)	mem 14853MB
[2022-11-07 13:04:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][300/1251]	eta 0:07:29 lr 0.000084	time 0.4656 (0.4731)	loss 2.7343 (2.9145)	grad_norm 3.0965 (2.6372)	mem 14853MB
[2022-11-07 13:05:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][350/1251]	eta 0:07:05 lr 0.000084	time 0.4754 (0.4721)	loss 3.7461 (2.9259)	grad_norm 2.6926 (2.6419)	mem 14853MB
[2022-11-07 13:05:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][400/1251]	eta 0:06:41 lr 0.000083	time 0.4589 (0.4714)	loss 3.2546 (2.9249)	grad_norm 2.4512 (2.6406)	mem 14853MB
[2022-11-07 13:05:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][450/1251]	eta 0:06:16 lr 0.000083	time 0.4605 (0.4706)	loss 2.1869 (2.9152)	grad_norm 3.1100 (2.6317)	mem 14853MB
[2022-11-07 13:06:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][500/1251]	eta 0:05:53 lr 0.000083	time 0.4640 (0.4701)	loss 3.2819 (2.9100)	grad_norm 3.0771 (2.6304)	mem 14853MB
[2022-11-07 13:06:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][550/1251]	eta 0:05:29 lr 0.000083	time 0.4591 (0.4702)	loss 3.2346 (2.9168)	grad_norm 2.6299 (2.6344)	mem 14853MB
[2022-11-07 13:07:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][600/1251]	eta 0:05:05 lr 0.000083	time 0.4700 (0.4697)	loss 2.0928 (2.9204)	grad_norm 2.4787 (2.6358)	mem 14853MB
[2022-11-07 13:07:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][650/1251]	eta 0:04:42 lr 0.000083	time 0.4713 (0.4698)	loss 3.4003 (2.9226)	grad_norm 2.3814 (2.6301)	mem 14853MB
[2022-11-07 13:07:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][700/1251]	eta 0:04:18 lr 0.000083	time 0.4658 (0.4694)	loss 2.0156 (2.9214)	grad_norm 2.7025 (2.6300)	mem 14853MB
[2022-11-07 13:08:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][750/1251]	eta 0:03:55 lr 0.000083	time 0.4651 (0.4691)	loss 2.9901 (2.9281)	grad_norm 2.3036 (2.6321)	mem 14853MB
[2022-11-07 13:08:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][800/1251]	eta 0:03:31 lr 0.000083	time 0.4612 (0.4691)	loss 2.2929 (2.9340)	grad_norm 3.0463 (2.6353)	mem 14853MB
[2022-11-07 13:09:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][850/1251]	eta 0:03:08 lr 0.000082	time 0.4637 (0.4688)	loss 3.1930 (2.9392)	grad_norm 3.1668 (2.6344)	mem 14853MB
[2022-11-07 13:09:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][900/1251]	eta 0:02:44 lr 0.000082	time 0.4720 (0.4688)	loss 2.2287 (2.9441)	grad_norm 2.5197 (2.6306)	mem 14853MB
[2022-11-07 13:09:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][950/1251]	eta 0:02:21 lr 0.000082	time 0.4659 (0.4685)	loss 3.0552 (2.9458)	grad_norm 2.5213 (2.6297)	mem 14853MB
[2022-11-07 13:10:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][1000/1251]	eta 0:01:57 lr 0.000082	time 0.4650 (0.4684)	loss 3.6010 (2.9412)	grad_norm 2.6225 (2.6310)	mem 14853MB
[2022-11-07 13:10:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][1050/1251]	eta 0:01:34 lr 0.000082	time 0.4667 (0.4684)	loss 2.9473 (2.9409)	grad_norm 2.4325 (2.6294)	mem 14853MB
[2022-11-07 13:11:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][1100/1251]	eta 0:01:10 lr 0.000082	time 0.4603 (0.4683)	loss 3.0652 (2.9409)	grad_norm 2.2872 (2.6248)	mem 14853MB
[2022-11-07 13:11:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][1150/1251]	eta 0:00:47 lr 0.000082	time 0.4655 (0.4682)	loss 2.9785 (2.9374)	grad_norm 2.8894 (2.6247)	mem 14853MB
[2022-11-07 13:11:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][1200/1251]	eta 0:00:23 lr 0.000082	time 0.4552 (0.4681)	loss 3.4523 (2.9349)	grad_norm 2.4087 (2.6279)	mem 14853MB
[2022-11-07 13:12:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [247/300][1250/1251]	eta 0:00:00 lr 0.000082	time 0.4563 (0.4680)	loss 1.7388 (2.9349)	grad_norm 2.5835 (2.6253)	mem 14853MB
[2022-11-07 13:12:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 247 training takes 0:09:45
[2022-11-07 13:12:11 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_247.pth saving......
[2022-11-07 13:12:11 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_247.pth saved !!!
[2022-11-07 13:12:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.571 (1.571)	Loss 0.8202 (0.8202)	Acc@1 81.055 (81.055)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 13:12:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.622 Acc@5 95.806
[2022-11-07 13:12:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.6%
[2022-11-07 13:12:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.643 (1.643)	Loss 0.7926 (0.7926)	Acc@1 80.566 (80.566)	Acc@5 96.289 (96.289)	Mem 14853MB
[2022-11-07 13:12:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.186 Acc@5 96.082
[2022-11-07 13:12:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-07 13:12:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.22% at 245 epoch
[2022-11-07 13:12:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][0/1251]	eta 0:41:03 lr 0.000082	time 1.9694 (1.9694)	loss 2.1420 (2.1420)	grad_norm 2.8026 (2.8026)	mem 14853MB
[2022-11-07 13:12:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][50/1251]	eta 0:10:00 lr 0.000081	time 0.4676 (0.4996)	loss 1.7699 (2.8471)	grad_norm 2.8785 (2.6293)	mem 14853MB
[2022-11-07 13:13:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][100/1251]	eta 0:09:18 lr 0.000081	time 0.4690 (0.4853)	loss 2.5505 (2.9064)	grad_norm 2.8088 (2.6767)	mem 14853MB
[2022-11-07 13:13:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][150/1251]	eta 0:08:47 lr 0.000081	time 0.4656 (0.4794)	loss 1.9873 (2.8951)	grad_norm 2.6032 (2.6854)	mem 14853MB
[2022-11-07 13:14:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][200/1251]	eta 0:08:20 lr 0.000081	time 0.4732 (0.4763)	loss 3.4519 (2.9265)	grad_norm 2.4841 (2.6843)	mem 14853MB
[2022-11-07 13:14:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][250/1251]	eta 0:07:54 lr 0.000081	time 0.4656 (0.4743)	loss 2.5026 (2.9208)	grad_norm 2.3144 (2.6859)	mem 14853MB
[2022-11-07 13:14:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][300/1251]	eta 0:07:30 lr 0.000081	time 0.4654 (0.4732)	loss 2.4497 (2.9113)	grad_norm 2.8644 (2.6699)	mem 14853MB
[2022-11-07 13:15:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][350/1251]	eta 0:07:05 lr 0.000081	time 0.4698 (0.4723)	loss 2.6442 (2.9113)	grad_norm 2.6650 (2.6650)	mem 14853MB
[2022-11-07 13:15:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][400/1251]	eta 0:06:41 lr 0.000081	time 0.4603 (0.4715)	loss 3.2019 (2.9171)	grad_norm 2.6874 (2.6533)	mem 14853MB
[2022-11-07 13:16:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][450/1251]	eta 0:06:17 lr 0.000081	time 0.4584 (0.4711)	loss 2.4293 (2.9223)	grad_norm 2.5181 (2.6392)	mem 14853MB
[2022-11-07 13:16:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][500/1251]	eta 0:05:53 lr 0.000081	time 0.4547 (0.4704)	loss 3.6111 (2.9277)	grad_norm 2.7977 (2.6390)	mem 14853MB
[2022-11-07 13:16:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][550/1251]	eta 0:05:29 lr 0.000080	time 0.4591 (0.4702)	loss 2.0309 (2.9311)	grad_norm 2.5227 (2.6347)	mem 14853MB
[2022-11-07 13:17:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][600/1251]	eta 0:05:05 lr 0.000080	time 0.4644 (0.4699)	loss 3.1581 (2.9336)	grad_norm 2.6024 (2.6400)	mem 14853MB
[2022-11-07 13:17:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][650/1251]	eta 0:04:42 lr 0.000080	time 0.4672 (0.4696)	loss 3.1048 (2.9339)	grad_norm 2.5481 (2.6462)	mem 14853MB
[2022-11-07 13:17:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][700/1251]	eta 0:04:18 lr 0.000080	time 0.4626 (0.4694)	loss 3.3759 (2.9306)	grad_norm 2.4236 (inf)	mem 14853MB
[2022-11-07 13:18:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][750/1251]	eta 0:03:55 lr 0.000080	time 0.4627 (0.4692)	loss 3.2408 (2.9191)	grad_norm 2.5073 (inf)	mem 14853MB
[2022-11-07 13:18:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][800/1251]	eta 0:03:31 lr 0.000080	time 0.4650 (0.4692)	loss 2.4805 (2.9219)	grad_norm 2.3824 (inf)	mem 14853MB
[2022-11-07 13:19:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][850/1251]	eta 0:03:08 lr 0.000080	time 0.4655 (0.4690)	loss 3.6043 (2.9256)	grad_norm 2.7203 (inf)	mem 14853MB
[2022-11-07 13:19:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][900/1251]	eta 0:02:44 lr 0.000080	time 0.4813 (0.4688)	loss 3.1411 (2.9199)	grad_norm 2.4851 (inf)	mem 14853MB
[2022-11-07 13:19:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][950/1251]	eta 0:02:21 lr 0.000080	time 0.4599 (0.4687)	loss 2.6404 (2.9188)	grad_norm 2.3118 (inf)	mem 14853MB
[2022-11-07 13:20:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][1000/1251]	eta 0:01:57 lr 0.000079	time 0.4627 (0.4686)	loss 3.2108 (2.9246)	grad_norm 2.4873 (inf)	mem 14853MB
[2022-11-07 13:20:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][1050/1251]	eta 0:01:34 lr 0.000079	time 0.4635 (0.4687)	loss 3.0337 (2.9265)	grad_norm 2.7143 (inf)	mem 14853MB
[2022-11-07 13:21:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][1100/1251]	eta 0:01:10 lr 0.000079	time 0.4594 (0.4687)	loss 3.2474 (2.9358)	grad_norm 2.4790 (inf)	mem 14853MB
[2022-11-07 13:21:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][1150/1251]	eta 0:00:47 lr 0.000079	time 0.4723 (0.4685)	loss 2.7577 (2.9364)	grad_norm 2.9381 (inf)	mem 14853MB
[2022-11-07 13:21:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][1200/1251]	eta 0:00:23 lr 0.000079	time 0.4562 (0.4684)	loss 3.5626 (2.9299)	grad_norm 2.6791 (inf)	mem 14853MB
[2022-11-07 13:22:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [248/300][1250/1251]	eta 0:00:00 lr 0.000079	time 0.4568 (0.4682)	loss 3.0168 (2.9306)	grad_norm 2.6234 (inf)	mem 14853MB
[2022-11-07 13:22:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 248 training takes 0:09:45
[2022-11-07 13:22:15 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_248.pth saving......
[2022-11-07 13:22:15 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_248.pth saved !!!
[2022-11-07 13:22:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.514 (1.514)	Loss 0.7128 (0.7128)	Acc@1 83.105 (83.105)	Acc@5 96.484 (96.484)	Mem 14853MB
[2022-11-07 13:22:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.678 Acc@5 95.818
[2022-11-07 13:22:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.7%
[2022-11-07 13:22:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.761 (1.761)	Loss 0.7521 (0.7521)	Acc@1 82.520 (82.520)	Acc@5 96.680 (96.680)	Mem 14853MB
[2022-11-07 13:22:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.190 Acc@5 96.072
[2022-11-07 13:22:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-07 13:22:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.22% at 245 epoch
[2022-11-07 13:22:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][0/1251]	eta 0:41:27 lr 0.000079	time 1.9883 (1.9883)	loss 2.7375 (2.7375)	grad_norm 2.6013 (2.6013)	mem 14853MB
[2022-11-07 13:22:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][50/1251]	eta 0:09:56 lr 0.000079	time 0.4691 (0.4971)	loss 3.4497 (2.7750)	grad_norm 2.5499 (2.6807)	mem 14853MB
[2022-11-07 13:23:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][100/1251]	eta 0:09:15 lr 0.000079	time 0.4568 (0.4823)	loss 3.2094 (2.7974)	grad_norm 2.6404 (2.6591)	mem 14853MB
[2022-11-07 13:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][150/1251]	eta 0:08:45 lr 0.000079	time 0.4607 (0.4773)	loss 3.0350 (2.7816)	grad_norm 2.4424 (2.6539)	mem 14853MB
[2022-11-07 13:24:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][200/1251]	eta 0:08:19 lr 0.000079	time 0.4725 (0.4753)	loss 3.7063 (2.8235)	grad_norm 2.6400 (2.6567)	mem 14853MB
[2022-11-07 13:24:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][250/1251]	eta 0:07:53 lr 0.000078	time 0.4631 (0.4735)	loss 2.8843 (2.8571)	grad_norm 2.5316 (2.6548)	mem 14853MB
[2022-11-07 13:24:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][300/1251]	eta 0:07:29 lr 0.000078	time 0.4629 (0.4725)	loss 3.0131 (2.8621)	grad_norm 2.3142 (2.6573)	mem 14853MB
[2022-11-07 13:25:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][350/1251]	eta 0:07:04 lr 0.000078	time 0.4805 (0.4714)	loss 3.5482 (2.8629)	grad_norm 2.7776 (2.6523)	mem 14853MB
[2022-11-07 13:25:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][400/1251]	eta 0:06:40 lr 0.000078	time 0.4605 (0.4709)	loss 2.6516 (2.8495)	grad_norm 2.6513 (2.6461)	mem 14853MB
[2022-11-07 13:26:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][450/1251]	eta 0:06:16 lr 0.000078	time 0.4812 (0.4701)	loss 2.9410 (2.8531)	grad_norm 3.4195 (2.6512)	mem 14853MB
[2022-11-07 13:26:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][500/1251]	eta 0:05:52 lr 0.000078	time 0.4690 (0.4696)	loss 1.7549 (2.8589)	grad_norm 2.7377 (2.6581)	mem 14853MB
[2022-11-07 13:26:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][550/1251]	eta 0:05:29 lr 0.000078	time 0.4561 (0.4694)	loss 3.1525 (2.8748)	grad_norm 2.7632 (2.6599)	mem 14853MB
[2022-11-07 13:27:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][600/1251]	eta 0:05:05 lr 0.000078	time 0.4522 (0.4691)	loss 3.2750 (2.8861)	grad_norm 2.5986 (2.6545)	mem 14853MB
[2022-11-07 13:27:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][650/1251]	eta 0:04:41 lr 0.000078	time 0.4737 (0.4689)	loss 3.0703 (2.8821)	grad_norm 2.9938 (2.6586)	mem 14853MB
[2022-11-07 13:28:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][700/1251]	eta 0:04:18 lr 0.000077	time 0.5325 (0.4688)	loss 3.1167 (2.8906)	grad_norm 2.7910 (2.6612)	mem 14853MB
[2022-11-07 13:28:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][750/1251]	eta 0:03:54 lr 0.000077	time 0.4733 (0.4687)	loss 3.4380 (2.8949)	grad_norm 2.7579 (2.6612)	mem 14853MB
[2022-11-07 13:28:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][800/1251]	eta 0:03:31 lr 0.000077	time 0.4670 (0.4687)	loss 2.1037 (2.8986)	grad_norm 2.6594 (2.6584)	mem 14853MB
[2022-11-07 13:29:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][850/1251]	eta 0:03:07 lr 0.000077	time 0.4768 (0.4686)	loss 2.5365 (2.9010)	grad_norm 2.9070 (2.6574)	mem 14853MB
[2022-11-07 13:29:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][900/1251]	eta 0:02:44 lr 0.000077	time 0.4651 (0.4684)	loss 2.9534 (2.9052)	grad_norm 2.6268 (2.6590)	mem 14853MB
[2022-11-07 13:29:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][950/1251]	eta 0:02:20 lr 0.000077	time 0.4523 (0.4683)	loss 2.8740 (2.9100)	grad_norm 2.5417 (2.6567)	mem 14853MB
[2022-11-07 13:30:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][1000/1251]	eta 0:01:57 lr 0.000077	time 0.4624 (0.4682)	loss 2.3610 (2.9025)	grad_norm 2.6002 (2.6555)	mem 14853MB
[2022-11-07 13:30:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][1050/1251]	eta 0:01:34 lr 0.000077	time 0.4596 (0.4681)	loss 2.9376 (2.9054)	grad_norm 2.6436 (2.6555)	mem 14853MB
[2022-11-07 13:31:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][1100/1251]	eta 0:01:10 lr 0.000077	time 0.4676 (0.4681)	loss 3.2995 (2.9072)	grad_norm 2.3774 (2.6574)	mem 14853MB
[2022-11-07 13:31:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][1150/1251]	eta 0:00:47 lr 0.000077	time 0.4675 (0.4680)	loss 1.9097 (2.9086)	grad_norm 2.3337 (2.6588)	mem 14853MB
[2022-11-07 13:31:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][1200/1251]	eta 0:00:23 lr 0.000076	time 0.5273 (0.4680)	loss 1.6573 (2.9071)	grad_norm 2.4786 (2.6610)	mem 14853MB
[2022-11-07 13:32:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [249/300][1250/1251]	eta 0:00:00 lr 0.000076	time 0.4573 (0.4679)	loss 3.1664 (2.9087)	grad_norm 3.0105 (2.6595)	mem 14853MB
[2022-11-07 13:32:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 249 training takes 0:09:45
[2022-11-07 13:32:18 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_249.pth saving......
[2022-11-07 13:32:19 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_249.pth saved !!!
[2022-11-07 13:32:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.621 (1.621)	Loss 0.7867 (0.7867)	Acc@1 80.859 (80.859)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 13:32:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.774 Acc@5 95.818
[2022-11-07 13:32:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.8%
[2022-11-07 13:32:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.634 (1.634)	Loss 0.8583 (0.8583)	Acc@1 80.078 (80.078)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 13:32:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.192 Acc@5 96.082
[2022-11-07 13:32:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-07 13:32:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.22% at 245 epoch
[2022-11-07 13:32:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][0/1251]	eta 0:43:02 lr 0.000076	time 2.0640 (2.0640)	loss 2.1302 (2.1302)	grad_norm 2.5700 (2.5700)	mem 14853MB
[2022-11-07 13:33:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][50/1251]	eta 0:10:01 lr 0.000076	time 0.4828 (0.5010)	loss 3.5200 (2.8386)	grad_norm 3.3002 (2.6751)	mem 14853MB
[2022-11-07 13:33:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][100/1251]	eta 0:09:18 lr 0.000076	time 0.4748 (0.4849)	loss 3.3523 (2.8459)	grad_norm 2.6502 (2.6471)	mem 14853MB
[2022-11-07 13:33:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][150/1251]	eta 0:08:47 lr 0.000076	time 0.5477 (0.4792)	loss 3.4046 (2.8814)	grad_norm 2.7785 (2.6587)	mem 14853MB
[2022-11-07 13:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][200/1251]	eta 0:08:20 lr 0.000076	time 0.4646 (0.4765)	loss 3.0052 (2.9077)	grad_norm 2.6054 (2.6480)	mem 14853MB
[2022-11-07 13:34:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][250/1251]	eta 0:07:54 lr 0.000076	time 0.4625 (0.4740)	loss 2.9353 (2.9022)	grad_norm 2.5229 (2.6363)	mem 14853MB
[2022-11-07 13:34:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][300/1251]	eta 0:07:29 lr 0.000076	time 0.4774 (0.4725)	loss 3.3083 (2.9062)	grad_norm 2.7020 (2.6528)	mem 14853MB
[2022-11-07 13:35:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][350/1251]	eta 0:07:05 lr 0.000076	time 0.4642 (0.4717)	loss 3.1935 (2.9021)	grad_norm 2.6948 (2.6493)	mem 14853MB
[2022-11-07 13:35:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][400/1251]	eta 0:06:40 lr 0.000075	time 0.4651 (0.4710)	loss 3.3049 (2.9074)	grad_norm 2.5685 (2.6587)	mem 14853MB
[2022-11-07 13:36:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][450/1251]	eta 0:06:16 lr 0.000075	time 0.4690 (0.4704)	loss 3.1803 (2.9133)	grad_norm 3.0881 (2.6589)	mem 14853MB
[2022-11-07 13:36:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][500/1251]	eta 0:05:52 lr 0.000075	time 0.4680 (0.4700)	loss 3.0544 (2.9164)	grad_norm 2.6578 (2.6549)	mem 14853MB
[2022-11-07 13:36:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][550/1251]	eta 0:05:29 lr 0.000075	time 0.4720 (0.4696)	loss 2.3695 (2.9090)	grad_norm 2.5804 (2.6522)	mem 14853MB
[2022-11-07 13:37:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][600/1251]	eta 0:05:05 lr 0.000075	time 0.5545 (0.4696)	loss 2.6665 (2.9131)	grad_norm 2.2305 (2.6623)	mem 14853MB
[2022-11-07 13:37:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][650/1251]	eta 0:04:42 lr 0.000075	time 0.4764 (0.4693)	loss 3.1674 (2.9186)	grad_norm 3.2242 (2.6675)	mem 14853MB
[2022-11-07 13:38:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][700/1251]	eta 0:04:18 lr 0.000075	time 0.4605 (0.4693)	loss 2.1913 (2.9185)	grad_norm 3.3930 (2.6728)	mem 14853MB
[2022-11-07 13:38:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][750/1251]	eta 0:03:55 lr 0.000075	time 0.4554 (0.4691)	loss 2.8385 (2.9143)	grad_norm 2.8755 (2.6771)	mem 14853MB
[2022-11-07 13:38:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][800/1251]	eta 0:03:31 lr 0.000075	time 0.4557 (0.4690)	loss 2.9121 (2.9205)	grad_norm 2.4285 (2.6806)	mem 14853MB
[2022-11-07 13:39:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][850/1251]	eta 0:03:07 lr 0.000075	time 0.4626 (0.4688)	loss 3.2160 (2.9208)	grad_norm 2.6574 (2.6803)	mem 14853MB
[2022-11-07 13:39:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][900/1251]	eta 0:02:44 lr 0.000074	time 0.4676 (0.4686)	loss 3.2012 (2.9241)	grad_norm 2.6220 (2.6826)	mem 14853MB
[2022-11-07 13:40:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][950/1251]	eta 0:02:21 lr 0.000074	time 0.4679 (0.4685)	loss 2.3641 (2.9233)	grad_norm 2.9168 (2.6774)	mem 14853MB
[2022-11-07 13:40:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][1000/1251]	eta 0:01:57 lr 0.000074	time 0.4608 (0.4685)	loss 3.1885 (2.9195)	grad_norm 2.6541 (2.6858)	mem 14853MB
[2022-11-07 13:40:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][1050/1251]	eta 0:01:34 lr 0.000074	time 0.4599 (0.4683)	loss 2.1095 (2.9142)	grad_norm 2.2333 (2.6889)	mem 14853MB
[2022-11-07 13:41:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][1100/1251]	eta 0:01:10 lr 0.000074	time 0.4750 (0.4683)	loss 3.1937 (2.9148)	grad_norm 2.4382 (2.6879)	mem 14853MB
[2022-11-07 13:41:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][1150/1251]	eta 0:00:47 lr 0.000074	time 0.4692 (0.4682)	loss 2.9882 (2.9178)	grad_norm 2.8367 (2.6893)	mem 14853MB
[2022-11-07 13:41:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][1200/1251]	eta 0:00:23 lr 0.000074	time 0.4714 (0.4682)	loss 2.6960 (2.9133)	grad_norm 2.3698 (2.6951)	mem 14853MB
[2022-11-07 13:42:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [250/300][1250/1251]	eta 0:00:00 lr 0.000074	time 0.4568 (0.4680)	loss 3.0073 (2.9123)	grad_norm 2.5536 (2.6939)	mem 14853MB
[2022-11-07 13:42:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 250 training takes 0:09:45
[2022-11-07 13:42:22 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_250.pth saving......
[2022-11-07 13:42:23 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_250.pth saved !!!
[2022-11-07 13:42:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.558 (1.558)	Loss 0.8662 (0.8662)	Acc@1 78.516 (78.516)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-07 13:42:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.774 Acc@5 95.802
[2022-11-07 13:42:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.8%
[2022-11-07 13:42:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.689 (1.689)	Loss 0.7652 (0.7652)	Acc@1 81.445 (81.445)	Acc@5 96.582 (96.582)	Mem 14853MB
[2022-11-07 13:42:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.206 Acc@5 96.064
[2022-11-07 13:42:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-07 13:42:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.22% at 245 epoch
[2022-11-07 13:42:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][0/1251]	eta 0:40:36 lr 0.000074	time 1.9474 (1.9474)	loss 3.4370 (3.4370)	grad_norm 2.5323 (2.5323)	mem 14853MB
[2022-11-07 13:43:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][50/1251]	eta 0:10:05 lr 0.000074	time 0.4659 (0.5038)	loss 3.5979 (2.8728)	grad_norm 2.7362 (2.6179)	mem 14853MB
[2022-11-07 13:43:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][100/1251]	eta 0:09:17 lr 0.000074	time 0.4736 (0.4842)	loss 2.7435 (2.8492)	grad_norm 2.7237 (2.6136)	mem 14853MB
[2022-11-07 13:43:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][150/1251]	eta 0:08:47 lr 0.000073	time 0.4602 (0.4789)	loss 2.2916 (2.8628)	grad_norm 2.3151 (2.6113)	mem 14853MB
[2022-11-07 13:44:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][200/1251]	eta 0:08:20 lr 0.000073	time 0.4673 (0.4761)	loss 2.4445 (2.8880)	grad_norm 2.7160 (2.6280)	mem 14853MB
[2022-11-07 13:44:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][250/1251]	eta 0:07:54 lr 0.000073	time 0.4725 (0.4739)	loss 2.3206 (2.8761)	grad_norm 2.9479 (2.6404)	mem 14853MB
[2022-11-07 13:45:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][300/1251]	eta 0:07:29 lr 0.000073	time 0.4615 (0.4723)	loss 2.8701 (2.8910)	grad_norm 2.4885 (2.6345)	mem 14853MB
[2022-11-07 13:45:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][350/1251]	eta 0:07:04 lr 0.000073	time 0.4695 (0.4712)	loss 3.5358 (2.8816)	grad_norm 2.7134 (2.6439)	mem 14853MB
[2022-11-07 13:45:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][400/1251]	eta 0:06:40 lr 0.000073	time 0.4601 (0.4702)	loss 2.9978 (2.8846)	grad_norm 2.5765 (2.6592)	mem 14853MB
[2022-11-07 13:46:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][450/1251]	eta 0:06:16 lr 0.000073	time 0.4560 (0.4699)	loss 2.4991 (2.8935)	grad_norm 2.6619 (2.6645)	mem 14853MB
[2022-11-07 13:46:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][500/1251]	eta 0:05:52 lr 0.000073	time 0.4640 (0.4698)	loss 3.1970 (2.8845)	grad_norm 2.3615 (2.6618)	mem 14853MB
[2022-11-07 13:46:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][550/1251]	eta 0:05:29 lr 0.000073	time 0.4620 (0.4695)	loss 3.4151 (2.8923)	grad_norm 3.0865 (2.6715)	mem 14853MB
[2022-11-07 13:47:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][600/1251]	eta 0:05:05 lr 0.000073	time 0.4565 (0.4691)	loss 3.0695 (2.8933)	grad_norm 2.5006 (2.6804)	mem 14853MB
[2022-11-07 13:47:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][650/1251]	eta 0:04:41 lr 0.000072	time 0.4755 (0.4689)	loss 3.2072 (2.8896)	grad_norm 2.4481 (inf)	mem 14853MB
[2022-11-07 13:48:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][700/1251]	eta 0:04:18 lr 0.000072	time 0.4710 (0.4686)	loss 3.5142 (2.8922)	grad_norm 2.7296 (inf)	mem 14853MB
[2022-11-07 13:48:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][750/1251]	eta 0:03:54 lr 0.000072	time 0.4662 (0.4687)	loss 2.1322 (2.8915)	grad_norm 2.4510 (inf)	mem 14853MB
[2022-11-07 13:48:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][800/1251]	eta 0:03:31 lr 0.000072	time 0.4620 (0.4688)	loss 2.1022 (2.8948)	grad_norm 2.3575 (inf)	mem 14853MB
[2022-11-07 13:49:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][850/1251]	eta 0:03:07 lr 0.000072	time 0.4670 (0.4686)	loss 3.4036 (2.8982)	grad_norm 2.8085 (inf)	mem 14853MB
[2022-11-07 13:49:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][900/1251]	eta 0:02:44 lr 0.000072	time 0.4544 (0.4685)	loss 2.1130 (2.8979)	grad_norm 2.9957 (inf)	mem 14853MB
[2022-11-07 13:50:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][950/1251]	eta 0:02:20 lr 0.000072	time 0.4614 (0.4682)	loss 3.2775 (2.8950)	grad_norm 2.2867 (inf)	mem 14853MB
[2022-11-07 13:50:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][1000/1251]	eta 0:01:57 lr 0.000072	time 0.4641 (0.4683)	loss 2.4967 (2.8934)	grad_norm 2.8285 (inf)	mem 14853MB
[2022-11-07 13:50:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][1050/1251]	eta 0:01:34 lr 0.000072	time 0.4620 (0.4684)	loss 3.2032 (2.8934)	grad_norm 2.4362 (inf)	mem 14853MB
[2022-11-07 13:51:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][1100/1251]	eta 0:01:10 lr 0.000072	time 0.4653 (0.4682)	loss 2.5040 (2.8955)	grad_norm 2.5026 (inf)	mem 14853MB
[2022-11-07 13:51:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][1150/1251]	eta 0:00:47 lr 0.000071	time 0.4666 (0.4681)	loss 2.9595 (2.8974)	grad_norm 3.0820 (inf)	mem 14853MB
[2022-11-07 13:52:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][1200/1251]	eta 0:00:23 lr 0.000071	time 0.4659 (0.4680)	loss 3.1327 (2.8953)	grad_norm 2.8271 (inf)	mem 14853MB
[2022-11-07 13:52:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [251/300][1250/1251]	eta 0:00:00 lr 0.000071	time 0.4577 (0.4678)	loss 3.5487 (2.8911)	grad_norm 2.9027 (inf)	mem 14853MB
[2022-11-07 13:52:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 251 training takes 0:09:45
[2022-11-07 13:52:26 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_251.pth saving......
[2022-11-07 13:52:26 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_251.pth saved !!!
[2022-11-07 13:52:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.624 (1.624)	Loss 0.7841 (0.7841)	Acc@1 80.371 (80.371)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 13:52:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.722 Acc@5 95.850
[2022-11-07 13:52:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.7%
[2022-11-07 13:52:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.632 (1.632)	Loss 0.7851 (0.7851)	Acc@1 81.348 (81.348)	Acc@5 96.289 (96.289)	Mem 14853MB
[2022-11-07 13:52:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.262 Acc@5 96.058
[2022-11-07 13:52:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 13:52:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.26% at 251 epoch
[2022-11-07 13:52:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][0/1251]	eta 0:42:38 lr 0.000071	time 2.0455 (2.0455)	loss 3.3227 (3.3227)	grad_norm 2.6379 (2.6379)	mem 14853MB
[2022-11-07 13:53:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][50/1251]	eta 0:10:08 lr 0.000071	time 0.4750 (0.5068)	loss 2.4417 (2.7952)	grad_norm 2.8289 (2.6578)	mem 14853MB
[2022-11-07 13:53:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][100/1251]	eta 0:09:20 lr 0.000071	time 0.4591 (0.4869)	loss 3.6156 (2.8628)	grad_norm 2.7692 (2.6902)	mem 14853MB
[2022-11-07 13:53:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][150/1251]	eta 0:08:49 lr 0.000071	time 0.4770 (0.4808)	loss 3.3542 (2.8726)	grad_norm 2.6834 (2.6886)	mem 14853MB
[2022-11-07 13:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][200/1251]	eta 0:08:21 lr 0.000071	time 0.4683 (0.4770)	loss 3.5070 (2.8938)	grad_norm 2.6148 (2.6852)	mem 14853MB
[2022-11-07 13:54:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][250/1251]	eta 0:07:55 lr 0.000071	time 0.4600 (0.4751)	loss 3.0547 (2.8788)	grad_norm 2.4557 (2.6995)	mem 14853MB
[2022-11-07 13:55:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][300/1251]	eta 0:07:30 lr 0.000071	time 0.4778 (0.4735)	loss 2.3949 (2.8936)	grad_norm 3.0815 (2.7018)	mem 14853MB
[2022-11-07 13:55:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][350/1251]	eta 0:07:05 lr 0.000071	time 0.4548 (0.4726)	loss 2.3760 (2.9008)	grad_norm 3.1106 (2.7043)	mem 14853MB
[2022-11-07 13:55:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][400/1251]	eta 0:06:41 lr 0.000070	time 0.4704 (0.4716)	loss 3.0618 (2.8984)	grad_norm 2.7841 (2.7098)	mem 14853MB
[2022-11-07 13:56:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][450/1251]	eta 0:06:17 lr 0.000070	time 0.4549 (0.4709)	loss 2.6453 (2.8958)	grad_norm 2.4266 (2.7023)	mem 14853MB
[2022-11-07 13:56:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][500/1251]	eta 0:05:53 lr 0.000070	time 0.4623 (0.4703)	loss 2.9602 (2.8940)	grad_norm 2.7603 (2.6987)	mem 14853MB
[2022-11-07 13:57:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][550/1251]	eta 0:05:29 lr 0.000070	time 0.4662 (0.4704)	loss 3.3681 (2.8965)	grad_norm 2.9737 (2.7029)	mem 14853MB
[2022-11-07 13:57:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][600/1251]	eta 0:05:06 lr 0.000070	time 0.4635 (0.4703)	loss 2.9210 (2.8962)	grad_norm 2.9069 (2.7076)	mem 14853MB
[2022-11-07 13:57:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][650/1251]	eta 0:04:42 lr 0.000070	time 0.4946 (0.4700)	loss 3.3171 (2.8957)	grad_norm 2.7700 (2.7049)	mem 14853MB
[2022-11-07 13:58:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][700/1251]	eta 0:04:18 lr 0.000070	time 0.4629 (0.4696)	loss 3.2820 (2.8926)	grad_norm 2.4713 (2.7007)	mem 14853MB
[2022-11-07 13:58:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][750/1251]	eta 0:03:55 lr 0.000070	time 0.4663 (0.4695)	loss 3.1696 (2.8994)	grad_norm 2.5946 (2.6996)	mem 14853MB
[2022-11-07 13:59:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][800/1251]	eta 0:03:31 lr 0.000070	time 0.4626 (0.4695)	loss 2.4866 (2.9069)	grad_norm 2.4974 (2.6990)	mem 14853MB
[2022-11-07 13:59:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][850/1251]	eta 0:03:08 lr 0.000070	time 0.4651 (0.4693)	loss 2.9446 (2.9105)	grad_norm 2.4612 (2.6932)	mem 14853MB
[2022-11-07 13:59:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][900/1251]	eta 0:02:44 lr 0.000069	time 0.4586 (0.4692)	loss 3.0915 (2.9093)	grad_norm 2.6959 (2.6911)	mem 14853MB
[2022-11-07 14:00:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][950/1251]	eta 0:02:21 lr 0.000069	time 0.4650 (0.4690)	loss 2.7632 (2.9080)	grad_norm 2.3284 (2.6893)	mem 14853MB
[2022-11-07 14:00:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][1000/1251]	eta 0:01:57 lr 0.000069	time 0.4556 (0.4688)	loss 2.5023 (2.9071)	grad_norm 2.6906 (2.6889)	mem 14853MB
[2022-11-07 14:00:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][1050/1251]	eta 0:01:34 lr 0.000069	time 0.4587 (0.4689)	loss 2.5586 (2.9077)	grad_norm 2.7844 (2.6907)	mem 14853MB
[2022-11-07 14:01:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][1100/1251]	eta 0:01:10 lr 0.000069	time 0.4522 (0.4689)	loss 2.2387 (2.9091)	grad_norm 2.9921 (2.6927)	mem 14853MB
[2022-11-07 14:01:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][1150/1251]	eta 0:00:47 lr 0.000069	time 0.4561 (0.4688)	loss 2.9952 (2.9096)	grad_norm 2.5985 (2.6928)	mem 14853MB
[2022-11-07 14:02:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][1200/1251]	eta 0:00:23 lr 0.000069	time 0.4795 (0.4686)	loss 2.9621 (2.9087)	grad_norm 2.6670 (2.6932)	mem 14853MB
[2022-11-07 14:02:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [252/300][1250/1251]	eta 0:00:00 lr 0.000069	time 0.4572 (0.4684)	loss 3.6122 (2.9055)	grad_norm 2.6910 (2.6936)	mem 14853MB
[2022-11-07 14:02:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 252 training takes 0:09:46
[2022-11-07 14:02:30 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_252.pth saving......
[2022-11-07 14:02:30 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_252.pth saved !!!
[2022-11-07 14:02:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.524 (1.524)	Loss 0.8200 (0.8200)	Acc@1 81.934 (81.934)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 14:02:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.696 Acc@5 95.790
[2022-11-07 14:02:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.7%
[2022-11-07 14:02:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.597 (1.597)	Loss 0.8142 (0.8142)	Acc@1 81.348 (81.348)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 14:02:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.294 Acc@5 96.066
[2022-11-07 14:02:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 14:02:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.29% at 252 epoch
[2022-11-07 14:02:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][0/1251]	eta 0:46:30 lr 0.000069	time 2.2303 (2.2303)	loss 3.3871 (3.3871)	grad_norm 2.6323 (2.6323)	mem 14853MB
[2022-11-07 14:03:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][50/1251]	eta 0:10:09 lr 0.000069	time 0.4685 (0.5077)	loss 3.4222 (2.8672)	grad_norm 3.3808 (2.7956)	mem 14853MB
[2022-11-07 14:03:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][100/1251]	eta 0:09:24 lr 0.000069	time 0.4646 (0.4905)	loss 3.0904 (2.8645)	grad_norm 2.2780 (2.7056)	mem 14853MB
[2022-11-07 14:04:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][150/1251]	eta 0:08:51 lr 0.000068	time 0.4726 (0.4823)	loss 2.3738 (2.8779)	grad_norm 2.6410 (2.6867)	mem 14853MB
[2022-11-07 14:04:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][200/1251]	eta 0:08:22 lr 0.000068	time 0.4676 (0.4782)	loss 2.0342 (2.8875)	grad_norm 2.8831 (2.6944)	mem 14853MB
[2022-11-07 14:04:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][250/1251]	eta 0:07:56 lr 0.000068	time 0.4749 (0.4758)	loss 2.9881 (2.8886)	grad_norm 2.7729 (2.6993)	mem 14853MB
[2022-11-07 14:05:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][300/1251]	eta 0:07:30 lr 0.000068	time 0.4669 (0.4740)	loss 3.4806 (2.9130)	grad_norm 2.8302 (2.7192)	mem 14853MB
[2022-11-07 14:05:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][350/1251]	eta 0:07:06 lr 0.000068	time 0.4724 (0.4729)	loss 2.0397 (2.9035)	grad_norm 2.8237 (2.7225)	mem 14853MB
[2022-11-07 14:05:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][400/1251]	eta 0:06:41 lr 0.000068	time 0.4602 (0.4720)	loss 2.9821 (2.9129)	grad_norm 2.5749 (2.7223)	mem 14853MB
[2022-11-07 14:06:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][450/1251]	eta 0:06:17 lr 0.000068	time 0.4655 (0.4714)	loss 3.4809 (2.9125)	grad_norm 2.7951 (2.7157)	mem 14853MB
[2022-11-07 14:06:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][500/1251]	eta 0:05:53 lr 0.000068	time 0.4631 (0.4707)	loss 3.4153 (2.9124)	grad_norm 2.7025 (2.7087)	mem 14853MB
[2022-11-07 14:07:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][550/1251]	eta 0:05:29 lr 0.000068	time 0.4619 (0.4707)	loss 2.9786 (2.9142)	grad_norm 3.0774 (2.7103)	mem 14853MB
[2022-11-07 14:07:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][600/1251]	eta 0:05:06 lr 0.000068	time 0.4563 (0.4705)	loss 3.4158 (2.9204)	grad_norm 2.9378 (2.7088)	mem 14853MB
[2022-11-07 14:07:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][650/1251]	eta 0:04:42 lr 0.000067	time 0.4568 (0.4701)	loss 3.1887 (2.9193)	grad_norm 2.4922 (2.7139)	mem 14853MB
[2022-11-07 14:08:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][700/1251]	eta 0:04:18 lr 0.000067	time 0.4671 (0.4699)	loss 3.0679 (2.9259)	grad_norm 2.5048 (2.7149)	mem 14853MB
[2022-11-07 14:08:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][750/1251]	eta 0:03:55 lr 0.000067	time 0.4638 (0.4696)	loss 2.7971 (2.9194)	grad_norm 2.9566 (2.7145)	mem 14853MB
[2022-11-07 14:09:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][800/1251]	eta 0:03:31 lr 0.000067	time 0.4643 (0.4695)	loss 2.3734 (2.9137)	grad_norm 2.6251 (inf)	mem 14853MB
[2022-11-07 14:09:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][850/1251]	eta 0:03:08 lr 0.000067	time 0.4609 (0.4694)	loss 3.1203 (2.9205)	grad_norm 2.9531 (inf)	mem 14853MB
[2022-11-07 14:09:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][900/1251]	eta 0:02:44 lr 0.000067	time 0.5420 (0.4692)	loss 2.3883 (2.9150)	grad_norm 2.7434 (inf)	mem 14853MB
[2022-11-07 14:10:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][950/1251]	eta 0:02:21 lr 0.000067	time 0.4631 (0.4690)	loss 2.9557 (2.9092)	grad_norm 2.4585 (inf)	mem 14853MB
[2022-11-07 14:10:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][1000/1251]	eta 0:01:57 lr 0.000067	time 0.4665 (0.4688)	loss 1.9260 (2.9100)	grad_norm 2.7603 (inf)	mem 14853MB
[2022-11-07 14:11:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][1050/1251]	eta 0:01:34 lr 0.000067	time 0.4622 (0.4690)	loss 1.7009 (2.9125)	grad_norm 2.8126 (inf)	mem 14853MB
[2022-11-07 14:11:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][1100/1251]	eta 0:01:10 lr 0.000067	time 0.4703 (0.4690)	loss 3.0128 (2.9089)	grad_norm 2.8555 (inf)	mem 14853MB
[2022-11-07 14:11:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][1150/1251]	eta 0:00:47 lr 0.000067	time 0.4587 (0.4690)	loss 2.0536 (2.9042)	grad_norm 2.4926 (inf)	mem 14853MB
[2022-11-07 14:12:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][1200/1251]	eta 0:00:23 lr 0.000066	time 0.4740 (0.4688)	loss 2.4466 (2.9005)	grad_norm 2.9197 (inf)	mem 14853MB
[2022-11-07 14:12:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [253/300][1250/1251]	eta 0:00:00 lr 0.000066	time 0.4575 (0.4686)	loss 3.0285 (2.8969)	grad_norm 2.3729 (inf)	mem 14853MB
[2022-11-07 14:12:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 253 training takes 0:09:46
[2022-11-07 14:12:34 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_253.pth saving......
[2022-11-07 14:12:35 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_253.pth saved !!!
[2022-11-07 14:12:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.746 (1.746)	Loss 0.7887 (0.7887)	Acc@1 82.422 (82.422)	Acc@5 95.117 (95.117)	Mem 14853MB
[2022-11-07 14:12:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.814 Acc@5 95.812
[2022-11-07 14:12:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.8%
[2022-11-07 14:12:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.721 (1.721)	Loss 0.7999 (0.7999)	Acc@1 81.348 (81.348)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 14:12:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.272 Acc@5 96.056
[2022-11-07 14:12:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 14:12:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.29% at 252 epoch
[2022-11-07 14:12:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][0/1251]	eta 0:42:04 lr 0.000066	time 2.0182 (2.0182)	loss 3.1588 (3.1588)	grad_norm 2.6414 (2.6414)	mem 14853MB
[2022-11-07 14:13:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][50/1251]	eta 0:10:00 lr 0.000066	time 0.4666 (0.5003)	loss 3.1772 (2.9643)	grad_norm 2.5933 (2.7147)	mem 14853MB
[2022-11-07 14:13:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][100/1251]	eta 0:09:17 lr 0.000066	time 0.4763 (0.4847)	loss 3.0924 (2.9457)	grad_norm 2.8298 (2.7695)	mem 14853MB
[2022-11-07 14:14:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][150/1251]	eta 0:08:46 lr 0.000066	time 0.4661 (0.4782)	loss 2.8763 (2.9401)	grad_norm 2.4185 (2.7413)	mem 14853MB
[2022-11-07 14:14:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][200/1251]	eta 0:08:18 lr 0.000066	time 0.4691 (0.4748)	loss 3.3915 (2.8976)	grad_norm 3.2813 (2.7341)	mem 14853MB
[2022-11-07 14:14:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][250/1251]	eta 0:07:54 lr 0.000066	time 0.4748 (0.4736)	loss 2.8817 (2.8721)	grad_norm 2.8645 (2.7273)	mem 14853MB
[2022-11-07 14:15:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][300/1251]	eta 0:07:29 lr 0.000066	time 0.4742 (0.4722)	loss 2.1443 (2.8758)	grad_norm 2.4954 (2.7165)	mem 14853MB
[2022-11-07 14:15:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][350/1251]	eta 0:07:04 lr 0.000066	time 0.4647 (0.4711)	loss 1.9295 (2.8739)	grad_norm 2.8500 (2.7223)	mem 14853MB
[2022-11-07 14:16:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][400/1251]	eta 0:06:40 lr 0.000066	time 0.4722 (0.4703)	loss 2.6835 (2.8739)	grad_norm 2.5129 (2.7257)	mem 14853MB
[2022-11-07 14:16:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][450/1251]	eta 0:06:16 lr 0.000065	time 0.4762 (0.4696)	loss 2.3541 (2.8692)	grad_norm 2.8020 (2.7364)	mem 14853MB
[2022-11-07 14:16:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][500/1251]	eta 0:05:52 lr 0.000065	time 0.4595 (0.4694)	loss 2.9193 (2.8668)	grad_norm 2.5090 (2.7292)	mem 14853MB
[2022-11-07 14:17:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][550/1251]	eta 0:05:28 lr 0.000065	time 0.4596 (0.4692)	loss 2.0221 (2.8611)	grad_norm 2.8423 (2.7292)	mem 14853MB
[2022-11-07 14:17:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][600/1251]	eta 0:05:05 lr 0.000065	time 0.4625 (0.4690)	loss 3.2970 (2.8628)	grad_norm 3.0366 (2.7320)	mem 14853MB
[2022-11-07 14:17:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][650/1251]	eta 0:04:41 lr 0.000065	time 0.4592 (0.4686)	loss 3.5182 (2.8640)	grad_norm 2.6392 (2.7335)	mem 14853MB
[2022-11-07 14:18:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][700/1251]	eta 0:04:18 lr 0.000065	time 0.4650 (0.4683)	loss 3.0491 (2.8639)	grad_norm 2.7666 (2.7288)	mem 14853MB
[2022-11-07 14:18:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][750/1251]	eta 0:03:54 lr 0.000065	time 0.4525 (0.4684)	loss 1.9567 (2.8577)	grad_norm 2.5630 (2.7233)	mem 14853MB
[2022-11-07 14:19:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][800/1251]	eta 0:03:31 lr 0.000065	time 0.4644 (0.4683)	loss 3.2300 (2.8554)	grad_norm 2.5664 (2.7205)	mem 14853MB
[2022-11-07 14:19:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][850/1251]	eta 0:03:07 lr 0.000065	time 0.4657 (0.4681)	loss 2.3009 (2.8589)	grad_norm 2.4371 (2.7242)	mem 14853MB
[2022-11-07 14:19:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][900/1251]	eta 0:02:44 lr 0.000065	time 0.4632 (0.4680)	loss 2.8324 (2.8639)	grad_norm 2.5276 (2.7310)	mem 14853MB
[2022-11-07 14:20:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][950/1251]	eta 0:02:20 lr 0.000065	time 0.4747 (0.4678)	loss 3.0800 (2.8694)	grad_norm 2.7298 (2.7308)	mem 14853MB
[2022-11-07 14:20:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][1000/1251]	eta 0:01:57 lr 0.000064	time 0.4623 (0.4678)	loss 3.0856 (2.8741)	grad_norm 2.9093 (2.7297)	mem 14853MB
[2022-11-07 14:21:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][1050/1251]	eta 0:01:34 lr 0.000064	time 0.4592 (0.4678)	loss 2.1507 (2.8770)	grad_norm 2.6242 (2.7281)	mem 14853MB
[2022-11-07 14:21:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][1100/1251]	eta 0:01:10 lr 0.000064	time 0.4656 (0.4677)	loss 2.1956 (2.8767)	grad_norm 2.3129 (2.7339)	mem 14853MB
[2022-11-07 14:21:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][1150/1251]	eta 0:00:47 lr 0.000064	time 0.4642 (0.4676)	loss 3.2023 (2.8763)	grad_norm 2.7751 (2.7367)	mem 14853MB
[2022-11-07 14:22:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][1200/1251]	eta 0:00:23 lr 0.000064	time 0.4553 (0.4675)	loss 2.6878 (2.8756)	grad_norm 2.3231 (2.7347)	mem 14853MB
[2022-11-07 14:22:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [254/300][1250/1251]	eta 0:00:00 lr 0.000064	time 0.4560 (0.4675)	loss 2.3859 (2.8773)	grad_norm 2.4736 (2.7350)	mem 14853MB
[2022-11-07 14:22:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 254 training takes 0:09:44
[2022-11-07 14:22:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_254.pth saving......
[2022-11-07 14:22:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_254.pth saved !!!
[2022-11-07 14:22:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.593 (1.593)	Loss 0.7882 (0.7882)	Acc@1 82.520 (82.520)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-07 14:22:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.788 Acc@5 95.846
[2022-11-07 14:22:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.8%
[2022-11-07 14:22:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.580 (1.580)	Loss 0.7750 (0.7750)	Acc@1 81.934 (81.934)	Acc@5 96.191 (96.191)	Mem 14853MB
[2022-11-07 14:22:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.280 Acc@5 96.048
[2022-11-07 14:22:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 14:22:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.29% at 252 epoch
[2022-11-07 14:22:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][0/1251]	eta 0:40:27 lr 0.000064	time 1.9401 (1.9401)	loss 3.3267 (3.3267)	grad_norm 2.6143 (2.6143)	mem 14853MB
[2022-11-07 14:23:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][50/1251]	eta 0:10:03 lr 0.000064	time 0.4765 (0.5022)	loss 2.3504 (2.9296)	grad_norm 2.4305 (2.8634)	mem 14853MB
[2022-11-07 14:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][100/1251]	eta 0:09:19 lr 0.000064	time 0.4763 (0.4859)	loss 3.4924 (2.9511)	grad_norm 2.9553 (2.8196)	mem 14853MB
[2022-11-07 14:24:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][150/1251]	eta 0:08:47 lr 0.000064	time 0.4646 (0.4790)	loss 2.5123 (2.9394)	grad_norm 2.6287 (2.7896)	mem 14853MB
[2022-11-07 14:24:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][200/1251]	eta 0:08:20 lr 0.000064	time 0.4610 (0.4759)	loss 2.0094 (2.9086)	grad_norm 2.6417 (2.7870)	mem 14853MB
[2022-11-07 14:24:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][250/1251]	eta 0:07:54 lr 0.000063	time 0.4579 (0.4742)	loss 3.3441 (2.9019)	grad_norm 2.5800 (2.7711)	mem 14853MB
[2022-11-07 14:25:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][300/1251]	eta 0:07:29 lr 0.000063	time 0.4585 (0.4729)	loss 1.9892 (2.8818)	grad_norm 2.7059 (2.7738)	mem 14853MB
[2022-11-07 14:25:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][350/1251]	eta 0:07:05 lr 0.000063	time 0.4726 (0.4719)	loss 3.1829 (2.8872)	grad_norm 2.2736 (2.7633)	mem 14853MB
[2022-11-07 14:26:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][400/1251]	eta 0:06:41 lr 0.000063	time 0.4616 (0.4712)	loss 3.1538 (2.8917)	grad_norm 3.4634 (2.7619)	mem 14853MB
[2022-11-07 14:26:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][450/1251]	eta 0:06:17 lr 0.000063	time 0.4691 (0.4708)	loss 2.8207 (2.8735)	grad_norm 2.5477 (2.7624)	mem 14853MB
[2022-11-07 14:26:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][500/1251]	eta 0:05:53 lr 0.000063	time 0.4604 (0.4701)	loss 3.0506 (2.8666)	grad_norm 2.8891 (2.7562)	mem 14853MB
[2022-11-07 14:27:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][550/1251]	eta 0:05:29 lr 0.000063	time 0.4692 (0.4702)	loss 3.5354 (2.8783)	grad_norm 2.5896 (2.7527)	mem 14853MB
[2022-11-07 14:27:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][600/1251]	eta 0:05:05 lr 0.000063	time 0.4610 (0.4697)	loss 3.2036 (2.8780)	grad_norm 2.6552 (2.7593)	mem 14853MB
[2022-11-07 14:28:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][650/1251]	eta 0:04:42 lr 0.000063	time 0.4751 (0.4694)	loss 2.1428 (2.8748)	grad_norm 2.4639 (2.7654)	mem 14853MB
[2022-11-07 14:28:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][700/1251]	eta 0:04:18 lr 0.000063	time 0.4729 (0.4694)	loss 3.2077 (2.8736)	grad_norm 2.6185 (2.7685)	mem 14853MB
[2022-11-07 14:28:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][750/1251]	eta 0:03:55 lr 0.000063	time 0.4584 (0.4692)	loss 1.9535 (2.8788)	grad_norm 2.7710 (inf)	mem 14853MB
[2022-11-07 14:29:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][800/1251]	eta 0:03:31 lr 0.000062	time 0.4718 (0.4692)	loss 3.4611 (2.8774)	grad_norm 2.7301 (inf)	mem 14853MB
[2022-11-07 14:29:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][850/1251]	eta 0:03:08 lr 0.000062	time 0.4607 (0.4690)	loss 2.6238 (2.8776)	grad_norm 2.5507 (inf)	mem 14853MB
[2022-11-07 14:29:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][900/1251]	eta 0:02:44 lr 0.000062	time 0.4584 (0.4689)	loss 3.0675 (2.8790)	grad_norm 2.8290 (inf)	mem 14853MB
[2022-11-07 14:30:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][950/1251]	eta 0:02:21 lr 0.000062	time 0.4552 (0.4688)	loss 2.8365 (2.8831)	grad_norm 3.0158 (inf)	mem 14853MB
[2022-11-07 14:30:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][1000/1251]	eta 0:01:57 lr 0.000062	time 0.4772 (0.4686)	loss 1.8981 (2.8829)	grad_norm 3.1529 (inf)	mem 14853MB
[2022-11-07 14:31:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][1050/1251]	eta 0:01:34 lr 0.000062	time 0.4539 (0.4687)	loss 3.4242 (2.8831)	grad_norm 2.7881 (nan)	mem 14853MB
[2022-11-07 14:31:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][1100/1251]	eta 0:01:10 lr 0.000062	time 0.4630 (0.4685)	loss 3.2921 (2.8828)	grad_norm 2.6549 (nan)	mem 14853MB
[2022-11-07 14:31:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][1150/1251]	eta 0:00:47 lr 0.000062	time 0.4669 (0.4684)	loss 3.2149 (2.8866)	grad_norm 3.3152 (nan)	mem 14853MB
[2022-11-07 14:32:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][1200/1251]	eta 0:00:23 lr 0.000062	time 0.4583 (0.4683)	loss 2.8344 (2.8895)	grad_norm 2.5669 (nan)	mem 14853MB
[2022-11-07 14:32:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [255/300][1250/1251]	eta 0:00:00 lr 0.000062	time 0.4568 (0.4681)	loss 3.3640 (2.8903)	grad_norm 3.0217 (nan)	mem 14853MB
[2022-11-07 14:32:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 255 training takes 0:09:45
[2022-11-07 14:32:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_255.pth saving......
[2022-11-07 14:32:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_255.pth saved !!!
[2022-11-07 14:32:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.499 (1.499)	Loss 0.7891 (0.7891)	Acc@1 82.031 (82.031)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 14:32:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.842 Acc@5 95.762
[2022-11-07 14:32:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.8%
[2022-11-07 14:32:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.811 (1.811)	Loss 0.7447 (0.7447)	Acc@1 82.422 (82.422)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 14:33:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.304 Acc@5 96.046
[2022-11-07 14:33:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 14:33:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.30% at 255 epoch
[2022-11-07 14:33:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][0/1251]	eta 0:41:02 lr 0.000062	time 1.9681 (1.9681)	loss 3.6384 (3.6384)	grad_norm 2.8036 (2.8036)	mem 14853MB
[2022-11-07 14:33:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][50/1251]	eta 0:09:59 lr 0.000062	time 0.4601 (0.4988)	loss 2.4442 (2.8463)	grad_norm 2.6715 (2.7051)	mem 14853MB
[2022-11-07 14:33:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][100/1251]	eta 0:09:16 lr 0.000061	time 0.4639 (0.4834)	loss 3.4944 (2.8241)	grad_norm 2.5089 (2.7385)	mem 14853MB
[2022-11-07 14:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][150/1251]	eta 0:08:47 lr 0.000061	time 0.4529 (0.4790)	loss 2.8946 (2.8463)	grad_norm 2.4642 (2.7394)	mem 14853MB
[2022-11-07 14:34:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][200/1251]	eta 0:08:20 lr 0.000061	time 0.4585 (0.4759)	loss 2.2017 (2.8527)	grad_norm 2.7888 (2.7526)	mem 14853MB
[2022-11-07 14:34:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][250/1251]	eta 0:07:54 lr 0.000061	time 0.4654 (0.4742)	loss 2.5484 (2.8440)	grad_norm 2.6306 (2.7433)	mem 14853MB
[2022-11-07 14:35:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][300/1251]	eta 0:07:29 lr 0.000061	time 0.4670 (0.4726)	loss 2.8798 (2.8553)	grad_norm 2.5848 (2.7400)	mem 14853MB
[2022-11-07 14:35:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][350/1251]	eta 0:07:04 lr 0.000061	time 0.4688 (0.4714)	loss 3.5884 (2.8751)	grad_norm 2.7067 (2.7407)	mem 14853MB
[2022-11-07 14:36:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][400/1251]	eta 0:06:40 lr 0.000061	time 0.4671 (0.4706)	loss 2.2164 (2.8745)	grad_norm 3.0336 (2.7515)	mem 14853MB
[2022-11-07 14:36:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][450/1251]	eta 0:06:16 lr 0.000061	time 0.4640 (0.4701)	loss 2.8095 (2.8799)	grad_norm 2.4600 (2.7530)	mem 14853MB
[2022-11-07 14:36:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][500/1251]	eta 0:05:52 lr 0.000061	time 0.4550 (0.4698)	loss 3.2266 (2.8834)	grad_norm 3.6378 (2.7602)	mem 14853MB
[2022-11-07 14:37:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][550/1251]	eta 0:05:29 lr 0.000061	time 0.5424 (0.4696)	loss 2.9502 (2.8732)	grad_norm 2.8967 (2.7666)	mem 14853MB
[2022-11-07 14:37:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][600/1251]	eta 0:05:05 lr 0.000061	time 0.4605 (0.4693)	loss 3.3431 (2.8726)	grad_norm 3.0119 (2.7686)	mem 14853MB
[2022-11-07 14:38:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][650/1251]	eta 0:04:41 lr 0.000060	time 0.4654 (0.4690)	loss 3.5122 (2.8747)	grad_norm 2.9322 (2.7711)	mem 14853MB
[2022-11-07 14:38:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][700/1251]	eta 0:04:18 lr 0.000060	time 0.4578 (0.4687)	loss 3.1812 (2.8768)	grad_norm 2.4607 (2.7686)	mem 14853MB
[2022-11-07 14:38:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][750/1251]	eta 0:03:54 lr 0.000060	time 0.4541 (0.4687)	loss 2.3985 (2.8754)	grad_norm 2.3985 (2.7732)	mem 14853MB
[2022-11-07 14:39:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][800/1251]	eta 0:03:31 lr 0.000060	time 0.4597 (0.4687)	loss 2.4074 (2.8765)	grad_norm 2.4091 (2.7764)	mem 14853MB
[2022-11-07 14:39:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][850/1251]	eta 0:03:07 lr 0.000060	time 0.4846 (0.4685)	loss 2.1237 (2.8751)	grad_norm 2.6918 (2.7816)	mem 14853MB
[2022-11-07 14:40:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][900/1251]	eta 0:02:44 lr 0.000060	time 0.4575 (0.4683)	loss 3.1963 (2.8794)	grad_norm 2.5690 (2.7783)	mem 14853MB
[2022-11-07 14:40:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][950/1251]	eta 0:02:20 lr 0.000060	time 0.4707 (0.4683)	loss 2.9345 (2.8775)	grad_norm 3.1123 (2.7815)	mem 14853MB
[2022-11-07 14:40:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][1000/1251]	eta 0:01:57 lr 0.000060	time 0.4600 (0.4682)	loss 2.9636 (2.8808)	grad_norm 2.6748 (2.7782)	mem 14853MB
[2022-11-07 14:41:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][1050/1251]	eta 0:01:34 lr 0.000060	time 0.4746 (0.4682)	loss 3.0097 (2.8854)	grad_norm 2.5833 (2.7761)	mem 14853MB
[2022-11-07 14:41:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][1100/1251]	eta 0:01:10 lr 0.000060	time 0.4589 (0.4681)	loss 3.0949 (2.8881)	grad_norm 3.3212 (2.7790)	mem 14853MB
[2022-11-07 14:41:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][1150/1251]	eta 0:00:47 lr 0.000060	time 0.4681 (0.4680)	loss 2.6414 (2.8853)	grad_norm 2.9783 (2.7791)	mem 14853MB
[2022-11-07 14:42:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][1200/1251]	eta 0:00:23 lr 0.000059	time 0.4628 (0.4680)	loss 3.2843 (2.8873)	grad_norm 2.6146 (2.7788)	mem 14853MB
[2022-11-07 14:42:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [256/300][1250/1251]	eta 0:00:00 lr 0.000059	time 0.4586 (0.4679)	loss 2.4993 (2.8901)	grad_norm 2.6029 (2.7776)	mem 14853MB
[2022-11-07 14:42:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 256 training takes 0:09:45
[2022-11-07 14:42:45 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_256.pth saving......
[2022-11-07 14:42:46 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_256.pth saved !!!
[2022-11-07 14:42:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.575 (1.575)	Loss 0.8615 (0.8615)	Acc@1 78.125 (78.125)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 14:42:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.742 Acc@5 95.834
[2022-11-07 14:42:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.7%
[2022-11-07 14:42:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.643 (1.643)	Loss 0.8142 (0.8142)	Acc@1 81.543 (81.543)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-07 14:43:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.276 Acc@5 96.042
[2022-11-07 14:43:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 14:43:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.30% at 255 epoch
[2022-11-07 14:43:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][0/1251]	eta 0:40:52 lr 0.000059	time 1.9601 (1.9601)	loss 3.4033 (3.4033)	grad_norm 2.6792 (2.6792)	mem 14853MB
[2022-11-07 14:43:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][50/1251]	eta 0:10:01 lr 0.000059	time 0.4569 (0.5008)	loss 3.0162 (2.8679)	grad_norm 2.7082 (2.8824)	mem 14853MB
[2022-11-07 14:43:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][100/1251]	eta 0:09:18 lr 0.000059	time 0.4673 (0.4856)	loss 3.0324 (2.8687)	grad_norm 2.5625 (2.7991)	mem 14853MB
[2022-11-07 14:44:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][150/1251]	eta 0:08:48 lr 0.000059	time 0.4760 (0.4796)	loss 2.8827 (2.8608)	grad_norm 2.7478 (2.7953)	mem 14853MB
[2022-11-07 14:44:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][200/1251]	eta 0:08:20 lr 0.000059	time 0.4785 (0.4762)	loss 2.1724 (2.8589)	grad_norm 2.5788 (2.7784)	mem 14853MB
[2022-11-07 14:45:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][250/1251]	eta 0:07:54 lr 0.000059	time 0.4597 (0.4740)	loss 2.8938 (2.8591)	grad_norm 2.4587 (2.7711)	mem 14853MB
[2022-11-07 14:45:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][300/1251]	eta 0:07:29 lr 0.000059	time 0.4659 (0.4725)	loss 2.7309 (2.8633)	grad_norm 2.9122 (2.7732)	mem 14853MB
[2022-11-07 14:45:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][350/1251]	eta 0:07:05 lr 0.000059	time 0.4737 (0.4722)	loss 3.1987 (2.8550)	grad_norm 2.4549 (2.7752)	mem 14853MB
[2022-11-07 14:46:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][400/1251]	eta 0:06:41 lr 0.000059	time 0.4571 (0.4712)	loss 3.2912 (2.8511)	grad_norm 2.7937 (2.7799)	mem 14853MB
[2022-11-07 14:46:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][450/1251]	eta 0:06:17 lr 0.000059	time 0.4646 (0.4708)	loss 2.9679 (2.8532)	grad_norm 2.9514 (2.7744)	mem 14853MB
[2022-11-07 14:46:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][500/1251]	eta 0:05:53 lr 0.000058	time 0.4637 (0.4701)	loss 2.6484 (2.8614)	grad_norm 2.6501 (2.7741)	mem 14853MB
[2022-11-07 14:47:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][550/1251]	eta 0:05:29 lr 0.000058	time 0.4779 (0.4699)	loss 2.9176 (2.8688)	grad_norm 2.6778 (2.7709)	mem 14853MB
[2022-11-07 14:47:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][600/1251]	eta 0:05:05 lr 0.000058	time 0.4556 (0.4695)	loss 2.9715 (2.8718)	grad_norm 2.8624 (2.7620)	mem 14853MB
[2022-11-07 14:48:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][650/1251]	eta 0:04:42 lr 0.000058	time 0.4639 (0.4694)	loss 3.1919 (2.8705)	grad_norm 2.7596 (2.7634)	mem 14853MB
[2022-11-07 14:48:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][700/1251]	eta 0:04:18 lr 0.000058	time 0.4683 (0.4691)	loss 3.6440 (2.8811)	grad_norm 3.0323 (2.7698)	mem 14853MB
[2022-11-07 14:48:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][750/1251]	eta 0:03:54 lr 0.000058	time 0.4710 (0.4688)	loss 3.5349 (2.8819)	grad_norm 2.7266 (2.7705)	mem 14853MB
[2022-11-07 14:49:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][800/1251]	eta 0:03:31 lr 0.000058	time 0.4615 (0.4688)	loss 3.0663 (2.8842)	grad_norm 2.5714 (2.7722)	mem 14853MB
[2022-11-07 14:49:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][850/1251]	eta 0:03:08 lr 0.000058	time 0.4707 (0.4689)	loss 2.4247 (2.8848)	grad_norm 2.9039 (2.7747)	mem 14853MB
[2022-11-07 14:50:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][900/1251]	eta 0:02:44 lr 0.000058	time 0.4729 (0.4687)	loss 3.0393 (2.8900)	grad_norm 3.0203 (2.7790)	mem 14853MB
[2022-11-07 14:50:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][950/1251]	eta 0:02:21 lr 0.000058	time 0.4703 (0.4687)	loss 2.4676 (2.8894)	grad_norm 2.7877 (2.7765)	mem 14853MB
[2022-11-07 14:50:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][1000/1251]	eta 0:01:57 lr 0.000058	time 0.4703 (0.4685)	loss 3.1989 (2.8873)	grad_norm 2.3879 (2.7896)	mem 14853MB
[2022-11-07 14:51:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][1050/1251]	eta 0:01:34 lr 0.000057	time 0.4631 (0.4685)	loss 3.3017 (2.8863)	grad_norm 3.0608 (2.7884)	mem 14853MB
[2022-11-07 14:51:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][1100/1251]	eta 0:01:10 lr 0.000057	time 0.4591 (0.4685)	loss 2.8654 (2.8873)	grad_norm 2.7073 (2.7868)	mem 14853MB
[2022-11-07 14:52:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][1150/1251]	eta 0:00:47 lr 0.000057	time 0.4625 (0.4684)	loss 3.2740 (2.8872)	grad_norm 3.0415 (2.7910)	mem 14853MB
[2022-11-07 14:52:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][1200/1251]	eta 0:00:23 lr 0.000057	time 0.4657 (0.4684)	loss 3.5496 (2.8901)	grad_norm 3.2155 (2.7934)	mem 14853MB
[2022-11-07 14:52:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [257/300][1250/1251]	eta 0:00:00 lr 0.000057	time 0.4572 (0.4681)	loss 2.9873 (2.8930)	grad_norm 2.8585 (2.7967)	mem 14853MB
[2022-11-07 14:52:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 257 training takes 0:09:45
[2022-11-07 14:52:49 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_257.pth saving......
[2022-11-07 14:52:50 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_257.pth saved !!!
[2022-11-07 14:52:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.533 (1.533)	Loss 0.7844 (0.7844)	Acc@1 82.227 (82.227)	Acc@5 96.777 (96.777)	Mem 14853MB
[2022-11-07 14:52:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.966 Acc@5 95.868
[2022-11-07 14:52:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.0%
[2022-11-07 14:53:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.692 (1.692)	Loss 0.7583 (0.7583)	Acc@1 82.422 (82.422)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 14:53:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.276 Acc@5 96.040
[2022-11-07 14:53:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 14:53:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.30% at 255 epoch
[2022-11-07 14:53:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][0/1251]	eta 0:40:08 lr 0.000057	time 1.9250 (1.9250)	loss 3.1079 (3.1079)	grad_norm 2.3692 (2.3692)	mem 14853MB
[2022-11-07 14:53:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][50/1251]	eta 0:09:58 lr 0.000057	time 0.4624 (0.4984)	loss 3.6559 (2.8315)	grad_norm 3.3624 (2.7866)	mem 14853MB
[2022-11-07 14:53:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][100/1251]	eta 0:09:16 lr 0.000057	time 0.4686 (0.4832)	loss 3.3800 (2.8349)	grad_norm 2.8436 (2.7836)	mem 14853MB
[2022-11-07 14:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][150/1251]	eta 0:08:47 lr 0.000057	time 0.4782 (0.4787)	loss 3.2858 (2.8555)	grad_norm 2.3447 (2.7670)	mem 14853MB
[2022-11-07 14:54:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][200/1251]	eta 0:08:19 lr 0.000057	time 0.4739 (0.4757)	loss 2.8915 (2.8662)	grad_norm 2.7055 (2.7760)	mem 14853MB
[2022-11-07 14:55:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][250/1251]	eta 0:07:54 lr 0.000057	time 0.4652 (0.4742)	loss 2.5362 (2.8647)	grad_norm 2.5857 (2.7899)	mem 14853MB
[2022-11-07 14:55:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][300/1251]	eta 0:07:29 lr 0.000057	time 0.4538 (0.4725)	loss 3.3617 (2.8656)	grad_norm 2.7182 (2.7930)	mem 14853MB
[2022-11-07 14:55:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][350/1251]	eta 0:07:04 lr 0.000056	time 0.4767 (0.4713)	loss 3.1249 (2.8684)	grad_norm 2.4154 (2.7786)	mem 14853MB
[2022-11-07 14:56:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][400/1251]	eta 0:06:40 lr 0.000056	time 0.4602 (0.4705)	loss 3.2524 (2.8421)	grad_norm 2.9504 (2.7737)	mem 14853MB
[2022-11-07 14:56:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][450/1251]	eta 0:06:16 lr 0.000056	time 0.4826 (0.4701)	loss 2.5531 (2.8421)	grad_norm 2.9783 (2.7790)	mem 14853MB
[2022-11-07 14:57:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][500/1251]	eta 0:05:52 lr 0.000056	time 0.4607 (0.4698)	loss 2.3089 (2.8495)	grad_norm 2.8767 (2.7807)	mem 14853MB
[2022-11-07 14:57:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][550/1251]	eta 0:05:29 lr 0.000056	time 0.4674 (0.4696)	loss 2.4092 (2.8519)	grad_norm 2.8657 (2.7767)	mem 14853MB
[2022-11-07 14:57:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][600/1251]	eta 0:05:05 lr 0.000056	time 0.4633 (0.4693)	loss 3.3326 (2.8536)	grad_norm 3.0072 (2.7759)	mem 14853MB
[2022-11-07 14:58:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][650/1251]	eta 0:04:41 lr 0.000056	time 0.4740 (0.4690)	loss 2.9358 (2.8608)	grad_norm 2.6270 (2.7775)	mem 14853MB
[2022-11-07 14:58:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][700/1251]	eta 0:04:18 lr 0.000056	time 0.4625 (0.4689)	loss 3.5524 (2.8593)	grad_norm 3.1807 (2.7813)	mem 14853MB
[2022-11-07 14:58:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][750/1251]	eta 0:03:54 lr 0.000056	time 0.4800 (0.4687)	loss 3.0206 (2.8578)	grad_norm 2.6381 (2.7864)	mem 14853MB
[2022-11-07 14:59:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][800/1251]	eta 0:03:31 lr 0.000056	time 0.4549 (0.4686)	loss 3.0005 (2.8565)	grad_norm 2.7093 (2.7857)	mem 14853MB
[2022-11-07 14:59:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][850/1251]	eta 0:03:07 lr 0.000056	time 0.4668 (0.4685)	loss 2.7010 (2.8600)	grad_norm 3.0682 (2.7875)	mem 14853MB
[2022-11-07 15:00:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][900/1251]	eta 0:02:44 lr 0.000056	time 0.4794 (0.4682)	loss 3.0764 (2.8622)	grad_norm 2.5246 (2.7874)	mem 14853MB
[2022-11-07 15:00:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][950/1251]	eta 0:02:20 lr 0.000055	time 0.4727 (0.4682)	loss 2.2737 (2.8650)	grad_norm 2.5059 (2.7896)	mem 14853MB
[2022-11-07 15:00:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][1000/1251]	eta 0:01:57 lr 0.000055	time 0.5438 (0.4681)	loss 3.0890 (2.8674)	grad_norm 2.9722 (2.7893)	mem 14853MB
[2022-11-07 15:01:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][1050/1251]	eta 0:01:34 lr 0.000055	time 0.4528 (0.4681)	loss 3.2371 (2.8674)	grad_norm 2.6068 (2.7906)	mem 14853MB
[2022-11-07 15:01:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][1100/1251]	eta 0:01:10 lr 0.000055	time 0.4747 (0.4680)	loss 3.2677 (2.8711)	grad_norm 2.6716 (2.7902)	mem 14853MB
[2022-11-07 15:02:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][1150/1251]	eta 0:00:47 lr 0.000055	time 0.4575 (0.4679)	loss 3.3149 (2.8702)	grad_norm 2.7011 (2.7904)	mem 14853MB
[2022-11-07 15:02:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][1200/1251]	eta 0:00:23 lr 0.000055	time 0.4669 (0.4680)	loss 3.6713 (2.8690)	grad_norm 3.0437 (2.7929)	mem 14853MB
[2022-11-07 15:02:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [258/300][1250/1251]	eta 0:00:00 lr 0.000055	time 0.4563 (0.4678)	loss 2.0673 (2.8703)	grad_norm 2.6785 (2.7941)	mem 14853MB
[2022-11-07 15:02:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 258 training takes 0:09:45
[2022-11-07 15:02:53 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_258.pth saving......
[2022-11-07 15:02:53 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_258.pth saved !!!
[2022-11-07 15:02:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.568 (1.568)	Loss 0.8126 (0.8126)	Acc@1 80.762 (80.762)	Acc@5 95.117 (95.117)	Mem 14853MB
[2022-11-07 15:03:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.884 Acc@5 95.878
[2022-11-07 15:03:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.9%
[2022-11-07 15:03:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.605 (1.605)	Loss 0.7180 (0.7180)	Acc@1 84.277 (84.277)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 15:03:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.290 Acc@5 96.048
[2022-11-07 15:03:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 15:03:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.30% at 255 epoch
[2022-11-07 15:03:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][0/1251]	eta 0:41:14 lr 0.000055	time 1.9784 (1.9784)	loss 2.3223 (2.3223)	grad_norm 2.7918 (2.7918)	mem 14853MB
[2022-11-07 15:03:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][50/1251]	eta 0:10:01 lr 0.000055	time 0.4652 (0.5005)	loss 2.9490 (2.8462)	grad_norm 2.6491 (2.7556)	mem 14853MB
[2022-11-07 15:04:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][100/1251]	eta 0:09:17 lr 0.000055	time 0.4705 (0.4845)	loss 3.1059 (2.8646)	grad_norm 2.8326 (2.8072)	mem 14853MB
[2022-11-07 15:04:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][150/1251]	eta 0:08:48 lr 0.000055	time 0.4568 (0.4800)	loss 2.5868 (2.8883)	grad_norm 3.0388 (2.8140)	mem 14853MB
[2022-11-07 15:04:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][200/1251]	eta 0:08:20 lr 0.000055	time 0.4586 (0.4765)	loss 1.9203 (2.8810)	grad_norm 2.5623 (2.8324)	mem 14853MB
[2022-11-07 15:05:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][250/1251]	eta 0:07:55 lr 0.000054	time 0.4613 (0.4747)	loss 1.9184 (2.8643)	grad_norm 2.8616 (2.8181)	mem 14853MB
[2022-11-07 15:05:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][300/1251]	eta 0:07:29 lr 0.000054	time 0.4635 (0.4730)	loss 2.9704 (2.8522)	grad_norm 2.5419 (2.8064)	mem 14853MB
[2022-11-07 15:05:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][350/1251]	eta 0:07:05 lr 0.000054	time 0.4642 (0.4718)	loss 2.7309 (2.8577)	grad_norm 2.5837 (2.7991)	mem 14853MB
[2022-11-07 15:06:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][400/1251]	eta 0:06:41 lr 0.000054	time 0.4668 (0.4712)	loss 3.1479 (2.8523)	grad_norm 2.9135 (2.7998)	mem 14853MB
[2022-11-07 15:06:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][450/1251]	eta 0:06:16 lr 0.000054	time 0.4674 (0.4706)	loss 3.1484 (2.8558)	grad_norm 2.3587 (2.8072)	mem 14853MB
[2022-11-07 15:07:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][500/1251]	eta 0:05:52 lr 0.000054	time 0.4661 (0.4700)	loss 3.1320 (2.8557)	grad_norm 2.4213 (2.8059)	mem 14853MB
[2022-11-07 15:07:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][550/1251]	eta 0:05:29 lr 0.000054	time 0.4628 (0.4699)	loss 2.7364 (2.8491)	grad_norm 2.9263 (2.8115)	mem 14853MB
[2022-11-07 15:07:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][600/1251]	eta 0:05:05 lr 0.000054	time 0.4608 (0.4696)	loss 3.0738 (2.8494)	grad_norm 2.6963 (2.8094)	mem 14853MB
[2022-11-07 15:08:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][650/1251]	eta 0:04:42 lr 0.000054	time 0.4504 (0.4696)	loss 2.9468 (2.8407)	grad_norm 2.7875 (2.8073)	mem 14853MB
[2022-11-07 15:08:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][700/1251]	eta 0:04:18 lr 0.000054	time 0.4722 (0.4692)	loss 3.1310 (2.8433)	grad_norm 2.8647 (2.8071)	mem 14853MB
[2022-11-07 15:09:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][750/1251]	eta 0:03:54 lr 0.000054	time 0.4545 (0.4689)	loss 2.8218 (2.8443)	grad_norm 2.6620 (2.8079)	mem 14853MB
[2022-11-07 15:09:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][800/1251]	eta 0:03:31 lr 0.000054	time 0.4710 (0.4689)	loss 1.9616 (2.8431)	grad_norm 2.9862 (2.8059)	mem 14853MB
[2022-11-07 15:09:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][850/1251]	eta 0:03:08 lr 0.000053	time 0.4580 (0.4689)	loss 2.3588 (2.8438)	grad_norm 3.9642 (2.8128)	mem 14853MB
[2022-11-07 15:10:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][900/1251]	eta 0:02:44 lr 0.000053	time 0.4811 (0.4689)	loss 2.0559 (2.8426)	grad_norm 3.1036 (2.8156)	mem 14853MB
[2022-11-07 15:10:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][950/1251]	eta 0:02:21 lr 0.000053	time 0.4609 (0.4687)	loss 2.6084 (2.8447)	grad_norm 2.3513 (2.8205)	mem 14853MB
[2022-11-07 15:11:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][1000/1251]	eta 0:01:57 lr 0.000053	time 0.4779 (0.4685)	loss 2.7036 (2.8503)	grad_norm 2.6296 (2.8215)	mem 14853MB
[2022-11-07 15:11:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][1050/1251]	eta 0:01:34 lr 0.000053	time 0.4549 (0.4686)	loss 2.7952 (2.8516)	grad_norm 2.4475 (2.8233)	mem 14853MB
[2022-11-07 15:11:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][1100/1251]	eta 0:01:10 lr 0.000053	time 0.4517 (0.4685)	loss 2.7519 (2.8513)	grad_norm 4.2273 (2.8230)	mem 14853MB
[2022-11-07 15:12:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][1150/1251]	eta 0:00:47 lr 0.000053	time 0.4664 (0.4685)	loss 2.8305 (2.8534)	grad_norm 2.6528 (2.8249)	mem 14853MB
[2022-11-07 15:12:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][1200/1251]	eta 0:00:23 lr 0.000053	time 0.4630 (0.4683)	loss 2.3028 (2.8586)	grad_norm 2.6025 (2.8242)	mem 14853MB
[2022-11-07 15:12:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [259/300][1250/1251]	eta 0:00:00 lr 0.000053	time 0.4354 (0.4680)	loss 2.5380 (2.8561)	grad_norm nan (nan)	mem 14853MB
[2022-11-07 15:12:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 259 training takes 0:09:45
[2022-11-07 15:12:56 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_259.pth saving......
[2022-11-07 15:12:57 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_259.pth saved !!!
[2022-11-07 15:12:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.740 (1.740)	Loss 0.8085 (0.8085)	Acc@1 82.422 (82.422)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 15:13:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.832 Acc@5 95.884
[2022-11-07 15:13:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.8%
[2022-11-07 15:13:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.645 (1.645)	Loss 0.7587 (0.7587)	Acc@1 82.812 (82.812)	Acc@5 96.484 (96.484)	Mem 14853MB
[2022-11-07 15:13:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.304 Acc@5 96.044
[2022-11-07 15:13:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 15:13:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.30% at 259 epoch
[2022-11-07 15:13:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][0/1251]	eta 0:40:11 lr 0.000053	time 1.9278 (1.9278)	loss 3.3508 (3.3508)	grad_norm 3.0286 (3.0286)	mem 14853MB
[2022-11-07 15:13:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][50/1251]	eta 0:09:59 lr 0.000053	time 0.4592 (0.4992)	loss 3.7260 (2.9304)	grad_norm 3.0551 (2.8507)	mem 14853MB
[2022-11-07 15:14:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][100/1251]	eta 0:09:16 lr 0.000053	time 0.4717 (0.4833)	loss 3.2804 (2.9318)	grad_norm 2.4716 (2.8302)	mem 14853MB
[2022-11-07 15:14:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][150/1251]	eta 0:08:46 lr 0.000053	time 0.4622 (0.4785)	loss 2.8604 (2.9196)	grad_norm 2.6129 (2.8455)	mem 14853MB
[2022-11-07 15:14:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][200/1251]	eta 0:08:19 lr 0.000052	time 0.4733 (0.4749)	loss 3.3515 (2.8862)	grad_norm 2.5149 (2.8497)	mem 14853MB
[2022-11-07 15:15:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][250/1251]	eta 0:07:53 lr 0.000052	time 0.4576 (0.4732)	loss 3.1809 (2.8968)	grad_norm 3.1073 (2.8352)	mem 14853MB
[2022-11-07 15:15:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][300/1251]	eta 0:07:29 lr 0.000052	time 0.4581 (0.4722)	loss 2.0361 (2.8852)	grad_norm 2.7181 (2.8324)	mem 14853MB
[2022-11-07 15:16:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][350/1251]	eta 0:07:04 lr 0.000052	time 0.4595 (0.4711)	loss 2.3241 (2.8806)	grad_norm 2.4960 (2.8354)	mem 14853MB
[2022-11-07 15:16:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][400/1251]	eta 0:06:40 lr 0.000052	time 0.4696 (0.4703)	loss 3.1174 (2.8846)	grad_norm 2.4939 (2.8383)	mem 14853MB
[2022-11-07 15:16:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][450/1251]	eta 0:06:16 lr 0.000052	time 0.4647 (0.4695)	loss 2.8795 (2.8846)	grad_norm 2.6962 (2.8329)	mem 14853MB
[2022-11-07 15:17:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][500/1251]	eta 0:05:52 lr 0.000052	time 0.4637 (0.4690)	loss 2.4362 (2.8812)	grad_norm 2.7132 (2.8299)	mem 14853MB
[2022-11-07 15:17:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][550/1251]	eta 0:05:28 lr 0.000052	time 0.4710 (0.4690)	loss 3.2994 (2.8835)	grad_norm 2.7680 (2.8298)	mem 14853MB
[2022-11-07 15:17:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][600/1251]	eta 0:05:05 lr 0.000052	time 0.4544 (0.4690)	loss 2.8127 (2.8867)	grad_norm 2.6818 (2.8285)	mem 14853MB
[2022-11-07 15:18:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][650/1251]	eta 0:04:41 lr 0.000052	time 0.4722 (0.4687)	loss 1.9055 (2.8824)	grad_norm 2.8779 (2.8335)	mem 14853MB
[2022-11-07 15:18:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][700/1251]	eta 0:04:18 lr 0.000052	time 0.4653 (0.4684)	loss 3.2533 (2.8941)	grad_norm 2.5856 (2.8307)	mem 14853MB
[2022-11-07 15:19:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][750/1251]	eta 0:03:54 lr 0.000052	time 0.4664 (0.4683)	loss 2.4039 (2.8940)	grad_norm 2.6154 (2.8324)	mem 14853MB
[2022-11-07 15:19:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][800/1251]	eta 0:03:31 lr 0.000051	time 0.4739 (0.4683)	loss 2.9768 (2.8978)	grad_norm 2.7129 (2.8350)	mem 14853MB
[2022-11-07 15:19:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][850/1251]	eta 0:03:07 lr 0.000051	time 0.4681 (0.4682)	loss 3.2973 (2.8956)	grad_norm 2.6301 (2.8359)	mem 14853MB
[2022-11-07 15:20:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][900/1251]	eta 0:02:44 lr 0.000051	time 0.4673 (0.4681)	loss 2.1124 (2.8879)	grad_norm 2.9421 (2.8341)	mem 14853MB
[2022-11-07 15:20:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][950/1251]	eta 0:02:20 lr 0.000051	time 0.4651 (0.4679)	loss 3.2772 (2.8898)	grad_norm 3.0775 (2.8369)	mem 14853MB
[2022-11-07 15:21:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][1000/1251]	eta 0:01:57 lr 0.000051	time 0.4618 (0.4678)	loss 2.5348 (2.8848)	grad_norm 2.6134 (2.8377)	mem 14853MB
[2022-11-07 15:21:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][1050/1251]	eta 0:01:34 lr 0.000051	time 0.4731 (0.4678)	loss 3.3025 (2.8876)	grad_norm 3.1597 (2.8342)	mem 14853MB
[2022-11-07 15:21:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][1100/1251]	eta 0:01:10 lr 0.000051	time 0.4653 (0.4678)	loss 1.9412 (2.8853)	grad_norm 2.8537 (2.8370)	mem 14853MB
[2022-11-07 15:22:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][1150/1251]	eta 0:00:47 lr 0.000051	time 0.4681 (0.4678)	loss 3.5523 (2.8846)	grad_norm 3.0744 (2.8376)	mem 14853MB
[2022-11-07 15:22:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][1200/1251]	eta 0:00:23 lr 0.000051	time 0.4660 (0.4676)	loss 3.0673 (2.8845)	grad_norm 2.4752 (2.8389)	mem 14853MB
[2022-11-07 15:22:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [260/300][1250/1251]	eta 0:00:00 lr 0.000051	time 0.4586 (0.4674)	loss 2.0395 (2.8841)	grad_norm 3.0609 (2.8418)	mem 14853MB
[2022-11-07 15:22:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 260 training takes 0:09:44
[2022-11-07 15:23:00 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_260.pth saving......
[2022-11-07 15:23:00 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_260.pth saved !!!
[2022-11-07 15:23:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.528 (1.528)	Loss 0.8496 (0.8496)	Acc@1 80.469 (80.469)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 15:23:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.974 Acc@5 95.878
[2022-11-07 15:23:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.0%
[2022-11-07 15:23:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.749 (1.749)	Loss 0.7535 (0.7535)	Acc@1 83.105 (83.105)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 15:23:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.300 Acc@5 96.058
[2022-11-07 15:23:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 15:23:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.30% at 259 epoch
[2022-11-07 15:23:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][0/1251]	eta 0:42:06 lr 0.000051	time 2.0192 (2.0192)	loss 3.2798 (3.2798)	grad_norm 2.5380 (2.5380)	mem 14853MB
[2022-11-07 15:23:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][50/1251]	eta 0:10:04 lr 0.000051	time 0.4774 (0.5033)	loss 2.9516 (2.8491)	grad_norm 2.9605 (2.8278)	mem 14853MB
[2022-11-07 15:24:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][100/1251]	eta 0:09:21 lr 0.000051	time 0.4614 (0.4880)	loss 2.9106 (2.8119)	grad_norm 2.8645 (2.8035)	mem 14853MB
[2022-11-07 15:24:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][150/1251]	eta 0:08:49 lr 0.000050	time 0.4623 (0.4807)	loss 2.6890 (2.8452)	grad_norm 2.6442 (2.8148)	mem 14853MB
[2022-11-07 15:24:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][200/1251]	eta 0:08:21 lr 0.000050	time 0.4581 (0.4773)	loss 3.3189 (2.8590)	grad_norm 2.6180 (2.8164)	mem 14853MB
[2022-11-07 15:25:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][250/1251]	eta 0:07:55 lr 0.000050	time 0.4676 (0.4750)	loss 2.1485 (2.8549)	grad_norm 2.7394 (2.8552)	mem 14853MB
[2022-11-07 15:25:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][300/1251]	eta 0:07:30 lr 0.000050	time 0.4612 (0.4732)	loss 2.6176 (2.8584)	grad_norm 2.5949 (2.8563)	mem 14853MB
[2022-11-07 15:26:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][350/1251]	eta 0:07:05 lr 0.000050	time 0.4625 (0.4722)	loss 3.3240 (2.8559)	grad_norm 2.7186 (2.8567)	mem 14853MB
[2022-11-07 15:26:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][400/1251]	eta 0:06:41 lr 0.000050	time 0.4627 (0.4716)	loss 2.6582 (2.8502)	grad_norm 3.0923 (2.8598)	mem 14853MB
[2022-11-07 15:26:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][450/1251]	eta 0:06:17 lr 0.000050	time 0.4558 (0.4709)	loss 3.0928 (2.8519)	grad_norm 2.6498 (2.8713)	mem 14853MB
[2022-11-07 15:27:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][500/1251]	eta 0:05:53 lr 0.000050	time 0.4634 (0.4703)	loss 3.2250 (2.8459)	grad_norm 2.6989 (2.8784)	mem 14853MB
[2022-11-07 15:27:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][550/1251]	eta 0:05:29 lr 0.000050	time 0.4621 (0.4703)	loss 1.8972 (2.8481)	grad_norm 2.6723 (2.8778)	mem 14853MB
[2022-11-07 15:28:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][600/1251]	eta 0:05:06 lr 0.000050	time 0.4561 (0.4702)	loss 2.5801 (2.8501)	grad_norm 2.7285 (2.8822)	mem 14853MB
[2022-11-07 15:28:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][650/1251]	eta 0:04:42 lr 0.000050	time 0.4701 (0.4699)	loss 1.9414 (2.8537)	grad_norm 2.3879 (2.8799)	mem 14853MB
[2022-11-07 15:28:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][700/1251]	eta 0:04:18 lr 0.000050	time 0.4629 (0.4697)	loss 3.1877 (2.8541)	grad_norm 3.1158 (2.8730)	mem 14853MB
[2022-11-07 15:29:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][750/1251]	eta 0:03:55 lr 0.000049	time 0.4762 (0.4694)	loss 2.9740 (2.8575)	grad_norm 4.0412 (2.8720)	mem 14853MB
[2022-11-07 15:29:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][800/1251]	eta 0:03:31 lr 0.000049	time 0.5421 (0.4697)	loss 3.4184 (2.8570)	grad_norm 2.8565 (2.8728)	mem 14853MB
[2022-11-07 15:29:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][850/1251]	eta 0:03:08 lr 0.000049	time 0.4726 (0.4694)	loss 3.3483 (2.8642)	grad_norm 2.9243 (2.8710)	mem 14853MB
[2022-11-07 15:30:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][900/1251]	eta 0:02:44 lr 0.000049	time 0.4649 (0.4693)	loss 2.9626 (2.8624)	grad_norm 2.4111 (inf)	mem 14853MB
[2022-11-07 15:30:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][950/1251]	eta 0:02:21 lr 0.000049	time 0.4668 (0.4691)	loss 2.2268 (2.8641)	grad_norm 2.4926 (inf)	mem 14853MB
[2022-11-07 15:31:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][1000/1251]	eta 0:01:57 lr 0.000049	time 0.4674 (0.4690)	loss 3.0142 (2.8686)	grad_norm 3.0556 (inf)	mem 14853MB
[2022-11-07 15:31:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][1050/1251]	eta 0:01:34 lr 0.000049	time 0.4709 (0.4692)	loss 3.2913 (2.8656)	grad_norm 2.4345 (inf)	mem 14853MB
[2022-11-07 15:31:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][1100/1251]	eta 0:01:10 lr 0.000049	time 0.4512 (0.4691)	loss 2.7698 (2.8695)	grad_norm 2.7652 (inf)	mem 14853MB
[2022-11-07 15:32:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][1150/1251]	eta 0:00:47 lr 0.000049	time 0.4579 (0.4689)	loss 2.3568 (2.8695)	grad_norm 2.4953 (inf)	mem 14853MB
[2022-11-07 15:32:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][1200/1251]	eta 0:00:23 lr 0.000049	time 0.4540 (0.4688)	loss 3.0934 (2.8714)	grad_norm 2.8336 (inf)	mem 14853MB
[2022-11-07 15:33:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [261/300][1250/1251]	eta 0:00:00 lr 0.000049	time 0.4580 (0.4686)	loss 3.2151 (2.8736)	grad_norm 3.0784 (inf)	mem 14853MB
[2022-11-07 15:33:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 261 training takes 0:09:46
[2022-11-07 15:33:04 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_261.pth saving......
[2022-11-07 15:33:05 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_261.pth saved !!!
[2022-11-07 15:33:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.796 (1.796)	Loss 0.7467 (0.7467)	Acc@1 83.105 (83.105)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 15:33:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.926 Acc@5 95.912
[2022-11-07 15:33:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.9%
[2022-11-07 15:33:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.693 (1.693)	Loss 0.8313 (0.8313)	Acc@1 80.859 (80.859)	Acc@5 94.727 (94.727)	Mem 14853MB
[2022-11-07 15:33:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.310 Acc@5 96.058
[2022-11-07 15:33:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 15:33:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.31% at 261 epoch
[2022-11-07 15:33:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][0/1251]	eta 0:40:31 lr 0.000049	time 1.9436 (1.9436)	loss 3.1703 (3.1703)	grad_norm 2.5649 (2.5649)	mem 14853MB
[2022-11-07 15:33:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][50/1251]	eta 0:10:05 lr 0.000049	time 0.4853 (0.5039)	loss 3.2188 (2.8713)	grad_norm 3.0152 (2.7933)	mem 14853MB
[2022-11-07 15:34:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][100/1251]	eta 0:09:19 lr 0.000049	time 0.4602 (0.4861)	loss 3.0679 (2.8222)	grad_norm 2.5030 (2.8711)	mem 14853MB
[2022-11-07 15:34:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][150/1251]	eta 0:08:48 lr 0.000048	time 0.5578 (0.4798)	loss 3.3242 (2.8082)	grad_norm 3.2543 (2.8701)	mem 14853MB
[2022-11-07 15:34:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][200/1251]	eta 0:08:20 lr 0.000048	time 0.4548 (0.4764)	loss 3.1790 (2.8324)	grad_norm 3.0845 (2.8673)	mem 14853MB
[2022-11-07 15:35:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][250/1251]	eta 0:07:54 lr 0.000048	time 0.4542 (0.4740)	loss 3.2372 (2.8153)	grad_norm 3.6612 (2.8676)	mem 14853MB
[2022-11-07 15:35:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][300/1251]	eta 0:07:29 lr 0.000048	time 0.4616 (0.4723)	loss 1.6353 (2.8291)	grad_norm 2.6258 (2.8651)	mem 14853MB
[2022-11-07 15:36:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][350/1251]	eta 0:07:04 lr 0.000048	time 0.4555 (0.4715)	loss 3.0107 (2.8290)	grad_norm 2.6663 (2.8597)	mem 14853MB
[2022-11-07 15:36:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][400/1251]	eta 0:06:40 lr 0.000048	time 0.4551 (0.4709)	loss 2.0309 (2.8355)	grad_norm 2.7607 (inf)	mem 14853MB
[2022-11-07 15:36:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][450/1251]	eta 0:06:16 lr 0.000048	time 0.4592 (0.4701)	loss 1.8626 (2.8493)	grad_norm 2.4408 (inf)	mem 14853MB
[2022-11-07 15:37:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][500/1251]	eta 0:05:52 lr 0.000048	time 0.4503 (0.4699)	loss 3.0870 (2.8508)	grad_norm 3.5932 (inf)	mem 14853MB
[2022-11-07 15:37:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][550/1251]	eta 0:05:29 lr 0.000048	time 0.4511 (0.4700)	loss 2.9174 (2.8520)	grad_norm 3.1644 (inf)	mem 14853MB
[2022-11-07 15:38:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][600/1251]	eta 0:05:05 lr 0.000048	time 0.4635 (0.4697)	loss 2.8386 (2.8602)	grad_norm 3.3488 (inf)	mem 14853MB
[2022-11-07 15:38:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][650/1251]	eta 0:04:42 lr 0.000048	time 0.4587 (0.4693)	loss 2.1027 (2.8577)	grad_norm 2.9366 (inf)	mem 14853MB
[2022-11-07 15:38:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][700/1251]	eta 0:04:18 lr 0.000048	time 0.4564 (0.4690)	loss 2.8933 (2.8590)	grad_norm 2.7478 (inf)	mem 14853MB
[2022-11-07 15:39:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][750/1251]	eta 0:03:54 lr 0.000047	time 0.5657 (0.4689)	loss 3.1117 (2.8538)	grad_norm 2.4988 (inf)	mem 14853MB
[2022-11-07 15:39:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][800/1251]	eta 0:03:31 lr 0.000047	time 0.4552 (0.4690)	loss 3.4672 (2.8589)	grad_norm 2.6492 (inf)	mem 14853MB
[2022-11-07 15:40:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][850/1251]	eta 0:03:08 lr 0.000047	time 0.4752 (0.4689)	loss 3.1171 (2.8654)	grad_norm 3.0012 (inf)	mem 14853MB
[2022-11-07 15:40:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][900/1251]	eta 0:02:44 lr 0.000047	time 0.4641 (0.4687)	loss 3.4178 (2.8652)	grad_norm 2.7117 (inf)	mem 14853MB
[2022-11-07 15:40:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][950/1251]	eta 0:02:21 lr 0.000047	time 0.4745 (0.4685)	loss 3.2480 (2.8707)	grad_norm 3.2960 (inf)	mem 14853MB
[2022-11-07 15:41:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][1000/1251]	eta 0:01:57 lr 0.000047	time 0.4638 (0.4684)	loss 2.8521 (2.8724)	grad_norm 3.4974 (inf)	mem 14853MB
[2022-11-07 15:41:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][1050/1251]	eta 0:01:34 lr 0.000047	time 0.4625 (0.4686)	loss 2.5494 (2.8779)	grad_norm 2.6384 (inf)	mem 14853MB
[2022-11-07 15:41:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][1100/1251]	eta 0:01:10 lr 0.000047	time 0.4651 (0.4685)	loss 2.0839 (2.8712)	grad_norm 3.1358 (inf)	mem 14853MB
[2022-11-07 15:42:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][1150/1251]	eta 0:00:47 lr 0.000047	time 0.4562 (0.4683)	loss 2.7017 (2.8745)	grad_norm 2.6089 (inf)	mem 14853MB
[2022-11-07 15:42:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][1200/1251]	eta 0:00:23 lr 0.000047	time 0.4682 (0.4682)	loss 2.2430 (2.8738)	grad_norm 2.7330 (inf)	mem 14853MB
[2022-11-07 15:43:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [262/300][1250/1251]	eta 0:00:00 lr 0.000047	time 0.4568 (0.4680)	loss 2.9752 (2.8721)	grad_norm 3.1318 (inf)	mem 14853MB
[2022-11-07 15:43:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 262 training takes 0:09:45
[2022-11-07 15:43:08 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_262.pth saving......
[2022-11-07 15:43:08 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_262.pth saved !!!
[2022-11-07 15:43:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.578 (1.578)	Loss 0.8350 (0.8350)	Acc@1 80.176 (80.176)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 15:43:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.892 Acc@5 95.862
[2022-11-07 15:43:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.9%
[2022-11-07 15:43:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.617 (1.617)	Loss 0.7957 (0.7957)	Acc@1 81.836 (81.836)	Acc@5 96.191 (96.191)	Mem 14853MB
[2022-11-07 15:43:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.320 Acc@5 96.062
[2022-11-07 15:43:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 15:43:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.32% at 262 epoch
[2022-11-07 15:43:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][0/1251]	eta 0:39:57 lr 0.000047	time 1.9162 (1.9162)	loss 3.1455 (3.1455)	grad_norm 2.9729 (2.9729)	mem 14853MB
[2022-11-07 15:43:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][50/1251]	eta 0:09:57 lr 0.000047	time 0.4616 (0.4978)	loss 2.7873 (2.8892)	grad_norm 2.7986 (2.9366)	mem 14853MB
[2022-11-07 15:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][100/1251]	eta 0:09:17 lr 0.000047	time 0.4576 (0.4847)	loss 2.7065 (2.8697)	grad_norm 3.3052 (2.9271)	mem 14853MB
[2022-11-07 15:44:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][150/1251]	eta 0:08:46 lr 0.000046	time 0.4679 (0.4782)	loss 2.9887 (2.8505)	grad_norm 2.9124 (2.8936)	mem 14853MB
[2022-11-07 15:45:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][200/1251]	eta 0:08:19 lr 0.000046	time 0.4584 (0.4752)	loss 2.9701 (2.8525)	grad_norm 2.5030 (2.8930)	mem 14853MB
[2022-11-07 15:45:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][250/1251]	eta 0:07:53 lr 0.000046	time 0.4645 (0.4731)	loss 2.7504 (2.8472)	grad_norm 2.4813 (2.9097)	mem 14853MB
[2022-11-07 15:45:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][300/1251]	eta 0:07:28 lr 0.000046	time 0.4618 (0.4716)	loss 2.5792 (2.8439)	grad_norm 3.0031 (2.9049)	mem 14853MB
[2022-11-07 15:46:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][350/1251]	eta 0:07:04 lr 0.000046	time 0.4730 (0.4710)	loss 3.1375 (2.8551)	grad_norm 2.7590 (2.8962)	mem 14853MB
[2022-11-07 15:46:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][400/1251]	eta 0:06:40 lr 0.000046	time 0.4564 (0.4702)	loss 3.0879 (2.8569)	grad_norm 2.6030 (2.8886)	mem 14853MB
[2022-11-07 15:46:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][450/1251]	eta 0:06:16 lr 0.000046	time 0.4563 (0.4696)	loss 3.2123 (2.8589)	grad_norm 2.4232 (2.8822)	mem 14853MB
[2022-11-07 15:47:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][500/1251]	eta 0:05:52 lr 0.000046	time 0.4745 (0.4690)	loss 2.9293 (2.8578)	grad_norm 3.3901 (2.8811)	mem 14853MB
[2022-11-07 15:47:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][550/1251]	eta 0:05:29 lr 0.000046	time 0.4705 (0.4694)	loss 2.7020 (2.8558)	grad_norm 2.9922 (2.8795)	mem 14853MB
[2022-11-07 15:48:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][600/1251]	eta 0:05:05 lr 0.000046	time 0.4714 (0.4690)	loss 2.2752 (2.8592)	grad_norm 2.7240 (2.8825)	mem 14853MB
[2022-11-07 15:48:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][650/1251]	eta 0:04:41 lr 0.000046	time 0.4624 (0.4687)	loss 2.9879 (2.8630)	grad_norm 2.8182 (2.8827)	mem 14853MB
[2022-11-07 15:48:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][700/1251]	eta 0:04:18 lr 0.000046	time 0.4665 (0.4685)	loss 2.3597 (2.8714)	grad_norm 2.7697 (2.8821)	mem 14853MB
[2022-11-07 15:49:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][750/1251]	eta 0:03:54 lr 0.000046	time 0.4658 (0.4682)	loss 2.8727 (2.8733)	grad_norm 2.8322 (2.8780)	mem 14853MB
[2022-11-07 15:49:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][800/1251]	eta 0:03:31 lr 0.000045	time 0.4577 (0.4683)	loss 3.0032 (2.8748)	grad_norm 2.7874 (2.8820)	mem 14853MB
[2022-11-07 15:50:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][850/1251]	eta 0:03:07 lr 0.000045	time 0.4647 (0.4681)	loss 3.1689 (2.8678)	grad_norm 2.8338 (2.8856)	mem 14853MB
[2022-11-07 15:50:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][900/1251]	eta 0:02:44 lr 0.000045	time 0.4594 (0.4680)	loss 2.3771 (2.8669)	grad_norm 2.4878 (2.8888)	mem 14853MB
[2022-11-07 15:50:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][950/1251]	eta 0:02:20 lr 0.000045	time 0.4769 (0.4680)	loss 2.9528 (2.8686)	grad_norm 2.8664 (2.8981)	mem 14853MB
[2022-11-07 15:51:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][1000/1251]	eta 0:01:57 lr 0.000045	time 0.4630 (0.4679)	loss 2.9893 (2.8699)	grad_norm 2.4760 (2.8959)	mem 14853MB
[2022-11-07 15:51:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][1050/1251]	eta 0:01:34 lr 0.000045	time 0.4614 (0.4679)	loss 1.6879 (2.8646)	grad_norm 2.4780 (2.8931)	mem 14853MB
[2022-11-07 15:52:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][1100/1251]	eta 0:01:10 lr 0.000045	time 0.4638 (0.4678)	loss 2.8645 (2.8651)	grad_norm 2.6300 (2.8904)	mem 14853MB
[2022-11-07 15:52:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][1150/1251]	eta 0:00:47 lr 0.000045	time 0.4639 (0.4676)	loss 3.3303 (2.8621)	grad_norm 3.1301 (2.8894)	mem 14853MB
[2022-11-07 15:52:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][1200/1251]	eta 0:00:23 lr 0.000045	time 0.4708 (0.4676)	loss 3.3803 (2.8645)	grad_norm 3.0527 (2.8921)	mem 14853MB
[2022-11-07 15:53:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [263/300][1250/1251]	eta 0:00:00 lr 0.000045	time 0.4568 (0.4675)	loss 2.4103 (2.8687)	grad_norm 3.0362 (2.8926)	mem 14853MB
[2022-11-07 15:53:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 263 training takes 0:09:44
[2022-11-07 15:53:11 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_263.pth saving......
[2022-11-07 15:53:12 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_263.pth saved !!!
[2022-11-07 15:53:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.635 (1.635)	Loss 0.7263 (0.7263)	Acc@1 83.691 (83.691)	Acc@5 96.680 (96.680)	Mem 14853MB
[2022-11-07 15:53:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.040 Acc@5 95.898
[2022-11-07 15:53:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.0%
[2022-11-07 15:53:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.545 (1.545)	Loss 0.7301 (0.7301)	Acc@1 83.301 (83.301)	Acc@5 96.680 (96.680)	Mem 14853MB
[2022-11-07 15:53:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.330 Acc@5 96.056
[2022-11-07 15:53:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 15:53:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.33% at 263 epoch
[2022-11-07 15:53:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][0/1251]	eta 0:41:22 lr 0.000045	time 1.9843 (1.9843)	loss 2.8649 (2.8649)	grad_norm 2.6410 (2.6410)	mem 14853MB
[2022-11-07 15:53:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][50/1251]	eta 0:10:02 lr 0.000045	time 0.4661 (0.5013)	loss 3.2943 (2.8418)	grad_norm 3.0821 (2.9926)	mem 14853MB
[2022-11-07 15:54:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][100/1251]	eta 0:09:17 lr 0.000045	time 0.4662 (0.4846)	loss 2.2931 (2.8627)	grad_norm 2.8353 (2.9333)	mem 14853MB
[2022-11-07 15:54:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][150/1251]	eta 0:08:48 lr 0.000045	time 0.4685 (0.4797)	loss 1.8242 (2.8577)	grad_norm 3.2994 (2.9265)	mem 14853MB
[2022-11-07 15:55:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][200/1251]	eta 0:08:20 lr 0.000044	time 0.4635 (0.4762)	loss 3.1920 (2.8583)	grad_norm 3.0193 (2.9202)	mem 14853MB
[2022-11-07 15:55:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][250/1251]	eta 0:07:54 lr 0.000044	time 0.4632 (0.4743)	loss 3.4726 (2.8682)	grad_norm 2.7305 (2.9238)	mem 14853MB
[2022-11-07 15:55:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][300/1251]	eta 0:07:29 lr 0.000044	time 0.4782 (0.4730)	loss 2.6624 (2.8583)	grad_norm 2.9683 (2.9249)	mem 14853MB
[2022-11-07 15:56:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][350/1251]	eta 0:07:05 lr 0.000044	time 0.4717 (0.4721)	loss 2.7404 (2.8565)	grad_norm 2.7578 (2.9122)	mem 14853MB
[2022-11-07 15:56:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][400/1251]	eta 0:06:40 lr 0.000044	time 0.4689 (0.4712)	loss 2.7932 (2.8680)	grad_norm 2.6986 (2.9021)	mem 14853MB
[2022-11-07 15:57:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][450/1251]	eta 0:06:17 lr 0.000044	time 0.4676 (0.4708)	loss 2.7873 (2.8733)	grad_norm 2.8558 (2.8960)	mem 14853MB
[2022-11-07 15:57:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][500/1251]	eta 0:05:53 lr 0.000044	time 0.4635 (0.4704)	loss 2.8436 (2.8552)	grad_norm 2.9658 (2.8956)	mem 14853MB
[2022-11-07 15:57:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][550/1251]	eta 0:05:29 lr 0.000044	time 0.4871 (0.4705)	loss 2.4160 (2.8539)	grad_norm 2.3790 (2.8912)	mem 14853MB
[2022-11-07 15:58:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][600/1251]	eta 0:05:06 lr 0.000044	time 0.4648 (0.4701)	loss 2.2074 (2.8600)	grad_norm 2.6339 (2.8914)	mem 14853MB
[2022-11-07 15:58:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][650/1251]	eta 0:04:42 lr 0.000044	time 0.4572 (0.4698)	loss 3.0585 (2.8603)	grad_norm 2.9068 (2.8855)	mem 14853MB
[2022-11-07 15:58:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][700/1251]	eta 0:04:18 lr 0.000044	time 0.4795 (0.4695)	loss 3.0990 (2.8578)	grad_norm 3.6511 (2.8797)	mem 14853MB
[2022-11-07 15:59:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][750/1251]	eta 0:03:55 lr 0.000044	time 0.4606 (0.4693)	loss 3.0606 (2.8527)	grad_norm 3.1346 (2.8724)	mem 14853MB
[2022-11-07 15:59:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][800/1251]	eta 0:03:31 lr 0.000044	time 0.4596 (0.4693)	loss 2.5046 (2.8499)	grad_norm 3.4698 (2.8751)	mem 14853MB
[2022-11-07 16:00:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][850/1251]	eta 0:03:08 lr 0.000043	time 0.4694 (0.4691)	loss 3.0389 (2.8491)	grad_norm 3.0628 (2.8807)	mem 14853MB
[2022-11-07 16:00:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][900/1251]	eta 0:02:44 lr 0.000043	time 0.4601 (0.4690)	loss 3.2471 (2.8512)	grad_norm 2.6259 (2.8786)	mem 14853MB
[2022-11-07 16:00:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][950/1251]	eta 0:02:21 lr 0.000043	time 0.4630 (0.4688)	loss 3.1682 (2.8545)	grad_norm 2.6451 (2.8778)	mem 14853MB
[2022-11-07 16:01:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][1000/1251]	eta 0:01:57 lr 0.000043	time 0.5435 (0.4688)	loss 3.0469 (2.8526)	grad_norm 2.9516 (2.8789)	mem 14853MB
[2022-11-07 16:01:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][1050/1251]	eta 0:01:34 lr 0.000043	time 0.5466 (0.4689)	loss 2.7017 (2.8516)	grad_norm 2.9499 (2.8786)	mem 14853MB
[2022-11-07 16:02:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][1100/1251]	eta 0:01:10 lr 0.000043	time 0.4576 (0.4687)	loss 3.4003 (2.8549)	grad_norm 2.4306 (2.8788)	mem 14853MB
[2022-11-07 16:02:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][1150/1251]	eta 0:00:47 lr 0.000043	time 0.4689 (0.4686)	loss 2.9954 (2.8539)	grad_norm 2.9947 (2.8762)	mem 14853MB
[2022-11-07 16:02:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][1200/1251]	eta 0:00:23 lr 0.000043	time 0.4622 (0.4684)	loss 3.3582 (2.8546)	grad_norm 2.7215 (2.8771)	mem 14853MB
[2022-11-07 16:03:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [264/300][1250/1251]	eta 0:00:00 lr 0.000043	time 0.4577 (0.4682)	loss 3.0729 (2.8564)	grad_norm 2.8319 (2.8794)	mem 14853MB
[2022-11-07 16:03:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 264 training takes 0:09:45
[2022-11-07 16:03:15 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_264.pth saving......
[2022-11-07 16:03:16 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_264.pth saved !!!
[2022-11-07 16:03:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.526 (1.526)	Loss 0.8973 (0.8973)	Acc@1 79.590 (79.590)	Acc@5 94.043 (94.043)	Mem 14853MB
[2022-11-07 16:03:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.984 Acc@5 95.892
[2022-11-07 16:03:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.0%
[2022-11-07 16:03:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.707 (1.707)	Loss 0.8509 (0.8509)	Acc@1 80.859 (80.859)	Acc@5 95.312 (95.312)	Mem 14853MB
[2022-11-07 16:03:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.306 Acc@5 96.054
[2022-11-07 16:03:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 16:03:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.33% at 263 epoch
[2022-11-07 16:03:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][0/1251]	eta 0:38:48 lr 0.000043	time 1.8612 (1.8612)	loss 3.4432 (3.4432)	grad_norm 3.4650 (3.4650)	mem 14853MB
[2022-11-07 16:03:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][50/1251]	eta 0:09:57 lr 0.000043	time 0.4765 (0.4974)	loss 3.4893 (2.9504)	grad_norm 2.9011 (2.9232)	mem 14853MB
[2022-11-07 16:04:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][100/1251]	eta 0:09:16 lr 0.000043	time 0.4702 (0.4838)	loss 3.1463 (2.9158)	grad_norm 2.8843 (2.9350)	mem 14853MB
[2022-11-07 16:04:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][150/1251]	eta 0:08:45 lr 0.000043	time 0.4751 (0.4775)	loss 1.6486 (2.8808)	grad_norm 2.5191 (2.9058)	mem 14853MB
[2022-11-07 16:05:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][200/1251]	eta 0:08:18 lr 0.000043	time 0.4726 (0.4748)	loss 3.0579 (2.8622)	grad_norm 3.0702 (2.8989)	mem 14853MB
[2022-11-07 16:05:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][250/1251]	eta 0:07:53 lr 0.000043	time 0.4644 (0.4733)	loss 2.9855 (2.8460)	grad_norm 2.5179 (2.8930)	mem 14853MB
[2022-11-07 16:05:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][300/1251]	eta 0:07:28 lr 0.000042	time 0.4592 (0.4718)	loss 2.8303 (2.8653)	grad_norm 2.4626 (2.8845)	mem 14853MB
[2022-11-07 16:06:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][350/1251]	eta 0:07:04 lr 0.000042	time 0.4632 (0.4708)	loss 2.5423 (2.8690)	grad_norm 2.6884 (2.8899)	mem 14853MB
[2022-11-07 16:06:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][400/1251]	eta 0:06:40 lr 0.000042	time 0.4727 (0.4705)	loss 2.9616 (2.8758)	grad_norm 4.4379 (2.9025)	mem 14853MB
[2022-11-07 16:07:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][450/1251]	eta 0:06:16 lr 0.000042	time 0.4642 (0.4700)	loss 2.9036 (2.8624)	grad_norm 3.0248 (2.8965)	mem 14853MB
[2022-11-07 16:07:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][500/1251]	eta 0:05:52 lr 0.000042	time 0.4530 (0.4695)	loss 2.3885 (2.8585)	grad_norm 3.0726 (2.8923)	mem 14853MB
[2022-11-07 16:07:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][550/1251]	eta 0:05:28 lr 0.000042	time 0.4600 (0.4692)	loss 3.2167 (2.8533)	grad_norm 3.4768 (2.8910)	mem 14853MB
[2022-11-07 16:08:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][600/1251]	eta 0:05:05 lr 0.000042	time 0.4752 (0.4691)	loss 3.3273 (2.8465)	grad_norm 3.1783 (2.8876)	mem 14853MB
[2022-11-07 16:08:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][650/1251]	eta 0:04:41 lr 0.000042	time 0.4622 (0.4689)	loss 3.3128 (2.8486)	grad_norm 2.5181 (2.8873)	mem 14853MB
[2022-11-07 16:09:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][700/1251]	eta 0:04:18 lr 0.000042	time 0.4644 (0.4686)	loss 3.5401 (2.8458)	grad_norm 2.9541 (2.8896)	mem 14853MB
[2022-11-07 16:09:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][750/1251]	eta 0:03:54 lr 0.000042	time 0.4641 (0.4684)	loss 2.3617 (2.8438)	grad_norm 2.5523 (2.8998)	mem 14853MB
[2022-11-07 16:09:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][800/1251]	eta 0:03:31 lr 0.000042	time 0.4619 (0.4683)	loss 3.3838 (2.8403)	grad_norm 3.2011 (2.8981)	mem 14853MB
[2022-11-07 16:10:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][850/1251]	eta 0:03:07 lr 0.000042	time 0.4577 (0.4683)	loss 2.0874 (2.8402)	grad_norm 2.4393 (2.9001)	mem 14853MB
[2022-11-07 16:10:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][900/1251]	eta 0:02:44 lr 0.000042	time 0.4551 (0.4682)	loss 2.5455 (2.8392)	grad_norm 2.6921 (2.9012)	mem 14853MB
[2022-11-07 16:10:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][950/1251]	eta 0:02:20 lr 0.000041	time 0.4507 (0.4682)	loss 3.0985 (2.8397)	grad_norm 3.5932 (2.9030)	mem 14853MB
[2022-11-07 16:11:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][1000/1251]	eta 0:01:57 lr 0.000041	time 0.4690 (0.4680)	loss 2.7101 (2.8436)	grad_norm 3.1085 (2.9045)	mem 14853MB
[2022-11-07 16:11:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][1050/1251]	eta 0:01:34 lr 0.000041	time 0.4652 (0.4680)	loss 3.2412 (2.8448)	grad_norm 2.7405 (inf)	mem 14853MB
[2022-11-07 16:12:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][1100/1251]	eta 0:01:10 lr 0.000041	time 0.4609 (0.4679)	loss 3.1692 (2.8474)	grad_norm 3.0900 (inf)	mem 14853MB
[2022-11-07 16:12:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][1150/1251]	eta 0:00:47 lr 0.000041	time 0.4742 (0.4680)	loss 2.9639 (2.8486)	grad_norm 2.6991 (inf)	mem 14853MB
[2022-11-07 16:12:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][1200/1251]	eta 0:00:23 lr 0.000041	time 0.4609 (0.4680)	loss 3.5828 (2.8519)	grad_norm 3.0320 (inf)	mem 14853MB
[2022-11-07 16:13:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [265/300][1250/1251]	eta 0:00:00 lr 0.000041	time 0.4564 (0.4678)	loss 2.3650 (2.8505)	grad_norm 2.9744 (inf)	mem 14853MB
[2022-11-07 16:13:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 265 training takes 0:09:45
[2022-11-07 16:13:19 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_265.pth saving......
[2022-11-07 16:13:19 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_265.pth saved !!!
[2022-11-07 16:13:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.437 (1.437)	Loss 0.7982 (0.7982)	Acc@1 82.129 (82.129)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-07 16:13:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.986 Acc@5 95.846
[2022-11-07 16:13:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.0%
[2022-11-07 16:13:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.553 (1.553)	Loss 0.8337 (0.8337)	Acc@1 80.762 (80.762)	Acc@5 95.215 (95.215)	Mem 14853MB
[2022-11-07 16:13:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.276 Acc@5 96.048
[2022-11-07 16:13:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 16:13:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.33% at 263 epoch
[2022-11-07 16:13:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][0/1251]	eta 0:41:36 lr 0.000041	time 1.9957 (1.9957)	loss 3.3068 (3.3068)	grad_norm 3.2680 (3.2680)	mem 14853MB
[2022-11-07 16:14:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][50/1251]	eta 0:10:00 lr 0.000041	time 0.4658 (0.5000)	loss 3.2523 (2.8599)	grad_norm 3.3186 (2.9464)	mem 14853MB
[2022-11-07 16:14:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][100/1251]	eta 0:09:19 lr 0.000041	time 0.4614 (0.4863)	loss 3.2566 (2.8431)	grad_norm 2.4416 (2.9877)	mem 14853MB
[2022-11-07 16:14:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][150/1251]	eta 0:08:47 lr 0.000041	time 0.4531 (0.4794)	loss 2.9219 (2.8506)	grad_norm 3.0393 (2.9788)	mem 14853MB
[2022-11-07 16:15:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][200/1251]	eta 0:08:19 lr 0.000041	time 0.4666 (0.4756)	loss 2.7834 (2.8611)	grad_norm 2.4779 (2.9628)	mem 14853MB
[2022-11-07 16:15:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][250/1251]	eta 0:07:54 lr 0.000041	time 0.4691 (0.4738)	loss 3.1315 (2.8734)	grad_norm 2.9815 (2.9636)	mem 14853MB
[2022-11-07 16:15:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][300/1251]	eta 0:07:29 lr 0.000041	time 0.4571 (0.4723)	loss 2.7906 (2.8639)	grad_norm 2.9576 (2.9526)	mem 14853MB
[2022-11-07 16:16:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][350/1251]	eta 0:07:04 lr 0.000041	time 0.4580 (0.4717)	loss 2.9670 (2.8679)	grad_norm 2.9276 (2.9544)	mem 14853MB
[2022-11-07 16:16:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][400/1251]	eta 0:06:40 lr 0.000040	time 0.4673 (0.4710)	loss 3.0317 (2.8671)	grad_norm 3.2681 (2.9481)	mem 14853MB
[2022-11-07 16:17:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][450/1251]	eta 0:06:17 lr 0.000040	time 0.4746 (0.4707)	loss 3.5634 (2.8736)	grad_norm 2.9595 (2.9435)	mem 14853MB
[2022-11-07 16:17:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][500/1251]	eta 0:05:53 lr 0.000040	time 0.4695 (0.4701)	loss 2.7931 (2.8491)	grad_norm 2.9569 (2.9421)	mem 14853MB
[2022-11-07 16:17:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][550/1251]	eta 0:05:29 lr 0.000040	time 0.4569 (0.4698)	loss 2.2778 (2.8447)	grad_norm 2.7995 (2.9381)	mem 14853MB
[2022-11-07 16:18:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][600/1251]	eta 0:05:05 lr 0.000040	time 0.4672 (0.4696)	loss 2.9115 (2.8367)	grad_norm 2.5412 (2.9324)	mem 14853MB
[2022-11-07 16:18:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][650/1251]	eta 0:04:41 lr 0.000040	time 0.4520 (0.4691)	loss 2.9651 (2.8328)	grad_norm 3.4894 (2.9360)	mem 14853MB
[2022-11-07 16:19:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][700/1251]	eta 0:04:18 lr 0.000040	time 0.4573 (0.4688)	loss 3.4368 (2.8320)	grad_norm 2.9460 (2.9346)	mem 14853MB
[2022-11-07 16:19:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][750/1251]	eta 0:03:54 lr 0.000040	time 0.4669 (0.4688)	loss 2.9097 (2.8319)	grad_norm 2.8226 (2.9383)	mem 14853MB
[2022-11-07 16:19:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][800/1251]	eta 0:03:31 lr 0.000040	time 0.4641 (0.4688)	loss 3.0198 (2.8255)	grad_norm 3.1256 (2.9343)	mem 14853MB
[2022-11-07 16:20:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][850/1251]	eta 0:03:07 lr 0.000040	time 0.4631 (0.4686)	loss 1.7817 (2.8296)	grad_norm 2.4920 (2.9314)	mem 14853MB
[2022-11-07 16:20:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][900/1251]	eta 0:02:44 lr 0.000040	time 0.4610 (0.4685)	loss 2.6802 (2.8359)	grad_norm 2.7731 (2.9343)	mem 14853MB
[2022-11-07 16:21:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][950/1251]	eta 0:02:20 lr 0.000040	time 0.4633 (0.4684)	loss 3.3836 (2.8385)	grad_norm 3.0835 (2.9344)	mem 14853MB
[2022-11-07 16:21:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][1000/1251]	eta 0:01:57 lr 0.000040	time 0.4582 (0.4684)	loss 2.0174 (2.8367)	grad_norm 2.4503 (2.9320)	mem 14853MB
[2022-11-07 16:21:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][1050/1251]	eta 0:01:34 lr 0.000040	time 0.4528 (0.4683)	loss 2.9924 (2.8367)	grad_norm 2.9424 (2.9307)	mem 14853MB
[2022-11-07 16:22:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][1100/1251]	eta 0:01:10 lr 0.000039	time 0.5513 (0.4683)	loss 2.7733 (2.8413)	grad_norm 2.7685 (inf)	mem 14853MB
[2022-11-07 16:22:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][1150/1251]	eta 0:00:47 lr 0.000039	time 0.4720 (0.4682)	loss 3.0314 (2.8451)	grad_norm 2.8871 (inf)	mem 14853MB
[2022-11-07 16:22:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][1200/1251]	eta 0:00:23 lr 0.000039	time 0.4617 (0.4681)	loss 2.8394 (2.8483)	grad_norm 2.9210 (inf)	mem 14853MB
[2022-11-07 16:23:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [266/300][1250/1251]	eta 0:00:00 lr 0.000039	time 0.4573 (0.4680)	loss 2.9102 (2.8491)	grad_norm 3.0421 (inf)	mem 14853MB
[2022-11-07 16:23:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 266 training takes 0:09:45
[2022-11-07 16:23:22 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_266.pth saving......
[2022-11-07 16:23:22 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_266.pth saved !!!
[2022-11-07 16:23:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.785 (1.785)	Loss 0.7281 (0.7281)	Acc@1 82.617 (82.617)	Acc@5 96.875 (96.875)	Mem 14853MB
[2022-11-07 16:23:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.040 Acc@5 95.882
[2022-11-07 16:23:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.0%
[2022-11-07 16:23:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.624 (1.624)	Loss 0.7127 (0.7127)	Acc@1 84.277 (84.277)	Acc@5 96.582 (96.582)	Mem 14853MB
[2022-11-07 16:23:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.274 Acc@5 96.044
[2022-11-07 16:23:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 16:23:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.33% at 263 epoch
[2022-11-07 16:23:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][0/1251]	eta 0:41:46 lr 0.000039	time 2.0038 (2.0038)	loss 3.1163 (3.1163)	grad_norm 2.6256 (2.6256)	mem 14853MB
[2022-11-07 16:24:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][50/1251]	eta 0:10:01 lr 0.000039	time 0.4678 (0.5011)	loss 2.8957 (2.8569)	grad_norm 3.7197 (2.9555)	mem 14853MB
[2022-11-07 16:24:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][100/1251]	eta 0:09:17 lr 0.000039	time 0.4677 (0.4844)	loss 3.0291 (2.8888)	grad_norm 2.8800 (2.9213)	mem 14853MB
[2022-11-07 16:24:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][150/1251]	eta 0:08:47 lr 0.000039	time 0.4557 (0.4787)	loss 2.4159 (2.9088)	grad_norm 2.3532 (2.9329)	mem 14853MB
[2022-11-07 16:25:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][200/1251]	eta 0:08:20 lr 0.000039	time 0.4671 (0.4765)	loss 3.0807 (2.8928)	grad_norm 2.6343 (2.9720)	mem 14853MB
[2022-11-07 16:25:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][250/1251]	eta 0:07:55 lr 0.000039	time 0.5412 (0.4750)	loss 3.0550 (2.8957)	grad_norm 2.6330 (2.9574)	mem 14853MB
[2022-11-07 16:26:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][300/1251]	eta 0:07:30 lr 0.000039	time 0.4691 (0.4734)	loss 2.8013 (2.8822)	grad_norm 3.3181 (2.9642)	mem 14853MB
[2022-11-07 16:26:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][350/1251]	eta 0:07:05 lr 0.000039	time 0.4630 (0.4724)	loss 3.3132 (2.8773)	grad_norm 2.7129 (2.9596)	mem 14853MB
[2022-11-07 16:26:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][400/1251]	eta 0:06:41 lr 0.000039	time 0.4608 (0.4714)	loss 1.9949 (2.8516)	grad_norm 3.2975 (2.9608)	mem 14853MB
[2022-11-07 16:27:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][450/1251]	eta 0:06:17 lr 0.000039	time 0.4781 (0.4716)	loss 3.2996 (2.8513)	grad_norm 3.5013 (2.9591)	mem 14853MB
[2022-11-07 16:27:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][500/1251]	eta 0:05:54 lr 0.000039	time 0.4618 (0.4715)	loss 2.7308 (2.8496)	grad_norm 2.4708 (2.9578)	mem 14853MB
[2022-11-07 16:28:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][550/1251]	eta 0:05:30 lr 0.000038	time 0.4711 (0.4711)	loss 3.3088 (2.8597)	grad_norm 2.8545 (2.9537)	mem 14853MB
[2022-11-07 16:28:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][600/1251]	eta 0:05:06 lr 0.000038	time 0.4632 (0.4705)	loss 3.1259 (2.8704)	grad_norm 2.9817 (2.9514)	mem 14853MB
[2022-11-07 16:28:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][650/1251]	eta 0:04:42 lr 0.000038	time 0.4543 (0.4703)	loss 2.8415 (2.8704)	grad_norm 2.6186 (2.9436)	mem 14853MB
[2022-11-07 16:29:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][700/1251]	eta 0:04:18 lr 0.000038	time 0.4720 (0.4700)	loss 3.2140 (2.8771)	grad_norm 3.8537 (2.9435)	mem 14853MB
[2022-11-07 16:29:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][750/1251]	eta 0:03:55 lr 0.000038	time 0.4559 (0.4697)	loss 3.2646 (2.8713)	grad_norm 3.0046 (2.9483)	mem 14853MB
[2022-11-07 16:29:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][800/1251]	eta 0:03:31 lr 0.000038	time 0.4612 (0.4696)	loss 3.1923 (2.8719)	grad_norm 2.9640 (2.9458)	mem 14853MB
[2022-11-07 16:30:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][850/1251]	eta 0:03:08 lr 0.000038	time 0.4837 (0.4695)	loss 3.5832 (2.8718)	grad_norm 2.7913 (2.9472)	mem 14853MB
[2022-11-07 16:30:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][900/1251]	eta 0:02:44 lr 0.000038	time 0.5319 (0.4692)	loss 2.9040 (2.8663)	grad_norm 3.2427 (2.9474)	mem 14853MB
[2022-11-07 16:31:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][950/1251]	eta 0:02:21 lr 0.000038	time 0.4757 (0.4692)	loss 3.1790 (2.8652)	grad_norm 3.0209 (2.9469)	mem 14853MB
[2022-11-07 16:31:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][1000/1251]	eta 0:01:57 lr 0.000038	time 0.4671 (0.4690)	loss 3.5313 (2.8668)	grad_norm 2.5874 (2.9489)	mem 14853MB
[2022-11-07 16:31:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][1050/1251]	eta 0:01:34 lr 0.000038	time 0.4607 (0.4688)	loss 3.2443 (2.8730)	grad_norm 3.1920 (2.9508)	mem 14853MB
[2022-11-07 16:32:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][1100/1251]	eta 0:01:10 lr 0.000038	time 0.4610 (0.4689)	loss 2.5854 (2.8715)	grad_norm 2.6889 (2.9487)	mem 14853MB
[2022-11-07 16:32:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][1150/1251]	eta 0:00:47 lr 0.000038	time 0.4615 (0.4688)	loss 2.1891 (2.8687)	grad_norm 3.2159 (2.9505)	mem 14853MB
[2022-11-07 16:33:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][1200/1251]	eta 0:00:23 lr 0.000038	time 0.4667 (0.4688)	loss 2.9283 (2.8710)	grad_norm 2.8171 (2.9524)	mem 14853MB
[2022-11-07 16:33:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [267/300][1250/1251]	eta 0:00:00 lr 0.000038	time 0.4584 (0.4685)	loss 2.7450 (2.8681)	grad_norm 3.4095 (2.9542)	mem 14853MB
[2022-11-07 16:33:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 267 training takes 0:09:46
[2022-11-07 16:33:26 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_267.pth saving......
[2022-11-07 16:33:27 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_267.pth saved !!!
[2022-11-07 16:33:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.513 (1.513)	Loss 0.7716 (0.7716)	Acc@1 81.445 (81.445)	Acc@5 95.215 (95.215)	Mem 14853MB
[2022-11-07 16:33:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.014 Acc@5 95.946
[2022-11-07 16:33:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.0%
[2022-11-07 16:33:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.744 (1.744)	Loss 0.7695 (0.7695)	Acc@1 81.934 (81.934)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 16:33:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.294 Acc@5 96.054
[2022-11-07 16:33:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 16:33:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.33% at 263 epoch
[2022-11-07 16:33:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][0/1251]	eta 0:41:00 lr 0.000038	time 1.9668 (1.9668)	loss 3.0430 (3.0430)	grad_norm 3.3273 (3.3273)	mem 14853MB
[2022-11-07 16:34:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][50/1251]	eta 0:09:59 lr 0.000037	time 0.4666 (0.4988)	loss 2.4473 (2.8852)	grad_norm 3.3195 (2.9587)	mem 14853MB
[2022-11-07 16:34:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][100/1251]	eta 0:09:18 lr 0.000037	time 0.4640 (0.4849)	loss 2.4204 (2.8280)	grad_norm 2.8822 (2.9418)	mem 14853MB
[2022-11-07 16:34:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][150/1251]	eta 0:08:46 lr 0.000037	time 0.4566 (0.4785)	loss 2.3787 (2.8445)	grad_norm 2.4366 (2.9358)	mem 14853MB
[2022-11-07 16:35:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][200/1251]	eta 0:08:19 lr 0.000037	time 0.4641 (0.4753)	loss 2.5106 (2.8429)	grad_norm 2.8570 (2.9324)	mem 14853MB
[2022-11-07 16:35:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][250/1251]	eta 0:07:54 lr 0.000037	time 0.4593 (0.4744)	loss 3.1028 (2.8453)	grad_norm 2.8547 (2.9296)	mem 14853MB
[2022-11-07 16:36:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][300/1251]	eta 0:07:29 lr 0.000037	time 0.4595 (0.4732)	loss 2.1485 (2.8564)	grad_norm 3.3035 (2.9409)	mem 14853MB
[2022-11-07 16:36:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][350/1251]	eta 0:07:05 lr 0.000037	time 0.4524 (0.4726)	loss 2.7951 (2.8566)	grad_norm 2.7249 (2.9371)	mem 14853MB
[2022-11-07 16:36:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][400/1251]	eta 0:06:41 lr 0.000037	time 0.4641 (0.4717)	loss 3.4047 (2.8639)	grad_norm 2.7434 (2.9347)	mem 14853MB
[2022-11-07 16:37:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][450/1251]	eta 0:06:17 lr 0.000037	time 0.4757 (0.4710)	loss 3.2298 (2.8565)	grad_norm 2.5706 (2.9557)	mem 14853MB
[2022-11-07 16:37:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][500/1251]	eta 0:05:53 lr 0.000037	time 0.4622 (0.4704)	loss 3.0544 (2.8578)	grad_norm 3.2166 (2.9591)	mem 14853MB
[2022-11-07 16:38:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][550/1251]	eta 0:05:29 lr 0.000037	time 0.4632 (0.4702)	loss 2.4073 (2.8574)	grad_norm 2.9956 (2.9612)	mem 14853MB
[2022-11-07 16:38:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][600/1251]	eta 0:05:05 lr 0.000037	time 0.5698 (0.4700)	loss 3.0203 (2.8540)	grad_norm 2.8181 (2.9603)	mem 14853MB
[2022-11-07 16:38:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][650/1251]	eta 0:04:42 lr 0.000037	time 0.4655 (0.4696)	loss 3.3220 (2.8633)	grad_norm 2.5976 (2.9580)	mem 14853MB
[2022-11-07 16:39:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][700/1251]	eta 0:04:18 lr 0.000037	time 0.4698 (0.4695)	loss 3.2014 (2.8594)	grad_norm 2.7450 (2.9600)	mem 14853MB
[2022-11-07 16:39:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][750/1251]	eta 0:03:55 lr 0.000037	time 0.4647 (0.4694)	loss 3.5091 (2.8661)	grad_norm 2.7568 (2.9623)	mem 14853MB
[2022-11-07 16:40:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][800/1251]	eta 0:03:31 lr 0.000036	time 0.4658 (0.4692)	loss 3.3798 (2.8632)	grad_norm 2.8276 (2.9602)	mem 14853MB
[2022-11-07 16:40:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][850/1251]	eta 0:03:08 lr 0.000036	time 0.4577 (0.4691)	loss 3.1673 (2.8572)	grad_norm 2.9476 (2.9642)	mem 14853MB
[2022-11-07 16:40:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][900/1251]	eta 0:02:44 lr 0.000036	time 0.4627 (0.4689)	loss 2.9448 (2.8578)	grad_norm 2.6663 (2.9646)	mem 14853MB
[2022-11-07 16:41:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][950/1251]	eta 0:02:21 lr 0.000036	time 0.4638 (0.4688)	loss 2.5136 (2.8535)	grad_norm 3.2386 (2.9660)	mem 14853MB
[2022-11-07 16:41:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][1000/1251]	eta 0:01:57 lr 0.000036	time 0.4598 (0.4688)	loss 2.3462 (2.8505)	grad_norm 2.9891 (2.9639)	mem 14853MB
[2022-11-07 16:41:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][1050/1251]	eta 0:01:34 lr 0.000036	time 0.4603 (0.4688)	loss 3.0471 (2.8520)	grad_norm 2.6099 (2.9624)	mem 14853MB
[2022-11-07 16:42:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][1100/1251]	eta 0:01:10 lr 0.000036	time 0.4677 (0.4688)	loss 3.1258 (2.8513)	grad_norm 3.1455 (2.9652)	mem 14853MB
[2022-11-07 16:42:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][1150/1251]	eta 0:00:47 lr 0.000036	time 0.4748 (0.4686)	loss 3.2774 (2.8550)	grad_norm 3.0852 (2.9606)	mem 14853MB
[2022-11-07 16:43:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][1200/1251]	eta 0:00:23 lr 0.000036	time 0.4752 (0.4686)	loss 2.8982 (2.8499)	grad_norm 2.5912 (2.9580)	mem 14853MB
[2022-11-07 16:43:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [268/300][1250/1251]	eta 0:00:00 lr 0.000036	time 0.4563 (0.4685)	loss 2.8772 (2.8518)	grad_norm 2.5795 (2.9554)	mem 14853MB
[2022-11-07 16:43:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 268 training takes 0:09:46
[2022-11-07 16:43:31 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_268.pth saving......
[2022-11-07 16:43:32 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_268.pth saved !!!
[2022-11-07 16:43:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.628 (1.628)	Loss 0.7393 (0.7393)	Acc@1 83.105 (83.105)	Acc@5 96.680 (96.680)	Mem 14853MB
[2022-11-07 16:43:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.056 Acc@5 95.876
[2022-11-07 16:43:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.1%
[2022-11-07 16:43:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.535 (1.535)	Loss 0.7560 (0.7560)	Acc@1 82.227 (82.227)	Acc@5 96.387 (96.387)	Mem 14853MB
[2022-11-07 16:43:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.286 Acc@5 96.058
[2022-11-07 16:43:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 16:43:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.33% at 263 epoch
[2022-11-07 16:43:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][0/1251]	eta 0:40:01 lr 0.000036	time 1.9195 (1.9195)	loss 3.1046 (3.1046)	grad_norm 3.0529 (3.0529)	mem 14853MB
[2022-11-07 16:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][50/1251]	eta 0:09:58 lr 0.000036	time 0.4656 (0.4983)	loss 3.0678 (2.8906)	grad_norm 2.6474 (2.9468)	mem 14853MB
[2022-11-07 16:44:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][100/1251]	eta 0:09:16 lr 0.000036	time 0.4652 (0.4832)	loss 3.4163 (2.8428)	grad_norm 3.0020 (inf)	mem 14853MB
[2022-11-07 16:45:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][150/1251]	eta 0:08:47 lr 0.000036	time 0.5566 (0.4789)	loss 2.6461 (2.8542)	grad_norm 3.4180 (inf)	mem 14853MB
[2022-11-07 16:45:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][200/1251]	eta 0:08:19 lr 0.000036	time 0.4632 (0.4755)	loss 2.6565 (2.8473)	grad_norm 3.9134 (inf)	mem 14853MB
[2022-11-07 16:45:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][250/1251]	eta 0:07:54 lr 0.000036	time 0.4720 (0.4737)	loss 2.5632 (2.8584)	grad_norm 2.9968 (inf)	mem 14853MB
[2022-11-07 16:46:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][300/1251]	eta 0:07:29 lr 0.000035	time 0.4640 (0.4723)	loss 2.9689 (2.8320)	grad_norm 2.7453 (inf)	mem 14853MB
[2022-11-07 16:46:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][350/1251]	eta 0:07:04 lr 0.000035	time 0.4623 (0.4713)	loss 3.0560 (2.8173)	grad_norm 3.4647 (inf)	mem 14853MB
[2022-11-07 16:46:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][400/1251]	eta 0:06:40 lr 0.000035	time 0.4596 (0.4707)	loss 2.2842 (2.8103)	grad_norm 2.4590 (inf)	mem 14853MB
[2022-11-07 16:47:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][450/1251]	eta 0:06:16 lr 0.000035	time 0.4614 (0.4704)	loss 2.1769 (2.8155)	grad_norm 2.9523 (inf)	mem 14853MB
[2022-11-07 16:47:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][500/1251]	eta 0:05:52 lr 0.000035	time 0.4611 (0.4698)	loss 2.7294 (2.8177)	grad_norm 2.8498 (inf)	mem 14853MB
[2022-11-07 16:48:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][550/1251]	eta 0:05:29 lr 0.000035	time 0.5206 (0.4696)	loss 3.5763 (2.8215)	grad_norm 3.3979 (inf)	mem 14853MB
[2022-11-07 16:48:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][600/1251]	eta 0:05:05 lr 0.000035	time 0.4658 (0.4695)	loss 2.9317 (2.8301)	grad_norm 3.0311 (inf)	mem 14853MB
[2022-11-07 16:48:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][650/1251]	eta 0:04:41 lr 0.000035	time 0.4600 (0.4692)	loss 2.7340 (2.8375)	grad_norm 2.6967 (inf)	mem 14853MB
[2022-11-07 16:49:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][700/1251]	eta 0:04:18 lr 0.000035	time 0.5376 (0.4690)	loss 2.6414 (2.8416)	grad_norm 3.1266 (inf)	mem 14853MB
[2022-11-07 16:49:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][750/1251]	eta 0:03:54 lr 0.000035	time 0.4618 (0.4688)	loss 2.6267 (2.8485)	grad_norm 2.9769 (inf)	mem 14853MB
[2022-11-07 16:50:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][800/1251]	eta 0:03:31 lr 0.000035	time 0.4739 (0.4688)	loss 2.4849 (2.8434)	grad_norm 2.8444 (inf)	mem 14853MB
[2022-11-07 16:50:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][850/1251]	eta 0:03:07 lr 0.000035	time 0.4526 (0.4688)	loss 3.0729 (2.8454)	grad_norm 3.1892 (inf)	mem 14853MB
[2022-11-07 16:50:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][900/1251]	eta 0:02:44 lr 0.000035	time 0.4682 (0.4686)	loss 1.8012 (2.8443)	grad_norm 2.9083 (inf)	mem 14853MB
[2022-11-07 16:51:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][950/1251]	eta 0:02:21 lr 0.000035	time 0.4613 (0.4686)	loss 2.7986 (2.8471)	grad_norm 2.6196 (inf)	mem 14853MB
[2022-11-07 16:51:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][1000/1251]	eta 0:01:57 lr 0.000035	time 0.4719 (0.4685)	loss 2.7382 (2.8428)	grad_norm 2.7502 (inf)	mem 14853MB
[2022-11-07 16:52:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][1050/1251]	eta 0:01:34 lr 0.000034	time 0.4610 (0.4685)	loss 2.2323 (2.8427)	grad_norm 2.9084 (inf)	mem 14853MB
[2022-11-07 16:52:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][1100/1251]	eta 0:01:10 lr 0.000034	time 0.4713 (0.4685)	loss 2.6141 (2.8478)	grad_norm 2.6397 (inf)	mem 14853MB
[2022-11-07 16:52:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][1150/1251]	eta 0:00:47 lr 0.000034	time 0.4714 (0.4682)	loss 2.9556 (2.8457)	grad_norm 2.9410 (inf)	mem 14853MB
[2022-11-07 16:53:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][1200/1251]	eta 0:00:23 lr 0.000034	time 0.5522 (0.4682)	loss 3.1059 (2.8452)	grad_norm 3.0898 (inf)	mem 14853MB
[2022-11-07 16:53:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [269/300][1250/1251]	eta 0:00:00 lr 0.000034	time 0.4569 (0.4680)	loss 3.5415 (2.8478)	grad_norm 3.1030 (inf)	mem 14853MB
[2022-11-07 16:53:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 269 training takes 0:09:45
[2022-11-07 16:53:35 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_269.pth saving......
[2022-11-07 16:53:36 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_269.pth saved !!!
[2022-11-07 16:53:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.564 (1.564)	Loss 0.7411 (0.7411)	Acc@1 83.789 (83.789)	Acc@5 96.484 (96.484)	Mem 14853MB
[2022-11-07 16:53:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.046 Acc@5 95.832
[2022-11-07 16:53:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.0%
[2022-11-07 16:53:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.669 (1.669)	Loss 0.7504 (0.7504)	Acc@1 83.594 (83.594)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 16:53:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.306 Acc@5 96.044
[2022-11-07 16:53:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 16:53:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.33% at 263 epoch
[2022-11-07 16:53:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][0/1251]	eta 0:41:19 lr 0.000034	time 1.9818 (1.9818)	loss 3.0801 (3.0801)	grad_norm 3.3614 (3.3614)	mem 14853MB
[2022-11-07 16:54:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][50/1251]	eta 0:09:58 lr 0.000034	time 0.4712 (0.4981)	loss 2.8920 (2.9040)	grad_norm 3.3242 (3.0010)	mem 14853MB
[2022-11-07 16:54:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][100/1251]	eta 0:09:18 lr 0.000034	time 0.4548 (0.4851)	loss 3.2487 (2.8874)	grad_norm 2.7565 (2.9631)	mem 14853MB
[2022-11-07 16:55:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][150/1251]	eta 0:08:47 lr 0.000034	time 0.4652 (0.4792)	loss 2.8500 (2.8569)	grad_norm 2.9523 (2.9704)	mem 14853MB
[2022-11-07 16:55:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][200/1251]	eta 0:08:20 lr 0.000034	time 0.4723 (0.4760)	loss 2.7615 (2.8624)	grad_norm 2.9997 (2.9937)	mem 14853MB
[2022-11-07 16:55:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][250/1251]	eta 0:07:54 lr 0.000034	time 0.4557 (0.4737)	loss 3.0968 (2.8445)	grad_norm 3.2629 (2.9942)	mem 14853MB
[2022-11-07 16:56:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][300/1251]	eta 0:07:29 lr 0.000034	time 0.4619 (0.4726)	loss 2.0739 (2.8492)	grad_norm 3.0093 (2.9851)	mem 14853MB
[2022-11-07 16:56:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][350/1251]	eta 0:07:04 lr 0.000034	time 0.4634 (0.4716)	loss 2.6083 (2.8541)	grad_norm 2.4375 (2.9881)	mem 14853MB
[2022-11-07 16:57:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][400/1251]	eta 0:06:40 lr 0.000034	time 0.4625 (0.4708)	loss 2.9194 (2.8581)	grad_norm 2.7037 (2.9892)	mem 14853MB
[2022-11-07 16:57:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][450/1251]	eta 0:06:16 lr 0.000034	time 0.4672 (0.4702)	loss 2.8467 (2.8627)	grad_norm 3.3555 (2.9990)	mem 14853MB
[2022-11-07 16:57:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][500/1251]	eta 0:05:52 lr 0.000034	time 0.4616 (0.4696)	loss 2.8609 (2.8568)	grad_norm 3.3317 (3.0009)	mem 14853MB
[2022-11-07 16:58:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][550/1251]	eta 0:05:29 lr 0.000034	time 0.4614 (0.4693)	loss 2.6919 (2.8415)	grad_norm 2.7549 (2.9983)	mem 14853MB
[2022-11-07 16:58:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][600/1251]	eta 0:05:05 lr 0.000033	time 0.4574 (0.4692)	loss 3.2327 (2.8494)	grad_norm 2.6486 (2.9983)	mem 14853MB
[2022-11-07 16:58:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][650/1251]	eta 0:04:41 lr 0.000033	time 0.4781 (0.4692)	loss 3.1953 (2.8527)	grad_norm 2.8614 (2.9866)	mem 14853MB
[2022-11-07 16:59:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][700/1251]	eta 0:04:18 lr 0.000033	time 0.4580 (0.4689)	loss 2.5627 (2.8478)	grad_norm 3.5458 (2.9874)	mem 14853MB
[2022-11-07 16:59:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][750/1251]	eta 0:03:54 lr 0.000033	time 0.4671 (0.4687)	loss 3.5438 (2.8521)	grad_norm 2.6708 (2.9870)	mem 14853MB
[2022-11-07 17:00:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][800/1251]	eta 0:03:31 lr 0.000033	time 0.5342 (0.4688)	loss 2.6904 (2.8544)	grad_norm 2.8636 (2.9798)	mem 14853MB
[2022-11-07 17:00:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][850/1251]	eta 0:03:07 lr 0.000033	time 0.4589 (0.4685)	loss 2.6209 (2.8565)	grad_norm 2.9559 (2.9798)	mem 14853MB
[2022-11-07 17:00:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][900/1251]	eta 0:02:44 lr 0.000033	time 0.5403 (0.4685)	loss 2.8112 (2.8552)	grad_norm 2.7564 (2.9782)	mem 14853MB
[2022-11-07 17:01:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][950/1251]	eta 0:02:20 lr 0.000033	time 0.4609 (0.4683)	loss 2.8602 (2.8574)	grad_norm 2.6938 (2.9780)	mem 14853MB
[2022-11-07 17:01:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][1000/1251]	eta 0:01:57 lr 0.000033	time 0.4621 (0.4681)	loss 1.8631 (2.8509)	grad_norm 3.4234 (2.9812)	mem 14853MB
[2022-11-07 17:02:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][1050/1251]	eta 0:01:34 lr 0.000033	time 0.4590 (0.4681)	loss 3.2753 (2.8516)	grad_norm 3.2499 (2.9851)	mem 14853MB
[2022-11-07 17:02:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][1100/1251]	eta 0:01:10 lr 0.000033	time 0.4680 (0.4680)	loss 2.5953 (2.8528)	grad_norm 3.1752 (2.9900)	mem 14853MB
[2022-11-07 17:02:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][1150/1251]	eta 0:00:47 lr 0.000033	time 0.4578 (0.4680)	loss 2.2487 (2.8525)	grad_norm 3.0952 (2.9869)	mem 14853MB
[2022-11-07 17:03:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][1200/1251]	eta 0:00:23 lr 0.000033	time 0.4626 (0.4678)	loss 2.9453 (2.8535)	grad_norm 2.8015 (2.9880)	mem 14853MB
[2022-11-07 17:03:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [270/300][1250/1251]	eta 0:00:00 lr 0.000033	time 0.4565 (0.4677)	loss 2.7429 (2.8538)	grad_norm 2.9868 (2.9897)	mem 14853MB
[2022-11-07 17:03:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 270 training takes 0:09:45
[2022-11-07 17:03:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_270.pth saving......
[2022-11-07 17:03:39 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_270.pth saved !!!
[2022-11-07 17:03:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.582 (1.582)	Loss 0.8094 (0.8094)	Acc@1 81.543 (81.543)	Acc@5 96.289 (96.289)	Mem 14853MB
[2022-11-07 17:03:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.142 Acc@5 95.950
[2022-11-07 17:03:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.1%
[2022-11-07 17:03:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.594 (1.594)	Loss 0.8174 (0.8174)	Acc@1 82.227 (82.227)	Acc@5 94.824 (94.824)	Mem 14853MB
[2022-11-07 17:03:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.320 Acc@5 96.046
[2022-11-07 17:03:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 17:03:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.33% at 263 epoch
[2022-11-07 17:03:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][0/1251]	eta 0:41:29 lr 0.000033	time 1.9899 (1.9899)	loss 3.3317 (3.3317)	grad_norm 2.9995 (2.9995)	mem 14853MB
[2022-11-07 17:04:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][50/1251]	eta 0:10:01 lr 0.000033	time 0.4613 (0.5009)	loss 3.1453 (2.7938)	grad_norm 3.1475 (3.0420)	mem 14853MB
[2022-11-07 17:04:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][100/1251]	eta 0:09:17 lr 0.000033	time 0.4660 (0.4847)	loss 3.2003 (2.8161)	grad_norm 2.4635 (2.9993)	mem 14853MB
[2022-11-07 17:05:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][150/1251]	eta 0:08:47 lr 0.000032	time 0.4605 (0.4795)	loss 2.6861 (2.8465)	grad_norm 3.0823 (3.0282)	mem 14853MB
[2022-11-07 17:05:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][200/1251]	eta 0:08:20 lr 0.000032	time 0.4593 (0.4764)	loss 3.0508 (2.8328)	grad_norm 3.3555 (3.0168)	mem 14853MB
[2022-11-07 17:05:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][250/1251]	eta 0:07:54 lr 0.000032	time 0.4595 (0.4742)	loss 3.0191 (2.8330)	grad_norm 3.2145 (3.0127)	mem 14853MB
[2022-11-07 17:06:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][300/1251]	eta 0:07:29 lr 0.000032	time 0.4660 (0.4727)	loss 3.1840 (2.8141)	grad_norm 2.5816 (2.9932)	mem 14853MB
[2022-11-07 17:06:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][350/1251]	eta 0:07:04 lr 0.000032	time 0.4630 (0.4716)	loss 1.8941 (2.8194)	grad_norm 3.0091 (3.0066)	mem 14853MB
[2022-11-07 17:07:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][400/1251]	eta 0:06:40 lr 0.000032	time 0.4732 (0.4708)	loss 2.2961 (2.8156)	grad_norm 2.4929 (3.0139)	mem 14853MB
[2022-11-07 17:07:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][450/1251]	eta 0:06:16 lr 0.000032	time 0.4678 (0.4703)	loss 2.7254 (2.8228)	grad_norm 3.0860 (3.0153)	mem 14853MB
[2022-11-07 17:07:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][500/1251]	eta 0:05:52 lr 0.000032	time 0.4610 (0.4697)	loss 3.3914 (2.8134)	grad_norm 2.7244 (3.0099)	mem 14853MB
[2022-11-07 17:08:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][550/1251]	eta 0:05:29 lr 0.000032	time 0.4572 (0.4697)	loss 2.3270 (2.8197)	grad_norm 2.9700 (3.0042)	mem 14853MB
[2022-11-07 17:08:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][600/1251]	eta 0:05:05 lr 0.000032	time 0.4499 (0.4696)	loss 2.9919 (2.8284)	grad_norm 2.7282 (2.9979)	mem 14853MB
[2022-11-07 17:09:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][650/1251]	eta 0:04:42 lr 0.000032	time 0.4762 (0.4693)	loss 3.3532 (2.8274)	grad_norm 3.2314 (3.0009)	mem 14853MB
[2022-11-07 17:09:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][700/1251]	eta 0:04:18 lr 0.000032	time 0.4780 (0.4690)	loss 2.2512 (2.8372)	grad_norm 2.7562 (3.0020)	mem 14853MB
[2022-11-07 17:09:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][750/1251]	eta 0:03:54 lr 0.000032	time 0.4523 (0.4689)	loss 2.6582 (2.8342)	grad_norm 4.1244 (3.0074)	mem 14853MB
[2022-11-07 17:10:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][800/1251]	eta 0:03:31 lr 0.000032	time 0.4662 (0.4688)	loss 2.6388 (2.8327)	grad_norm 3.0896 (3.0074)	mem 14853MB
[2022-11-07 17:10:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][850/1251]	eta 0:03:07 lr 0.000032	time 0.4645 (0.4688)	loss 2.3967 (2.8361)	grad_norm 2.6999 (3.0118)	mem 14853MB
[2022-11-07 17:10:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][900/1251]	eta 0:02:44 lr 0.000032	time 0.4680 (0.4686)	loss 3.3213 (2.8315)	grad_norm 3.4536 (3.0138)	mem 14853MB
[2022-11-07 17:11:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][950/1251]	eta 0:02:21 lr 0.000031	time 0.4601 (0.4685)	loss 3.0111 (2.8305)	grad_norm 3.1569 (3.0114)	mem 14853MB
[2022-11-07 17:11:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][1000/1251]	eta 0:01:57 lr 0.000031	time 0.4696 (0.4684)	loss 2.3506 (2.8256)	grad_norm 3.1750 (3.0117)	mem 14853MB
[2022-11-07 17:12:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][1050/1251]	eta 0:01:34 lr 0.000031	time 0.4607 (0.4685)	loss 3.2693 (2.8249)	grad_norm 3.0220 (3.0108)	mem 14853MB
[2022-11-07 17:12:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][1100/1251]	eta 0:01:10 lr 0.000031	time 0.4729 (0.4684)	loss 3.2913 (2.8274)	grad_norm 3.0739 (3.0110)	mem 14853MB
[2022-11-07 17:12:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][1150/1251]	eta 0:00:47 lr 0.000031	time 0.4724 (0.4683)	loss 2.8673 (2.8261)	grad_norm 2.8989 (3.0137)	mem 14853MB
[2022-11-07 17:13:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][1200/1251]	eta 0:00:23 lr 0.000031	time 0.4667 (0.4682)	loss 3.3746 (2.8251)	grad_norm 2.9099 (3.0103)	mem 14853MB
[2022-11-07 17:13:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [271/300][1250/1251]	eta 0:00:00 lr 0.000031	time 0.4571 (0.4680)	loss 3.1454 (2.8243)	grad_norm 2.9569 (3.0130)	mem 14853MB
[2022-11-07 17:13:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 271 training takes 0:09:45
[2022-11-07 17:13:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_271.pth saving......
[2022-11-07 17:13:43 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_271.pth saved !!!
[2022-11-07 17:13:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.673 (1.673)	Loss 0.6811 (0.6811)	Acc@1 84.668 (84.668)	Acc@5 96.680 (96.680)	Mem 14853MB
[2022-11-07 17:13:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.182 Acc@5 95.974
[2022-11-07 17:13:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.2%
[2022-11-07 17:13:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.735 (1.735)	Loss 0.7557 (0.7557)	Acc@1 82.715 (82.715)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-07 17:14:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.334 Acc@5 96.046
[2022-11-07 17:14:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-07 17:14:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.33% at 271 epoch
[2022-11-07 17:14:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][0/1251]	eta 0:39:54 lr 0.000031	time 1.9138 (1.9138)	loss 3.0651 (3.0651)	grad_norm 2.9586 (2.9586)	mem 14853MB
[2022-11-07 17:14:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][50/1251]	eta 0:09:58 lr 0.000031	time 0.4573 (0.4980)	loss 2.1516 (2.8020)	grad_norm 2.6656 (2.9980)	mem 14853MB
[2022-11-07 17:14:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][100/1251]	eta 0:09:17 lr 0.000031	time 0.4561 (0.4845)	loss 3.2148 (2.8046)	grad_norm 2.9505 (3.0410)	mem 14853MB
[2022-11-07 17:15:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][150/1251]	eta 0:08:46 lr 0.000031	time 0.4668 (0.4784)	loss 2.9989 (2.8320)	grad_norm 2.8271 (3.0192)	mem 14853MB
[2022-11-07 17:15:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][200/1251]	eta 0:08:19 lr 0.000031	time 0.4606 (0.4751)	loss 2.6865 (2.8302)	grad_norm 2.9269 (2.9989)	mem 14853MB
[2022-11-07 17:15:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][250/1251]	eta 0:07:53 lr 0.000031	time 0.4793 (0.4729)	loss 2.1742 (2.8527)	grad_norm 3.6149 (2.9983)	mem 14853MB
[2022-11-07 17:16:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][300/1251]	eta 0:07:28 lr 0.000031	time 0.4604 (0.4716)	loss 3.2286 (2.8442)	grad_norm 3.2509 (3.0012)	mem 14853MB
[2022-11-07 17:16:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][350/1251]	eta 0:07:04 lr 0.000031	time 0.4649 (0.4709)	loss 3.1988 (2.8567)	grad_norm 3.3256 (inf)	mem 14853MB
[2022-11-07 17:17:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][400/1251]	eta 0:06:40 lr 0.000031	time 0.4653 (0.4703)	loss 2.9869 (2.8521)	grad_norm 3.0234 (inf)	mem 14853MB
[2022-11-07 17:17:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][450/1251]	eta 0:06:16 lr 0.000031	time 0.4648 (0.4698)	loss 2.8886 (2.8513)	grad_norm 2.6053 (inf)	mem 14853MB
[2022-11-07 17:17:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][500/1251]	eta 0:05:52 lr 0.000031	time 0.4624 (0.4692)	loss 2.7106 (2.8579)	grad_norm 2.6622 (inf)	mem 14853MB
[2022-11-07 17:18:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][550/1251]	eta 0:05:28 lr 0.000030	time 0.4737 (0.4692)	loss 2.5976 (2.8520)	grad_norm 3.1486 (inf)	mem 14853MB
[2022-11-07 17:18:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][600/1251]	eta 0:05:05 lr 0.000030	time 0.4650 (0.4690)	loss 1.9480 (2.8504)	grad_norm 3.4368 (inf)	mem 14853MB
[2022-11-07 17:19:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][650/1251]	eta 0:04:41 lr 0.000030	time 0.4587 (0.4687)	loss 2.3495 (2.8478)	grad_norm 3.4254 (inf)	mem 14853MB
[2022-11-07 17:19:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][700/1251]	eta 0:04:18 lr 0.000030	time 0.4705 (0.4685)	loss 2.5285 (2.8447)	grad_norm 3.0295 (inf)	mem 14853MB
[2022-11-07 17:19:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][750/1251]	eta 0:03:54 lr 0.000030	time 0.4698 (0.4682)	loss 2.3593 (2.8412)	grad_norm 2.6859 (inf)	mem 14853MB
[2022-11-07 17:20:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][800/1251]	eta 0:03:31 lr 0.000030	time 0.4699 (0.4683)	loss 3.2697 (2.8384)	grad_norm 2.9463 (inf)	mem 14853MB
[2022-11-07 17:20:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][850/1251]	eta 0:03:07 lr 0.000030	time 0.4650 (0.4683)	loss 3.1495 (2.8359)	grad_norm 2.7874 (inf)	mem 14853MB
[2022-11-07 17:21:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][900/1251]	eta 0:02:44 lr 0.000030	time 0.4743 (0.4682)	loss 3.1017 (2.8343)	grad_norm 2.8624 (inf)	mem 14853MB
[2022-11-07 17:21:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][950/1251]	eta 0:02:20 lr 0.000030	time 0.4643 (0.4680)	loss 2.4235 (2.8368)	grad_norm 3.2737 (inf)	mem 14853MB
[2022-11-07 17:21:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][1000/1251]	eta 0:01:57 lr 0.000030	time 0.4628 (0.4679)	loss 1.7021 (2.8347)	grad_norm 3.2532 (inf)	mem 14853MB
[2022-11-07 17:22:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][1050/1251]	eta 0:01:34 lr 0.000030	time 0.4669 (0.4679)	loss 2.3982 (2.8359)	grad_norm 2.9329 (inf)	mem 14853MB
[2022-11-07 17:22:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][1100/1251]	eta 0:01:10 lr 0.000030	time 0.4711 (0.4679)	loss 1.7318 (2.8360)	grad_norm 2.8570 (inf)	mem 14853MB
[2022-11-07 17:22:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][1150/1251]	eta 0:00:47 lr 0.000030	time 0.4694 (0.4678)	loss 2.4916 (2.8355)	grad_norm 3.1840 (inf)	mem 14853MB
[2022-11-07 17:23:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][1200/1251]	eta 0:00:23 lr 0.000030	time 0.4693 (0.4678)	loss 3.1114 (2.8332)	grad_norm 2.7977 (inf)	mem 14853MB
[2022-11-07 17:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [272/300][1250/1251]	eta 0:00:00 lr 0.000030	time 0.4573 (0.4676)	loss 2.1579 (2.8284)	grad_norm 2.9532 (inf)	mem 14853MB
[2022-11-07 17:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 272 training takes 0:09:45
[2022-11-07 17:23:46 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_272.pth saving......
[2022-11-07 17:23:46 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_272.pth saved !!!
[2022-11-07 17:23:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.693 (1.693)	Loss 0.8060 (0.8060)	Acc@1 81.250 (81.250)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 17:23:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.178 Acc@5 95.900
[2022-11-07 17:23:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.2%
[2022-11-07 17:23:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.616 (1.616)	Loss 0.7761 (0.7761)	Acc@1 81.836 (81.836)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 17:24:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.350 Acc@5 96.036
[2022-11-07 17:24:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 17:24:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.35% at 272 epoch
[2022-11-07 17:24:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][0/1251]	eta 0:40:29 lr 0.000030	time 1.9420 (1.9420)	loss 3.1932 (3.1932)	grad_norm 2.7402 (2.7402)	mem 14853MB
[2022-11-07 17:24:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][50/1251]	eta 0:10:00 lr 0.000030	time 0.4624 (0.5001)	loss 2.6109 (2.7679)	grad_norm 3.4974 (3.0463)	mem 14853MB
[2022-11-07 17:24:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][100/1251]	eta 0:09:17 lr 0.000030	time 0.4612 (0.4844)	loss 2.5590 (2.8111)	grad_norm 3.1991 (3.0719)	mem 14853MB
[2022-11-07 17:25:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][150/1251]	eta 0:08:47 lr 0.000029	time 0.4630 (0.4787)	loss 2.5366 (2.8190)	grad_norm 3.1672 (3.0780)	mem 14853MB
[2022-11-07 17:25:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][200/1251]	eta 0:08:19 lr 0.000029	time 0.4683 (0.4755)	loss 3.3378 (2.8003)	grad_norm 2.8551 (3.0762)	mem 14853MB
[2022-11-07 17:26:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][250/1251]	eta 0:07:54 lr 0.000029	time 0.4550 (0.4738)	loss 1.7905 (2.8024)	grad_norm 2.5425 (3.0739)	mem 14853MB
[2022-11-07 17:26:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][300/1251]	eta 0:07:29 lr 0.000029	time 0.4549 (0.4724)	loss 3.4538 (2.8109)	grad_norm 3.2347 (3.0662)	mem 14853MB
[2022-11-07 17:26:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][350/1251]	eta 0:07:04 lr 0.000029	time 0.4622 (0.4714)	loss 3.1782 (2.8191)	grad_norm 2.7989 (3.0712)	mem 14853MB
[2022-11-07 17:27:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][400/1251]	eta 0:06:40 lr 0.000029	time 0.4784 (0.4707)	loss 3.0034 (2.8142)	grad_norm 2.7941 (3.0670)	mem 14853MB
[2022-11-07 17:27:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][450/1251]	eta 0:06:16 lr 0.000029	time 0.4572 (0.4703)	loss 3.1267 (2.8086)	grad_norm 3.4705 (3.0580)	mem 14853MB
[2022-11-07 17:27:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][500/1251]	eta 0:05:52 lr 0.000029	time 0.4617 (0.4699)	loss 2.5494 (2.8149)	grad_norm 3.0563 (3.0562)	mem 14853MB
[2022-11-07 17:28:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][550/1251]	eta 0:05:29 lr 0.000029	time 0.4628 (0.4700)	loss 1.9130 (2.8183)	grad_norm 2.9055 (3.0514)	mem 14853MB
[2022-11-07 17:28:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][600/1251]	eta 0:05:05 lr 0.000029	time 0.4624 (0.4697)	loss 3.2655 (2.8138)	grad_norm 3.2475 (3.0523)	mem 14853MB
[2022-11-07 17:29:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][650/1251]	eta 0:04:42 lr 0.000029	time 0.4648 (0.4694)	loss 2.9694 (2.8166)	grad_norm 2.8921 (3.0538)	mem 14853MB
[2022-11-07 17:29:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][700/1251]	eta 0:04:18 lr 0.000029	time 0.4644 (0.4693)	loss 2.3645 (2.8142)	grad_norm 2.9450 (3.0536)	mem 14853MB
[2022-11-07 17:29:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][750/1251]	eta 0:03:55 lr 0.000029	time 0.4590 (0.4693)	loss 3.2464 (2.8169)	grad_norm 2.6340 (3.0561)	mem 14853MB
[2022-11-07 17:30:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][800/1251]	eta 0:03:31 lr 0.000029	time 0.4724 (0.4693)	loss 3.1730 (2.8113)	grad_norm 2.7974 (3.0510)	mem 14853MB
[2022-11-07 17:30:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][850/1251]	eta 0:03:08 lr 0.000029	time 0.4637 (0.4691)	loss 3.6924 (2.8120)	grad_norm 2.9648 (3.0496)	mem 14853MB
[2022-11-07 17:31:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][900/1251]	eta 0:02:44 lr 0.000029	time 0.4660 (0.4689)	loss 2.3983 (2.8184)	grad_norm 2.8110 (3.0496)	mem 14853MB
[2022-11-07 17:31:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][950/1251]	eta 0:02:21 lr 0.000029	time 0.4817 (0.4688)	loss 2.8999 (2.8193)	grad_norm 2.8244 (3.0520)	mem 14853MB
[2022-11-07 17:31:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][1000/1251]	eta 0:01:57 lr 0.000029	time 0.4616 (0.4687)	loss 2.8255 (2.8186)	grad_norm 3.3523 (3.0586)	mem 14853MB
[2022-11-07 17:32:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][1050/1251]	eta 0:01:34 lr 0.000028	time 0.4587 (0.4688)	loss 2.8005 (2.8147)	grad_norm 3.3335 (3.0613)	mem 14853MB
[2022-11-07 17:32:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][1100/1251]	eta 0:01:10 lr 0.000028	time 0.4656 (0.4688)	loss 2.9485 (2.8186)	grad_norm 3.1530 (3.0614)	mem 14853MB
[2022-11-07 17:33:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][1150/1251]	eta 0:00:47 lr 0.000028	time 0.4644 (0.4686)	loss 2.6195 (2.8143)	grad_norm 2.8993 (inf)	mem 14853MB
[2022-11-07 17:33:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][1200/1251]	eta 0:00:23 lr 0.000028	time 0.4737 (0.4686)	loss 3.4162 (2.8166)	grad_norm 3.8063 (inf)	mem 14853MB
[2022-11-07 17:33:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [273/300][1250/1251]	eta 0:00:00 lr 0.000028	time 0.4643 (0.4684)	loss 3.2976 (2.8213)	grad_norm 2.9854 (inf)	mem 14853MB
[2022-11-07 17:33:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 273 training takes 0:09:46
[2022-11-07 17:33:50 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_273.pth saving......
[2022-11-07 17:33:50 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_273.pth saved !!!
[2022-11-07 17:33:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.686 (1.686)	Loss 0.8492 (0.8492)	Acc@1 81.055 (81.055)	Acc@5 95.215 (95.215)	Mem 14853MB
[2022-11-07 17:33:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.106 Acc@5 95.980
[2022-11-07 17:33:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.1%
[2022-11-07 17:34:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.755 (1.755)	Loss 0.7620 (0.7620)	Acc@1 83.203 (83.203)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 17:34:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.372 Acc@5 96.042
[2022-11-07 17:34:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 17:34:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.37% at 273 epoch
[2022-11-07 17:34:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][0/1251]	eta 0:40:26 lr 0.000028	time 1.9398 (1.9398)	loss 3.0885 (3.0885)	grad_norm 2.6770 (2.6770)	mem 14853MB
[2022-11-07 17:34:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][50/1251]	eta 0:10:02 lr 0.000028	time 0.4682 (0.5013)	loss 2.6878 (2.7278)	grad_norm 3.1381 (3.0509)	mem 14853MB
[2022-11-07 17:34:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][100/1251]	eta 0:09:19 lr 0.000028	time 0.4715 (0.4858)	loss 3.3144 (2.7761)	grad_norm 3.9188 (3.0570)	mem 14853MB
[2022-11-07 17:35:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][150/1251]	eta 0:08:48 lr 0.000028	time 0.4635 (0.4804)	loss 3.1021 (2.7552)	grad_norm 2.7397 (3.0498)	mem 14853MB
[2022-11-07 17:35:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][200/1251]	eta 0:08:21 lr 0.000028	time 0.4823 (0.4769)	loss 2.7820 (2.7581)	grad_norm 2.6460 (3.0331)	mem 14853MB
[2022-11-07 17:36:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][250/1251]	eta 0:07:55 lr 0.000028	time 0.4749 (0.4746)	loss 3.1339 (2.7822)	grad_norm 2.8003 (3.0266)	mem 14853MB
[2022-11-07 17:36:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][300/1251]	eta 0:07:30 lr 0.000028	time 0.4545 (0.4735)	loss 3.4757 (2.7863)	grad_norm 2.8523 (3.0154)	mem 14853MB
[2022-11-07 17:36:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][350/1251]	eta 0:07:05 lr 0.000028	time 0.4649 (0.4725)	loss 3.0170 (2.8073)	grad_norm 2.8367 (3.0143)	mem 14853MB
[2022-11-07 17:37:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][400/1251]	eta 0:06:41 lr 0.000028	time 0.4690 (0.4716)	loss 2.7982 (2.8205)	grad_norm 2.3105 (3.0251)	mem 14853MB
[2022-11-07 17:37:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][450/1251]	eta 0:06:17 lr 0.000028	time 0.4681 (0.4709)	loss 1.7484 (2.8258)	grad_norm 2.8277 (3.0274)	mem 14853MB
[2022-11-07 17:38:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][500/1251]	eta 0:05:53 lr 0.000028	time 0.4537 (0.4702)	loss 2.7119 (2.8346)	grad_norm 2.9055 (3.0316)	mem 14853MB
[2022-11-07 17:38:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][550/1251]	eta 0:05:29 lr 0.000028	time 0.4636 (0.4702)	loss 2.0462 (2.8360)	grad_norm 3.3735 (3.0366)	mem 14853MB
[2022-11-07 17:38:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][600/1251]	eta 0:05:05 lr 0.000028	time 0.4588 (0.4700)	loss 2.7239 (2.8280)	grad_norm 2.6467 (3.0414)	mem 14853MB
[2022-11-07 17:39:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][650/1251]	eta 0:04:42 lr 0.000028	time 0.4627 (0.4698)	loss 2.8598 (2.8278)	grad_norm 3.1729 (3.0480)	mem 14853MB
[2022-11-07 17:39:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][700/1251]	eta 0:04:18 lr 0.000027	time 0.4664 (0.4694)	loss 3.2153 (2.8269)	grad_norm 2.8978 (3.0536)	mem 14853MB
[2022-11-07 17:40:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][750/1251]	eta 0:03:55 lr 0.000027	time 0.4634 (0.4692)	loss 3.4052 (2.8316)	grad_norm 3.0077 (3.0540)	mem 14853MB
[2022-11-07 17:40:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][800/1251]	eta 0:03:31 lr 0.000027	time 0.4656 (0.4694)	loss 3.0962 (2.8375)	grad_norm 2.6493 (3.0616)	mem 14853MB
[2022-11-07 17:40:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][850/1251]	eta 0:03:08 lr 0.000027	time 0.4699 (0.4692)	loss 1.9244 (2.8393)	grad_norm 3.2641 (3.0657)	mem 14853MB
[2022-11-07 17:41:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][900/1251]	eta 0:02:44 lr 0.000027	time 0.4645 (0.4691)	loss 2.3986 (2.8406)	grad_norm 2.8951 (3.0632)	mem 14853MB
[2022-11-07 17:41:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][950/1251]	eta 0:02:21 lr 0.000027	time 0.4535 (0.4689)	loss 2.7572 (2.8424)	grad_norm 3.5371 (3.0649)	mem 14853MB
[2022-11-07 17:41:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][1000/1251]	eta 0:01:57 lr 0.000027	time 0.4546 (0.4687)	loss 3.0368 (2.8428)	grad_norm 3.1584 (3.0602)	mem 14853MB
[2022-11-07 17:42:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][1050/1251]	eta 0:01:34 lr 0.000027	time 0.4626 (0.4688)	loss 3.2640 (2.8438)	grad_norm 2.8860 (3.0599)	mem 14853MB
[2022-11-07 17:42:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][1100/1251]	eta 0:01:10 lr 0.000027	time 0.4543 (0.4687)	loss 1.9494 (2.8394)	grad_norm 2.6985 (3.0615)	mem 14853MB
[2022-11-07 17:43:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][1150/1251]	eta 0:00:47 lr 0.000027	time 0.4696 (0.4687)	loss 1.7896 (2.8361)	grad_norm 2.9083 (3.0592)	mem 14853MB
[2022-11-07 17:43:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][1200/1251]	eta 0:00:23 lr 0.000027	time 0.4583 (0.4686)	loss 3.0479 (2.8348)	grad_norm 3.3308 (3.0582)	mem 14853MB
[2022-11-07 17:43:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [274/300][1250/1251]	eta 0:00:00 lr 0.000027	time 0.4571 (0.4683)	loss 2.6388 (2.8391)	grad_norm 2.7514 (3.0617)	mem 14853MB
[2022-11-07 17:43:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 274 training takes 0:09:46
[2022-11-07 17:43:54 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_274.pth saving......
[2022-11-07 17:43:55 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_274.pth saved !!!
[2022-11-07 17:43:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.612 (1.612)	Loss 0.8269 (0.8269)	Acc@1 80.957 (80.957)	Acc@5 95.312 (95.312)	Mem 14853MB
[2022-11-07 17:44:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.192 Acc@5 95.952
[2022-11-07 17:44:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.2%
[2022-11-07 17:44:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.742 (1.742)	Loss 0.7368 (0.7368)	Acc@1 82.520 (82.520)	Acc@5 96.191 (96.191)	Mem 14853MB
[2022-11-07 17:44:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.384 Acc@5 96.036
[2022-11-07 17:44:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 17:44:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.38% at 274 epoch
[2022-11-07 17:44:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][0/1251]	eta 0:43:22 lr 0.000027	time 2.0801 (2.0801)	loss 2.9891 (2.9891)	grad_norm 3.9020 (3.9020)	mem 14853MB
[2022-11-07 17:44:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][50/1251]	eta 0:10:03 lr 0.000027	time 0.4694 (0.5022)	loss 2.9749 (2.9297)	grad_norm 3.6580 (3.0870)	mem 14853MB
[2022-11-07 17:45:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][100/1251]	eta 0:09:18 lr 0.000027	time 0.4712 (0.4850)	loss 2.7341 (2.8788)	grad_norm 2.8517 (3.1466)	mem 14853MB
[2022-11-07 17:45:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][150/1251]	eta 0:08:47 lr 0.000027	time 0.4747 (0.4794)	loss 3.0499 (2.8290)	grad_norm 2.6927 (3.1363)	mem 14853MB
[2022-11-07 17:45:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][200/1251]	eta 0:08:20 lr 0.000027	time 0.4562 (0.4766)	loss 3.0651 (2.8313)	grad_norm 3.2531 (3.1112)	mem 14853MB
[2022-11-07 17:46:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][250/1251]	eta 0:07:54 lr 0.000027	time 0.4592 (0.4742)	loss 3.0620 (2.8420)	grad_norm 2.4920 (3.1025)	mem 14853MB
[2022-11-07 17:46:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][300/1251]	eta 0:07:29 lr 0.000027	time 0.4690 (0.4726)	loss 2.5743 (2.8282)	grad_norm 2.7857 (3.1110)	mem 14853MB
[2022-11-07 17:46:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][350/1251]	eta 0:07:04 lr 0.000026	time 0.4777 (0.4713)	loss 2.7768 (2.8194)	grad_norm 3.1191 (3.1119)	mem 14853MB
[2022-11-07 17:47:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][400/1251]	eta 0:06:40 lr 0.000026	time 0.4617 (0.4703)	loss 3.1162 (2.8294)	grad_norm 2.4882 (3.1028)	mem 14853MB
[2022-11-07 17:47:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][450/1251]	eta 0:06:16 lr 0.000026	time 0.4580 (0.4699)	loss 3.6187 (2.8332)	grad_norm 2.8587 (3.0908)	mem 14853MB
[2022-11-07 17:48:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][500/1251]	eta 0:05:52 lr 0.000026	time 0.4569 (0.4695)	loss 2.1624 (2.8231)	grad_norm 2.8246 (3.0897)	mem 14853MB
[2022-11-07 17:48:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][550/1251]	eta 0:05:28 lr 0.000026	time 0.4792 (0.4693)	loss 2.9892 (2.8272)	grad_norm 2.8436 (3.0946)	mem 14853MB
[2022-11-07 17:48:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][600/1251]	eta 0:05:05 lr 0.000026	time 0.4643 (0.4691)	loss 2.8897 (2.8312)	grad_norm 3.7808 (3.1019)	mem 14853MB
[2022-11-07 17:49:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][650/1251]	eta 0:04:41 lr 0.000026	time 0.4603 (0.4687)	loss 3.1310 (2.8283)	grad_norm 3.8639 (3.0993)	mem 14853MB
[2022-11-07 17:49:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][700/1251]	eta 0:04:18 lr 0.000026	time 0.4705 (0.4684)	loss 1.7117 (2.8235)	grad_norm 2.9698 (3.0912)	mem 14853MB
[2022-11-07 17:50:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][750/1251]	eta 0:03:54 lr 0.000026	time 0.4628 (0.4683)	loss 2.0476 (2.8283)	grad_norm 2.4742 (inf)	mem 14853MB
[2022-11-07 17:50:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][800/1251]	eta 0:03:31 lr 0.000026	time 0.4774 (0.4683)	loss 1.8898 (2.8299)	grad_norm 3.0505 (inf)	mem 14853MB
[2022-11-07 17:50:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][850/1251]	eta 0:03:07 lr 0.000026	time 0.4539 (0.4682)	loss 3.2530 (2.8294)	grad_norm 2.9442 (inf)	mem 14853MB
[2022-11-07 17:51:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][900/1251]	eta 0:02:44 lr 0.000026	time 0.4630 (0.4680)	loss 3.1575 (2.8281)	grad_norm 3.1024 (inf)	mem 14853MB
[2022-11-07 17:51:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][950/1251]	eta 0:02:20 lr 0.000026	time 0.4731 (0.4678)	loss 2.0030 (2.8298)	grad_norm 2.7934 (inf)	mem 14853MB
[2022-11-07 17:52:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][1000/1251]	eta 0:01:57 lr 0.000026	time 0.4604 (0.4678)	loss 2.5377 (2.8284)	grad_norm 3.2047 (inf)	mem 14853MB
[2022-11-07 17:52:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][1050/1251]	eta 0:01:34 lr 0.000026	time 0.4606 (0.4678)	loss 2.6494 (2.8281)	grad_norm 2.9500 (inf)	mem 14853MB
[2022-11-07 17:52:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][1100/1251]	eta 0:01:10 lr 0.000026	time 0.4596 (0.4678)	loss 3.0675 (2.8321)	grad_norm 2.7209 (inf)	mem 14853MB
[2022-11-07 17:53:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][1150/1251]	eta 0:00:47 lr 0.000026	time 0.4672 (0.4677)	loss 2.1925 (2.8319)	grad_norm 3.0944 (inf)	mem 14853MB
[2022-11-07 17:53:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][1200/1251]	eta 0:00:23 lr 0.000026	time 0.4662 (0.4676)	loss 2.5124 (2.8337)	grad_norm 2.8404 (inf)	mem 14853MB
[2022-11-07 17:53:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [275/300][1250/1251]	eta 0:00:00 lr 0.000026	time 0.4563 (0.4674)	loss 3.1260 (2.8294)	grad_norm 2.8563 (inf)	mem 14853MB
[2022-11-07 17:53:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 275 training takes 0:09:44
[2022-11-07 17:53:57 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_275.pth saving......
[2022-11-07 17:53:58 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_275.pth saved !!!
[2022-11-07 17:53:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.634 (1.634)	Loss 0.7726 (0.7726)	Acc@1 83.203 (83.203)	Acc@5 95.703 (95.703)	Mem 14853MB
[2022-11-07 17:54:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.278 Acc@5 95.902
[2022-11-07 17:54:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.3%
[2022-11-07 17:54:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.618 (1.618)	Loss 0.7856 (0.7856)	Acc@1 82.715 (82.715)	Acc@5 95.020 (95.020)	Mem 14853MB
[2022-11-07 17:54:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.388 Acc@5 96.040
[2022-11-07 17:54:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 17:54:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.39% at 275 epoch
[2022-11-07 17:54:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][0/1251]	eta 0:39:18 lr 0.000026	time 1.8850 (1.8850)	loss 3.3784 (3.3784)	grad_norm 3.3102 (3.3102)	mem 14853MB
[2022-11-07 17:54:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][50/1251]	eta 0:09:58 lr 0.000025	time 0.4679 (0.4984)	loss 3.3015 (2.8327)	grad_norm 2.6686 (3.0684)	mem 14853MB
[2022-11-07 17:55:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][100/1251]	eta 0:09:18 lr 0.000025	time 0.4718 (0.4849)	loss 3.0921 (2.8307)	grad_norm 2.6353 (3.0709)	mem 14853MB
[2022-11-07 17:55:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][150/1251]	eta 0:08:47 lr 0.000025	time 0.4623 (0.4788)	loss 2.1437 (2.8109)	grad_norm 3.0474 (3.0814)	mem 14853MB
[2022-11-07 17:55:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][200/1251]	eta 0:08:19 lr 0.000025	time 0.4645 (0.4756)	loss 3.2407 (2.8323)	grad_norm 3.4616 (3.0823)	mem 14853MB
[2022-11-07 17:56:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][250/1251]	eta 0:07:54 lr 0.000025	time 0.4637 (0.4738)	loss 2.1675 (2.8405)	grad_norm 3.2502 (3.0898)	mem 14853MB
[2022-11-07 17:56:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][300/1251]	eta 0:07:29 lr 0.000025	time 0.4704 (0.4722)	loss 3.2574 (2.8202)	grad_norm 2.8725 (3.0978)	mem 14853MB
[2022-11-07 17:57:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][350/1251]	eta 0:07:04 lr 0.000025	time 0.4595 (0.4712)	loss 3.2166 (2.8430)	grad_norm 2.9309 (3.0862)	mem 14853MB
[2022-11-07 17:57:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][400/1251]	eta 0:06:40 lr 0.000025	time 0.4618 (0.4705)	loss 3.0100 (2.8407)	grad_norm 2.8568 (3.0840)	mem 14853MB
[2022-11-07 17:57:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][450/1251]	eta 0:06:16 lr 0.000025	time 0.4716 (0.4698)	loss 3.0544 (2.8471)	grad_norm 2.7766 (3.0801)	mem 14853MB
[2022-11-07 17:58:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][500/1251]	eta 0:05:52 lr 0.000025	time 0.4693 (0.4695)	loss 3.2247 (2.8431)	grad_norm 2.8500 (3.0724)	mem 14853MB
[2022-11-07 17:58:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][550/1251]	eta 0:05:29 lr 0.000025	time 0.4607 (0.4694)	loss 2.9266 (2.8421)	grad_norm 2.5676 (3.0694)	mem 14853MB
[2022-11-07 17:58:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][600/1251]	eta 0:05:05 lr 0.000025	time 0.4536 (0.4693)	loss 2.8745 (2.8462)	grad_norm 2.5498 (3.0727)	mem 14853MB
[2022-11-07 17:59:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][650/1251]	eta 0:04:41 lr 0.000025	time 0.4684 (0.4692)	loss 3.0933 (2.8415)	grad_norm 3.0666 (3.0758)	mem 14853MB
[2022-11-07 17:59:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][700/1251]	eta 0:04:18 lr 0.000025	time 0.4699 (0.4689)	loss 2.1945 (2.8417)	grad_norm 3.1692 (3.0744)	mem 14853MB
[2022-11-07 18:00:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][750/1251]	eta 0:03:54 lr 0.000025	time 0.4583 (0.4689)	loss 3.1075 (2.8458)	grad_norm 3.2322 (3.0789)	mem 14853MB
[2022-11-07 18:00:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][800/1251]	eta 0:03:31 lr 0.000025	time 0.4587 (0.4688)	loss 1.7039 (2.8459)	grad_norm 2.7874 (3.0840)	mem 14853MB
[2022-11-07 18:00:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][850/1251]	eta 0:03:07 lr 0.000025	time 0.4792 (0.4688)	loss 3.3468 (2.8445)	grad_norm 3.3203 (3.0901)	mem 14853MB
[2022-11-07 18:01:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][900/1251]	eta 0:02:44 lr 0.000025	time 0.4707 (0.4686)	loss 3.2177 (2.8477)	grad_norm 2.8988 (3.0857)	mem 14853MB
[2022-11-07 18:01:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][950/1251]	eta 0:02:20 lr 0.000025	time 0.4621 (0.4684)	loss 2.8936 (2.8441)	grad_norm 2.7324 (3.0879)	mem 14853MB
[2022-11-07 18:02:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][1000/1251]	eta 0:01:57 lr 0.000025	time 0.4554 (0.4683)	loss 1.8165 (2.8427)	grad_norm 2.6671 (3.0921)	mem 14853MB
[2022-11-07 18:02:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][1050/1251]	eta 0:01:34 lr 0.000024	time 0.4597 (0.4684)	loss 3.3019 (2.8436)	grad_norm 3.1166 (3.0946)	mem 14853MB
[2022-11-07 18:02:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][1100/1251]	eta 0:01:10 lr 0.000024	time 0.4597 (0.4683)	loss 3.2599 (2.8432)	grad_norm 2.9550 (3.0958)	mem 14853MB
[2022-11-07 18:03:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][1150/1251]	eta 0:00:47 lr 0.000024	time 0.4736 (0.4682)	loss 2.9638 (2.8432)	grad_norm 3.4743 (3.0961)	mem 14853MB
[2022-11-07 18:03:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][1200/1251]	eta 0:00:23 lr 0.000024	time 0.4561 (0.4681)	loss 3.0420 (2.8406)	grad_norm 2.9107 (3.0898)	mem 14853MB
[2022-11-07 18:04:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [276/300][1250/1251]	eta 0:00:00 lr 0.000024	time 0.4556 (0.4679)	loss 1.8857 (2.8383)	grad_norm 3.5220 (3.0868)	mem 14853MB
[2022-11-07 18:04:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 276 training takes 0:09:45
[2022-11-07 18:04:01 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_276.pth saving......
[2022-11-07 18:04:01 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_276.pth saved !!!
[2022-11-07 18:04:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.562 (1.562)	Loss 0.7587 (0.7587)	Acc@1 82.520 (82.520)	Acc@5 96.387 (96.387)	Mem 14853MB
[2022-11-07 18:04:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.254 Acc@5 95.912
[2022-11-07 18:04:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.3%
[2022-11-07 18:04:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.607 (1.607)	Loss 0.7797 (0.7797)	Acc@1 82.422 (82.422)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 18:04:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.394 Acc@5 96.038
[2022-11-07 18:04:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 18:04:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.39% at 276 epoch
[2022-11-07 18:04:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][0/1251]	eta 0:42:51 lr 0.000024	time 2.0557 (2.0557)	loss 2.2769 (2.2769)	grad_norm 2.8034 (2.8034)	mem 14853MB
[2022-11-07 18:04:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][50/1251]	eta 0:10:05 lr 0.000024	time 0.4586 (0.5041)	loss 2.7703 (2.7531)	grad_norm 2.7573 (3.1130)	mem 14853MB
[2022-11-07 18:05:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][100/1251]	eta 0:09:19 lr 0.000024	time 0.4647 (0.4864)	loss 2.9082 (2.8073)	grad_norm 2.8410 (3.1096)	mem 14853MB
[2022-11-07 18:05:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][150/1251]	eta 0:08:48 lr 0.000024	time 0.4673 (0.4799)	loss 2.0512 (2.8168)	grad_norm 2.7648 (3.0981)	mem 14853MB
[2022-11-07 18:05:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][200/1251]	eta 0:08:21 lr 0.000024	time 0.4571 (0.4771)	loss 2.1302 (2.7838)	grad_norm 3.2911 (3.0989)	mem 14853MB
[2022-11-07 18:06:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][250/1251]	eta 0:07:55 lr 0.000024	time 0.4664 (0.4746)	loss 3.1733 (2.8053)	grad_norm 3.2985 (3.0975)	mem 14853MB
[2022-11-07 18:06:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][300/1251]	eta 0:07:29 lr 0.000024	time 0.4558 (0.4730)	loss 2.6497 (2.8076)	grad_norm 3.7648 (3.0829)	mem 14853MB
[2022-11-07 18:07:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][350/1251]	eta 0:07:05 lr 0.000024	time 0.4685 (0.4721)	loss 2.9775 (2.8133)	grad_norm 2.9075 (3.0749)	mem 14853MB
[2022-11-07 18:07:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][400/1251]	eta 0:06:41 lr 0.000024	time 0.4653 (0.4713)	loss 3.1971 (2.8245)	grad_norm 2.8721 (3.0724)	mem 14853MB
[2022-11-07 18:07:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][450/1251]	eta 0:06:17 lr 0.000024	time 0.4669 (0.4708)	loss 3.2965 (2.8249)	grad_norm 3.0646 (inf)	mem 14853MB
[2022-11-07 18:08:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][500/1251]	eta 0:05:53 lr 0.000024	time 0.4656 (0.4702)	loss 2.6787 (2.8315)	grad_norm 2.8323 (inf)	mem 14853MB
[2022-11-07 18:08:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][550/1251]	eta 0:05:29 lr 0.000024	time 0.4511 (0.4703)	loss 2.9509 (2.8311)	grad_norm 3.4145 (inf)	mem 14853MB
[2022-11-07 18:09:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][600/1251]	eta 0:05:05 lr 0.000024	time 0.4728 (0.4700)	loss 2.5767 (2.8310)	grad_norm 3.4238 (inf)	mem 14853MB
[2022-11-07 18:09:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][650/1251]	eta 0:04:42 lr 0.000024	time 0.4657 (0.4696)	loss 2.6238 (2.8288)	grad_norm 2.7343 (inf)	mem 14853MB
[2022-11-07 18:09:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][700/1251]	eta 0:04:18 lr 0.000024	time 0.4649 (0.4694)	loss 2.5693 (2.8324)	grad_norm 3.2711 (inf)	mem 14853MB
[2022-11-07 18:10:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][750/1251]	eta 0:03:55 lr 0.000024	time 0.4754 (0.4692)	loss 3.1408 (2.8340)	grad_norm 3.5479 (inf)	mem 14853MB
[2022-11-07 18:10:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][800/1251]	eta 0:03:31 lr 0.000024	time 0.4794 (0.4694)	loss 2.9888 (2.8325)	grad_norm 2.9705 (inf)	mem 14853MB
[2022-11-07 18:10:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][850/1251]	eta 0:03:08 lr 0.000023	time 0.4591 (0.4692)	loss 2.9809 (2.8348)	grad_norm 4.4423 (inf)	mem 14853MB
[2022-11-07 18:11:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][900/1251]	eta 0:02:44 lr 0.000023	time 0.4698 (0.4690)	loss 2.4566 (2.8344)	grad_norm 2.7819 (inf)	mem 14853MB
[2022-11-07 18:11:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][950/1251]	eta 0:02:21 lr 0.000023	time 0.4563 (0.4688)	loss 3.3966 (2.8290)	grad_norm 3.2107 (inf)	mem 14853MB
[2022-11-07 18:12:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][1000/1251]	eta 0:01:57 lr 0.000023	time 0.4532 (0.4685)	loss 2.7935 (2.8253)	grad_norm 3.8446 (inf)	mem 14853MB
[2022-11-07 18:12:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][1050/1251]	eta 0:01:34 lr 0.000023	time 0.4633 (0.4687)	loss 3.1806 (2.8180)	grad_norm 3.3000 (inf)	mem 14853MB
[2022-11-07 18:12:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][1100/1251]	eta 0:01:10 lr 0.000023	time 0.4615 (0.4687)	loss 2.5834 (2.8207)	grad_norm 2.9169 (inf)	mem 14853MB
[2022-11-07 18:13:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][1150/1251]	eta 0:00:47 lr 0.000023	time 0.4743 (0.4685)	loss 3.1440 (2.8192)	grad_norm 4.3594 (inf)	mem 14853MB
[2022-11-07 18:13:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][1200/1251]	eta 0:00:23 lr 0.000023	time 0.4580 (0.4684)	loss 2.0912 (2.8209)	grad_norm 3.0944 (inf)	mem 14853MB
[2022-11-07 18:14:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [277/300][1250/1251]	eta 0:00:00 lr 0.000023	time 0.4574 (0.4682)	loss 2.7130 (2.8230)	grad_norm 2.9531 (inf)	mem 14853MB
[2022-11-07 18:14:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 277 training takes 0:09:45
[2022-11-07 18:14:05 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_277.pth saving......
[2022-11-07 18:14:05 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_277.pth saved !!!
[2022-11-07 18:14:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.630 (1.630)	Loss 0.7191 (0.7191)	Acc@1 83.008 (83.008)	Acc@5 97.070 (97.070)	Mem 14853MB
[2022-11-07 18:14:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.272 Acc@5 95.946
[2022-11-07 18:14:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.3%
[2022-11-07 18:14:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.591 (1.591)	Loss 0.7826 (0.7826)	Acc@1 81.543 (81.543)	Acc@5 95.215 (95.215)	Mem 14853MB
[2022-11-07 18:14:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.394 Acc@5 96.044
[2022-11-07 18:14:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 18:14:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.39% at 277 epoch
[2022-11-07 18:14:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][0/1251]	eta 0:40:11 lr 0.000023	time 1.9273 (1.9273)	loss 2.9615 (2.9615)	grad_norm 3.1688 (3.1688)	mem 14853MB
[2022-11-07 18:14:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][50/1251]	eta 0:09:59 lr 0.000023	time 0.4765 (0.4988)	loss 2.7217 (2.8164)	grad_norm 3.1472 (3.0935)	mem 14853MB
[2022-11-07 18:15:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][100/1251]	eta 0:09:18 lr 0.000023	time 0.4617 (0.4848)	loss 3.0518 (2.8270)	grad_norm 3.4413 (3.0800)	mem 14853MB
[2022-11-07 18:15:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][150/1251]	eta 0:08:46 lr 0.000023	time 0.4635 (0.4786)	loss 2.6618 (2.8364)	grad_norm 3.2184 (3.0929)	mem 14853MB
[2022-11-07 18:15:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][200/1251]	eta 0:08:19 lr 0.000023	time 0.4654 (0.4751)	loss 3.0339 (2.8240)	grad_norm 3.2241 (3.0779)	mem 14853MB
[2022-11-07 18:16:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][250/1251]	eta 0:07:54 lr 0.000023	time 0.4622 (0.4737)	loss 2.9280 (2.8292)	grad_norm 2.8300 (3.0966)	mem 14853MB
[2022-11-07 18:16:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][300/1251]	eta 0:07:29 lr 0.000023	time 0.4669 (0.4722)	loss 3.1153 (2.8302)	grad_norm 2.8632 (3.0952)	mem 14853MB
[2022-11-07 18:17:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][350/1251]	eta 0:07:04 lr 0.000023	time 0.4590 (0.4711)	loss 3.3300 (2.8520)	grad_norm 2.9951 (3.0803)	mem 14853MB
[2022-11-07 18:17:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][400/1251]	eta 0:06:40 lr 0.000023	time 0.4583 (0.4703)	loss 3.0216 (2.8433)	grad_norm 3.9517 (3.0914)	mem 14853MB
[2022-11-07 18:17:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][450/1251]	eta 0:06:16 lr 0.000023	time 0.4763 (0.4696)	loss 3.1324 (2.8364)	grad_norm 3.5484 (3.1012)	mem 14853MB
[2022-11-07 18:18:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][500/1251]	eta 0:05:52 lr 0.000023	time 0.4631 (0.4691)	loss 3.0682 (2.8448)	grad_norm 3.1723 (3.1075)	mem 14853MB
[2022-11-07 18:18:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][550/1251]	eta 0:05:29 lr 0.000023	time 0.4666 (0.4694)	loss 2.0341 (2.8334)	grad_norm 3.1700 (3.1054)	mem 14853MB
[2022-11-07 18:19:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][600/1251]	eta 0:05:05 lr 0.000023	time 0.4653 (0.4691)	loss 3.2866 (2.8419)	grad_norm 3.1042 (3.1175)	mem 14853MB
[2022-11-07 18:19:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][650/1251]	eta 0:04:41 lr 0.000022	time 0.4516 (0.4689)	loss 2.8412 (2.8414)	grad_norm 3.0493 (3.1164)	mem 14853MB
[2022-11-07 18:19:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][700/1251]	eta 0:04:18 lr 0.000022	time 0.4670 (0.4687)	loss 2.8073 (2.8395)	grad_norm 3.2322 (3.1206)	mem 14853MB
[2022-11-07 18:20:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][750/1251]	eta 0:03:54 lr 0.000022	time 0.4753 (0.4684)	loss 3.1680 (2.8346)	grad_norm 2.7201 (3.1164)	mem 14853MB
[2022-11-07 18:20:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][800/1251]	eta 0:03:31 lr 0.000022	time 0.5444 (0.4687)	loss 2.9657 (2.8393)	grad_norm 3.2783 (3.1169)	mem 14853MB
[2022-11-07 18:21:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][850/1251]	eta 0:03:07 lr 0.000022	time 0.4632 (0.4686)	loss 1.9640 (2.8414)	grad_norm 2.8106 (3.1182)	mem 14853MB
[2022-11-07 18:21:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][900/1251]	eta 0:02:44 lr 0.000022	time 0.4677 (0.4683)	loss 2.3999 (2.8423)	grad_norm 2.8364 (3.1175)	mem 14853MB
[2022-11-07 18:21:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][950/1251]	eta 0:02:20 lr 0.000022	time 0.4759 (0.4682)	loss 2.8061 (2.8449)	grad_norm 3.4853 (3.1181)	mem 14853MB
[2022-11-07 18:22:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][1000/1251]	eta 0:01:57 lr 0.000022	time 0.4642 (0.4680)	loss 3.0721 (2.8426)	grad_norm 3.0249 (3.1200)	mem 14853MB
[2022-11-07 18:22:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][1050/1251]	eta 0:01:34 lr 0.000022	time 0.4619 (0.4682)	loss 2.5132 (2.8400)	grad_norm 3.3883 (3.1191)	mem 14853MB
[2022-11-07 18:22:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][1100/1251]	eta 0:01:10 lr 0.000022	time 0.4699 (0.4681)	loss 2.6634 (2.8414)	grad_norm 3.0877 (3.1224)	mem 14853MB
[2022-11-07 18:23:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][1150/1251]	eta 0:00:47 lr 0.000022	time 0.4641 (0.4681)	loss 3.0451 (2.8432)	grad_norm 2.9804 (3.1236)	mem 14853MB
[2022-11-07 18:23:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][1200/1251]	eta 0:00:23 lr 0.000022	time 0.4625 (0.4680)	loss 2.1868 (2.8399)	grad_norm 2.7622 (3.1237)	mem 14853MB
[2022-11-07 18:24:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [278/300][1250/1251]	eta 0:00:00 lr 0.000022	time 0.4569 (0.4678)	loss 3.1947 (2.8418)	grad_norm 3.2435 (inf)	mem 14853MB
[2022-11-07 18:24:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 278 training takes 0:09:45
[2022-11-07 18:24:08 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_278.pth saving......
[2022-11-07 18:24:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_278.pth saved !!!
[2022-11-07 18:24:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.579 (1.579)	Loss 0.7428 (0.7428)	Acc@1 82.910 (82.910)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 18:24:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.426 Acc@5 95.916
[2022-11-07 18:24:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-07 18:24:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.653 (1.653)	Loss 0.8521 (0.8521)	Acc@1 79.785 (79.785)	Acc@5 94.922 (94.922)	Mem 14853MB
[2022-11-07 18:24:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.392 Acc@5 96.026
[2022-11-07 18:24:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 18:24:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.43% at 278 epoch
[2022-11-07 18:24:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][0/1251]	eta 0:41:22 lr 0.000022	time 1.9841 (1.9841)	loss 2.8666 (2.8666)	grad_norm 3.0956 (3.0956)	mem 14853MB
[2022-11-07 18:24:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][50/1251]	eta 0:10:06 lr 0.000022	time 0.4611 (0.5048)	loss 1.9756 (2.7852)	grad_norm 2.7379 (3.1643)	mem 14853MB
[2022-11-07 18:25:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][100/1251]	eta 0:09:20 lr 0.000022	time 0.4655 (0.4868)	loss 2.1584 (2.8316)	grad_norm 4.4980 (3.1486)	mem 14853MB
[2022-11-07 18:25:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][150/1251]	eta 0:08:49 lr 0.000022	time 0.4663 (0.4812)	loss 3.1892 (2.8186)	grad_norm 3.0911 (3.1478)	mem 14853MB
[2022-11-07 18:26:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][200/1251]	eta 0:08:21 lr 0.000022	time 0.4694 (0.4772)	loss 3.2718 (2.8124)	grad_norm 3.1512 (3.1448)	mem 14853MB
[2022-11-07 18:26:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][250/1251]	eta 0:07:55 lr 0.000022	time 0.4601 (0.4747)	loss 2.8105 (2.7997)	grad_norm 3.1238 (3.1630)	mem 14853MB
[2022-11-07 18:26:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][300/1251]	eta 0:07:29 lr 0.000022	time 0.4578 (0.4731)	loss 2.9947 (2.8144)	grad_norm 2.6856 (3.1646)	mem 14853MB
[2022-11-07 18:27:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][350/1251]	eta 0:07:05 lr 0.000022	time 0.4583 (0.4725)	loss 3.1397 (2.8149)	grad_norm 2.6590 (3.1560)	mem 14853MB
[2022-11-07 18:27:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][400/1251]	eta 0:06:41 lr 0.000022	time 0.4680 (0.4716)	loss 3.0416 (2.8063)	grad_norm 2.7910 (3.1473)	mem 14853MB
[2022-11-07 18:27:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][450/1251]	eta 0:06:17 lr 0.000022	time 0.4620 (0.4708)	loss 2.8554 (2.8058)	grad_norm 2.8871 (3.1348)	mem 14853MB
[2022-11-07 18:28:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][500/1251]	eta 0:05:53 lr 0.000021	time 0.4574 (0.4702)	loss 3.2683 (2.7986)	grad_norm 2.9843 (3.1333)	mem 14853MB
[2022-11-07 18:28:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][550/1251]	eta 0:05:29 lr 0.000021	time 0.4674 (0.4702)	loss 3.0560 (2.8025)	grad_norm 2.9240 (3.1277)	mem 14853MB
[2022-11-07 18:29:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][600/1251]	eta 0:05:05 lr 0.000021	time 0.4708 (0.4699)	loss 2.6326 (2.8091)	grad_norm 3.2837 (3.1291)	mem 14853MB
[2022-11-07 18:29:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][650/1251]	eta 0:04:42 lr 0.000021	time 0.4628 (0.4698)	loss 3.4226 (2.8194)	grad_norm 2.9584 (3.1346)	mem 14853MB
[2022-11-07 18:29:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][700/1251]	eta 0:04:18 lr 0.000021	time 0.4669 (0.4694)	loss 2.9127 (2.8245)	grad_norm 3.6321 (3.1351)	mem 14853MB
[2022-11-07 18:30:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][750/1251]	eta 0:03:55 lr 0.000021	time 0.4630 (0.4693)	loss 3.0356 (2.8231)	grad_norm 2.9372 (3.1363)	mem 14853MB
[2022-11-07 18:30:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][800/1251]	eta 0:03:31 lr 0.000021	time 0.4594 (0.4693)	loss 3.0481 (2.8254)	grad_norm 2.9792 (3.1348)	mem 14853MB
[2022-11-07 18:31:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][850/1251]	eta 0:03:08 lr 0.000021	time 0.4709 (0.4692)	loss 2.7927 (2.8264)	grad_norm 2.9299 (3.1375)	mem 14853MB
[2022-11-07 18:31:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][900/1251]	eta 0:02:44 lr 0.000021	time 0.4751 (0.4690)	loss 2.4999 (2.8285)	grad_norm 2.6996 (3.1368)	mem 14853MB
[2022-11-07 18:31:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][950/1251]	eta 0:02:21 lr 0.000021	time 0.4605 (0.4689)	loss 2.9843 (2.8268)	grad_norm 2.4822 (3.1304)	mem 14853MB
[2022-11-07 18:32:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][1000/1251]	eta 0:01:57 lr 0.000021	time 0.4606 (0.4687)	loss 2.5995 (2.8208)	grad_norm 2.7529 (3.1282)	mem 14853MB
[2022-11-07 18:32:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][1050/1251]	eta 0:01:34 lr 0.000021	time 0.4616 (0.4688)	loss 2.9940 (2.8212)	grad_norm 3.1358 (3.1257)	mem 14853MB
[2022-11-07 18:33:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][1100/1251]	eta 0:01:10 lr 0.000021	time 0.4684 (0.4687)	loss 2.4126 (2.8208)	grad_norm 2.8043 (3.1252)	mem 14853MB
[2022-11-07 18:33:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][1150/1251]	eta 0:00:47 lr 0.000021	time 0.4629 (0.4686)	loss 3.1813 (2.8178)	grad_norm 3.0231 (3.1252)	mem 14853MB
[2022-11-07 18:33:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][1200/1251]	eta 0:00:23 lr 0.000021	time 0.4579 (0.4685)	loss 3.0890 (2.8207)	grad_norm 3.0270 (3.1246)	mem 14853MB
[2022-11-07 18:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [279/300][1250/1251]	eta 0:00:00 lr 0.000021	time 0.4583 (0.4682)	loss 3.1735 (2.8224)	grad_norm 3.8266 (3.1275)	mem 14853MB
[2022-11-07 18:34:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 279 training takes 0:09:45
[2022-11-07 18:34:12 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_279.pth saving......
[2022-11-07 18:34:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_279.pth saved !!!
[2022-11-07 18:34:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.590 (1.590)	Loss 0.7238 (0.7238)	Acc@1 83.789 (83.789)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 18:34:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.278 Acc@5 95.932
[2022-11-07 18:34:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.3%
[2022-11-07 18:34:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.705 (1.705)	Loss 0.7277 (0.7277)	Acc@1 83.691 (83.691)	Acc@5 96.680 (96.680)	Mem 14853MB
[2022-11-07 18:34:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.402 Acc@5 96.026
[2022-11-07 18:34:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 18:34:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.43% at 278 epoch
[2022-11-07 18:34:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][0/1251]	eta 0:39:57 lr 0.000021	time 1.9163 (1.9163)	loss 2.3326 (2.3326)	grad_norm 3.0961 (3.0961)	mem 14853MB
[2022-11-07 18:34:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][50/1251]	eta 0:09:57 lr 0.000021	time 0.4543 (0.4978)	loss 3.2731 (2.8149)	grad_norm 3.0533 (3.1318)	mem 14853MB
[2022-11-07 18:35:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][100/1251]	eta 0:09:17 lr 0.000021	time 0.4760 (0.4840)	loss 3.1242 (2.8159)	grad_norm 2.9022 (3.1860)	mem 14853MB
[2022-11-07 18:35:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][150/1251]	eta 0:08:46 lr 0.000021	time 0.4541 (0.4782)	loss 3.1554 (2.7914)	grad_norm 3.1021 (3.1446)	mem 14853MB
[2022-11-07 18:36:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][200/1251]	eta 0:08:19 lr 0.000021	time 0.4660 (0.4757)	loss 2.9814 (2.8043)	grad_norm 3.0695 (3.1215)	mem 14853MB
[2022-11-07 18:36:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][250/1251]	eta 0:07:54 lr 0.000021	time 0.4570 (0.4742)	loss 3.0418 (2.8222)	grad_norm 3.5651 (3.1613)	mem 14853MB
[2022-11-07 18:36:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][300/1251]	eta 0:07:29 lr 0.000021	time 0.4628 (0.4725)	loss 2.5532 (2.8125)	grad_norm 3.0848 (3.1623)	mem 14853MB
[2022-11-07 18:37:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][350/1251]	eta 0:07:04 lr 0.000021	time 0.4714 (0.4716)	loss 3.2321 (2.8225)	grad_norm 2.7575 (3.1711)	mem 14853MB
[2022-11-07 18:37:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][400/1251]	eta 0:06:40 lr 0.000020	time 0.4663 (0.4707)	loss 3.1709 (2.8236)	grad_norm 2.7103 (3.1722)	mem 14853MB
[2022-11-07 18:38:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][450/1251]	eta 0:06:16 lr 0.000020	time 0.4613 (0.4701)	loss 3.2564 (2.8161)	grad_norm 2.7890 (3.1602)	mem 14853MB
[2022-11-07 18:38:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][500/1251]	eta 0:05:52 lr 0.000020	time 0.4644 (0.4697)	loss 2.9470 (2.8083)	grad_norm 3.1432 (3.1627)	mem 14853MB
[2022-11-07 18:38:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][550/1251]	eta 0:05:29 lr 0.000020	time 0.4619 (0.4698)	loss 2.8522 (2.8089)	grad_norm 2.9555 (3.1575)	mem 14853MB
[2022-11-07 18:39:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][600/1251]	eta 0:05:05 lr 0.000020	time 0.4765 (0.4694)	loss 3.1887 (2.8157)	grad_norm 2.9874 (3.1646)	mem 14853MB
[2022-11-07 18:39:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][650/1251]	eta 0:04:41 lr 0.000020	time 0.4609 (0.4690)	loss 1.8382 (2.8084)	grad_norm 3.5924 (3.1579)	mem 14853MB
[2022-11-07 18:39:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][700/1251]	eta 0:04:18 lr 0.000020	time 0.5358 (0.4689)	loss 2.8615 (2.8088)	grad_norm 2.9142 (3.1597)	mem 14853MB
[2022-11-07 18:40:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][750/1251]	eta 0:03:54 lr 0.000020	time 0.4663 (0.4687)	loss 2.1851 (2.8098)	grad_norm 3.1298 (3.1627)	mem 14853MB
[2022-11-07 18:40:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][800/1251]	eta 0:03:31 lr 0.000020	time 0.4558 (0.4686)	loss 2.6331 (2.8069)	grad_norm 2.9110 (3.1667)	mem 14853MB
[2022-11-07 18:41:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][850/1251]	eta 0:03:07 lr 0.000020	time 0.4663 (0.4685)	loss 3.2876 (2.8153)	grad_norm 3.7893 (3.1652)	mem 14853MB
[2022-11-07 18:41:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][900/1251]	eta 0:02:44 lr 0.000020	time 0.4714 (0.4684)	loss 2.1475 (2.8147)	grad_norm 3.0995 (3.1707)	mem 14853MB
[2022-11-07 18:41:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][950/1251]	eta 0:02:20 lr 0.000020	time 0.4711 (0.4683)	loss 2.4993 (2.8129)	grad_norm 3.4540 (inf)	mem 14853MB
[2022-11-07 18:42:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][1000/1251]	eta 0:01:57 lr 0.000020	time 0.4540 (0.4681)	loss 3.0176 (2.8148)	grad_norm 2.6379 (inf)	mem 14853MB
[2022-11-07 18:42:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][1050/1251]	eta 0:01:34 lr 0.000020	time 0.4608 (0.4681)	loss 3.1759 (2.8160)	grad_norm 3.2230 (inf)	mem 14853MB
[2022-11-07 18:43:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][1100/1251]	eta 0:01:10 lr 0.000020	time 0.4729 (0.4681)	loss 3.1578 (2.8164)	grad_norm 2.9617 (inf)	mem 14853MB
[2022-11-07 18:43:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][1150/1251]	eta 0:00:47 lr 0.000020	time 0.4633 (0.4680)	loss 3.3065 (2.8170)	grad_norm 3.6220 (inf)	mem 14853MB
[2022-11-07 18:43:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][1200/1251]	eta 0:00:23 lr 0.000020	time 0.5346 (0.4680)	loss 2.6981 (2.8149)	grad_norm 2.9665 (inf)	mem 14853MB
[2022-11-07 18:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [280/300][1250/1251]	eta 0:00:00 lr 0.000020	time 0.4566 (0.4677)	loss 2.1949 (2.8151)	grad_norm 3.1983 (inf)	mem 14853MB
[2022-11-07 18:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 280 training takes 0:09:45
[2022-11-07 18:44:16 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_280.pth saving......
[2022-11-07 18:44:16 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_280.pth saved !!!
[2022-11-07 18:44:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.718 (1.718)	Loss 0.8304 (0.8304)	Acc@1 81.445 (81.445)	Acc@5 96.191 (96.191)	Mem 14853MB
[2022-11-07 18:44:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.286 Acc@5 95.932
[2022-11-07 18:44:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.3%
[2022-11-07 18:44:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.710 (1.710)	Loss 0.7832 (0.7832)	Acc@1 81.934 (81.934)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 18:44:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.414 Acc@5 96.030
[2022-11-07 18:44:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 18:44:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.43% at 278 epoch
[2022-11-07 18:44:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][0/1251]	eta 0:40:33 lr 0.000020	time 1.9451 (1.9451)	loss 3.0704 (3.0704)	grad_norm 2.7617 (2.7617)	mem 14853MB
[2022-11-07 18:44:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][50/1251]	eta 0:10:00 lr 0.000020	time 0.4667 (0.4996)	loss 2.9077 (2.7453)	grad_norm 3.4487 (3.1607)	mem 14853MB
[2022-11-07 18:45:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][100/1251]	eta 0:09:17 lr 0.000020	time 0.4626 (0.4846)	loss 2.7228 (2.8324)	grad_norm 3.1290 (3.1809)	mem 14853MB
[2022-11-07 18:45:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][150/1251]	eta 0:08:47 lr 0.000020	time 0.4679 (0.4787)	loss 3.1188 (2.8393)	grad_norm 2.7971 (3.1705)	mem 14853MB
[2022-11-07 18:46:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][200/1251]	eta 0:08:19 lr 0.000020	time 0.4591 (0.4751)	loss 3.0825 (2.8249)	grad_norm 2.9184 (3.1502)	mem 14853MB
[2022-11-07 18:46:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][250/1251]	eta 0:07:54 lr 0.000020	time 0.4623 (0.4736)	loss 2.8960 (2.8169)	grad_norm 3.4016 (3.1341)	mem 14853MB
[2022-11-07 18:46:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][300/1251]	eta 0:07:29 lr 0.000020	time 0.4679 (0.4723)	loss 2.8908 (2.8100)	grad_norm 2.9436 (3.1374)	mem 14853MB
[2022-11-07 18:47:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][350/1251]	eta 0:07:04 lr 0.000019	time 0.4587 (0.4715)	loss 2.8678 (2.8154)	grad_norm 3.0405 (3.1345)	mem 14853MB
[2022-11-07 18:47:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][400/1251]	eta 0:06:40 lr 0.000019	time 0.4595 (0.4708)	loss 3.0266 (2.8107)	grad_norm 3.0423 (3.1358)	mem 14853MB
[2022-11-07 18:48:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][450/1251]	eta 0:06:16 lr 0.000019	time 0.4637 (0.4701)	loss 2.1353 (2.8021)	grad_norm 3.1855 (3.1391)	mem 14853MB
[2022-11-07 18:48:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][500/1251]	eta 0:05:52 lr 0.000019	time 0.4748 (0.4696)	loss 2.8203 (2.8019)	grad_norm 3.2713 (3.1366)	mem 14853MB
[2022-11-07 18:48:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][550/1251]	eta 0:05:29 lr 0.000019	time 0.4658 (0.4695)	loss 3.2658 (2.7969)	grad_norm 3.3179 (3.1420)	mem 14853MB
[2022-11-07 18:49:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][600/1251]	eta 0:05:05 lr 0.000019	time 0.4657 (0.4691)	loss 3.2412 (2.7954)	grad_norm 2.7410 (3.1378)	mem 14853MB
[2022-11-07 18:49:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][650/1251]	eta 0:04:41 lr 0.000019	time 0.4713 (0.4691)	loss 2.2766 (2.7954)	grad_norm 2.9698 (3.1298)	mem 14853MB
[2022-11-07 18:50:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][700/1251]	eta 0:04:18 lr 0.000019	time 0.4679 (0.4690)	loss 3.3341 (2.7974)	grad_norm 2.7650 (3.1338)	mem 14853MB
[2022-11-07 18:50:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][750/1251]	eta 0:03:54 lr 0.000019	time 0.4629 (0.4688)	loss 2.9131 (2.8001)	grad_norm 3.1645 (3.1408)	mem 14853MB
[2022-11-07 18:50:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][800/1251]	eta 0:03:31 lr 0.000019	time 0.4687 (0.4687)	loss 2.9168 (2.8061)	grad_norm 3.1593 (3.1407)	mem 14853MB
[2022-11-07 18:51:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][850/1251]	eta 0:03:07 lr 0.000019	time 0.4649 (0.4685)	loss 2.9920 (2.8061)	grad_norm 2.7704 (3.1384)	mem 14853MB
[2022-11-07 18:51:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][900/1251]	eta 0:02:44 lr 0.000019	time 0.4649 (0.4685)	loss 2.1959 (2.8073)	grad_norm 2.9075 (3.1359)	mem 14853MB
[2022-11-07 18:51:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][950/1251]	eta 0:02:21 lr 0.000019	time 0.4721 (0.4685)	loss 2.9072 (2.8096)	grad_norm 2.9747 (3.1391)	mem 14853MB
[2022-11-07 18:52:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][1000/1251]	eta 0:01:57 lr 0.000019	time 0.4653 (0.4684)	loss 2.9540 (2.8118)	grad_norm 3.0584 (3.1379)	mem 14853MB
[2022-11-07 18:52:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][1050/1251]	eta 0:01:34 lr 0.000019	time 0.4746 (0.4684)	loss 2.9922 (2.8108)	grad_norm 3.7595 (3.1351)	mem 14853MB
[2022-11-07 18:53:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][1100/1251]	eta 0:01:10 lr 0.000019	time 0.4610 (0.4682)	loss 2.3085 (2.8120)	grad_norm 2.6823 (3.1336)	mem 14853MB
[2022-11-07 18:53:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][1150/1251]	eta 0:00:47 lr 0.000019	time 0.4529 (0.4682)	loss 3.3739 (2.8131)	grad_norm 2.8806 (3.1315)	mem 14853MB
[2022-11-07 18:53:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][1200/1251]	eta 0:00:23 lr 0.000019	time 0.4684 (0.4681)	loss 2.4032 (2.8119)	grad_norm 2.9113 (3.1337)	mem 14853MB
[2022-11-07 18:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [281/300][1250/1251]	eta 0:00:00 lr 0.000019	time 0.4609 (0.4680)	loss 3.4294 (2.8131)	grad_norm 3.7158 (3.1357)	mem 14853MB
[2022-11-07 18:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 281 training takes 0:09:45
[2022-11-07 18:54:19 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_281.pth saving......
[2022-11-07 18:54:20 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_281.pth saved !!!
[2022-11-07 18:54:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.643 (1.643)	Loss 0.8225 (0.8225)	Acc@1 80.566 (80.566)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 18:54:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.350 Acc@5 95.974
[2022-11-07 18:54:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.3%
[2022-11-07 18:54:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.636 (1.636)	Loss 0.7121 (0.7121)	Acc@1 83.398 (83.398)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 18:54:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.406 Acc@5 96.024
[2022-11-07 18:54:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 18:54:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.43% at 278 epoch
[2022-11-07 18:54:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][0/1251]	eta 0:42:28 lr 0.000019	time 2.0371 (2.0371)	loss 3.2115 (3.2115)	grad_norm 3.0203 (3.0203)	mem 14853MB
[2022-11-07 18:55:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][50/1251]	eta 0:10:07 lr 0.000019	time 0.4670 (0.5056)	loss 2.6442 (2.7476)	grad_norm 2.8587 (3.1209)	mem 14853MB
[2022-11-07 18:55:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][100/1251]	eta 0:09:19 lr 0.000019	time 0.4777 (0.4859)	loss 3.0095 (2.7859)	grad_norm 2.7681 (3.1602)	mem 14853MB
[2022-11-07 18:55:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][150/1251]	eta 0:08:48 lr 0.000019	time 0.4595 (0.4799)	loss 2.8170 (2.8058)	grad_norm 3.1304 (3.1245)	mem 14853MB
[2022-11-07 18:56:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][200/1251]	eta 0:08:20 lr 0.000019	time 0.4734 (0.4766)	loss 2.5870 (2.8041)	grad_norm 3.1200 (3.1569)	mem 14853MB
[2022-11-07 18:56:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][250/1251]	eta 0:07:54 lr 0.000019	time 0.4614 (0.4742)	loss 3.2934 (2.8027)	grad_norm 3.2204 (3.1476)	mem 14853MB
[2022-11-07 18:57:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][300/1251]	eta 0:07:29 lr 0.000019	time 0.4615 (0.4730)	loss 3.2730 (2.8002)	grad_norm 3.2224 (3.1531)	mem 14853MB
[2022-11-07 18:57:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][350/1251]	eta 0:07:05 lr 0.000018	time 0.4570 (0.4720)	loss 2.5810 (2.8179)	grad_norm 3.0628 (3.1520)	mem 14853MB
[2022-11-07 18:57:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][400/1251]	eta 0:06:40 lr 0.000018	time 0.4634 (0.4711)	loss 3.2712 (2.8125)	grad_norm 3.3693 (3.1412)	mem 14853MB
[2022-11-07 18:58:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][450/1251]	eta 0:06:16 lr 0.000018	time 0.4604 (0.4704)	loss 3.1921 (2.8117)	grad_norm 3.0323 (3.1448)	mem 14853MB
[2022-11-07 18:58:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][500/1251]	eta 0:05:52 lr 0.000018	time 0.4645 (0.4700)	loss 2.7114 (2.8158)	grad_norm 3.4434 (inf)	mem 14853MB
[2022-11-07 18:58:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][550/1251]	eta 0:05:29 lr 0.000018	time 0.4615 (0.4698)	loss 2.2564 (2.8096)	grad_norm 3.1063 (inf)	mem 14853MB
[2022-11-07 18:59:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][600/1251]	eta 0:05:05 lr 0.000018	time 0.4696 (0.4697)	loss 3.1393 (2.8145)	grad_norm 3.4241 (inf)	mem 14853MB
[2022-11-07 18:59:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][650/1251]	eta 0:04:42 lr 0.000018	time 0.4697 (0.4693)	loss 3.1959 (2.8073)	grad_norm 3.2737 (inf)	mem 14853MB
[2022-11-07 19:00:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][700/1251]	eta 0:04:18 lr 0.000018	time 0.4611 (0.4689)	loss 3.2726 (2.8079)	grad_norm 2.9662 (inf)	mem 14853MB
[2022-11-07 19:00:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][750/1251]	eta 0:03:54 lr 0.000018	time 0.4724 (0.4687)	loss 3.1105 (2.8114)	grad_norm 3.5225 (inf)	mem 14853MB
[2022-11-07 19:00:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][800/1251]	eta 0:03:31 lr 0.000018	time 0.4668 (0.4689)	loss 2.8368 (2.8111)	grad_norm 3.5432 (inf)	mem 14853MB
[2022-11-07 19:01:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][850/1251]	eta 0:03:08 lr 0.000018	time 0.4886 (0.4689)	loss 2.9613 (2.8152)	grad_norm 2.5268 (inf)	mem 14853MB
[2022-11-07 19:01:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][900/1251]	eta 0:02:44 lr 0.000018	time 0.4629 (0.4687)	loss 3.0994 (2.8158)	grad_norm 3.0950 (inf)	mem 14853MB
[2022-11-07 19:02:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][950/1251]	eta 0:02:21 lr 0.000018	time 0.4544 (0.4685)	loss 2.8513 (2.8132)	grad_norm 2.9733 (inf)	mem 14853MB
[2022-11-07 19:02:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][1000/1251]	eta 0:01:57 lr 0.000018	time 0.4605 (0.4683)	loss 2.5200 (2.8122)	grad_norm 3.1956 (inf)	mem 14853MB
[2022-11-07 19:02:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][1050/1251]	eta 0:01:34 lr 0.000018	time 0.4659 (0.4684)	loss 2.9487 (2.8122)	grad_norm 2.9380 (inf)	mem 14853MB
[2022-11-07 19:03:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][1100/1251]	eta 0:01:10 lr 0.000018	time 0.4591 (0.4685)	loss 2.8474 (2.8124)	grad_norm 3.3309 (inf)	mem 14853MB
[2022-11-07 19:03:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][1150/1251]	eta 0:00:47 lr 0.000018	time 0.4617 (0.4684)	loss 3.3798 (2.8141)	grad_norm 3.1404 (inf)	mem 14853MB
[2022-11-07 19:04:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][1200/1251]	eta 0:00:23 lr 0.000018	time 0.4603 (0.4683)	loss 2.4586 (2.8148)	grad_norm 2.8377 (inf)	mem 14853MB
[2022-11-07 19:04:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [282/300][1250/1251]	eta 0:00:00 lr 0.000018	time 0.4559 (0.4680)	loss 2.8681 (2.8133)	grad_norm 3.2621 (inf)	mem 14853MB
[2022-11-07 19:04:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 282 training takes 0:09:45
[2022-11-07 19:04:23 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_282.pth saving......
[2022-11-07 19:04:24 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_282.pth saved !!!
[2022-11-07 19:04:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.624 (1.624)	Loss 0.7884 (0.7884)	Acc@1 82.422 (82.422)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 19:04:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.412 Acc@5 95.918
[2022-11-07 19:04:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-07 19:04:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.660 (1.660)	Loss 0.7733 (0.7733)	Acc@1 83.008 (83.008)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 19:04:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.402 Acc@5 96.002
[2022-11-07 19:04:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 19:04:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.43% at 278 epoch
[2022-11-07 19:04:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][0/1251]	eta 0:42:33 lr 0.000018	time 2.0411 (2.0411)	loss 1.9569 (1.9569)	grad_norm 3.5058 (3.5058)	mem 14853MB
[2022-11-07 19:05:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][50/1251]	eta 0:10:04 lr 0.000018	time 0.4637 (0.5035)	loss 2.3118 (2.7514)	grad_norm 3.5123 (3.1527)	mem 14853MB
[2022-11-07 19:05:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][100/1251]	eta 0:09:20 lr 0.000018	time 0.4759 (0.4868)	loss 1.7917 (2.7561)	grad_norm 2.8860 (3.1345)	mem 14853MB
[2022-11-07 19:05:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][150/1251]	eta 0:08:47 lr 0.000018	time 0.4622 (0.4794)	loss 1.8948 (2.7366)	grad_norm 2.9205 (3.1685)	mem 14853MB
[2022-11-07 19:06:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][200/1251]	eta 0:08:20 lr 0.000018	time 0.4642 (0.4762)	loss 3.3548 (2.7865)	grad_norm 2.7585 (3.1642)	mem 14853MB
[2022-11-07 19:06:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][250/1251]	eta 0:07:54 lr 0.000018	time 0.4654 (0.4741)	loss 2.0208 (2.7944)	grad_norm 2.9494 (3.1829)	mem 14853MB
[2022-11-07 19:07:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][300/1251]	eta 0:07:29 lr 0.000018	time 0.4592 (0.4726)	loss 2.9831 (2.8102)	grad_norm 2.7868 (3.1845)	mem 14853MB
[2022-11-07 19:07:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][350/1251]	eta 0:07:05 lr 0.000018	time 0.4630 (0.4721)	loss 2.3706 (2.8109)	grad_norm 3.3668 (3.1833)	mem 14853MB
[2022-11-07 19:07:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][400/1251]	eta 0:06:40 lr 0.000018	time 0.4548 (0.4711)	loss 3.1421 (2.8122)	grad_norm 3.5933 (3.1850)	mem 14853MB
[2022-11-07 19:08:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][450/1251]	eta 0:06:16 lr 0.000017	time 0.4564 (0.4705)	loss 2.6740 (2.8149)	grad_norm 3.1985 (3.1783)	mem 14853MB
[2022-11-07 19:08:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][500/1251]	eta 0:05:53 lr 0.000017	time 0.4571 (0.4701)	loss 3.1985 (2.8071)	grad_norm 3.2933 (3.1854)	mem 14853MB
[2022-11-07 19:09:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][550/1251]	eta 0:05:29 lr 0.000017	time 0.4642 (0.4699)	loss 3.1410 (2.8079)	grad_norm 2.5582 (3.1853)	mem 14853MB
[2022-11-07 19:09:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][600/1251]	eta 0:05:05 lr 0.000017	time 0.4544 (0.4698)	loss 3.0194 (2.8084)	grad_norm 3.3741 (3.1815)	mem 14853MB
[2022-11-07 19:09:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][650/1251]	eta 0:04:42 lr 0.000017	time 0.4638 (0.4695)	loss 2.9792 (2.8095)	grad_norm 2.9389 (3.1826)	mem 14853MB
[2022-11-07 19:10:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][700/1251]	eta 0:04:18 lr 0.000017	time 0.4659 (0.4693)	loss 2.2330 (2.8051)	grad_norm 4.4545 (3.1912)	mem 14853MB
[2022-11-07 19:10:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][750/1251]	eta 0:03:54 lr 0.000017	time 0.4633 (0.4689)	loss 1.9304 (2.8104)	grad_norm 3.2051 (3.1930)	mem 14853MB
[2022-11-07 19:10:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][800/1251]	eta 0:03:31 lr 0.000017	time 0.4543 (0.4689)	loss 3.2087 (2.8146)	grad_norm 3.1369 (3.1949)	mem 14853MB
[2022-11-07 19:11:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][850/1251]	eta 0:03:07 lr 0.000017	time 0.4599 (0.4688)	loss 2.0731 (2.8096)	grad_norm 3.2318 (3.1942)	mem 14853MB
[2022-11-07 19:11:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][900/1251]	eta 0:02:44 lr 0.000017	time 0.4675 (0.4687)	loss 3.3384 (2.8107)	grad_norm 3.5265 (3.1956)	mem 14853MB
[2022-11-07 19:12:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][950/1251]	eta 0:02:21 lr 0.000017	time 0.4610 (0.4686)	loss 2.8370 (2.8058)	grad_norm 3.6853 (3.1910)	mem 14853MB
[2022-11-07 19:12:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][1000/1251]	eta 0:01:57 lr 0.000017	time 0.4717 (0.4685)	loss 2.5991 (2.8062)	grad_norm 3.1121 (3.1905)	mem 14853MB
[2022-11-07 19:12:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][1050/1251]	eta 0:01:34 lr 0.000017	time 0.4666 (0.4684)	loss 2.9632 (2.8086)	grad_norm 3.2738 (3.1895)	mem 14853MB
[2022-11-07 19:13:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][1100/1251]	eta 0:01:10 lr 0.000017	time 0.4686 (0.4684)	loss 1.9792 (2.8054)	grad_norm 2.7214 (3.1835)	mem 14853MB
[2022-11-07 19:13:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][1150/1251]	eta 0:00:47 lr 0.000017	time 0.4619 (0.4682)	loss 3.1740 (2.8072)	grad_norm 2.7208 (3.1800)	mem 14853MB
[2022-11-07 19:14:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][1200/1251]	eta 0:00:23 lr 0.000017	time 0.5528 (0.4683)	loss 3.2557 (2.8062)	grad_norm 3.5061 (3.1786)	mem 14853MB
[2022-11-07 19:14:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [283/300][1250/1251]	eta 0:00:00 lr 0.000017	time 0.4570 (0.4681)	loss 2.5286 (2.8081)	grad_norm 2.7477 (3.1779)	mem 14853MB
[2022-11-07 19:14:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 283 training takes 0:09:45
[2022-11-07 19:14:27 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_283.pth saving......
[2022-11-07 19:14:28 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_283.pth saved !!!
[2022-11-07 19:14:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.427 (1.427)	Loss 0.8126 (0.8126)	Acc@1 82.617 (82.617)	Acc@5 95.117 (95.117)	Mem 14853MB
[2022-11-07 19:14:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.396 Acc@5 95.906
[2022-11-07 19:14:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-07 19:14:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.758 (1.758)	Loss 0.7435 (0.7435)	Acc@1 83.887 (83.887)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 19:14:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.426 Acc@5 95.994
[2022-11-07 19:14:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 19:14:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.43% at 278 epoch
[2022-11-07 19:14:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][0/1251]	eta 0:40:06 lr 0.000017	time 1.9236 (1.9236)	loss 3.3438 (3.3438)	grad_norm 2.7742 (2.7742)	mem 14853MB
[2022-11-07 19:15:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][50/1251]	eta 0:09:57 lr 0.000017	time 0.4821 (0.4975)	loss 2.5021 (2.7441)	grad_norm 3.2240 (inf)	mem 14853MB
[2022-11-07 19:15:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][100/1251]	eta 0:09:16 lr 0.000017	time 0.4613 (0.4835)	loss 2.7802 (2.7807)	grad_norm 3.2901 (inf)	mem 14853MB
[2022-11-07 19:15:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][150/1251]	eta 0:08:46 lr 0.000017	time 0.4669 (0.4780)	loss 3.2312 (2.7895)	grad_norm 3.1453 (inf)	mem 14853MB
[2022-11-07 19:16:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][200/1251]	eta 0:08:19 lr 0.000017	time 0.4639 (0.4756)	loss 2.0632 (2.8113)	grad_norm 2.7344 (inf)	mem 14853MB
[2022-11-07 19:16:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][250/1251]	eta 0:07:53 lr 0.000017	time 0.4646 (0.4735)	loss 2.8748 (2.7906)	grad_norm 2.9304 (inf)	mem 14853MB
[2022-11-07 19:17:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][300/1251]	eta 0:07:29 lr 0.000017	time 0.4620 (0.4724)	loss 2.4435 (2.7941)	grad_norm 3.1127 (inf)	mem 14853MB
[2022-11-07 19:17:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][350/1251]	eta 0:07:04 lr 0.000017	time 0.4631 (0.4716)	loss 3.1100 (2.7870)	grad_norm 3.2480 (inf)	mem 14853MB
[2022-11-07 19:17:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][400/1251]	eta 0:06:40 lr 0.000017	time 0.4571 (0.4706)	loss 2.0574 (2.7901)	grad_norm 3.8093 (inf)	mem 14853MB
[2022-11-07 19:18:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][450/1251]	eta 0:06:16 lr 0.000017	time 0.4740 (0.4704)	loss 3.0661 (2.7957)	grad_norm 3.3615 (inf)	mem 14853MB
[2022-11-07 19:18:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][500/1251]	eta 0:05:52 lr 0.000017	time 0.4544 (0.4697)	loss 2.0831 (2.7980)	grad_norm 3.0807 (inf)	mem 14853MB
[2022-11-07 19:19:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][550/1251]	eta 0:05:29 lr 0.000017	time 0.5288 (0.4695)	loss 2.4033 (2.8077)	grad_norm 3.3397 (inf)	mem 14853MB
[2022-11-07 19:19:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][600/1251]	eta 0:05:05 lr 0.000017	time 0.4722 (0.4691)	loss 2.9180 (2.8087)	grad_norm 3.0735 (inf)	mem 14853MB
[2022-11-07 19:19:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][650/1251]	eta 0:04:41 lr 0.000016	time 0.4580 (0.4687)	loss 2.1788 (2.8047)	grad_norm 3.3373 (inf)	mem 14853MB
[2022-11-07 19:20:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][700/1251]	eta 0:04:18 lr 0.000016	time 0.4550 (0.4688)	loss 2.6799 (2.8081)	grad_norm 2.9617 (inf)	mem 14853MB
[2022-11-07 19:20:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][750/1251]	eta 0:03:54 lr 0.000016	time 0.4698 (0.4686)	loss 2.7545 (2.8121)	grad_norm 3.3509 (inf)	mem 14853MB
[2022-11-07 19:21:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][800/1251]	eta 0:03:31 lr 0.000016	time 0.4618 (0.4686)	loss 2.3468 (2.8047)	grad_norm 3.1354 (inf)	mem 14853MB
[2022-11-07 19:21:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][850/1251]	eta 0:03:07 lr 0.000016	time 0.4722 (0.4685)	loss 2.8595 (2.8118)	grad_norm 2.9125 (inf)	mem 14853MB
[2022-11-07 19:21:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][900/1251]	eta 0:02:44 lr 0.000016	time 0.4697 (0.4684)	loss 2.9226 (2.8127)	grad_norm 3.1124 (inf)	mem 14853MB
[2022-11-07 19:22:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][950/1251]	eta 0:02:20 lr 0.000016	time 0.4654 (0.4682)	loss 3.4312 (2.8189)	grad_norm 2.6567 (inf)	mem 14853MB
[2022-11-07 19:22:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][1000/1251]	eta 0:01:57 lr 0.000016	time 0.4644 (0.4681)	loss 2.9140 (2.8160)	grad_norm 2.7287 (inf)	mem 14853MB
[2022-11-07 19:22:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][1050/1251]	eta 0:01:34 lr 0.000016	time 0.5342 (0.4682)	loss 2.4693 (2.8126)	grad_norm 3.2839 (inf)	mem 14853MB
[2022-11-07 19:23:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][1100/1251]	eta 0:01:10 lr 0.000016	time 0.4598 (0.4681)	loss 2.9254 (2.8137)	grad_norm 2.7327 (inf)	mem 14853MB
[2022-11-07 19:23:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][1150/1251]	eta 0:00:47 lr 0.000016	time 0.4673 (0.4679)	loss 3.0125 (2.8099)	grad_norm 2.7860 (inf)	mem 14853MB
[2022-11-07 19:24:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][1200/1251]	eta 0:00:23 lr 0.000016	time 0.4702 (0.4679)	loss 3.1371 (2.8110)	grad_norm 2.8374 (inf)	mem 14853MB
[2022-11-07 19:24:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [284/300][1250/1251]	eta 0:00:00 lr 0.000016	time 0.4568 (0.4677)	loss 3.0409 (2.8077)	grad_norm 3.0800 (inf)	mem 14853MB
[2022-11-07 19:24:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 284 training takes 0:09:45
[2022-11-07 19:24:30 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_284.pth saving......
[2022-11-07 19:24:31 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_284.pth saved !!!
[2022-11-07 19:24:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.697 (1.697)	Loss 0.7634 (0.7634)	Acc@1 83.398 (83.398)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 19:24:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.428 Acc@5 95.944
[2022-11-07 19:24:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-07 19:24:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.626 (1.626)	Loss 0.7993 (0.7993)	Acc@1 83.301 (83.301)	Acc@5 96.094 (96.094)	Mem 14853MB
[2022-11-07 19:24:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.444 Acc@5 96.002
[2022-11-07 19:24:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 19:24:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.44% at 284 epoch
[2022-11-07 19:24:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][0/1251]	eta 0:46:03 lr 0.000016	time 2.2094 (2.2094)	loss 2.8557 (2.8557)	grad_norm 3.8853 (3.8853)	mem 14853MB
[2022-11-07 19:25:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][50/1251]	eta 0:10:02 lr 0.000016	time 0.4691 (0.5019)	loss 2.9109 (2.8908)	grad_norm 3.7549 (3.0790)	mem 14853MB
[2022-11-07 19:25:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][100/1251]	eta 0:09:18 lr 0.000016	time 0.4566 (0.4855)	loss 3.3788 (2.8337)	grad_norm 3.4149 (3.0884)	mem 14853MB
[2022-11-07 19:26:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][150/1251]	eta 0:08:47 lr 0.000016	time 0.4680 (0.4792)	loss 3.2863 (2.8231)	grad_norm 3.0690 (3.0955)	mem 14853MB
[2022-11-07 19:26:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][200/1251]	eta 0:08:21 lr 0.000016	time 0.4636 (0.4770)	loss 2.6052 (2.7950)	grad_norm 3.1555 (3.0992)	mem 14853MB
[2022-11-07 19:26:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][250/1251]	eta 0:07:55 lr 0.000016	time 0.4706 (0.4751)	loss 1.9634 (2.7985)	grad_norm 2.8814 (3.1078)	mem 14853MB
[2022-11-07 19:27:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][300/1251]	eta 0:07:30 lr 0.000016	time 0.4695 (0.4732)	loss 2.1287 (2.7988)	grad_norm 2.8663 (3.1096)	mem 14853MB
[2022-11-07 19:27:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][350/1251]	eta 0:07:05 lr 0.000016	time 0.4633 (0.4724)	loss 2.8731 (2.8262)	grad_norm 2.9077 (3.1174)	mem 14853MB
[2022-11-07 19:27:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][400/1251]	eta 0:06:41 lr 0.000016	time 0.4619 (0.4716)	loss 2.6207 (2.8259)	grad_norm 3.2189 (3.1272)	mem 14853MB
[2022-11-07 19:28:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][450/1251]	eta 0:06:17 lr 0.000016	time 0.4661 (0.4709)	loss 3.1428 (2.8182)	grad_norm 3.9038 (3.1358)	mem 14853MB
[2022-11-07 19:28:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][500/1251]	eta 0:05:53 lr 0.000016	time 0.4740 (0.4707)	loss 2.6697 (2.8091)	grad_norm 2.6740 (3.1529)	mem 14853MB
[2022-11-07 19:29:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][550/1251]	eta 0:05:29 lr 0.000016	time 0.4609 (0.4703)	loss 3.1085 (2.8133)	grad_norm 3.0216 (3.1584)	mem 14853MB
[2022-11-07 19:29:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][600/1251]	eta 0:05:05 lr 0.000016	time 0.4646 (0.4699)	loss 3.1549 (2.8099)	grad_norm 2.9698 (3.1526)	mem 14853MB
[2022-11-07 19:29:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][650/1251]	eta 0:04:42 lr 0.000016	time 0.4575 (0.4697)	loss 3.2751 (2.8090)	grad_norm 3.2327 (3.1558)	mem 14853MB
[2022-11-07 19:30:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][700/1251]	eta 0:04:18 lr 0.000016	time 0.4660 (0.4695)	loss 3.3171 (2.8091)	grad_norm 3.1607 (3.1551)	mem 14853MB
[2022-11-07 19:30:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][750/1251]	eta 0:03:55 lr 0.000016	time 0.4618 (0.4692)	loss 3.3899 (2.8182)	grad_norm 3.3596 (3.1579)	mem 14853MB
[2022-11-07 19:31:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][800/1251]	eta 0:03:31 lr 0.000016	time 0.4670 (0.4691)	loss 3.2162 (2.8224)	grad_norm 3.0263 (3.1582)	mem 14853MB
[2022-11-07 19:31:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][850/1251]	eta 0:03:08 lr 0.000016	time 0.5367 (0.4690)	loss 3.1557 (2.8210)	grad_norm 3.6257 (3.1576)	mem 14853MB
[2022-11-07 19:31:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][900/1251]	eta 0:02:44 lr 0.000016	time 0.4692 (0.4688)	loss 3.2340 (2.8186)	grad_norm 3.4071 (inf)	mem 14853MB
[2022-11-07 19:32:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][950/1251]	eta 0:02:21 lr 0.000015	time 0.4614 (0.4688)	loss 2.0162 (2.8203)	grad_norm 3.4700 (inf)	mem 14853MB
[2022-11-07 19:32:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][1000/1251]	eta 0:01:57 lr 0.000015	time 0.4705 (0.4686)	loss 2.2830 (2.8234)	grad_norm 2.9147 (inf)	mem 14853MB
[2022-11-07 19:33:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][1050/1251]	eta 0:01:34 lr 0.000015	time 0.4719 (0.4685)	loss 2.6566 (2.8253)	grad_norm 2.8155 (inf)	mem 14853MB
[2022-11-07 19:33:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][1100/1251]	eta 0:01:10 lr 0.000015	time 0.4787 (0.4684)	loss 1.7414 (2.8278)	grad_norm 3.5942 (inf)	mem 14853MB
[2022-11-07 19:33:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][1150/1251]	eta 0:00:47 lr 0.000015	time 0.4705 (0.4684)	loss 3.1251 (2.8248)	grad_norm 2.8903 (inf)	mem 14853MB
[2022-11-07 19:34:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][1200/1251]	eta 0:00:23 lr 0.000015	time 0.4615 (0.4684)	loss 3.3731 (2.8274)	grad_norm 3.2323 (inf)	mem 14853MB
[2022-11-07 19:34:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [285/300][1250/1251]	eta 0:00:00 lr 0.000015	time 0.4569 (0.4681)	loss 2.7201 (2.8279)	grad_norm 3.0191 (inf)	mem 14853MB
[2022-11-07 19:34:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 285 training takes 0:09:45
[2022-11-07 19:34:34 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_285.pth saving......
[2022-11-07 19:34:35 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_285.pth saved !!!
[2022-11-07 19:34:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.574 (1.574)	Loss 0.8529 (0.8529)	Acc@1 81.055 (81.055)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 19:34:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.380 Acc@5 95.978
[2022-11-07 19:34:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-07 19:34:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.574 (1.574)	Loss 0.7565 (0.7565)	Acc@1 83.398 (83.398)	Acc@5 96.191 (96.191)	Mem 14853MB
[2022-11-07 19:34:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.442 Acc@5 96.014
[2022-11-07 19:34:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-07 19:34:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.44% at 284 epoch
[2022-11-07 19:34:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][0/1251]	eta 0:40:53 lr 0.000015	time 1.9609 (1.9609)	loss 3.1040 (3.1040)	grad_norm 3.3777 (3.3777)	mem 14853MB
[2022-11-07 19:35:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][50/1251]	eta 0:10:02 lr 0.000015	time 0.4661 (0.5019)	loss 2.6249 (2.8138)	grad_norm 3.0360 (3.2260)	mem 14853MB
[2022-11-07 19:35:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][100/1251]	eta 0:09:19 lr 0.000015	time 0.5614 (0.4859)	loss 2.0305 (2.8303)	grad_norm 2.9776 (3.1847)	mem 14853MB
[2022-11-07 19:36:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][150/1251]	eta 0:08:47 lr 0.000015	time 0.4720 (0.4791)	loss 3.0640 (2.8287)	grad_norm 2.8087 (3.1928)	mem 14853MB
[2022-11-07 19:36:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][200/1251]	eta 0:08:19 lr 0.000015	time 0.4589 (0.4754)	loss 3.1280 (2.8295)	grad_norm 3.4228 (3.1823)	mem 14853MB
[2022-11-07 19:36:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][250/1251]	eta 0:07:54 lr 0.000015	time 0.4680 (0.4737)	loss 3.0260 (2.8392)	grad_norm 2.9397 (3.1781)	mem 14853MB
[2022-11-07 19:37:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][300/1251]	eta 0:07:29 lr 0.000015	time 0.4600 (0.4727)	loss 3.1666 (2.8409)	grad_norm 3.2606 (3.1919)	mem 14853MB
[2022-11-07 19:37:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][350/1251]	eta 0:07:05 lr 0.000015	time 0.4667 (0.4720)	loss 2.2969 (2.8232)	grad_norm 3.0279 (3.1894)	mem 14853MB
[2022-11-07 19:38:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][400/1251]	eta 0:06:40 lr 0.000015	time 0.4562 (0.4711)	loss 3.0042 (2.8189)	grad_norm 3.1445 (3.1863)	mem 14853MB
[2022-11-07 19:38:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][450/1251]	eta 0:06:16 lr 0.000015	time 0.4653 (0.4705)	loss 3.3588 (2.8174)	grad_norm 4.3495 (3.1879)	mem 14853MB
[2022-11-07 19:38:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][500/1251]	eta 0:05:52 lr 0.000015	time 0.4819 (0.4700)	loss 3.1328 (2.8220)	grad_norm 3.2569 (3.1867)	mem 14853MB
[2022-11-07 19:39:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][550/1251]	eta 0:05:29 lr 0.000015	time 0.4661 (0.4698)	loss 3.3669 (2.8223)	grad_norm 3.3213 (3.1849)	mem 14853MB
[2022-11-07 19:39:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][600/1251]	eta 0:05:05 lr 0.000015	time 0.4524 (0.4695)	loss 2.4952 (2.8201)	grad_norm 3.4595 (3.1816)	mem 14853MB
[2022-11-07 19:39:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][650/1251]	eta 0:04:42 lr 0.000015	time 0.4644 (0.4693)	loss 2.9257 (2.8220)	grad_norm 3.8437 (3.1931)	mem 14853MB
[2022-11-07 19:40:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][700/1251]	eta 0:04:18 lr 0.000015	time 0.5268 (0.4692)	loss 3.1568 (2.8172)	grad_norm 3.2725 (3.1967)	mem 14853MB
[2022-11-07 19:40:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][750/1251]	eta 0:03:54 lr 0.000015	time 0.4627 (0.4689)	loss 3.0113 (2.8207)	grad_norm 3.0787 (3.2031)	mem 14853MB
[2022-11-07 19:41:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][800/1251]	eta 0:03:31 lr 0.000015	time 0.5374 (0.4691)	loss 2.1883 (2.8227)	grad_norm 3.1113 (3.2005)	mem 14853MB
[2022-11-07 19:41:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][850/1251]	eta 0:03:08 lr 0.000015	time 0.4626 (0.4690)	loss 2.7677 (2.8244)	grad_norm 2.8953 (3.1987)	mem 14853MB
[2022-11-07 19:41:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][900/1251]	eta 0:02:44 lr 0.000015	time 0.4732 (0.4688)	loss 3.0418 (2.8285)	grad_norm 3.6163 (3.1985)	mem 14853MB
[2022-11-07 19:42:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][950/1251]	eta 0:02:21 lr 0.000015	time 0.4581 (0.4688)	loss 3.3733 (2.8284)	grad_norm 3.3824 (3.2029)	mem 14853MB
[2022-11-07 19:42:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][1000/1251]	eta 0:01:57 lr 0.000015	time 0.4709 (0.4686)	loss 2.6286 (2.8257)	grad_norm 3.4117 (3.2048)	mem 14853MB
[2022-11-07 19:43:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][1050/1251]	eta 0:01:34 lr 0.000015	time 0.4610 (0.4687)	loss 2.1299 (2.8268)	grad_norm 3.2319 (3.2035)	mem 14853MB
[2022-11-07 19:43:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][1100/1251]	eta 0:01:10 lr 0.000015	time 0.4675 (0.4686)	loss 2.1805 (2.8249)	grad_norm 2.9736 (3.2029)	mem 14853MB
[2022-11-07 19:43:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][1150/1251]	eta 0:00:47 lr 0.000015	time 0.4661 (0.4685)	loss 2.3126 (2.8221)	grad_norm 3.1946 (3.2007)	mem 14853MB
[2022-11-07 19:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][1200/1251]	eta 0:00:23 lr 0.000015	time 0.5378 (0.4683)	loss 2.9386 (2.8218)	grad_norm 3.0596 (3.1969)	mem 14853MB
[2022-11-07 19:44:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [286/300][1250/1251]	eta 0:00:00 lr 0.000015	time 0.4562 (0.4681)	loss 3.1464 (2.8215)	grad_norm 3.4992 (3.1979)	mem 14853MB
[2022-11-07 19:44:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 286 training takes 0:09:45
[2022-11-07 19:44:38 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_286.pth saving......
[2022-11-07 19:44:39 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_286.pth saved !!!
[2022-11-07 19:44:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.530 (1.530)	Loss 0.7950 (0.7950)	Acc@1 82.520 (82.520)	Acc@5 95.020 (95.020)	Mem 14853MB
[2022-11-07 19:44:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.374 Acc@5 95.930
[2022-11-07 19:44:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-07 19:44:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.611 (1.611)	Loss 0.7868 (0.7868)	Acc@1 81.641 (81.641)	Acc@5 95.508 (95.508)	Mem 14853MB
[2022-11-07 19:44:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.470 Acc@5 96.012
[2022-11-07 19:44:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 19:44:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.47% at 286 epoch
[2022-11-07 19:44:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][0/1251]	eta 0:41:26 lr 0.000015	time 1.9873 (1.9873)	loss 2.2563 (2.2563)	grad_norm 3.4151 (3.4151)	mem 14853MB
[2022-11-07 19:45:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][50/1251]	eta 0:09:59 lr 0.000015	time 0.4602 (0.4996)	loss 3.2747 (2.8853)	grad_norm 4.0920 (3.1237)	mem 14853MB
[2022-11-07 19:45:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][100/1251]	eta 0:09:17 lr 0.000015	time 0.4681 (0.4845)	loss 3.1873 (2.8223)	grad_norm 4.1402 (3.1693)	mem 14853MB
[2022-11-07 19:46:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][150/1251]	eta 0:08:47 lr 0.000014	time 0.4524 (0.4794)	loss 2.7528 (2.8221)	grad_norm 2.8413 (3.2186)	mem 14853MB
[2022-11-07 19:46:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][200/1251]	eta 0:08:20 lr 0.000014	time 0.4719 (0.4762)	loss 3.2579 (2.8312)	grad_norm 3.2639 (3.1949)	mem 14853MB
[2022-11-07 19:46:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][250/1251]	eta 0:07:54 lr 0.000014	time 0.4556 (0.4745)	loss 3.1366 (2.8307)	grad_norm 2.8991 (3.1897)	mem 14853MB
[2022-11-07 19:47:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][300/1251]	eta 0:07:29 lr 0.000014	time 0.4702 (0.4730)	loss 3.0937 (2.8304)	grad_norm 3.5423 (3.1914)	mem 14853MB
[2022-11-07 19:47:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][350/1251]	eta 0:07:05 lr 0.000014	time 0.4694 (0.4720)	loss 3.1458 (2.8298)	grad_norm 2.6162 (3.1933)	mem 14853MB
[2022-11-07 19:48:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][400/1251]	eta 0:06:40 lr 0.000014	time 0.4582 (0.4711)	loss 3.3360 (2.8417)	grad_norm 3.3607 (3.2010)	mem 14853MB
[2022-11-07 19:48:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][450/1251]	eta 0:06:16 lr 0.000014	time 0.4616 (0.4704)	loss 2.8492 (2.8419)	grad_norm 3.8489 (3.2091)	mem 14853MB
[2022-11-07 19:48:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][500/1251]	eta 0:05:53 lr 0.000014	time 0.4679 (0.4701)	loss 3.2361 (2.8451)	grad_norm 2.9411 (inf)	mem 14853MB
[2022-11-07 19:49:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][550/1251]	eta 0:05:29 lr 0.000014	time 0.4660 (0.4702)	loss 3.1712 (2.8420)	grad_norm 3.3634 (inf)	mem 14853MB
[2022-11-07 19:49:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][600/1251]	eta 0:05:05 lr 0.000014	time 0.4618 (0.4698)	loss 2.8820 (2.8418)	grad_norm 3.7359 (inf)	mem 14853MB
[2022-11-07 19:50:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][650/1251]	eta 0:04:42 lr 0.000014	time 0.4667 (0.4695)	loss 2.8348 (2.8380)	grad_norm 3.5610 (inf)	mem 14853MB
[2022-11-07 19:50:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][700/1251]	eta 0:04:18 lr 0.000014	time 0.4652 (0.4693)	loss 3.3238 (2.8462)	grad_norm 3.5626 (inf)	mem 14853MB
[2022-11-07 19:50:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][750/1251]	eta 0:03:54 lr 0.000014	time 0.4685 (0.4690)	loss 3.3878 (2.8466)	grad_norm 3.6253 (inf)	mem 14853MB
[2022-11-07 19:51:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][800/1251]	eta 0:03:31 lr 0.000014	time 0.4609 (0.4690)	loss 2.6996 (2.8488)	grad_norm 3.9667 (inf)	mem 14853MB
[2022-11-07 19:51:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][850/1251]	eta 0:03:08 lr 0.000014	time 0.4654 (0.4689)	loss 2.8805 (2.8384)	grad_norm 2.9561 (inf)	mem 14853MB
[2022-11-07 19:51:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][900/1251]	eta 0:02:44 lr 0.000014	time 0.5443 (0.4688)	loss 2.8132 (2.8401)	grad_norm 3.1425 (inf)	mem 14853MB
[2022-11-07 19:52:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][950/1251]	eta 0:02:21 lr 0.000014	time 0.4622 (0.4687)	loss 2.2010 (2.8392)	grad_norm 2.9202 (inf)	mem 14853MB
[2022-11-07 19:52:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][1000/1251]	eta 0:01:57 lr 0.000014	time 0.4565 (0.4685)	loss 3.1354 (2.8316)	grad_norm 2.6369 (inf)	mem 14853MB
[2022-11-07 19:53:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][1050/1251]	eta 0:01:34 lr 0.000014	time 0.4631 (0.4686)	loss 2.6679 (2.8279)	grad_norm 2.8345 (inf)	mem 14853MB
[2022-11-07 19:53:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][1100/1251]	eta 0:01:10 lr 0.000014	time 0.4671 (0.4686)	loss 3.1583 (2.8230)	grad_norm 3.1269 (inf)	mem 14853MB
[2022-11-07 19:53:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][1150/1251]	eta 0:00:47 lr 0.000014	time 0.4602 (0.4685)	loss 3.2616 (2.8258)	grad_norm 3.9737 (inf)	mem 14853MB
[2022-11-07 19:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][1200/1251]	eta 0:00:23 lr 0.000014	time 0.4725 (0.4683)	loss 2.5684 (2.8281)	grad_norm 3.1747 (inf)	mem 14853MB
[2022-11-07 19:54:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [287/300][1250/1251]	eta 0:00:00 lr 0.000014	time 0.4563 (0.4682)	loss 2.9756 (2.8279)	grad_norm 2.7472 (inf)	mem 14853MB
[2022-11-07 19:54:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 287 training takes 0:09:45
[2022-11-07 19:54:42 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_287.pth saving......
[2022-11-07 19:54:43 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_287.pth saved !!!
[2022-11-07 19:54:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.700 (1.700)	Loss 0.8093 (0.8093)	Acc@1 82.617 (82.617)	Acc@5 95.410 (95.410)	Mem 14853MB
[2022-11-07 19:54:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.354 Acc@5 95.942
[2022-11-07 19:54:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-07 19:54:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.586 (1.586)	Loss 0.8372 (0.8372)	Acc@1 82.617 (82.617)	Acc@5 94.531 (94.531)	Mem 14853MB
[2022-11-07 19:55:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.474 Acc@5 96.006
[2022-11-07 19:55:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 19:55:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.47% at 287 epoch
[2022-11-07 19:55:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][0/1251]	eta 0:41:45 lr 0.000014	time 2.0025 (2.0025)	loss 2.6140 (2.6140)	grad_norm 3.4821 (3.4821)	mem 14853MB
[2022-11-07 19:55:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][50/1251]	eta 0:10:04 lr 0.000014	time 0.4641 (0.5032)	loss 2.6223 (2.7794)	grad_norm 3.4914 (3.2318)	mem 14853MB
[2022-11-07 19:55:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][100/1251]	eta 0:09:19 lr 0.000014	time 0.4615 (0.4857)	loss 2.5343 (2.7993)	grad_norm 2.8488 (3.2074)	mem 14853MB
[2022-11-07 19:56:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][150/1251]	eta 0:08:48 lr 0.000014	time 0.4678 (0.4798)	loss 3.1226 (2.8286)	grad_norm 3.0130 (3.2109)	mem 14853MB
[2022-11-07 19:56:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][200/1251]	eta 0:08:20 lr 0.000014	time 0.4643 (0.4764)	loss 2.7961 (2.8034)	grad_norm 3.2687 (3.2126)	mem 14853MB
[2022-11-07 19:56:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][250/1251]	eta 0:07:54 lr 0.000014	time 0.4724 (0.4744)	loss 3.0119 (2.8176)	grad_norm 3.1945 (3.2004)	mem 14853MB
[2022-11-07 19:57:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][300/1251]	eta 0:07:29 lr 0.000014	time 0.4683 (0.4730)	loss 2.7417 (2.8031)	grad_norm 2.7625 (3.2170)	mem 14853MB
[2022-11-07 19:57:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][350/1251]	eta 0:07:05 lr 0.000014	time 0.4659 (0.4722)	loss 2.9706 (2.8015)	grad_norm 2.8436 (3.2119)	mem 14853MB
[2022-11-07 19:58:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][400/1251]	eta 0:06:41 lr 0.000014	time 0.4661 (0.4712)	loss 3.1640 (2.7999)	grad_norm 3.0534 (3.2124)	mem 14853MB
[2022-11-07 19:58:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][450/1251]	eta 0:06:16 lr 0.000014	time 0.4672 (0.4704)	loss 2.8730 (2.8118)	grad_norm 2.9339 (inf)	mem 14853MB
[2022-11-07 19:58:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][500/1251]	eta 0:05:52 lr 0.000014	time 0.4629 (0.4698)	loss 1.9576 (2.8099)	grad_norm 3.0248 (inf)	mem 14853MB
[2022-11-07 19:59:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][550/1251]	eta 0:05:29 lr 0.000014	time 0.4680 (0.4695)	loss 3.4274 (2.8097)	grad_norm 3.3486 (inf)	mem 14853MB
[2022-11-07 19:59:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][600/1251]	eta 0:05:05 lr 0.000014	time 0.4693 (0.4695)	loss 3.0972 (2.8176)	grad_norm 3.1418 (inf)	mem 14853MB
[2022-11-07 20:00:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][650/1251]	eta 0:04:42 lr 0.000014	time 0.4756 (0.4693)	loss 2.3285 (2.8224)	grad_norm 3.0600 (inf)	mem 14853MB
[2022-11-07 20:00:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][700/1251]	eta 0:04:18 lr 0.000014	time 0.5343 (0.4692)	loss 2.9959 (2.8156)	grad_norm 2.8742 (inf)	mem 14853MB
[2022-11-07 20:00:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][750/1251]	eta 0:03:54 lr 0.000014	time 0.4725 (0.4689)	loss 2.9391 (2.8137)	grad_norm 2.9458 (inf)	mem 14853MB
[2022-11-07 20:01:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][800/1251]	eta 0:03:31 lr 0.000013	time 0.4594 (0.4690)	loss 1.9268 (2.8125)	grad_norm 3.6830 (inf)	mem 14853MB
[2022-11-07 20:01:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][850/1251]	eta 0:03:08 lr 0.000013	time 0.4744 (0.4690)	loss 2.5733 (2.8105)	grad_norm 3.3363 (inf)	mem 14853MB
[2022-11-07 20:02:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][900/1251]	eta 0:02:44 lr 0.000013	time 0.4629 (0.4688)	loss 3.1297 (2.8154)	grad_norm 3.1260 (inf)	mem 14853MB
[2022-11-07 20:02:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][950/1251]	eta 0:02:21 lr 0.000013	time 0.4551 (0.4687)	loss 3.4582 (2.8200)	grad_norm 2.9007 (inf)	mem 14853MB
[2022-11-07 20:02:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][1000/1251]	eta 0:01:57 lr 0.000013	time 0.4715 (0.4686)	loss 2.3154 (2.8108)	grad_norm 4.1157 (inf)	mem 14853MB
[2022-11-07 20:03:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][1050/1251]	eta 0:01:34 lr 0.000013	time 0.4676 (0.4686)	loss 2.1965 (2.8083)	grad_norm 3.2302 (inf)	mem 14853MB
[2022-11-07 20:03:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][1100/1251]	eta 0:01:10 lr 0.000013	time 0.4639 (0.4686)	loss 3.2946 (2.8122)	grad_norm 3.4449 (inf)	mem 14853MB
[2022-11-07 20:04:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][1150/1251]	eta 0:00:47 lr 0.000013	time 0.4621 (0.4685)	loss 2.0522 (2.8066)	grad_norm 3.1732 (inf)	mem 14853MB
[2022-11-07 20:04:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][1200/1251]	eta 0:00:23 lr 0.000013	time 0.5317 (0.4684)	loss 2.6495 (2.8098)	grad_norm 2.8595 (inf)	mem 14853MB
[2022-11-07 20:04:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [288/300][1250/1251]	eta 0:00:00 lr 0.000013	time 0.4578 (0.4682)	loss 2.8318 (2.8111)	grad_norm 2.9467 (inf)	mem 14853MB
[2022-11-07 20:04:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 288 training takes 0:09:45
[2022-11-07 20:04:46 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_288.pth saving......
[2022-11-07 20:04:47 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_288.pth saved !!!
[2022-11-07 20:04:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.601 (1.601)	Loss 0.8034 (0.8034)	Acc@1 81.445 (81.445)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 20:04:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.422 Acc@5 95.950
[2022-11-07 20:04:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-07 20:04:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.645 (1.645)	Loss 0.7434 (0.7434)	Acc@1 84.082 (84.082)	Acc@5 95.605 (95.605)	Mem 14853MB
[2022-11-07 20:05:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.466 Acc@5 96.000
[2022-11-07 20:05:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 20:05:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.47% at 287 epoch
[2022-11-07 20:05:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][0/1251]	eta 0:41:42 lr 0.000013	time 2.0007 (2.0007)	loss 2.8799 (2.8799)	grad_norm 2.8102 (2.8102)	mem 14853MB
[2022-11-07 20:05:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][50/1251]	eta 0:10:01 lr 0.000013	time 0.4591 (0.5007)	loss 2.0044 (2.7026)	grad_norm 3.7969 (3.1458)	mem 14853MB
[2022-11-07 20:05:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][100/1251]	eta 0:09:18 lr 0.000013	time 0.4678 (0.4852)	loss 2.0761 (2.7389)	grad_norm 3.0757 (3.1769)	mem 14853MB
[2022-11-07 20:06:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][150/1251]	eta 0:08:47 lr 0.000013	time 0.4651 (0.4791)	loss 2.8954 (2.7596)	grad_norm 3.0945 (3.1772)	mem 14853MB
[2022-11-07 20:06:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][200/1251]	eta 0:08:19 lr 0.000013	time 0.4522 (0.4755)	loss 2.0999 (2.7683)	grad_norm 2.9241 (3.1807)	mem 14853MB
[2022-11-07 20:07:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][250/1251]	eta 0:07:54 lr 0.000013	time 0.4643 (0.4742)	loss 2.2820 (2.7993)	grad_norm 2.9382 (3.1921)	mem 14853MB
[2022-11-07 20:07:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][300/1251]	eta 0:07:29 lr 0.000013	time 0.4558 (0.4731)	loss 3.1078 (2.8058)	grad_norm 3.2415 (3.1963)	mem 14853MB
[2022-11-07 20:07:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][350/1251]	eta 0:07:05 lr 0.000013	time 0.4719 (0.4720)	loss 2.8885 (2.7949)	grad_norm 3.0401 (3.2010)	mem 14853MB
[2022-11-07 20:08:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][400/1251]	eta 0:06:41 lr 0.000013	time 0.4721 (0.4712)	loss 2.8421 (2.7889)	grad_norm 3.3293 (3.2085)	mem 14853MB
[2022-11-07 20:08:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][450/1251]	eta 0:06:16 lr 0.000013	time 0.4738 (0.4705)	loss 2.0539 (2.7809)	grad_norm 3.2870 (3.2132)	mem 14853MB
[2022-11-07 20:09:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][500/1251]	eta 0:05:53 lr 0.000013	time 0.4578 (0.4701)	loss 3.1269 (2.7861)	grad_norm 3.1515 (3.2128)	mem 14853MB
[2022-11-07 20:09:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][550/1251]	eta 0:05:29 lr 0.000013	time 0.4626 (0.4700)	loss 2.8495 (2.7930)	grad_norm 3.3726 (3.2301)	mem 14853MB
[2022-11-07 20:09:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][600/1251]	eta 0:05:05 lr 0.000013	time 0.4684 (0.4698)	loss 2.8487 (2.7928)	grad_norm 2.8879 (3.2298)	mem 14853MB
[2022-11-07 20:10:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][650/1251]	eta 0:04:42 lr 0.000013	time 0.4616 (0.4696)	loss 2.9884 (2.7876)	grad_norm 3.3144 (3.2287)	mem 14853MB
[2022-11-07 20:10:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][700/1251]	eta 0:04:18 lr 0.000013	time 0.4611 (0.4693)	loss 3.2843 (2.7907)	grad_norm 3.9361 (3.2294)	mem 14853MB
[2022-11-07 20:10:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][750/1251]	eta 0:03:55 lr 0.000013	time 0.4707 (0.4691)	loss 3.2144 (2.7930)	grad_norm 3.0728 (3.2320)	mem 14853MB
[2022-11-07 20:11:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][800/1251]	eta 0:03:31 lr 0.000013	time 0.4601 (0.4692)	loss 3.0301 (2.7941)	grad_norm 3.1272 (3.2410)	mem 14853MB
[2022-11-07 20:11:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][850/1251]	eta 0:03:08 lr 0.000013	time 0.4662 (0.4691)	loss 2.6821 (2.7969)	grad_norm 3.5529 (3.2417)	mem 14853MB
[2022-11-07 20:12:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][900/1251]	eta 0:02:44 lr 0.000013	time 0.4642 (0.4690)	loss 2.2001 (2.7990)	grad_norm 3.3168 (3.2369)	mem 14853MB
[2022-11-07 20:12:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][950/1251]	eta 0:02:21 lr 0.000013	time 0.4657 (0.4688)	loss 2.9075 (2.8044)	grad_norm 2.6785 (3.2361)	mem 14853MB
[2022-11-07 20:12:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][1000/1251]	eta 0:01:57 lr 0.000013	time 0.4540 (0.4687)	loss 2.8173 (2.8032)	grad_norm 2.8701 (3.2367)	mem 14853MB
[2022-11-07 20:13:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][1050/1251]	eta 0:01:34 lr 0.000013	time 0.4622 (0.4687)	loss 2.6772 (2.8032)	grad_norm 3.3783 (3.2367)	mem 14853MB
[2022-11-07 20:13:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][1100/1251]	eta 0:01:10 lr 0.000013	time 0.4596 (0.4687)	loss 2.9060 (2.8017)	grad_norm 3.0883 (3.2350)	mem 14853MB
[2022-11-07 20:14:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][1150/1251]	eta 0:00:47 lr 0.000013	time 0.4618 (0.4686)	loss 1.9154 (2.7994)	grad_norm 3.0617 (3.2327)	mem 14853MB
[2022-11-07 20:14:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][1200/1251]	eta 0:00:23 lr 0.000013	time 0.4587 (0.4684)	loss 3.1217 (2.8014)	grad_norm 3.1788 (3.2364)	mem 14853MB
[2022-11-07 20:14:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [289/300][1250/1251]	eta 0:00:00 lr 0.000013	time 0.4575 (0.4683)	loss 3.2897 (2.8013)	grad_norm 3.6966 (3.2403)	mem 14853MB
[2022-11-07 20:14:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 289 training takes 0:09:45
[2022-11-07 20:14:50 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_289.pth saving......
[2022-11-07 20:14:51 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_289.pth saved !!!
[2022-11-07 20:14:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.531 (1.531)	Loss 0.8161 (0.8161)	Acc@1 80.957 (80.957)	Acc@5 95.898 (95.898)	Mem 14853MB
[2022-11-07 20:15:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.374 Acc@5 95.914
[2022-11-07 20:15:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-07 20:15:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.556 (1.556)	Loss 0.7295 (0.7295)	Acc@1 83.105 (83.105)	Acc@5 96.289 (96.289)	Mem 14853MB
[2022-11-07 20:15:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.456 Acc@5 96.004
[2022-11-07 20:15:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 20:15:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.47% at 287 epoch
[2022-11-07 20:15:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][0/1251]	eta 0:40:52 lr 0.000013	time 1.9604 (1.9604)	loss 2.9262 (2.9262)	grad_norm 3.1014 (3.1014)	mem 14853MB
[2022-11-07 20:15:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][50/1251]	eta 0:09:59 lr 0.000013	time 0.4565 (0.4996)	loss 2.6305 (2.7746)	grad_norm 2.8917 (3.2476)	mem 14853MB
[2022-11-07 20:15:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][100/1251]	eta 0:09:20 lr 0.000013	time 0.4687 (0.4874)	loss 2.0067 (2.7649)	grad_norm 3.5174 (3.2678)	mem 14853MB
[2022-11-07 20:16:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][150/1251]	eta 0:08:49 lr 0.000013	time 0.4733 (0.4809)	loss 3.4225 (2.7838)	grad_norm 3.3744 (3.2837)	mem 14853MB
[2022-11-07 20:16:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][200/1251]	eta 0:08:21 lr 0.000013	time 0.4666 (0.4770)	loss 1.8837 (2.7992)	grad_norm 2.9582 (3.2637)	mem 14853MB
[2022-11-07 20:17:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][250/1251]	eta 0:07:55 lr 0.000013	time 0.4719 (0.4748)	loss 3.0921 (2.7869)	grad_norm 3.0322 (3.2418)	mem 14853MB
[2022-11-07 20:17:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][300/1251]	eta 0:07:29 lr 0.000013	time 0.4598 (0.4732)	loss 2.5224 (2.7846)	grad_norm 3.1412 (3.2342)	mem 14853MB
[2022-11-07 20:17:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][350/1251]	eta 0:07:05 lr 0.000013	time 0.4696 (0.4720)	loss 2.7729 (2.8022)	grad_norm 3.4049 (3.2318)	mem 14853MB
[2022-11-07 20:18:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][400/1251]	eta 0:06:41 lr 0.000013	time 0.4646 (0.4715)	loss 1.9170 (2.8064)	grad_norm 2.9408 (3.2367)	mem 14853MB
[2022-11-07 20:18:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][450/1251]	eta 0:06:17 lr 0.000013	time 0.4656 (0.4708)	loss 2.9684 (2.8104)	grad_norm 3.2259 (3.2326)	mem 14853MB
[2022-11-07 20:19:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][500/1251]	eta 0:05:53 lr 0.000012	time 0.4733 (0.4701)	loss 2.7783 (2.8131)	grad_norm 3.7138 (3.2328)	mem 14853MB
[2022-11-07 20:19:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][550/1251]	eta 0:05:29 lr 0.000012	time 0.4630 (0.4699)	loss 2.7039 (2.8096)	grad_norm 3.0488 (3.2351)	mem 14853MB
[2022-11-07 20:19:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][600/1251]	eta 0:05:05 lr 0.000012	time 0.4669 (0.4699)	loss 3.0679 (2.8132)	grad_norm 3.3250 (3.2311)	mem 14853MB
[2022-11-07 20:20:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][650/1251]	eta 0:04:42 lr 0.000012	time 0.4730 (0.4696)	loss 3.1589 (2.8106)	grad_norm 2.8698 (3.2389)	mem 14853MB
[2022-11-07 20:20:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][700/1251]	eta 0:04:18 lr 0.000012	time 0.4590 (0.4692)	loss 2.3770 (2.8084)	grad_norm 3.1397 (3.2385)	mem 14853MB
[2022-11-07 20:21:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][750/1251]	eta 0:03:54 lr 0.000012	time 0.4614 (0.4689)	loss 2.1172 (2.8021)	grad_norm 3.1932 (3.2358)	mem 14853MB
[2022-11-07 20:21:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][800/1251]	eta 0:03:31 lr 0.000012	time 0.4741 (0.4689)	loss 3.1229 (2.8002)	grad_norm 3.3530 (3.2416)	mem 14853MB
[2022-11-07 20:21:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][850/1251]	eta 0:03:08 lr 0.000012	time 0.4692 (0.4689)	loss 3.3235 (2.8054)	grad_norm 2.6362 (3.2438)	mem 14853MB
[2022-11-07 20:22:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][900/1251]	eta 0:02:44 lr 0.000012	time 0.4564 (0.4688)	loss 3.4511 (2.8027)	grad_norm 3.6835 (3.2426)	mem 14853MB
[2022-11-07 20:22:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][950/1251]	eta 0:02:21 lr 0.000012	time 0.4721 (0.4686)	loss 2.1682 (2.8065)	grad_norm 3.2096 (3.2392)	mem 14853MB
[2022-11-07 20:22:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][1000/1251]	eta 0:01:57 lr 0.000012	time 0.4605 (0.4684)	loss 3.1357 (2.8042)	grad_norm 3.5957 (3.2400)	mem 14853MB
[2022-11-07 20:23:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][1050/1251]	eta 0:01:34 lr 0.000012	time 0.4652 (0.4684)	loss 3.0134 (2.8028)	grad_norm 3.5178 (3.2403)	mem 14853MB
[2022-11-07 20:23:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][1100/1251]	eta 0:01:10 lr 0.000012	time 0.4639 (0.4684)	loss 2.2938 (2.7997)	grad_norm 3.4192 (3.2409)	mem 14853MB
[2022-11-07 20:24:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][1150/1251]	eta 0:00:47 lr 0.000012	time 0.4608 (0.4684)	loss 2.4176 (2.8004)	grad_norm 3.1854 (3.2413)	mem 14853MB
[2022-11-07 20:24:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][1200/1251]	eta 0:00:23 lr 0.000012	time 0.4556 (0.4683)	loss 2.3599 (2.7973)	grad_norm 3.3172 (3.2416)	mem 14853MB
[2022-11-07 20:24:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [290/300][1250/1251]	eta 0:00:00 lr 0.000012	time 0.4571 (0.4681)	loss 3.2222 (2.7983)	grad_norm 3.9110 (3.2406)	mem 14853MB
[2022-11-07 20:24:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 290 training takes 0:09:45
[2022-11-07 20:24:54 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_290.pth saving......
[2022-11-07 20:24:55 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_290.pth saved !!!
[2022-11-07 20:24:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.501 (1.501)	Loss 0.7106 (0.7106)	Acc@1 83.301 (83.301)	Acc@5 97.070 (97.070)	Mem 14853MB
[2022-11-07 20:25:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.472 Acc@5 95.930
[2022-11-07 20:25:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.5%
[2022-11-07 20:25:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.679 (1.679)	Loss 0.7109 (0.7109)	Acc@1 84.082 (84.082)	Acc@5 96.582 (96.582)	Mem 14853MB
[2022-11-07 20:25:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.462 Acc@5 96.004
[2022-11-07 20:25:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 20:25:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.47% at 287 epoch
[2022-11-07 20:25:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][0/1251]	eta 0:40:29 lr 0.000012	time 1.9423 (1.9423)	loss 3.0467 (3.0467)	grad_norm 3.3741 (3.3741)	mem 14853MB
[2022-11-07 20:25:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][50/1251]	eta 0:10:03 lr 0.000012	time 0.4650 (0.5022)	loss 2.3867 (2.7714)	grad_norm 2.8880 (3.2068)	mem 14853MB
[2022-11-07 20:26:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][100/1251]	eta 0:09:19 lr 0.000012	time 0.4578 (0.4862)	loss 2.3759 (2.7959)	grad_norm 3.0795 (3.2003)	mem 14853MB
[2022-11-07 20:26:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][150/1251]	eta 0:08:48 lr 0.000012	time 0.4676 (0.4797)	loss 3.1547 (2.8065)	grad_norm 3.2191 (3.2144)	mem 14853MB
[2022-11-07 20:26:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][200/1251]	eta 0:08:20 lr 0.000012	time 0.4626 (0.4766)	loss 2.7923 (2.8078)	grad_norm 3.2313 (3.2257)	mem 14853MB
[2022-11-07 20:27:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][250/1251]	eta 0:07:54 lr 0.000012	time 0.4774 (0.4745)	loss 2.7447 (2.8343)	grad_norm 3.1216 (3.2303)	mem 14853MB
[2022-11-07 20:27:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][300/1251]	eta 0:07:29 lr 0.000012	time 0.4654 (0.4730)	loss 2.2378 (2.8374)	grad_norm 4.0918 (3.2252)	mem 14853MB
[2022-11-07 20:27:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][350/1251]	eta 0:07:05 lr 0.000012	time 0.4635 (0.4721)	loss 2.4713 (2.8195)	grad_norm 3.1328 (3.2372)	mem 14853MB
[2022-11-07 20:28:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][400/1251]	eta 0:06:41 lr 0.000012	time 0.4723 (0.4712)	loss 3.0577 (2.8204)	grad_norm 3.2302 (3.2385)	mem 14853MB
[2022-11-07 20:28:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][450/1251]	eta 0:06:16 lr 0.000012	time 0.4557 (0.4705)	loss 2.7272 (2.8208)	grad_norm 3.1144 (3.2415)	mem 14853MB
[2022-11-07 20:29:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][500/1251]	eta 0:05:53 lr 0.000012	time 0.4636 (0.4703)	loss 3.2848 (2.8117)	grad_norm 2.9164 (3.2372)	mem 14853MB
[2022-11-07 20:29:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][550/1251]	eta 0:05:29 lr 0.000012	time 0.5382 (0.4702)	loss 3.1631 (2.8017)	grad_norm 3.1397 (3.2370)	mem 14853MB
[2022-11-07 20:29:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][600/1251]	eta 0:05:05 lr 0.000012	time 0.4606 (0.4699)	loss 1.6671 (2.8006)	grad_norm 2.9583 (3.2434)	mem 14853MB
[2022-11-07 20:30:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][650/1251]	eta 0:04:42 lr 0.000012	time 0.4593 (0.4695)	loss 2.5980 (2.7980)	grad_norm 3.5281 (3.2444)	mem 14853MB
[2022-11-07 20:30:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][700/1251]	eta 0:04:18 lr 0.000012	time 0.4725 (0.4691)	loss 3.3012 (2.8064)	grad_norm 3.0296 (inf)	mem 14853MB
[2022-11-07 20:31:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][750/1251]	eta 0:03:54 lr 0.000012	time 0.4621 (0.4688)	loss 3.0135 (2.8081)	grad_norm 2.9730 (inf)	mem 14853MB
[2022-11-07 20:31:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][800/1251]	eta 0:03:31 lr 0.000012	time 0.4652 (0.4690)	loss 2.4503 (2.8081)	grad_norm 3.8943 (inf)	mem 14853MB
[2022-11-07 20:31:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][850/1251]	eta 0:03:08 lr 0.000012	time 0.4767 (0.4689)	loss 2.2008 (2.8080)	grad_norm 3.7700 (inf)	mem 14853MB
[2022-11-07 20:32:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][900/1251]	eta 0:02:44 lr 0.000012	time 0.4760 (0.4687)	loss 2.7040 (2.8138)	grad_norm 3.2626 (inf)	mem 14853MB
[2022-11-07 20:32:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][950/1251]	eta 0:02:21 lr 0.000012	time 0.4692 (0.4685)	loss 3.2580 (2.8168)	grad_norm 3.0515 (inf)	mem 14853MB
[2022-11-07 20:33:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][1000/1251]	eta 0:01:57 lr 0.000012	time 0.5438 (0.4684)	loss 2.9988 (2.8185)	grad_norm 3.1112 (inf)	mem 14853MB
[2022-11-07 20:33:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][1050/1251]	eta 0:01:34 lr 0.000012	time 0.4580 (0.4684)	loss 3.2662 (2.8228)	grad_norm 3.0407 (inf)	mem 14853MB
[2022-11-07 20:33:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][1100/1251]	eta 0:01:10 lr 0.000012	time 0.4743 (0.4684)	loss 2.2946 (2.8226)	grad_norm 3.4633 (inf)	mem 14853MB
[2022-11-07 20:34:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][1150/1251]	eta 0:00:47 lr 0.000012	time 0.4554 (0.4683)	loss 2.8314 (2.8230)	grad_norm 2.7909 (inf)	mem 14853MB
[2022-11-07 20:34:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][1200/1251]	eta 0:00:23 lr 0.000012	time 0.4721 (0.4681)	loss 3.3496 (2.8209)	grad_norm 3.8754 (inf)	mem 14853MB
[2022-11-07 20:34:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [291/300][1250/1251]	eta 0:00:00 lr 0.000012	time 0.4557 (0.4680)	loss 2.9941 (2.8228)	grad_norm 3.2051 (inf)	mem 14853MB
[2022-11-07 20:34:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 291 training takes 0:09:45
[2022-11-07 20:34:58 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_291.pth saving......
[2022-11-07 20:34:58 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_291.pth saved !!!
[2022-11-07 20:35:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.568 (1.568)	Loss 0.7791 (0.7791)	Acc@1 82.715 (82.715)	Acc@5 95.801 (95.801)	Mem 14853MB
[2022-11-07 20:35:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.430 Acc@5 95.940
[2022-11-07 20:35:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-07 20:35:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.754 (1.754)	Loss 0.6597 (0.6597)	Acc@1 84.863 (84.863)	Acc@5 95.996 (95.996)	Mem 14853MB
[2022-11-07 20:35:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.472 Acc@5 95.986
[2022-11-07 20:35:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 20:35:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.47% at 287 epoch
[2022-11-07 20:35:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][0/1251]	eta 0:40:11 lr 0.000012	time 1.9276 (1.9276)	loss 3.4763 (3.4763)	grad_norm 3.2812 (3.2812)	mem 14854MB
[2022-11-07 20:35:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][50/1251]	eta 0:09:57 lr 0.000012	time 0.4698 (0.4972)	loss 3.2821 (2.8564)	grad_norm 3.2973 (3.1499)	mem 14854MB
[2022-11-07 20:36:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][100/1251]	eta 0:09:18 lr 0.000012	time 0.4698 (0.4849)	loss 3.1208 (2.8622)	grad_norm 4.1742 (3.1899)	mem 14854MB
[2022-11-07 20:36:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][150/1251]	eta 0:08:46 lr 0.000012	time 0.4642 (0.4783)	loss 2.9108 (2.8825)	grad_norm 2.9510 (3.1571)	mem 14854MB
[2022-11-07 20:36:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][200/1251]	eta 0:08:20 lr 0.000012	time 0.4606 (0.4758)	loss 2.9702 (2.8873)	grad_norm 3.2434 (3.1824)	mem 14854MB
[2022-11-07 20:37:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][250/1251]	eta 0:07:54 lr 0.000012	time 0.4667 (0.4741)	loss 3.3086 (2.8823)	grad_norm 3.3890 (3.1855)	mem 14854MB
[2022-11-07 20:37:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][300/1251]	eta 0:07:29 lr 0.000012	time 0.4635 (0.4726)	loss 2.8334 (2.8741)	grad_norm 3.0484 (3.1907)	mem 14854MB
[2022-11-07 20:38:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][350/1251]	eta 0:07:05 lr 0.000012	time 0.4634 (0.4719)	loss 3.2580 (2.8585)	grad_norm 2.9163 (3.2057)	mem 14854MB
[2022-11-07 20:38:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][400/1251]	eta 0:06:40 lr 0.000012	time 0.4716 (0.4712)	loss 3.1633 (2.8591)	grad_norm 2.8921 (3.2033)	mem 14854MB
[2022-11-07 20:38:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][450/1251]	eta 0:06:16 lr 0.000012	time 0.4695 (0.4704)	loss 2.9773 (2.8684)	grad_norm 2.9967 (3.1976)	mem 14854MB
[2022-11-07 20:39:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][500/1251]	eta 0:05:52 lr 0.000012	time 0.4599 (0.4699)	loss 3.0222 (2.8710)	grad_norm 3.3864 (3.2014)	mem 14854MB
[2022-11-07 20:39:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][550/1251]	eta 0:05:29 lr 0.000012	time 0.4608 (0.4696)	loss 2.6571 (2.8728)	grad_norm 2.6721 (3.2014)	mem 14854MB
[2022-11-07 20:39:58 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][600/1251]	eta 0:05:05 lr 0.000012	time 0.5596 (0.4695)	loss 2.7356 (2.8669)	grad_norm 3.0335 (3.2085)	mem 14854MB
[2022-11-07 20:40:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][650/1251]	eta 0:04:41 lr 0.000012	time 0.4564 (0.4691)	loss 2.4847 (2.8545)	grad_norm 3.0996 (3.2172)	mem 14854MB
[2022-11-07 20:40:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][700/1251]	eta 0:04:18 lr 0.000012	time 0.4743 (0.4691)	loss 3.1651 (2.8463)	grad_norm 4.1201 (3.2189)	mem 14854MB
[2022-11-07 20:41:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][750/1251]	eta 0:03:54 lr 0.000011	time 0.4619 (0.4687)	loss 1.8775 (2.8397)	grad_norm 3.4179 (3.2190)	mem 14854MB
[2022-11-07 20:41:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][800/1251]	eta 0:03:31 lr 0.000011	time 0.4549 (0.4686)	loss 2.6818 (2.8304)	grad_norm 3.2398 (3.2228)	mem 14854MB
[2022-11-07 20:41:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][850/1251]	eta 0:03:07 lr 0.000011	time 0.4647 (0.4685)	loss 3.0402 (2.8275)	grad_norm 2.8239 (3.2224)	mem 14854MB
[2022-11-07 20:42:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][900/1251]	eta 0:02:44 lr 0.000011	time 0.4666 (0.4684)	loss 3.3184 (2.8259)	grad_norm 3.4331 (3.2172)	mem 14854MB
[2022-11-07 20:42:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][950/1251]	eta 0:02:21 lr 0.000011	time 0.4646 (0.4685)	loss 2.8216 (2.8175)	grad_norm 3.8594 (3.2264)	mem 14854MB
[2022-11-07 20:43:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][1000/1251]	eta 0:01:57 lr 0.000011	time 0.4538 (0.4683)	loss 3.2286 (2.8149)	grad_norm 3.6128 (3.2316)	mem 14854MB
[2022-11-07 20:43:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][1050/1251]	eta 0:01:34 lr 0.000011	time 0.4597 (0.4683)	loss 2.4310 (2.8118)	grad_norm 3.0174 (3.2324)	mem 14854MB
[2022-11-07 20:43:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][1100/1251]	eta 0:01:10 lr 0.000011	time 0.4617 (0.4681)	loss 3.2077 (2.8068)	grad_norm 2.9916 (3.2330)	mem 14854MB
[2022-11-07 20:44:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][1150/1251]	eta 0:00:47 lr 0.000011	time 0.4660 (0.4680)	loss 3.1235 (2.8094)	grad_norm 3.0641 (3.2347)	mem 14854MB
[2022-11-07 20:44:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][1200/1251]	eta 0:00:23 lr 0.000011	time 0.4673 (0.4680)	loss 2.7072 (2.8061)	grad_norm 2.7625 (3.2363)	mem 14854MB
[2022-11-07 20:45:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [292/300][1250/1251]	eta 0:00:00 lr 0.000011	time 0.4580 (0.4679)	loss 1.9645 (2.8075)	grad_norm 3.1352 (3.2384)	mem 14854MB
[2022-11-07 20:45:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 292 training takes 0:09:45
[2022-11-07 20:45:02 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_292.pth saving......
[2022-11-07 20:45:02 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_292.pth saved !!!
[2022-11-07 20:45:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.568 (1.568)	Loss 0.7204 (0.7204)	Acc@1 83.984 (83.984)	Acc@5 96.191 (96.191)	Mem 14854MB
[2022-11-07 20:45:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.470 Acc@5 95.954
[2022-11-07 20:45:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.5%
[2022-11-07 20:45:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.571 (1.571)	Loss 0.7916 (0.7916)	Acc@1 81.934 (81.934)	Acc@5 95.508 (95.508)	Mem 14854MB
[2022-11-07 20:45:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.486 Acc@5 95.992
[2022-11-07 20:45:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 20:45:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.49% at 292 epoch
[2022-11-07 20:45:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][0/1251]	eta 0:43:09 lr 0.000011	time 2.0700 (2.0700)	loss 2.7059 (2.7059)	grad_norm 3.6245 (3.6245)	mem 14854MB
[2022-11-07 20:45:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][50/1251]	eta 0:10:04 lr 0.000011	time 0.4645 (0.5029)	loss 2.6987 (2.7670)	grad_norm 2.8402 (3.2438)	mem 14854MB
[2022-11-07 20:46:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][100/1251]	eta 0:09:19 lr 0.000011	time 0.4548 (0.4860)	loss 3.3743 (2.8121)	grad_norm 2.9587 (3.2435)	mem 14854MB
[2022-11-07 20:46:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][150/1251]	eta 0:08:48 lr 0.000011	time 0.4714 (0.4801)	loss 2.7049 (2.7879)	grad_norm 3.5060 (3.2301)	mem 14854MB
[2022-11-07 20:46:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][200/1251]	eta 0:08:21 lr 0.000011	time 0.4660 (0.4769)	loss 3.2072 (2.7770)	grad_norm 3.1063 (3.2402)	mem 14854MB
[2022-11-07 20:47:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][250/1251]	eta 0:07:55 lr 0.000011	time 0.4517 (0.4746)	loss 3.5329 (2.7829)	grad_norm 3.2660 (inf)	mem 14854MB
[2022-11-07 20:47:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][300/1251]	eta 0:07:29 lr 0.000011	time 0.4737 (0.4730)	loss 3.3710 (2.7915)	grad_norm 3.2165 (inf)	mem 14854MB
[2022-11-07 20:48:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][350/1251]	eta 0:07:05 lr 0.000011	time 0.4578 (0.4720)	loss 3.2488 (2.8148)	grad_norm 3.0668 (inf)	mem 14854MB
[2022-11-07 20:48:28 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][400/1251]	eta 0:06:41 lr 0.000011	time 0.4628 (0.4712)	loss 2.7770 (2.8195)	grad_norm 3.3449 (inf)	mem 14854MB
[2022-11-07 20:48:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][450/1251]	eta 0:06:17 lr 0.000011	time 0.4717 (0.4708)	loss 3.4696 (2.8197)	grad_norm 3.6892 (inf)	mem 14854MB
[2022-11-07 20:49:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][500/1251]	eta 0:05:53 lr 0.000011	time 0.4632 (0.4702)	loss 3.3225 (2.8165)	grad_norm 3.1722 (inf)	mem 14854MB
[2022-11-07 20:49:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][550/1251]	eta 0:05:29 lr 0.000011	time 0.4678 (0.4701)	loss 2.7808 (2.8165)	grad_norm 3.0866 (inf)	mem 14854MB
[2022-11-07 20:50:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][600/1251]	eta 0:05:05 lr 0.000011	time 0.4622 (0.4696)	loss 3.4888 (2.8162)	grad_norm 3.2644 (inf)	mem 14854MB
[2022-11-07 20:50:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][650/1251]	eta 0:04:42 lr 0.000011	time 0.4598 (0.4696)	loss 2.0373 (2.8075)	grad_norm 2.7846 (inf)	mem 14854MB
[2022-11-07 20:50:48 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][700/1251]	eta 0:04:18 lr 0.000011	time 0.4652 (0.4693)	loss 1.9495 (2.8050)	grad_norm 3.1576 (inf)	mem 14854MB
[2022-11-07 20:51:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][750/1251]	eta 0:03:55 lr 0.000011	time 0.4639 (0.4692)	loss 2.6203 (2.8014)	grad_norm 3.1513 (inf)	mem 14854MB
[2022-11-07 20:51:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][800/1251]	eta 0:03:31 lr 0.000011	time 0.4598 (0.4691)	loss 1.9929 (2.7999)	grad_norm 3.1698 (inf)	mem 14854MB
[2022-11-07 20:51:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][850/1251]	eta 0:03:08 lr 0.000011	time 0.4654 (0.4690)	loss 2.2668 (2.7997)	grad_norm 3.3395 (inf)	mem 14854MB
[2022-11-07 20:52:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][900/1251]	eta 0:02:44 lr 0.000011	time 0.4670 (0.4689)	loss 2.9672 (2.7991)	grad_norm 3.4577 (inf)	mem 14854MB
[2022-11-07 20:52:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][950/1251]	eta 0:02:21 lr 0.000011	time 0.4746 (0.4687)	loss 2.8271 (2.8042)	grad_norm 4.8714 (inf)	mem 14854MB
[2022-11-07 20:53:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][1000/1251]	eta 0:01:57 lr 0.000011	time 0.4554 (0.4687)	loss 1.9262 (2.8031)	grad_norm 3.6263 (inf)	mem 14854MB
[2022-11-07 20:53:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][1050/1251]	eta 0:01:34 lr 0.000011	time 0.4672 (0.4687)	loss 2.6279 (2.7968)	grad_norm 2.9200 (inf)	mem 14854MB
[2022-11-07 20:53:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][1100/1251]	eta 0:01:10 lr 0.000011	time 0.4772 (0.4686)	loss 2.9653 (2.7956)	grad_norm 3.7910 (inf)	mem 14854MB
[2022-11-07 20:54:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][1150/1251]	eta 0:00:47 lr 0.000011	time 0.4666 (0.4686)	loss 2.8879 (2.7935)	grad_norm 3.4850 (inf)	mem 14854MB
[2022-11-07 20:54:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][1200/1251]	eta 0:00:23 lr 0.000011	time 0.4639 (0.4684)	loss 3.0765 (2.7892)	grad_norm 3.3704 (inf)	mem 14854MB
[2022-11-07 20:55:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [293/300][1250/1251]	eta 0:00:00 lr 0.000011	time 0.4566 (0.4682)	loss 1.9559 (2.7853)	grad_norm 3.4348 (inf)	mem 14854MB
[2022-11-07 20:55:05 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 293 training takes 0:09:45
[2022-11-07 20:55:05 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_293.pth saving......
[2022-11-07 20:55:06 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_293.pth saved !!!
[2022-11-07 20:55:08 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.588 (1.588)	Loss 0.6682 (0.6682)	Acc@1 85.059 (85.059)	Acc@5 97.168 (97.168)	Mem 14854MB
[2022-11-07 20:55:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.458 Acc@5 96.004
[2022-11-07 20:55:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.5%
[2022-11-07 20:55:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.669 (1.669)	Loss 0.7708 (0.7708)	Acc@1 82.031 (82.031)	Acc@5 96.777 (96.777)	Mem 14854MB
[2022-11-07 20:55:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.488 Acc@5 95.984
[2022-11-07 20:55:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 20:55:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.49% at 293 epoch
[2022-11-07 20:55:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][0/1251]	eta 0:41:53 lr 0.000011	time 2.0092 (2.0092)	loss 3.0652 (3.0652)	grad_norm 3.3173 (3.3173)	mem 14854MB
[2022-11-07 20:55:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][50/1251]	eta 0:10:03 lr 0.000011	time 0.5315 (0.5025)	loss 1.7142 (2.8599)	grad_norm 3.5529 (3.3038)	mem 14854MB
[2022-11-07 20:56:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][100/1251]	eta 0:09:18 lr 0.000011	time 0.4649 (0.4848)	loss 2.6876 (2.7919)	grad_norm 2.9379 (3.2510)	mem 14854MB
[2022-11-07 20:56:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][150/1251]	eta 0:08:47 lr 0.000011	time 0.4758 (0.4793)	loss 2.3957 (2.7938)	grad_norm 3.0801 (3.2573)	mem 14854MB
[2022-11-07 20:56:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][200/1251]	eta 0:08:19 lr 0.000011	time 0.4610 (0.4755)	loss 2.5752 (2.8143)	grad_norm 2.8223 (3.2752)	mem 14854MB
[2022-11-07 20:57:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][250/1251]	eta 0:07:54 lr 0.000011	time 0.4666 (0.4738)	loss 3.0830 (2.7879)	grad_norm 3.5679 (3.2620)	mem 14854MB
[2022-11-07 20:57:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][300/1251]	eta 0:07:29 lr 0.000011	time 0.4660 (0.4726)	loss 1.7849 (2.7901)	grad_norm 2.8964 (3.2631)	mem 14854MB
[2022-11-07 20:58:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][350/1251]	eta 0:07:04 lr 0.000011	time 0.4786 (0.4716)	loss 3.1978 (2.8034)	grad_norm 3.4369 (3.2769)	mem 14854MB
[2022-11-07 20:58:32 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][400/1251]	eta 0:06:40 lr 0.000011	time 0.4692 (0.4706)	loss 2.7023 (2.8036)	grad_norm 2.9547 (3.2803)	mem 14854MB
[2022-11-07 20:58:55 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][450/1251]	eta 0:06:16 lr 0.000011	time 0.4523 (0.4700)	loss 3.0037 (2.8016)	grad_norm 3.4961 (3.2851)	mem 14854MB
[2022-11-07 20:59:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][500/1251]	eta 0:05:52 lr 0.000011	time 0.4628 (0.4697)	loss 2.3512 (2.7957)	grad_norm 3.4441 (3.2845)	mem 14854MB
[2022-11-07 20:59:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][550/1251]	eta 0:05:29 lr 0.000011	time 0.4635 (0.4695)	loss 2.6743 (2.7968)	grad_norm 3.1318 (3.2775)	mem 14854MB
[2022-11-07 21:00:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][600/1251]	eta 0:05:05 lr 0.000011	time 0.4697 (0.4695)	loss 3.1167 (2.7929)	grad_norm 3.1869 (3.2711)	mem 14854MB
[2022-11-07 21:00:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][650/1251]	eta 0:04:41 lr 0.000011	time 0.4599 (0.4691)	loss 2.6339 (2.7939)	grad_norm 2.9504 (3.2712)	mem 14854MB
[2022-11-07 21:00:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][700/1251]	eta 0:04:18 lr 0.000011	time 0.4595 (0.4688)	loss 3.1815 (2.7988)	grad_norm 3.6567 (3.2786)	mem 14854MB
[2022-11-07 21:01:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][750/1251]	eta 0:03:54 lr 0.000011	time 0.4671 (0.4686)	loss 2.7857 (2.8014)	grad_norm 2.9773 (3.2762)	mem 14854MB
[2022-11-07 21:01:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][800/1251]	eta 0:03:31 lr 0.000011	time 0.4654 (0.4687)	loss 2.3359 (2.7992)	grad_norm 2.8547 (3.2760)	mem 14854MB
[2022-11-07 21:02:02 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][850/1251]	eta 0:03:07 lr 0.000011	time 0.4686 (0.4686)	loss 3.2063 (2.8047)	grad_norm 3.0850 (3.2779)	mem 14854MB
[2022-11-07 21:02:25 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][900/1251]	eta 0:02:44 lr 0.000011	time 0.4616 (0.4684)	loss 2.5975 (2.8036)	grad_norm 3.2907 (3.2830)	mem 14854MB
[2022-11-07 21:02:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][950/1251]	eta 0:02:20 lr 0.000011	time 0.4727 (0.4683)	loss 2.8779 (2.8059)	grad_norm 3.2974 (3.2813)	mem 14854MB
[2022-11-07 21:03:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][1000/1251]	eta 0:01:57 lr 0.000011	time 0.5421 (0.4683)	loss 1.8123 (2.8050)	grad_norm 4.9395 (inf)	mem 14854MB
[2022-11-07 21:03:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][1050/1251]	eta 0:01:34 lr 0.000011	time 0.4712 (0.4682)	loss 2.7586 (2.8034)	grad_norm 2.9110 (inf)	mem 14854MB
[2022-11-07 21:03:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][1100/1251]	eta 0:01:10 lr 0.000011	time 0.4625 (0.4683)	loss 3.1804 (2.8032)	grad_norm 2.8414 (inf)	mem 14854MB
[2022-11-07 21:04:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][1150/1251]	eta 0:00:47 lr 0.000011	time 0.4603 (0.4681)	loss 2.2616 (2.7998)	grad_norm 3.1196 (inf)	mem 14854MB
[2022-11-07 21:04:45 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][1200/1251]	eta 0:00:23 lr 0.000011	time 0.4671 (0.4680)	loss 3.3851 (2.8021)	grad_norm 3.1505 (inf)	mem 14854MB
[2022-11-07 21:05:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [294/300][1250/1251]	eta 0:00:00 lr 0.000011	time 0.4574 (0.4679)	loss 3.2176 (2.8022)	grad_norm 2.9610 (inf)	mem 14854MB
[2022-11-07 21:05:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 294 training takes 0:09:45
[2022-11-07 21:05:09 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_294.pth saving......
[2022-11-07 21:05:10 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_294.pth saved !!!
[2022-11-07 21:05:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.436 (1.436)	Loss 0.8799 (0.8799)	Acc@1 79.297 (79.297)	Acc@5 95.215 (95.215)	Mem 14854MB
[2022-11-07 21:05:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.434 Acc@5 95.996
[2022-11-07 21:05:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-07 21:05:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.660 (1.660)	Loss 0.7228 (0.7228)	Acc@1 82.812 (82.812)	Acc@5 96.875 (96.875)	Mem 14854MB
[2022-11-07 21:05:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.494 Acc@5 95.990
[2022-11-07 21:05:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 21:05:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.49% at 294 epoch
[2022-11-07 21:05:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][0/1251]	eta 0:41:54 lr 0.000011	time 2.0101 (2.0101)	loss 1.9500 (1.9500)	grad_norm 3.6924 (3.6924)	mem 14854MB
[2022-11-07 21:05:52 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][50/1251]	eta 0:10:01 lr 0.000011	time 0.4732 (0.5010)	loss 2.7962 (2.8015)	grad_norm 3.4966 (3.2428)	mem 14854MB
[2022-11-07 21:06:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][100/1251]	eta 0:09:21 lr 0.000011	time 0.4555 (0.4874)	loss 2.7759 (2.7845)	grad_norm 3.8529 (3.2541)	mem 14854MB
[2022-11-07 21:06:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][150/1251]	eta 0:08:49 lr 0.000011	time 0.4657 (0.4811)	loss 3.3152 (2.8109)	grad_norm 3.0693 (3.2928)	mem 14854MB
[2022-11-07 21:07:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][200/1251]	eta 0:08:21 lr 0.000011	time 0.4693 (0.4773)	loss 3.2623 (2.8234)	grad_norm 3.5998 (3.2889)	mem 14854MB
[2022-11-07 21:07:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][250/1251]	eta 0:07:55 lr 0.000011	time 0.4787 (0.4748)	loss 1.7456 (2.7936)	grad_norm 3.0053 (3.3018)	mem 14854MB
[2022-11-07 21:07:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][300/1251]	eta 0:07:29 lr 0.000011	time 0.4624 (0.4730)	loss 2.8082 (2.8085)	grad_norm 3.1539 (3.2735)	mem 14854MB
[2022-11-07 21:08:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][350/1251]	eta 0:07:05 lr 0.000011	time 0.4626 (0.4721)	loss 2.0292 (2.8111)	grad_norm 3.1252 (3.2710)	mem 14854MB
[2022-11-07 21:08:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][400/1251]	eta 0:06:41 lr 0.000011	time 0.4699 (0.4715)	loss 2.0853 (2.7949)	grad_norm 2.8849 (3.2739)	mem 14854MB
[2022-11-07 21:08:59 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][450/1251]	eta 0:06:17 lr 0.000011	time 0.4662 (0.4708)	loss 3.2765 (2.7982)	grad_norm 3.6787 (3.2805)	mem 14854MB
[2022-11-07 21:09:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][500/1251]	eta 0:05:53 lr 0.000011	time 0.4689 (0.4704)	loss 2.9526 (2.7940)	grad_norm 2.9344 (3.2820)	mem 14854MB
[2022-11-07 21:09:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][550/1251]	eta 0:05:29 lr 0.000011	time 0.4786 (0.4700)	loss 2.4022 (2.8028)	grad_norm 3.2385 (3.2818)	mem 14854MB
[2022-11-07 21:10:09 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][600/1251]	eta 0:05:05 lr 0.000011	time 0.4572 (0.4699)	loss 2.4216 (2.7973)	grad_norm 3.7583 (3.2849)	mem 14854MB
[2022-11-07 21:10:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][650/1251]	eta 0:04:42 lr 0.000011	time 0.4534 (0.4696)	loss 3.1068 (2.7978)	grad_norm 2.9424 (3.2870)	mem 14854MB
[2022-11-07 21:10:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][700/1251]	eta 0:04:18 lr 0.000011	time 0.4513 (0.4692)	loss 3.1200 (2.7975)	grad_norm 3.4316 (3.2832)	mem 14854MB
[2022-11-07 21:11:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][750/1251]	eta 0:03:55 lr 0.000011	time 0.4689 (0.4692)	loss 3.2292 (2.7926)	grad_norm 3.2989 (3.2846)	mem 14854MB
[2022-11-07 21:11:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][800/1251]	eta 0:03:31 lr 0.000011	time 0.5387 (0.4690)	loss 3.0170 (2.7904)	grad_norm 3.2662 (3.2839)	mem 14854MB
[2022-11-07 21:12:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][850/1251]	eta 0:03:08 lr 0.000011	time 0.4651 (0.4690)	loss 2.7133 (2.7869)	grad_norm 2.9826 (3.2859)	mem 14854MB
[2022-11-07 21:12:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][900/1251]	eta 0:02:44 lr 0.000010	time 0.4607 (0.4688)	loss 3.2983 (2.7912)	grad_norm 3.6155 (3.2902)	mem 14854MB
[2022-11-07 21:12:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][950/1251]	eta 0:02:21 lr 0.000010	time 0.4641 (0.4686)	loss 3.0422 (2.7914)	grad_norm 3.1667 (3.2870)	mem 14854MB
[2022-11-07 21:13:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][1000/1251]	eta 0:01:57 lr 0.000010	time 0.4683 (0.4685)	loss 3.3324 (2.7953)	grad_norm 2.9283 (3.2846)	mem 14854MB
[2022-11-07 21:13:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][1050/1251]	eta 0:01:34 lr 0.000010	time 0.4711 (0.4685)	loss 2.8221 (2.7982)	grad_norm 3.3794 (3.2825)	mem 14854MB
[2022-11-07 21:14:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][1100/1251]	eta 0:01:10 lr 0.000010	time 0.4611 (0.4685)	loss 2.9938 (2.7997)	grad_norm 3.7445 (3.2869)	mem 14854MB
[2022-11-07 21:14:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][1150/1251]	eta 0:00:47 lr 0.000010	time 0.4655 (0.4683)	loss 3.3066 (2.8007)	grad_norm 3.7498 (3.2902)	mem 14854MB
[2022-11-07 21:14:49 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][1200/1251]	eta 0:00:23 lr 0.000010	time 0.4632 (0.4682)	loss 2.3723 (2.7977)	grad_norm 3.1537 (3.2918)	mem 14854MB
[2022-11-07 21:15:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [295/300][1250/1251]	eta 0:00:00 lr 0.000010	time 0.4564 (0.4679)	loss 2.4456 (2.7982)	grad_norm 3.2827 (3.2922)	mem 14854MB
[2022-11-07 21:15:12 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 295 training takes 0:09:45
[2022-11-07 21:15:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_295.pth saving......
[2022-11-07 21:15:13 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_295.pth saved !!!
[2022-11-07 21:15:15 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.589 (1.589)	Loss 0.8093 (0.8093)	Acc@1 81.641 (81.641)	Acc@5 95.996 (95.996)	Mem 14854MB
[2022-11-07 21:15:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.472 Acc@5 95.958
[2022-11-07 21:15:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.5%
[2022-11-07 21:15:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.545 (1.545)	Loss 0.7630 (0.7630)	Acc@1 82.422 (82.422)	Acc@5 96.191 (96.191)	Mem 14854MB
[2022-11-07 21:15:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.506 Acc@5 95.980
[2022-11-07 21:15:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 21:15:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.51% at 295 epoch
[2022-11-07 21:15:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][0/1251]	eta 0:41:12 lr 0.000010	time 1.9766 (1.9766)	loss 2.9587 (2.9587)	grad_norm 4.1805 (4.1805)	mem 14854MB
[2022-11-07 21:15:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][50/1251]	eta 0:10:02 lr 0.000010	time 0.4664 (0.5020)	loss 3.3734 (2.8543)	grad_norm 3.1283 (3.2788)	mem 14854MB
[2022-11-07 21:16:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][100/1251]	eta 0:09:18 lr 0.000010	time 0.4629 (0.4855)	loss 2.1591 (2.7938)	grad_norm 3.1659 (3.3167)	mem 14854MB
[2022-11-07 21:16:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][150/1251]	eta 0:08:46 lr 0.000010	time 0.4650 (0.4786)	loss 2.8272 (2.7784)	grad_norm 3.2382 (3.3184)	mem 14854MB
[2022-11-07 21:17:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][200/1251]	eta 0:08:19 lr 0.000010	time 0.4607 (0.4752)	loss 2.1491 (2.7701)	grad_norm 3.1046 (3.3309)	mem 14854MB
[2022-11-07 21:17:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][250/1251]	eta 0:07:54 lr 0.000010	time 0.4592 (0.4741)	loss 3.3487 (2.7963)	grad_norm 3.2509 (3.3210)	mem 14854MB
[2022-11-07 21:17:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][300/1251]	eta 0:07:29 lr 0.000010	time 0.4621 (0.4727)	loss 2.7731 (2.8010)	grad_norm 3.8164 (3.3232)	mem 14854MB
[2022-11-07 21:18:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][350/1251]	eta 0:07:05 lr 0.000010	time 0.4682 (0.4718)	loss 2.0309 (2.7842)	grad_norm 3.3309 (3.3114)	mem 14854MB
[2022-11-07 21:18:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][400/1251]	eta 0:06:40 lr 0.000010	time 0.4665 (0.4709)	loss 2.9020 (2.7883)	grad_norm 3.1693 (3.3170)	mem 14854MB
[2022-11-07 21:19:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][450/1251]	eta 0:06:16 lr 0.000010	time 0.4719 (0.4703)	loss 2.4325 (2.7882)	grad_norm 2.9505 (3.3216)	mem 14854MB
[2022-11-07 21:19:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][500/1251]	eta 0:05:52 lr 0.000010	time 0.4525 (0.4698)	loss 2.4010 (2.7915)	grad_norm 2.8636 (3.3180)	mem 14854MB
[2022-11-07 21:19:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][550/1251]	eta 0:05:29 lr 0.000010	time 0.4681 (0.4699)	loss 3.5235 (2.7839)	grad_norm 2.8605 (inf)	mem 14854MB
[2022-11-07 21:20:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][600/1251]	eta 0:05:05 lr 0.000010	time 0.4623 (0.4696)	loss 2.6992 (2.7857)	grad_norm 2.9426 (inf)	mem 14854MB
[2022-11-07 21:20:36 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][650/1251]	eta 0:04:41 lr 0.000010	time 0.4597 (0.4692)	loss 2.9551 (2.7975)	grad_norm 3.1983 (inf)	mem 14854MB
[2022-11-07 21:21:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][700/1251]	eta 0:04:18 lr 0.000010	time 0.5479 (0.4690)	loss 2.5736 (2.7845)	grad_norm 3.3251 (inf)	mem 14854MB
[2022-11-07 21:21:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][750/1251]	eta 0:03:54 lr 0.000010	time 0.4636 (0.4689)	loss 2.2567 (2.7839)	grad_norm 3.1084 (inf)	mem 14854MB
[2022-11-07 21:21:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][800/1251]	eta 0:03:31 lr 0.000010	time 0.5475 (0.4690)	loss 3.1678 (2.7887)	grad_norm 2.7995 (inf)	mem 14854MB
[2022-11-07 21:22:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][850/1251]	eta 0:03:07 lr 0.000010	time 0.4670 (0.4687)	loss 3.2022 (2.7834)	grad_norm 3.1786 (inf)	mem 14854MB
[2022-11-07 21:22:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][900/1251]	eta 0:02:44 lr 0.000010	time 0.4651 (0.4686)	loss 3.0034 (2.7807)	grad_norm 2.8363 (inf)	mem 14854MB
[2022-11-07 21:22:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][950/1251]	eta 0:02:20 lr 0.000010	time 0.4615 (0.4684)	loss 3.0664 (2.7873)	grad_norm 3.0194 (inf)	mem 14854MB
[2022-11-07 21:23:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][1000/1251]	eta 0:01:57 lr 0.000010	time 0.4640 (0.4682)	loss 3.1839 (2.7892)	grad_norm 3.1788 (inf)	mem 14854MB
[2022-11-07 21:23:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][1050/1251]	eta 0:01:34 lr 0.000010	time 0.4686 (0.4685)	loss 3.1662 (2.7919)	grad_norm 2.9857 (inf)	mem 14854MB
[2022-11-07 21:24:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][1100/1251]	eta 0:01:10 lr 0.000010	time 0.4729 (0.4683)	loss 1.9120 (2.7930)	grad_norm 2.9747 (inf)	mem 14854MB
[2022-11-07 21:24:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][1150/1251]	eta 0:00:47 lr 0.000010	time 0.4738 (0.4681)	loss 2.9624 (2.7867)	grad_norm 4.3908 (inf)	mem 14854MB
[2022-11-07 21:24:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][1200/1251]	eta 0:00:23 lr 0.000010	time 0.5335 (0.4681)	loss 2.2373 (2.7893)	grad_norm 4.0230 (inf)	mem 14854MB
[2022-11-07 21:25:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [296/300][1250/1251]	eta 0:00:00 lr 0.000010	time 0.4570 (0.4679)	loss 3.2142 (2.7928)	grad_norm 3.2195 (inf)	mem 14854MB
[2022-11-07 21:25:16 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 296 training takes 0:09:45
[2022-11-07 21:25:16 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_296.pth saving......
[2022-11-07 21:25:17 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_296.pth saved !!!
[2022-11-07 21:25:19 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.567 (1.567)	Loss 0.7492 (0.7492)	Acc@1 82.617 (82.617)	Acc@5 96.875 (96.875)	Mem 14854MB
[2022-11-07 21:25:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.452 Acc@5 95.966
[2022-11-07 21:25:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.5%
[2022-11-07 21:25:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.592 (1.592)	Loss 0.7616 (0.7616)	Acc@1 82.715 (82.715)	Acc@5 96.289 (96.289)	Mem 14854MB
[2022-11-07 21:25:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.524 Acc@5 95.980
[2022-11-07 21:25:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 21:25:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.52% at 296 epoch
[2022-11-07 21:25:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][0/1251]	eta 0:45:00 lr 0.000010	time 2.1583 (2.1583)	loss 3.2294 (3.2294)	grad_norm 2.7073 (2.7073)	mem 14854MB
[2022-11-07 21:26:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][50/1251]	eta 0:10:06 lr 0.000010	time 0.4723 (0.5049)	loss 3.3256 (2.9396)	grad_norm 3.6234 (3.2835)	mem 14854MB
[2022-11-07 21:26:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][100/1251]	eta 0:09:21 lr 0.000010	time 0.4626 (0.4883)	loss 2.6585 (2.9097)	grad_norm 2.7639 (3.2410)	mem 14854MB
[2022-11-07 21:26:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][150/1251]	eta 0:08:49 lr 0.000010	time 0.4665 (0.4810)	loss 3.0252 (2.8764)	grad_norm 2.8787 (3.2771)	mem 14854MB
[2022-11-07 21:27:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][200/1251]	eta 0:08:21 lr 0.000010	time 0.4633 (0.4775)	loss 2.9722 (2.8372)	grad_norm 3.1464 (3.2807)	mem 14854MB
[2022-11-07 21:27:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][250/1251]	eta 0:07:55 lr 0.000010	time 0.4519 (0.4750)	loss 3.4219 (2.8132)	grad_norm 3.8831 (3.2822)	mem 14854MB
[2022-11-07 21:27:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][300/1251]	eta 0:07:30 lr 0.000010	time 0.4657 (0.4734)	loss 2.7529 (2.8188)	grad_norm 4.2713 (3.2882)	mem 14854MB
[2022-11-07 21:28:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][350/1251]	eta 0:07:05 lr 0.000010	time 0.4553 (0.4723)	loss 2.1853 (2.8152)	grad_norm 3.5522 (3.2985)	mem 14854MB
[2022-11-07 21:28:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][400/1251]	eta 0:06:41 lr 0.000010	time 0.4577 (0.4715)	loss 2.2433 (2.8097)	grad_norm 3.4558 (3.2960)	mem 14854MB
[2022-11-07 21:29:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][450/1251]	eta 0:06:17 lr 0.000010	time 0.4672 (0.4708)	loss 3.0144 (2.8154)	grad_norm 3.1532 (3.2951)	mem 14854MB
[2022-11-07 21:29:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][500/1251]	eta 0:05:53 lr 0.000010	time 0.4660 (0.4702)	loss 2.7723 (2.8069)	grad_norm 3.1170 (3.2939)	mem 14854MB
[2022-11-07 21:29:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][550/1251]	eta 0:05:29 lr 0.000010	time 0.4687 (0.4702)	loss 3.2981 (2.8125)	grad_norm 3.5116 (3.2975)	mem 14854MB
[2022-11-07 21:30:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][600/1251]	eta 0:05:05 lr 0.000010	time 0.4641 (0.4697)	loss 3.1353 (2.8070)	grad_norm 3.3607 (3.2932)	mem 14854MB
[2022-11-07 21:30:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][650/1251]	eta 0:04:42 lr 0.000010	time 0.4595 (0.4694)	loss 2.0294 (2.8004)	grad_norm 3.2696 (3.2913)	mem 14854MB
[2022-11-07 21:31:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][700/1251]	eta 0:04:18 lr 0.000010	time 0.4677 (0.4693)	loss 2.2130 (2.7996)	grad_norm 3.9310 (3.2878)	mem 14854MB
[2022-11-07 21:31:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][750/1251]	eta 0:03:55 lr 0.000010	time 0.4615 (0.4691)	loss 2.7922 (2.8013)	grad_norm 3.0026 (3.2856)	mem 14854MB
[2022-11-07 21:31:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][800/1251]	eta 0:03:31 lr 0.000010	time 0.4595 (0.4690)	loss 2.1798 (2.8019)	grad_norm 4.1264 (3.2908)	mem 14854MB
[2022-11-07 21:32:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][850/1251]	eta 0:03:07 lr 0.000010	time 0.4654 (0.4688)	loss 2.0973 (2.7917)	grad_norm 3.2291 (3.2922)	mem 14854MB
[2022-11-07 21:32:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][900/1251]	eta 0:02:44 lr 0.000010	time 0.4586 (0.4686)	loss 2.2704 (2.7891)	grad_norm 3.3832 (3.2958)	mem 14854MB
[2022-11-07 21:33:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][950/1251]	eta 0:02:20 lr 0.000010	time 0.4705 (0.4684)	loss 3.0074 (2.7886)	grad_norm 3.1909 (3.2958)	mem 14854MB
[2022-11-07 21:33:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][1000/1251]	eta 0:01:57 lr 0.000010	time 0.4667 (0.4683)	loss 2.1576 (2.7892)	grad_norm 3.1313 (3.2931)	mem 14854MB
[2022-11-07 21:33:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][1050/1251]	eta 0:01:34 lr 0.000010	time 0.4606 (0.4683)	loss 2.0020 (2.7883)	grad_norm 3.1097 (3.2939)	mem 14854MB
[2022-11-07 21:34:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][1100/1251]	eta 0:01:10 lr 0.000010	time 0.4717 (0.4682)	loss 3.0724 (2.7902)	grad_norm 2.9308 (3.2992)	mem 14854MB
[2022-11-07 21:34:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][1150/1251]	eta 0:00:47 lr 0.000010	time 0.4661 (0.4681)	loss 2.9604 (2.7879)	grad_norm 3.0723 (3.3006)	mem 14854MB
[2022-11-07 21:34:56 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][1200/1251]	eta 0:00:23 lr 0.000010	time 0.4587 (0.4680)	loss 2.8144 (2.7856)	grad_norm 3.0209 (3.2993)	mem 14854MB
[2022-11-07 21:35:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [297/300][1250/1251]	eta 0:00:00 lr 0.000010	time 0.4563 (0.4678)	loss 3.0871 (2.7843)	grad_norm 3.0339 (3.2978)	mem 14854MB
[2022-11-07 21:35:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 297 training takes 0:09:45
[2022-11-07 21:35:20 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_297.pth saving......
[2022-11-07 21:35:21 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_297.pth saved !!!
[2022-11-07 21:35:22 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.594 (1.594)	Loss 0.7784 (0.7784)	Acc@1 82.910 (82.910)	Acc@5 95.410 (95.410)	Mem 14854MB
[2022-11-07 21:35:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.410 Acc@5 95.958
[2022-11-07 21:35:29 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-07 21:35:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.571 (1.571)	Loss 0.7508 (0.7508)	Acc@1 83.105 (83.105)	Acc@5 95.801 (95.801)	Mem 14854MB
[2022-11-07 21:35:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.544 Acc@5 95.996
[2022-11-07 21:35:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 21:35:38 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.54% at 297 epoch
[2022-11-07 21:35:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][0/1251]	eta 0:40:22 lr 0.000010	time 1.9368 (1.9368)	loss 2.8113 (2.8113)	grad_norm 3.0133 (3.0133)	mem 14854MB
[2022-11-07 21:36:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][50/1251]	eta 0:09:57 lr 0.000010	time 0.4580 (0.4977)	loss 2.9447 (2.8183)	grad_norm 2.9976 (3.4078)	mem 14854MB
[2022-11-07 21:36:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][100/1251]	eta 0:09:16 lr 0.000010	time 0.4709 (0.4837)	loss 3.1847 (2.7600)	grad_norm 3.0519 (3.3572)	mem 14854MB
[2022-11-07 21:36:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][150/1251]	eta 0:08:46 lr 0.000010	time 0.4685 (0.4785)	loss 2.9430 (2.7701)	grad_norm 3.1561 (3.3601)	mem 14854MB
[2022-11-07 21:37:13 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][200/1251]	eta 0:08:19 lr 0.000010	time 0.4621 (0.4749)	loss 2.9845 (2.7986)	grad_norm 3.0752 (3.3594)	mem 14854MB
[2022-11-07 21:37:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][250/1251]	eta 0:07:53 lr 0.000010	time 0.4585 (0.4733)	loss 2.5669 (2.7917)	grad_norm 3.1349 (3.3553)	mem 14854MB
[2022-11-07 21:38:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][300/1251]	eta 0:07:28 lr 0.000010	time 0.4600 (0.4719)	loss 2.9463 (2.8050)	grad_norm 3.1407 (3.3624)	mem 14854MB
[2022-11-07 21:38:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][350/1251]	eta 0:07:04 lr 0.000010	time 0.4563 (0.4710)	loss 2.3089 (2.8112)	grad_norm 3.0102 (3.3466)	mem 14854MB
[2022-11-07 21:38:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][400/1251]	eta 0:06:40 lr 0.000010	time 0.4694 (0.4707)	loss 3.4128 (2.8227)	grad_norm 3.1644 (3.3428)	mem 14854MB
[2022-11-07 21:39:10 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][450/1251]	eta 0:06:16 lr 0.000010	time 0.4659 (0.4702)	loss 2.6902 (2.8197)	grad_norm 3.3394 (3.3471)	mem 14854MB
[2022-11-07 21:39:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][500/1251]	eta 0:05:52 lr 0.000010	time 0.4727 (0.4697)	loss 3.1088 (2.8092)	grad_norm 3.2990 (3.3459)	mem 14854MB
[2022-11-07 21:39:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][550/1251]	eta 0:05:29 lr 0.000010	time 0.4682 (0.4694)	loss 3.2270 (2.8031)	grad_norm 2.8476 (3.3391)	mem 14854MB
[2022-11-07 21:40:20 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][600/1251]	eta 0:05:05 lr 0.000010	time 0.4730 (0.4693)	loss 2.2362 (2.8001)	grad_norm 3.2408 (3.3302)	mem 14854MB
[2022-11-07 21:40:43 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][650/1251]	eta 0:04:41 lr 0.000010	time 0.4587 (0.4690)	loss 1.7808 (2.7939)	grad_norm 3.2152 (3.3248)	mem 14854MB
[2022-11-07 21:41:06 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][700/1251]	eta 0:04:18 lr 0.000010	time 0.4696 (0.4688)	loss 2.3481 (2.7864)	grad_norm 3.9045 (3.3294)	mem 14854MB
[2022-11-07 21:41:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][750/1251]	eta 0:03:54 lr 0.000010	time 0.4555 (0.4685)	loss 2.5348 (2.7902)	grad_norm 3.3960 (3.3339)	mem 14854MB
[2022-11-07 21:41:53 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][800/1251]	eta 0:03:31 lr 0.000010	time 0.4509 (0.4685)	loss 3.2349 (2.7924)	grad_norm 3.4036 (3.3283)	mem 14854MB
[2022-11-07 21:42:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][850/1251]	eta 0:03:07 lr 0.000010	time 0.4527 (0.4686)	loss 2.5764 (2.7891)	grad_norm 3.7722 (3.3281)	mem 14854MB
[2022-11-07 21:42:40 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][900/1251]	eta 0:02:44 lr 0.000010	time 0.4581 (0.4684)	loss 3.1161 (2.7858)	grad_norm 3.9888 (3.3333)	mem 14854MB
[2022-11-07 21:43:03 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][950/1251]	eta 0:02:20 lr 0.000010	time 0.4626 (0.4684)	loss 3.3343 (2.7847)	grad_norm 3.6021 (3.3347)	mem 14854MB
[2022-11-07 21:43:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][1000/1251]	eta 0:01:57 lr 0.000010	time 0.4702 (0.4682)	loss 2.6223 (2.7833)	grad_norm 3.0890 (3.3373)	mem 14854MB
[2022-11-07 21:43:50 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][1050/1251]	eta 0:01:34 lr 0.000010	time 0.4658 (0.4683)	loss 2.7755 (2.7827)	grad_norm 3.3403 (3.3335)	mem 14854MB
[2022-11-07 21:44:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][1100/1251]	eta 0:01:10 lr 0.000010	time 0.4667 (0.4684)	loss 3.1329 (2.7880)	grad_norm 3.4143 (3.3344)	mem 14854MB
[2022-11-07 21:44:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][1150/1251]	eta 0:00:47 lr 0.000010	time 0.4658 (0.4682)	loss 1.9027 (2.7855)	grad_norm 3.5315 (3.3345)	mem 14854MB
[2022-11-07 21:45:00 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][1200/1251]	eta 0:00:23 lr 0.000010	time 0.4659 (0.4681)	loss 3.2181 (2.7901)	grad_norm 2.8593 (3.3387)	mem 14854MB
[2022-11-07 21:45:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [298/300][1250/1251]	eta 0:00:00 lr 0.000010	time 0.4567 (0.4680)	loss 1.8986 (2.7890)	grad_norm 3.3203 (3.3388)	mem 14854MB
[2022-11-07 21:45:23 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 298 training takes 0:09:45
[2022-11-07 21:45:24 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_298.pth saving......
[2022-11-07 21:45:24 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_298.pth saved !!!
[2022-11-07 21:45:26 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.546 (1.546)	Loss 0.7859 (0.7859)	Acc@1 81.543 (81.543)	Acc@5 95.508 (95.508)	Mem 14854MB
[2022-11-07 21:45:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.452 Acc@5 95.990
[2022-11-07 21:45:33 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.5%
[2022-11-07 21:45:35 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.687 (1.687)	Loss 0.7588 (0.7588)	Acc@1 82.520 (82.520)	Acc@5 95.801 (95.801)	Mem 14854MB
[2022-11-07 21:45:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.492 Acc@5 96.000
[2022-11-07 21:45:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 21:45:42 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.54% at 297 epoch
[2022-11-07 21:45:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][0/1251]	eta 0:40:25 lr 0.000010	time 1.9390 (1.9390)	loss 2.9080 (2.9080)	grad_norm 3.2569 (3.2569)	mem 14854MB
[2022-11-07 21:46:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][50/1251]	eta 0:09:58 lr 0.000010	time 0.4668 (0.4982)	loss 2.4412 (2.8863)	grad_norm 3.3961 (3.4164)	mem 14854MB
[2022-11-07 21:46:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][100/1251]	eta 0:09:17 lr 0.000010	time 0.4598 (0.4839)	loss 2.1610 (2.8252)	grad_norm 3.0125 (3.3475)	mem 14854MB
[2022-11-07 21:46:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][150/1251]	eta 0:08:46 lr 0.000010	time 0.4664 (0.4779)	loss 2.6386 (2.8504)	grad_norm 3.6065 (3.3639)	mem 14854MB
[2022-11-07 21:47:17 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][200/1251]	eta 0:08:18 lr 0.000010	time 0.4600 (0.4748)	loss 1.8841 (2.8071)	grad_norm 3.5806 (3.3633)	mem 14854MB
[2022-11-07 21:47:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][250/1251]	eta 0:07:54 lr 0.000010	time 0.4637 (0.4738)	loss 2.9792 (2.8042)	grad_norm 3.3740 (3.3522)	mem 14854MB
[2022-11-07 21:48:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][300/1251]	eta 0:07:29 lr 0.000010	time 0.4664 (0.4729)	loss 2.7624 (2.7897)	grad_norm 3.3396 (3.3442)	mem 14854MB
[2022-11-07 21:48:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][350/1251]	eta 0:07:05 lr 0.000010	time 0.4693 (0.4719)	loss 3.1651 (2.7900)	grad_norm 3.1540 (3.3516)	mem 14854MB
[2022-11-07 21:48:51 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][400/1251]	eta 0:06:41 lr 0.000010	time 0.4567 (0.4712)	loss 3.0022 (2.7961)	grad_norm 3.1860 (3.3528)	mem 14854MB
[2022-11-07 21:49:14 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][450/1251]	eta 0:06:16 lr 0.000010	time 0.4520 (0.4706)	loss 3.2186 (2.7933)	grad_norm 3.5269 (3.3432)	mem 14854MB
[2022-11-07 21:49:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][500/1251]	eta 0:05:53 lr 0.000010	time 0.5434 (0.4702)	loss 2.2567 (2.7912)	grad_norm 3.3801 (3.3335)	mem 14854MB
[2022-11-07 21:50:01 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][550/1251]	eta 0:05:29 lr 0.000010	time 0.4826 (0.4701)	loss 2.9179 (2.7943)	grad_norm 3.4523 (3.3251)	mem 14854MB
[2022-11-07 21:50:24 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][600/1251]	eta 0:05:05 lr 0.000010	time 0.4520 (0.4696)	loss 3.0997 (2.8025)	grad_norm 3.2821 (3.3239)	mem 14854MB
[2022-11-07 21:50:47 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][650/1251]	eta 0:04:42 lr 0.000010	time 0.4637 (0.4693)	loss 2.9749 (2.8071)	grad_norm 3.4727 (3.3279)	mem 14854MB
[2022-11-07 21:51:11 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][700/1251]	eta 0:04:18 lr 0.000010	time 0.4666 (0.4691)	loss 2.9230 (2.8029)	grad_norm 4.2916 (3.3320)	mem 14854MB
[2022-11-07 21:51:34 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][750/1251]	eta 0:03:54 lr 0.000010	time 0.4831 (0.4689)	loss 2.0918 (2.7991)	grad_norm 2.9019 (3.3278)	mem 14854MB
[2022-11-07 21:51:57 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][800/1251]	eta 0:03:31 lr 0.000010	time 0.4768 (0.4690)	loss 3.1947 (2.7972)	grad_norm 3.5421 (3.3244)	mem 14854MB
[2022-11-07 21:52:21 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][850/1251]	eta 0:03:08 lr 0.000010	time 0.4647 (0.4688)	loss 2.7083 (2.7986)	grad_norm 3.2835 (inf)	mem 14854MB
[2022-11-07 21:52:44 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][900/1251]	eta 0:02:44 lr 0.000010	time 0.4705 (0.4687)	loss 3.0803 (2.7980)	grad_norm 2.8021 (inf)	mem 14854MB
[2022-11-07 21:53:07 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][950/1251]	eta 0:02:21 lr 0.000010	time 0.4618 (0.4686)	loss 2.6609 (2.7926)	grad_norm 3.1641 (inf)	mem 14854MB
[2022-11-07 21:53:31 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][1000/1251]	eta 0:01:57 lr 0.000010	time 0.4638 (0.4685)	loss 2.7431 (2.7915)	grad_norm 3.4103 (inf)	mem 14854MB
[2022-11-07 21:53:54 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][1050/1251]	eta 0:01:34 lr 0.000010	time 0.4742 (0.4686)	loss 3.1315 (2.7935)	grad_norm 5.1482 (inf)	mem 14854MB
[2022-11-07 21:54:18 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][1100/1251]	eta 0:01:10 lr 0.000010	time 0.4708 (0.4685)	loss 2.9499 (2.7938)	grad_norm 2.6659 (inf)	mem 14854MB
[2022-11-07 21:54:41 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][1150/1251]	eta 0:00:47 lr 0.000010	time 0.4690 (0.4684)	loss 3.3178 (2.7954)	grad_norm 2.5978 (inf)	mem 14854MB
[2022-11-07 21:55:04 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][1200/1251]	eta 0:00:23 lr 0.000010	time 0.4680 (0.4684)	loss 1.9211 (2.7921)	grad_norm 3.0897 (inf)	mem 14854MB
[2022-11-07 21:55:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 312): INFO Train: [299/300][1250/1251]	eta 0:00:00 lr 0.000010	time 0.4561 (0.4681)	loss 3.4173 (2.7962)	grad_norm 3.6816 (inf)	mem 14854MB
[2022-11-07 21:55:27 QFormer_transformer_tiny_patch4_window7_224] (main.py 320): INFO EPOCH 299 training takes 0:09:45
[2022-11-07 21:55:28 QFormer_transformer_tiny_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_299.pth saving......
[2022-11-07 21:55:28 QFormer_transformer_tiny_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_tiny_patch4_window7_224/1024-dpr20-coords_lambda1e-1/ckpt_epoch_299.pth saved !!!
[2022-11-07 21:55:30 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.503 (1.503)	Loss 0.7777 (0.7777)	Acc@1 82.031 (82.031)	Acc@5 95.801 (95.801)	Mem 14854MB
[2022-11-07 21:55:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.520 Acc@5 96.000
[2022-11-07 21:55:37 QFormer_transformer_tiny_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.5%
[2022-11-07 21:55:39 QFormer_transformer_tiny_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.568 (1.568)	Loss 0.7947 (0.7947)	Acc@1 82.031 (82.031)	Acc@5 95.801 (95.801)	Mem 14854MB
[2022-11-07 21:55:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.506 Acc@5 95.998
[2022-11-07 21:55:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-07 21:55:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.54% at 297 epoch
[2022-11-07 21:55:46 QFormer_transformer_tiny_patch4_window7_224] (main.py 232): INFO Training time 2 days, 2:20:14

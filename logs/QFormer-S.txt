[2022-11-10 23:41:35 QFormer_transformer_small_patch4_window7_224] (main.py 442): INFO Full config saved to output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/config.json
[2022-11-10 23:41:35 QFormer_transformer_small_patch4_window7_224] (main.py 445): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
  SCALE: null
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EMA:
  EMA_DECAY: 0.9999200015999999
  EMA_FORCE_CPU: false
  ENABLE_EMA: true
ENABLE_WANDB: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  EMSWIN:
    EM_FACTOR: 0.9
    EM_ITERS: 3
    INSTANCE_TOKENS:
    - 10
    - 10
    - 10
    - 0
  LABEL_SMOOTHING: 0.1
  NAME: QFormer_transformer_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  QuadrangleAttention:
    context_size: null
    pyramid_size:
    - 1
    rpe: v1
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    LAYER_RATIO:
    - 1
    - 2
    - 3
    - 4
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    RELATIVE_POS_EMBEDDING: true
    SHIFT: true
    WINDOW_SIZE: 7
  TYPE: qformer
OUTPUT: output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1
PRINT_FREQ: 50
RESUME_OPTIMIZER: true
SAVE_FREQ: 1
SEED: 0
TAG: 1024-dpr30-coords_lambda1e-1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.001
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05
  coords_lambda: 0.1

[2022-11-10 23:41:41 QFormer_transformer_small_patch4_window7_224] (main.py 99): INFO Creating model:qformer/QFormer_transformer_small_patch4_window7_224
[2022-11-10 23:41:42 QFormer_transformer_small_patch4_window7_224] (main.py 102): INFO QFormer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(96, 27, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(96, 27, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(192, 54, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(192, 54, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(768, 216, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(768, 216, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-11-10 23:41:42 QFormer_transformer_small_patch4_window7_224] (main.py 115): INFO enable EMA model
[2022-11-10 23:41:42 QFormer_transformer_small_patch4_window7_224] (main.py 129): INFO number of params: 51164188
[2022-11-10 23:41:42 QFormer_transformer_small_patch4_window7_224] (main.py 132): INFO number of GFLOPs: 0.015670272
[2022-11-10 23:42:12 QFormer_transformer_small_patch4_window7_224] (main.py 175): INFO no checkpoint found in output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1, ignoring auto resume
[2022-11-10 23:42:12 QFormer_transformer_small_patch4_window7_224] (main.py 201): INFO Start training
[2022-11-10 23:42:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][0/1251]	eta 2:46:10 lr 0.000001	time 7.9704 (7.9704)	loss 6.9436 (6.9436)	grad_norm 0.9305 (0.9305)	mem 23205MB
[2022-11-10 23:42:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][50/1251]	eta 0:18:30 lr 0.000003	time 0.7546 (0.9246)	loss 6.9206 (6.9310)	grad_norm 0.7326 (0.8231)	mem 23815MB
[2022-11-10 23:43:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][100/1251]	eta 0:16:03 lr 0.000005	time 0.7365 (0.8368)	loss 6.9359 (6.9263)	grad_norm 0.6266 (0.7710)	mem 23815MB
[2022-11-10 23:44:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][150/1251]	eta 0:14:47 lr 0.000007	time 0.7360 (0.8063)	loss 6.9262 (6.9226)	grad_norm 1.0181 (0.7366)	mem 23815MB
[2022-11-10 23:44:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][200/1251]	eta 0:13:50 lr 0.000009	time 0.8562 (0.7905)	loss 6.8942 (6.9200)	grad_norm 0.6913 (0.7073)	mem 23815MB
[2022-11-10 23:45:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][250/1251]	eta 0:13:00 lr 0.000011	time 0.7349 (0.7800)	loss 6.9162 (6.9173)	grad_norm 0.5459 (0.6831)	mem 23817MB
[2022-11-10 23:46:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][300/1251]	eta 0:12:15 lr 0.000013	time 0.7417 (0.7734)	loss 6.9204 (6.9144)	grad_norm 0.5664 (0.6626)	mem 23817MB
[2022-11-10 23:46:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][350/1251]	eta 0:11:32 lr 0.000015	time 0.7318 (0.7684)	loss 6.8833 (6.9120)	grad_norm 0.5609 (0.6502)	mem 23817MB
[2022-11-10 23:47:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][400/1251]	eta 0:10:51 lr 0.000017	time 0.7432 (0.7651)	loss 6.9136 (6.9089)	grad_norm 0.6767 (0.6454)	mem 23817MB
[2022-11-10 23:47:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][450/1251]	eta 0:10:10 lr 0.000019	time 0.7394 (0.7626)	loss 6.8781 (6.9057)	grad_norm 0.5910 (0.6489)	mem 23817MB
[2022-11-10 23:48:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][500/1251]	eta 0:09:30 lr 0.000021	time 0.7451 (0.7602)	loss 6.8858 (6.9017)	grad_norm 1.0157 (0.6546)	mem 23817MB
[2022-11-10 23:49:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][550/1251]	eta 0:08:51 lr 0.000023	time 0.8173 (0.7584)	loss 6.8956 (6.8983)	grad_norm 0.7150 (0.6616)	mem 23817MB
[2022-11-10 23:49:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][600/1251]	eta 0:08:12 lr 0.000025	time 0.7387 (0.7568)	loss 6.8182 (6.8948)	grad_norm 1.0148 (0.6696)	mem 23817MB
[2022-11-10 23:50:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][650/1251]	eta 0:07:34 lr 0.000027	time 0.7328 (0.7555)	loss 6.8326 (6.8910)	grad_norm 1.1446 (0.6829)	mem 23817MB
[2022-11-10 23:51:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][700/1251]	eta 0:06:55 lr 0.000029	time 0.7327 (0.7544)	loss 6.8447 (6.8872)	grad_norm 1.3337 (0.6963)	mem 23817MB
[2022-11-10 23:51:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][750/1251]	eta 0:06:17 lr 0.000031	time 0.7402 (0.7534)	loss 6.8539 (6.8837)	grad_norm 0.7632 (0.7125)	mem 23817MB
[2022-11-10 23:52:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][800/1251]	eta 0:05:39 lr 0.000033	time 0.7374 (0.7529)	loss 6.8094 (6.8801)	grad_norm 0.9151 (0.7337)	mem 23817MB
[2022-11-10 23:52:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][850/1251]	eta 0:05:01 lr 0.000035	time 0.7306 (0.7521)	loss 6.7939 (6.8750)	grad_norm 0.9075 (0.7637)	mem 23817MB
[2022-11-10 23:53:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][900/1251]	eta 0:04:23 lr 0.000037	time 0.7377 (0.7514)	loss 6.8080 (6.8701)	grad_norm 1.4339 (0.7905)	mem 23817MB
[2022-11-10 23:54:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][950/1251]	eta 0:03:46 lr 0.000039	time 0.7383 (0.7509)	loss 6.7251 (6.8643)	grad_norm 1.9695 (0.8181)	mem 23817MB
[2022-11-10 23:54:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][1000/1251]	eta 0:03:08 lr 0.000041	time 0.7404 (0.7505)	loss 6.7454 (6.8598)	grad_norm 1.0236 (0.8402)	mem 23817MB
[2022-11-10 23:55:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][1050/1251]	eta 0:02:30 lr 0.000043	time 0.7351 (0.7499)	loss 6.6438 (6.8549)	grad_norm 1.4641 (0.8647)	mem 23817MB
[2022-11-10 23:55:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][1100/1251]	eta 0:01:53 lr 0.000045	time 0.7362 (0.7495)	loss 6.7827 (6.8498)	grad_norm 1.3948 (0.8867)	mem 23817MB
[2022-11-10 23:56:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][1150/1251]	eta 0:01:15 lr 0.000047	time 0.7321 (0.7490)	loss 6.8012 (6.8451)	grad_norm 1.3261 (0.9060)	mem 23817MB
[2022-11-10 23:57:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][1200/1251]	eta 0:00:38 lr 0.000049	time 0.7408 (0.7487)	loss 6.7407 (6.8404)	grad_norm 1.0400 (0.9290)	mem 23817MB
[2022-11-10 23:57:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [0/300][1250/1251]	eta 0:00:00 lr 0.000051	time 0.7317 (0.7482)	loss 6.6587 (6.8351)	grad_norm 1.8252 (0.9515)	mem 23817MB
[2022-11-10 23:57:48 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 0 training takes 0:15:36
[2022-11-10 23:57:49 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_0.pth saving......
[2022-11-10 23:57:50 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_0.pth saved !!!
[2022-11-10 23:57:51 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.765 (1.765)	Loss 6.2661 (6.2661)	Acc@1 1.855 (1.855)	Acc@5 7.910 (7.910)	Mem 23817MB
[2022-11-10 23:58:04 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 1.980 Acc@5 7.050
[2022-11-10 23:58:04 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 2.0%
[2022-11-10 23:58:06 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.831 (1.831)	Loss 6.9348 (6.9348)	Acc@1 0.000 (0.000)	Acc@5 0.391 (0.391)	Mem 23817MB
[2022-11-10 23:58:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.092 Acc@5 0.552
[2022-11-10 23:58:17 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.1%
[2022-11-10 23:58:17 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 1.98% at 0 epoch
[2022-11-10 23:58:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][0/1251]	eta 1:14:05 lr 0.000051	time 3.5539 (3.5539)	loss 6.6313 (6.6313)	grad_norm 1.3922 (1.3922)	mem 23862MB
[2022-11-10 23:58:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][50/1251]	eta 0:15:57 lr 0.000053	time 0.7323 (0.7970)	loss 6.5091 (6.7113)	grad_norm 2.1071 (1.4608)	mem 23862MB
[2022-11-10 23:59:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][100/1251]	eta 0:14:44 lr 0.000055	time 0.7360 (0.7687)	loss 6.6894 (6.6903)	grad_norm 1.9232 (1.4950)	mem 23862MB
[2022-11-11 00:00:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][150/1251]	eta 0:13:57 lr 0.000057	time 0.7337 (0.7603)	loss 6.7960 (6.6904)	grad_norm 1.4998 (1.5575)	mem 23862MB
[2022-11-11 00:00:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][200/1251]	eta 0:13:13 lr 0.000059	time 0.7387 (0.7546)	loss 6.6549 (6.6839)	grad_norm 1.6853 (1.5825)	mem 23862MB
[2022-11-11 00:01:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][250/1251]	eta 0:12:32 lr 0.000061	time 0.7330 (0.7514)	loss 6.7253 (6.6738)	grad_norm 1.4534 (1.6113)	mem 23862MB
[2022-11-11 00:02:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][300/1251]	eta 0:11:52 lr 0.000063	time 0.7305 (0.7496)	loss 6.6092 (6.6694)	grad_norm 1.4140 (1.6164)	mem 23862MB
[2022-11-11 00:02:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][350/1251]	eta 0:11:13 lr 0.000065	time 0.7400 (0.7480)	loss 6.6601 (6.6623)	grad_norm 2.6085 (1.6217)	mem 23862MB
[2022-11-11 00:03:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][400/1251]	eta 0:10:35 lr 0.000067	time 0.7318 (0.7471)	loss 6.3743 (6.6549)	grad_norm 1.5519 (1.6557)	mem 23862MB
[2022-11-11 00:03:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][450/1251]	eta 0:09:57 lr 0.000069	time 0.7377 (0.7461)	loss 6.8128 (6.6473)	grad_norm 1.5056 (1.6744)	mem 23862MB
[2022-11-11 00:04:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][500/1251]	eta 0:09:19 lr 0.000071	time 0.7368 (0.7455)	loss 6.8360 (6.6428)	grad_norm 1.7554 (1.6715)	mem 23862MB
[2022-11-11 00:05:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][550/1251]	eta 0:08:42 lr 0.000073	time 0.7436 (0.7452)	loss 6.5574 (6.6387)	grad_norm 1.7394 (1.6853)	mem 23862MB
[2022-11-11 00:05:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][600/1251]	eta 0:08:04 lr 0.000075	time 0.7363 (0.7448)	loss 6.4440 (6.6314)	grad_norm 1.5590 (1.6880)	mem 23862MB
[2022-11-11 00:06:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][650/1251]	eta 0:07:27 lr 0.000077	time 0.7300 (0.7443)	loss 6.5963 (6.6251)	grad_norm 1.8566 (1.6981)	mem 23862MB
[2022-11-11 00:06:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][700/1251]	eta 0:06:49 lr 0.000079	time 0.7353 (0.7439)	loss 6.5403 (6.6174)	grad_norm 1.4671 (1.6949)	mem 23862MB
[2022-11-11 00:07:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][750/1251]	eta 0:06:12 lr 0.000081	time 0.7435 (0.7437)	loss 6.3128 (6.6100)	grad_norm 1.5038 (1.6979)	mem 23862MB
[2022-11-11 00:08:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][800/1251]	eta 0:05:35 lr 0.000083	time 0.7362 (0.7436)	loss 6.6600 (6.6077)	grad_norm 2.0103 (inf)	mem 23862MB
[2022-11-11 00:08:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][850/1251]	eta 0:04:58 lr 0.000085	time 0.7355 (0.7434)	loss 6.4204 (6.6021)	grad_norm 1.5541 (inf)	mem 23862MB
[2022-11-11 00:09:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][900/1251]	eta 0:04:20 lr 0.000087	time 0.7402 (0.7432)	loss 6.7393 (6.5993)	grad_norm 1.5294 (inf)	mem 23862MB
[2022-11-11 00:10:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][950/1251]	eta 0:03:43 lr 0.000089	time 0.7368 (0.7431)	loss 6.5452 (6.5944)	grad_norm 2.4713 (inf)	mem 23862MB
[2022-11-11 00:10:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][1000/1251]	eta 0:03:06 lr 0.000091	time 0.7424 (0.7430)	loss 6.6238 (6.5872)	grad_norm 2.0239 (inf)	mem 23862MB
[2022-11-11 00:11:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][1050/1251]	eta 0:02:29 lr 0.000093	time 0.7393 (0.7428)	loss 6.3373 (6.5818)	grad_norm 1.7860 (inf)	mem 23862MB
[2022-11-11 00:11:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][1100/1251]	eta 0:01:52 lr 0.000095	time 0.7365 (0.7427)	loss 6.2669 (6.5783)	grad_norm 1.7878 (inf)	mem 23862MB
[2022-11-11 00:12:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][1150/1251]	eta 0:01:15 lr 0.000097	time 0.7359 (0.7427)	loss 6.6505 (6.5732)	grad_norm 2.0168 (inf)	mem 23862MB
[2022-11-11 00:13:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][1200/1251]	eta 0:00:37 lr 0.000099	time 0.7202 (0.7426)	loss 6.6087 (6.5694)	grad_norm 1.6866 (inf)	mem 23862MB
[2022-11-11 00:13:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [1/300][1250/1251]	eta 0:00:00 lr 0.000101	time 0.7297 (0.7424)	loss 6.2012 (6.5642)	grad_norm 2.6364 (inf)	mem 23862MB
[2022-11-11 00:13:46 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 1 training takes 0:15:28
[2022-11-11 00:13:46 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_1.pth saving......
[2022-11-11 00:13:47 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_1.pth saved !!!
[2022-11-11 00:13:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.694 (1.694)	Loss 5.5823 (5.5823)	Acc@1 6.543 (6.543)	Acc@5 16.504 (16.504)	Mem 23862MB
[2022-11-11 00:13:59 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 5.716 Acc@5 16.748
[2022-11-11 00:13:59 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 5.7%
[2022-11-11 00:14:01 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.805 (1.805)	Loss 6.9248 (6.9248)	Acc@1 0.195 (0.195)	Acc@5 0.293 (0.293)	Mem 23862MB
[2022-11-11 00:14:12 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.122 Acc@5 0.558
[2022-11-11 00:14:12 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.1%
[2022-11-11 00:14:12 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 5.72% at 1 epoch
[2022-11-11 00:14:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][0/1251]	eta 0:51:06 lr 0.000101	time 2.4511 (2.4511)	loss 6.3812 (6.3812)	grad_norm 2.0059 (2.0059)	mem 23869MB
[2022-11-11 00:14:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][50/1251]	eta 0:15:33 lr 0.000103	time 0.7391 (0.7771)	loss 6.4927 (6.4550)	grad_norm 1.9049 (1.9755)	mem 23869MB
[2022-11-11 00:15:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][100/1251]	eta 0:14:35 lr 0.000105	time 0.7411 (0.7604)	loss 6.4074 (6.4726)	grad_norm 2.2106 (2.0112)	mem 23869MB
[2022-11-11 00:16:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][150/1251]	eta 0:13:50 lr 0.000107	time 0.7431 (0.7539)	loss 6.1623 (6.4472)	grad_norm 1.3576 (1.9723)	mem 23869MB
[2022-11-11 00:16:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][200/1251]	eta 0:13:08 lr 0.000109	time 0.7461 (0.7506)	loss 6.4908 (6.4560)	grad_norm 2.1476 (1.9577)	mem 23869MB
[2022-11-11 00:17:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][250/1251]	eta 0:12:29 lr 0.000111	time 0.7390 (0.7489)	loss 6.0152 (6.4426)	grad_norm 1.8612 (1.9626)	mem 23869MB
[2022-11-11 00:17:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][300/1251]	eta 0:11:51 lr 0.000113	time 0.7364 (0.7479)	loss 6.5780 (6.4329)	grad_norm 1.6911 (1.9486)	mem 23869MB
[2022-11-11 00:18:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][350/1251]	eta 0:11:13 lr 0.000115	time 0.7371 (0.7474)	loss 6.4532 (6.4259)	grad_norm 1.6837 (1.9435)	mem 23869MB
[2022-11-11 00:19:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][400/1251]	eta 0:10:35 lr 0.000117	time 0.7396 (0.7469)	loss 6.0482 (6.4132)	grad_norm 1.6080 (1.9552)	mem 23869MB
[2022-11-11 00:19:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][450/1251]	eta 0:09:57 lr 0.000119	time 0.8397 (0.7465)	loss 6.6085 (6.4059)	grad_norm 1.7809 (1.9467)	mem 23869MB
[2022-11-11 00:20:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][500/1251]	eta 0:09:20 lr 0.000121	time 0.7313 (0.7459)	loss 6.4609 (6.4035)	grad_norm 1.8390 (1.9502)	mem 23869MB
[2022-11-11 00:21:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][550/1251]	eta 0:08:42 lr 0.000123	time 0.7421 (0.7459)	loss 6.1831 (6.3980)	grad_norm 2.0449 (1.9611)	mem 23869MB
[2022-11-11 00:21:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][600/1251]	eta 0:08:05 lr 0.000125	time 0.7352 (0.7455)	loss 6.2430 (6.3931)	grad_norm 1.8871 (1.9597)	mem 23869MB
[2022-11-11 00:22:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][650/1251]	eta 0:07:28 lr 0.000127	time 0.7445 (0.7454)	loss 6.6322 (6.3897)	grad_norm 1.7863 (1.9731)	mem 23869MB
[2022-11-11 00:22:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][700/1251]	eta 0:06:50 lr 0.000129	time 0.7410 (0.7452)	loss 6.3950 (6.3864)	grad_norm 1.7516 (1.9898)	mem 23869MB
[2022-11-11 00:23:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][750/1251]	eta 0:06:13 lr 0.000131	time 0.7306 (0.7449)	loss 5.9721 (6.3858)	grad_norm 1.5687 (1.9850)	mem 23869MB
[2022-11-11 00:24:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][800/1251]	eta 0:05:35 lr 0.000133	time 0.7384 (0.7450)	loss 6.5170 (6.3811)	grad_norm 1.9446 (1.9869)	mem 23869MB
[2022-11-11 00:24:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][850/1251]	eta 0:04:58 lr 0.000135	time 0.7382 (0.7449)	loss 6.4766 (6.3761)	grad_norm 1.6333 (1.9911)	mem 23869MB
[2022-11-11 00:25:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][900/1251]	eta 0:04:21 lr 0.000137	time 0.7440 (0.7448)	loss 6.4385 (6.3713)	grad_norm 1.9146 (1.9973)	mem 23869MB
[2022-11-11 00:26:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][950/1251]	eta 0:03:44 lr 0.000139	time 0.7415 (0.7448)	loss 6.4663 (6.3671)	grad_norm 2.3916 (2.0039)	mem 23869MB
[2022-11-11 00:26:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][1000/1251]	eta 0:03:06 lr 0.000141	time 0.7424 (0.7446)	loss 6.4163 (6.3607)	grad_norm 2.2062 (2.0007)	mem 23869MB
[2022-11-11 00:27:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][1050/1251]	eta 0:02:29 lr 0.000143	time 0.7496 (0.7447)	loss 6.4173 (6.3548)	grad_norm 1.8789 (2.0023)	mem 23869MB
[2022-11-11 00:27:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][1100/1251]	eta 0:01:52 lr 0.000145	time 0.7337 (0.7446)	loss 5.8549 (6.3513)	grad_norm 1.6378 (2.0007)	mem 23869MB
[2022-11-11 00:28:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][1150/1251]	eta 0:01:15 lr 0.000147	time 0.7417 (0.7444)	loss 6.4861 (6.3510)	grad_norm 2.3868 (inf)	mem 23869MB
[2022-11-11 00:29:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][1200/1251]	eta 0:00:37 lr 0.000149	time 0.7418 (0.7445)	loss 6.3640 (6.3482)	grad_norm 2.6641 (inf)	mem 23869MB
[2022-11-11 00:29:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [2/300][1250/1251]	eta 0:00:00 lr 0.000151	time 0.7265 (0.7442)	loss 6.5196 (6.3443)	grad_norm 2.0505 (inf)	mem 23869MB
[2022-11-11 00:29:43 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 2 training takes 0:15:31
[2022-11-11 00:29:43 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_2.pth saving......
[2022-11-11 00:29:44 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_2.pth saved !!!
[2022-11-11 00:29:46 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.714 (1.714)	Loss 5.0140 (5.0140)	Acc@1 10.449 (10.449)	Acc@5 28.613 (28.613)	Mem 23869MB
[2022-11-11 00:29:56 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 10.910 Acc@5 27.100
[2022-11-11 00:29:56 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 10.9%
[2022-11-11 00:29:58 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.933 (1.933)	Loss 6.9220 (6.9220)	Acc@1 0.195 (0.195)	Acc@5 0.293 (0.293)	Mem 23869MB
[2022-11-11 00:30:09 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.134 Acc@5 0.612
[2022-11-11 00:30:09 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.1%
[2022-11-11 00:30:09 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 10.91% at 2 epoch
[2022-11-11 00:30:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][0/1251]	eta 0:50:31 lr 0.000151	time 2.4232 (2.4232)	loss 6.2324 (6.2324)	grad_norm 2.2504 (2.2504)	mem 23874MB
[2022-11-11 00:30:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][50/1251]	eta 0:15:33 lr 0.000153	time 0.7495 (0.7770)	loss 6.0705 (6.2682)	grad_norm 2.0438 (2.0902)	mem 23874MB
[2022-11-11 00:31:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][100/1251]	eta 0:14:36 lr 0.000155	time 0.7349 (0.7614)	loss 6.0365 (6.2422)	grad_norm 1.6020 (2.0673)	mem 23874MB
[2022-11-11 00:32:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][150/1251]	eta 0:13:50 lr 0.000157	time 0.7404 (0.7539)	loss 6.3126 (6.2119)	grad_norm 2.2132 (2.0889)	mem 23874MB
[2022-11-11 00:32:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][200/1251]	eta 0:13:10 lr 0.000159	time 0.7353 (0.7521)	loss 6.2880 (6.1942)	grad_norm 2.2676 (2.0893)	mem 23874MB
[2022-11-11 00:33:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][250/1251]	eta 0:12:31 lr 0.000161	time 0.7396 (0.7509)	loss 6.5220 (6.1975)	grad_norm 1.8015 (2.0943)	mem 23874MB
[2022-11-11 00:33:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][300/1251]	eta 0:11:53 lr 0.000163	time 0.7361 (0.7498)	loss 6.0276 (6.2024)	grad_norm 2.0435 (2.0886)	mem 23874MB
[2022-11-11 00:34:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][350/1251]	eta 0:11:14 lr 0.000165	time 0.7370 (0.7490)	loss 6.3859 (6.1870)	grad_norm 1.7639 (2.1089)	mem 23874MB
[2022-11-11 00:35:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][400/1251]	eta 0:10:36 lr 0.000167	time 0.7357 (0.7484)	loss 5.6600 (6.1829)	grad_norm 2.4264 (2.1081)	mem 23874MB
[2022-11-11 00:35:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][450/1251]	eta 0:09:58 lr 0.000169	time 0.7401 (0.7477)	loss 6.0447 (6.1797)	grad_norm 1.6549 (2.1036)	mem 23874MB
[2022-11-11 00:36:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][500/1251]	eta 0:09:21 lr 0.000171	time 0.7429 (0.7474)	loss 6.3272 (6.1786)	grad_norm 2.1925 (2.1126)	mem 23874MB
[2022-11-11 00:37:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][550/1251]	eta 0:08:43 lr 0.000173	time 0.7348 (0.7472)	loss 6.2812 (6.1710)	grad_norm 2.2138 (2.1107)	mem 23874MB
[2022-11-11 00:37:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][600/1251]	eta 0:08:06 lr 0.000175	time 0.7419 (0.7471)	loss 6.2038 (6.1675)	grad_norm 2.4248 (2.1159)	mem 23874MB
[2022-11-11 00:38:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][650/1251]	eta 0:07:28 lr 0.000177	time 0.7368 (0.7467)	loss 6.3577 (6.1663)	grad_norm 2.0073 (2.1187)	mem 23874MB
[2022-11-11 00:38:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][700/1251]	eta 0:06:51 lr 0.000179	time 0.7366 (0.7464)	loss 5.8719 (6.1636)	grad_norm 2.4373 (2.1221)	mem 23874MB
[2022-11-11 00:39:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][750/1251]	eta 0:06:13 lr 0.000181	time 0.7370 (0.7463)	loss 5.7260 (6.1603)	grad_norm 2.1633 (2.1242)	mem 23874MB
[2022-11-11 00:40:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][800/1251]	eta 0:05:36 lr 0.000183	time 0.7431 (0.7462)	loss 6.3821 (6.1578)	grad_norm 1.9733 (2.1182)	mem 23874MB
[2022-11-11 00:40:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][850/1251]	eta 0:04:59 lr 0.000185	time 0.7297 (0.7462)	loss 6.2264 (6.1579)	grad_norm 1.8623 (2.1210)	mem 23874MB
[2022-11-11 00:41:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][900/1251]	eta 0:04:21 lr 0.000187	time 0.7359 (0.7461)	loss 6.2748 (6.1534)	grad_norm 1.8686 (2.1242)	mem 23874MB
[2022-11-11 00:41:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][950/1251]	eta 0:03:44 lr 0.000189	time 0.7596 (0.7459)	loss 6.2232 (6.1538)	grad_norm 2.1751 (2.1269)	mem 23874MB
[2022-11-11 00:42:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][1000/1251]	eta 0:03:07 lr 0.000191	time 0.7362 (0.7458)	loss 6.4219 (6.1501)	grad_norm 1.8630 (2.1320)	mem 23874MB
[2022-11-11 00:43:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][1050/1251]	eta 0:02:29 lr 0.000193	time 0.7360 (0.7458)	loss 6.4339 (6.1456)	grad_norm 1.8901 (2.1374)	mem 23874MB
[2022-11-11 00:43:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][1100/1251]	eta 0:01:52 lr 0.000195	time 0.7378 (0.7458)	loss 6.3658 (6.1402)	grad_norm 1.9857 (2.1495)	mem 23874MB
[2022-11-11 00:44:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][1150/1251]	eta 0:01:15 lr 0.000197	time 0.7405 (0.7458)	loss 5.6654 (6.1308)	grad_norm 1.9225 (2.1552)	mem 23874MB
[2022-11-11 00:45:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][1200/1251]	eta 0:00:38 lr 0.000199	time 0.7376 (0.7458)	loss 6.2705 (6.1291)	grad_norm 1.7642 (2.1575)	mem 23874MB
[2022-11-11 00:45:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [3/300][1250/1251]	eta 0:00:00 lr 0.000201	time 0.7255 (0.7455)	loss 6.2525 (6.1260)	grad_norm 2.0352 (2.1560)	mem 23874MB
[2022-11-11 00:45:42 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 3 training takes 0:15:32
[2022-11-11 00:45:42 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_3.pth saving......
[2022-11-11 00:45:43 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_3.pth saved !!!
[2022-11-11 00:45:45 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.619 (1.619)	Loss 4.4284 (4.4284)	Acc@1 17.188 (17.188)	Acc@5 38.379 (38.379)	Mem 23874MB
[2022-11-11 00:45:56 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 17.320 Acc@5 38.114
[2022-11-11 00:45:56 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 17.3%
[2022-11-11 00:45:57 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.785 (1.785)	Loss 6.9245 (6.9245)	Acc@1 0.195 (0.195)	Acc@5 0.781 (0.781)	Mem 23874MB
[2022-11-11 00:46:08 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.192 Acc@5 0.822
[2022-11-11 00:46:08 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.2%
[2022-11-11 00:46:08 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 17.32% at 3 epoch
[2022-11-11 00:46:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][0/1251]	eta 0:50:12 lr 0.000201	time 2.4080 (2.4080)	loss 6.2394 (6.2394)	grad_norm 2.4586 (2.4586)	mem 23874MB
[2022-11-11 00:46:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][50/1251]	eta 0:15:35 lr 0.000203	time 0.7415 (0.7791)	loss 6.1172 (5.9668)	grad_norm 2.1961 (2.1520)	mem 23874MB
[2022-11-11 00:47:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][100/1251]	eta 0:14:41 lr 0.000205	time 0.7492 (0.7657)	loss 6.4439 (5.9657)	grad_norm 2.2092 (2.1516)	mem 23874MB
[2022-11-11 00:48:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][150/1251]	eta 0:13:55 lr 0.000207	time 0.7360 (0.7590)	loss 5.8868 (5.9795)	grad_norm 1.7860 (2.1862)	mem 23874MB
[2022-11-11 00:48:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][200/1251]	eta 0:13:14 lr 0.000209	time 0.7382 (0.7562)	loss 6.2929 (5.9873)	grad_norm 2.2134 (2.2111)	mem 23874MB
[2022-11-11 00:49:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][250/1251]	eta 0:12:35 lr 0.000211	time 0.8080 (0.7552)	loss 6.3122 (5.9955)	grad_norm 2.6361 (2.1948)	mem 23874MB
[2022-11-11 00:49:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][300/1251]	eta 0:11:56 lr 0.000213	time 0.7355 (0.7530)	loss 5.8282 (5.9976)	grad_norm 2.5436 (2.2125)	mem 23874MB
[2022-11-11 00:50:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][350/1251]	eta 0:11:18 lr 0.000215	time 0.7436 (0.7525)	loss 6.3043 (5.9907)	grad_norm 2.2101 (2.2327)	mem 23874MB
[2022-11-11 00:51:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][400/1251]	eta 0:10:39 lr 0.000217	time 0.7413 (0.7517)	loss 6.4933 (5.9851)	grad_norm 2.0239 (2.2291)	mem 23874MB
[2022-11-11 00:51:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][450/1251]	eta 0:10:01 lr 0.000219	time 0.7413 (0.7512)	loss 5.6609 (5.9808)	grad_norm 3.0617 (2.2171)	mem 23874MB
[2022-11-11 00:52:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][500/1251]	eta 0:09:24 lr 0.000221	time 0.8342 (0.7512)	loss 6.1256 (5.9783)	grad_norm 1.7728 (2.2222)	mem 23874MB
[2022-11-11 00:53:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][550/1251]	eta 0:08:46 lr 0.000223	time 0.7444 (0.7506)	loss 6.2019 (5.9686)	grad_norm 2.0060 (2.2243)	mem 23874MB
[2022-11-11 00:53:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][600/1251]	eta 0:08:08 lr 0.000225	time 0.7419 (0.7502)	loss 5.1354 (5.9615)	grad_norm 1.7815 (2.2182)	mem 23874MB
[2022-11-11 00:54:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][650/1251]	eta 0:07:30 lr 0.000227	time 0.8096 (0.7501)	loss 6.0550 (5.9586)	grad_norm 1.9760 (inf)	mem 23874MB
[2022-11-11 00:54:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][700/1251]	eta 0:06:53 lr 0.000229	time 0.7417 (0.7497)	loss 5.7182 (5.9544)	grad_norm 1.9770 (inf)	mem 23874MB
[2022-11-11 00:55:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][750/1251]	eta 0:06:15 lr 0.000231	time 0.7344 (0.7496)	loss 5.6729 (5.9464)	grad_norm 1.9154 (inf)	mem 23874MB
[2022-11-11 00:56:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][800/1251]	eta 0:05:37 lr 0.000233	time 0.7468 (0.7494)	loss 6.2900 (5.9415)	grad_norm 2.2133 (inf)	mem 23874MB
[2022-11-11 00:56:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][850/1251]	eta 0:05:00 lr 0.000235	time 0.7380 (0.7494)	loss 6.0727 (5.9397)	grad_norm 3.0273 (inf)	mem 23874MB
[2022-11-11 00:57:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][900/1251]	eta 0:04:23 lr 0.000237	time 0.7386 (0.7493)	loss 6.2784 (5.9402)	grad_norm 1.9820 (inf)	mem 23874MB
[2022-11-11 00:58:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][950/1251]	eta 0:03:45 lr 0.000239	time 0.7434 (0.7491)	loss 5.4823 (5.9389)	grad_norm 2.1774 (inf)	mem 23874MB
[2022-11-11 00:58:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][1000/1251]	eta 0:03:07 lr 0.000241	time 0.7364 (0.7489)	loss 5.9342 (5.9350)	grad_norm 2.3389 (inf)	mem 23874MB
[2022-11-11 00:59:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][1050/1251]	eta 0:02:30 lr 0.000243	time 0.8102 (0.7491)	loss 5.6588 (5.9285)	grad_norm 2.2545 (inf)	mem 23874MB
[2022-11-11 00:59:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][1100/1251]	eta 0:01:53 lr 0.000245	time 0.7419 (0.7488)	loss 5.4213 (5.9255)	grad_norm 2.2358 (inf)	mem 23874MB
[2022-11-11 01:00:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][1150/1251]	eta 0:01:15 lr 0.000247	time 0.7516 (0.7489)	loss 6.2129 (5.9173)	grad_norm 1.8467 (inf)	mem 23874MB
[2022-11-11 01:01:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][1200/1251]	eta 0:00:38 lr 0.000249	time 0.7384 (0.7489)	loss 5.3977 (5.9158)	grad_norm 2.2280 (inf)	mem 23874MB
[2022-11-11 01:01:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [4/300][1250/1251]	eta 0:00:00 lr 0.000251	time 0.7259 (0.7487)	loss 5.4543 (5.9107)	grad_norm 2.3336 (inf)	mem 23874MB
[2022-11-11 01:01:45 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 4 training takes 0:15:36
[2022-11-11 01:01:45 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_4.pth saving......
[2022-11-11 01:01:46 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_4.pth saved !!!
[2022-11-11 01:01:48 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.611 (1.611)	Loss 3.7888 (3.7888)	Acc@1 26.074 (26.074)	Acc@5 50.879 (50.879)	Mem 23874MB
[2022-11-11 01:01:59 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 24.752 Acc@5 48.348
[2022-11-11 01:01:59 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 24.8%
[2022-11-11 01:02:00 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.890 (1.890)	Loss 6.8833 (6.8833)	Acc@1 0.000 (0.000)	Acc@5 1.074 (1.074)	Mem 23874MB
[2022-11-11 01:02:11 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.194 Acc@5 0.910
[2022-11-11 01:02:11 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.2%
[2022-11-11 01:02:11 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 24.75% at 4 epoch
[2022-11-11 01:02:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][0/1251]	eta 0:49:52 lr 0.000251	time 2.3920 (2.3920)	loss 5.9654 (5.9654)	grad_norm 1.8240 (1.8240)	mem 23874MB
[2022-11-11 01:02:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][50/1251]	eta 0:15:35 lr 0.000253	time 0.7403 (0.7789)	loss 6.1023 (5.9307)	grad_norm 2.3887 (2.2662)	mem 23874MB
[2022-11-11 01:03:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][100/1251]	eta 0:14:39 lr 0.000255	time 0.7365 (0.7644)	loss 5.4062 (5.8668)	grad_norm 2.4169 (2.2828)	mem 23874MB
[2022-11-11 01:04:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][150/1251]	eta 0:13:54 lr 0.000257	time 0.7345 (0.7576)	loss 6.2955 (5.8575)	grad_norm 1.9366 (2.2494)	mem 23874MB
[2022-11-11 01:04:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][200/1251]	eta 0:13:13 lr 0.000259	time 0.7380 (0.7551)	loss 4.8991 (5.8205)	grad_norm 2.4552 (2.2579)	mem 23874MB
[2022-11-11 01:05:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][250/1251]	eta 0:12:33 lr 0.000261	time 0.8106 (0.7532)	loss 6.0675 (5.7916)	grad_norm 1.5338 (2.2534)	mem 23874MB
[2022-11-11 01:05:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][300/1251]	eta 0:11:55 lr 0.000263	time 0.7391 (0.7521)	loss 5.5273 (5.7919)	grad_norm 2.3848 (2.2595)	mem 23874MB
[2022-11-11 01:06:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][350/1251]	eta 0:11:16 lr 0.000265	time 0.7345 (0.7513)	loss 5.1007 (5.7813)	grad_norm 2.1496 (2.2393)	mem 23874MB
[2022-11-11 01:07:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][400/1251]	eta 0:10:38 lr 0.000267	time 0.7385 (0.7506)	loss 5.8423 (5.7758)	grad_norm 2.0841 (2.2333)	mem 23874MB
[2022-11-11 01:07:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][450/1251]	eta 0:10:00 lr 0.000269	time 0.7379 (0.7501)	loss 6.0623 (5.7683)	grad_norm 2.4365 (2.2286)	mem 23874MB
[2022-11-11 01:08:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][500/1251]	eta 0:09:22 lr 0.000271	time 0.7424 (0.7495)	loss 6.1891 (5.7657)	grad_norm 1.9407 (2.2323)	mem 23874MB
[2022-11-11 01:09:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][550/1251]	eta 0:08:45 lr 0.000273	time 0.7422 (0.7490)	loss 5.0424 (5.7603)	grad_norm 2.1320 (2.2338)	mem 23874MB
[2022-11-11 01:09:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][600/1251]	eta 0:08:07 lr 0.000275	time 0.7359 (0.7487)	loss 5.4546 (5.7491)	grad_norm 1.6910 (2.2328)	mem 23874MB
[2022-11-11 01:10:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][650/1251]	eta 0:07:29 lr 0.000277	time 0.7364 (0.7487)	loss 4.7609 (5.7391)	grad_norm 2.0516 (2.2290)	mem 23874MB
[2022-11-11 01:10:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][700/1251]	eta 0:06:52 lr 0.000279	time 0.7416 (0.7485)	loss 5.4978 (5.7291)	grad_norm 2.6012 (2.2218)	mem 23874MB
[2022-11-11 01:11:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][750/1251]	eta 0:06:14 lr 0.000281	time 0.7363 (0.7484)	loss 5.3286 (5.7188)	grad_norm 2.1861 (2.2190)	mem 23874MB
[2022-11-11 01:12:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][800/1251]	eta 0:05:37 lr 0.000283	time 0.7353 (0.7482)	loss 6.0100 (5.7129)	grad_norm 2.9861 (2.2225)	mem 23874MB
[2022-11-11 01:12:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][850/1251]	eta 0:04:59 lr 0.000285	time 0.7441 (0.7480)	loss 5.3019 (5.7110)	grad_norm 2.6320 (2.2243)	mem 23874MB
[2022-11-11 01:13:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][900/1251]	eta 0:04:22 lr 0.000287	time 0.7386 (0.7480)	loss 5.9350 (5.7108)	grad_norm 1.6906 (2.2261)	mem 23874MB
[2022-11-11 01:14:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][950/1251]	eta 0:03:45 lr 0.000289	time 0.8085 (0.7480)	loss 4.9067 (5.7078)	grad_norm 2.5660 (2.2357)	mem 23874MB
[2022-11-11 01:14:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][1000/1251]	eta 0:03:07 lr 0.000291	time 0.7408 (0.7478)	loss 5.6476 (5.7035)	grad_norm 2.2279 (2.2304)	mem 23874MB
[2022-11-11 01:15:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][1050/1251]	eta 0:02:30 lr 0.000293	time 0.7414 (0.7479)	loss 5.3531 (5.7012)	grad_norm 2.9751 (2.2314)	mem 23874MB
[2022-11-11 01:15:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][1100/1251]	eta 0:01:52 lr 0.000295	time 0.7362 (0.7477)	loss 6.1152 (5.6982)	grad_norm 2.1998 (2.2274)	mem 23874MB
[2022-11-11 01:16:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][1150/1251]	eta 0:01:15 lr 0.000297	time 0.7364 (0.7479)	loss 5.4276 (5.6921)	grad_norm 2.6282 (2.2303)	mem 23874MB
[2022-11-11 01:17:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][1200/1251]	eta 0:00:38 lr 0.000299	time 0.7370 (0.7477)	loss 6.0316 (5.6871)	grad_norm 2.2171 (2.2247)	mem 23874MB
[2022-11-11 01:17:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [5/300][1250/1251]	eta 0:00:00 lr 0.000301	time 0.7273 (0.7476)	loss 5.5598 (5.6826)	grad_norm 3.4951 (2.2257)	mem 23874MB
[2022-11-11 01:17:47 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 5 training takes 0:15:35
[2022-11-11 01:17:47 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_5.pth saving......
[2022-11-11 01:17:48 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_5.pth saved !!!
[2022-11-11 01:17:50 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.606 (1.606)	Loss 3.4160 (3.4160)	Acc@1 31.641 (31.641)	Acc@5 56.055 (56.055)	Mem 23874MB
[2022-11-11 01:18:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 31.514 Acc@5 56.580
[2022-11-11 01:18:00 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 31.5%
[2022-11-11 01:18:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.866 (1.866)	Loss 6.8859 (6.8859)	Acc@1 0.098 (0.098)	Acc@5 1.270 (1.270)	Mem 23874MB
[2022-11-11 01:18:13 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.248 Acc@5 1.076
[2022-11-11 01:18:13 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.2%
[2022-11-11 01:18:13 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 31.51% at 5 epoch
[2022-11-11 01:18:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][0/1251]	eta 0:50:14 lr 0.000301	time 2.4093 (2.4093)	loss 4.8763 (4.8763)	grad_norm 2.1849 (2.1849)	mem 23874MB
[2022-11-11 01:18:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][50/1251]	eta 0:15:42 lr 0.000303	time 0.7388 (0.7849)	loss 4.5660 (5.4910)	grad_norm 1.8254 (2.2276)	mem 23874MB
[2022-11-11 01:19:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][100/1251]	eta 0:14:40 lr 0.000305	time 0.7386 (0.7651)	loss 5.9477 (5.5283)	grad_norm 2.3741 (2.1451)	mem 23874MB
[2022-11-11 01:20:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][150/1251]	eta 0:13:56 lr 0.000307	time 0.7419 (0.7594)	loss 5.9151 (5.5432)	grad_norm 2.4285 (2.1594)	mem 23874MB
[2022-11-11 01:20:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][200/1251]	eta 0:13:14 lr 0.000309	time 0.8147 (0.7560)	loss 5.4457 (5.5589)	grad_norm 2.1519 (inf)	mem 23874MB
[2022-11-11 01:21:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][250/1251]	eta 0:12:34 lr 0.000311	time 0.7356 (0.7534)	loss 5.7207 (5.5327)	grad_norm 1.8527 (inf)	mem 23874MB
[2022-11-11 01:21:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][300/1251]	eta 0:11:55 lr 0.000313	time 0.7473 (0.7522)	loss 5.8103 (5.5351)	grad_norm 2.0176 (inf)	mem 23874MB
[2022-11-11 01:22:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][350/1251]	eta 0:11:16 lr 0.000315	time 0.7443 (0.7513)	loss 5.6636 (5.5310)	grad_norm 1.5891 (inf)	mem 23874MB
[2022-11-11 01:23:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][400/1251]	eta 0:10:38 lr 0.000317	time 0.7354 (0.7505)	loss 6.0425 (5.5429)	grad_norm 1.7813 (inf)	mem 23874MB
[2022-11-11 01:23:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][450/1251]	eta 0:10:00 lr 0.000319	time 0.7410 (0.7502)	loss 5.8815 (5.5327)	grad_norm 2.0869 (inf)	mem 23874MB
[2022-11-11 01:24:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][500/1251]	eta 0:09:22 lr 0.000321	time 0.7381 (0.7495)	loss 5.7417 (5.5271)	grad_norm 2.2340 (inf)	mem 23874MB
[2022-11-11 01:25:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][550/1251]	eta 0:08:45 lr 0.000323	time 0.7449 (0.7493)	loss 4.4698 (5.5126)	grad_norm 1.8730 (inf)	mem 23874MB
[2022-11-11 01:25:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][600/1251]	eta 0:08:07 lr 0.000325	time 0.7329 (0.7489)	loss 5.8129 (5.5172)	grad_norm 1.6882 (inf)	mem 23874MB
[2022-11-11 01:26:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][650/1251]	eta 0:07:29 lr 0.000327	time 0.7423 (0.7486)	loss 5.5854 (5.5119)	grad_norm 2.3607 (inf)	mem 23874MB
[2022-11-11 01:26:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][700/1251]	eta 0:06:52 lr 0.000329	time 0.7381 (0.7486)	loss 5.8151 (5.5104)	grad_norm 2.2560 (inf)	mem 23874MB
[2022-11-11 01:27:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][750/1251]	eta 0:06:14 lr 0.000331	time 0.7402 (0.7483)	loss 5.9231 (5.5029)	grad_norm 1.7345 (inf)	mem 23874MB
[2022-11-11 01:28:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][800/1251]	eta 0:05:37 lr 0.000333	time 0.7468 (0.7481)	loss 4.6980 (5.5025)	grad_norm 2.2044 (inf)	mem 23874MB
[2022-11-11 01:28:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][850/1251]	eta 0:04:59 lr 0.000335	time 0.7375 (0.7479)	loss 5.8695 (5.4999)	grad_norm 2.0854 (inf)	mem 23874MB
[2022-11-11 01:29:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][900/1251]	eta 0:04:22 lr 0.000337	time 0.7428 (0.7478)	loss 5.2989 (5.5014)	grad_norm 1.7333 (inf)	mem 23874MB
[2022-11-11 01:30:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][950/1251]	eta 0:03:45 lr 0.000339	time 0.7379 (0.7477)	loss 5.6388 (5.5004)	grad_norm 1.9944 (inf)	mem 23874MB
[2022-11-11 01:30:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][1000/1251]	eta 0:03:07 lr 0.000341	time 0.8105 (0.7477)	loss 6.1365 (5.4983)	grad_norm 2.3092 (inf)	mem 23874MB
[2022-11-11 01:31:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][1050/1251]	eta 0:02:30 lr 0.000343	time 0.7415 (0.7475)	loss 5.6217 (5.4978)	grad_norm 1.9966 (inf)	mem 23874MB
[2022-11-11 01:31:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][1100/1251]	eta 0:01:52 lr 0.000345	time 0.7407 (0.7474)	loss 5.7264 (5.4963)	grad_norm 3.2730 (inf)	mem 23874MB
[2022-11-11 01:32:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][1150/1251]	eta 0:01:15 lr 0.000347	time 0.7391 (0.7473)	loss 5.0136 (5.4922)	grad_norm 1.8973 (inf)	mem 23874MB
[2022-11-11 01:33:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][1200/1251]	eta 0:00:38 lr 0.000349	time 0.7404 (0.7473)	loss 4.9845 (5.4853)	grad_norm 2.0119 (inf)	mem 23874MB
[2022-11-11 01:33:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [6/300][1250/1251]	eta 0:00:00 lr 0.000351	time 0.7281 (0.7472)	loss 5.5145 (5.4862)	grad_norm 2.1961 (inf)	mem 23874MB
[2022-11-11 01:33:48 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 6 training takes 0:15:34
[2022-11-11 01:33:48 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_6.pth saving......
[2022-11-11 01:33:49 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_6.pth saved !!!
[2022-11-11 01:33:51 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.704 (1.704)	Loss 3.0067 (3.0067)	Acc@1 35.547 (35.547)	Acc@5 62.891 (62.891)	Mem 23874MB
[2022-11-11 01:34:01 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 37.908 Acc@5 63.498
[2022-11-11 01:34:01 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 37.9%
[2022-11-11 01:34:03 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.931 (1.931)	Loss 6.8615 (6.8615)	Acc@1 0.195 (0.195)	Acc@5 1.172 (1.172)	Mem 23874MB
[2022-11-11 01:34:14 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.372 Acc@5 1.494
[2022-11-11 01:34:14 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.4%
[2022-11-11 01:34:14 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 37.91% at 6 epoch
[2022-11-11 01:34:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][0/1251]	eta 0:52:51 lr 0.000351	time 2.5353 (2.5353)	loss 5.7509 (5.7509)	grad_norm 1.4937 (1.4937)	mem 23874MB
[2022-11-11 01:34:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][50/1251]	eta 0:15:39 lr 0.000353	time 0.7494 (0.7822)	loss 5.2541 (5.3798)	grad_norm 2.1582 (2.1116)	mem 23874MB
[2022-11-11 01:35:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][100/1251]	eta 0:14:39 lr 0.000355	time 0.7412 (0.7642)	loss 4.9558 (5.3825)	grad_norm 2.0875 (2.0764)	mem 23874MB
[2022-11-11 01:36:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][150/1251]	eta 0:13:54 lr 0.000357	time 0.7417 (0.7580)	loss 5.8923 (5.3588)	grad_norm 2.1041 (2.1033)	mem 23874MB
[2022-11-11 01:36:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][200/1251]	eta 0:13:14 lr 0.000359	time 0.8455 (0.7560)	loss 5.8076 (5.3702)	grad_norm 1.6303 (2.0933)	mem 23874MB
[2022-11-11 01:37:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][250/1251]	eta 0:12:35 lr 0.000361	time 0.7434 (0.7543)	loss 4.4309 (5.3556)	grad_norm 1.6508 (2.0742)	mem 23874MB
[2022-11-11 01:38:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][300/1251]	eta 0:11:55 lr 0.000363	time 0.7437 (0.7528)	loss 5.0093 (5.3474)	grad_norm 1.8259 (2.0900)	mem 23874MB
[2022-11-11 01:38:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][350/1251]	eta 0:11:17 lr 0.000365	time 0.7430 (0.7522)	loss 5.6877 (5.3473)	grad_norm 1.4900 (2.0681)	mem 23874MB
[2022-11-11 01:39:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][400/1251]	eta 0:10:39 lr 0.000367	time 0.7542 (0.7517)	loss 5.1735 (5.3340)	grad_norm 1.6328 (2.0701)	mem 23874MB
[2022-11-11 01:39:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][450/1251]	eta 0:10:01 lr 0.000369	time 0.7379 (0.7512)	loss 5.5664 (5.3262)	grad_norm 2.3648 (2.0583)	mem 23874MB
[2022-11-11 01:40:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][500/1251]	eta 0:09:23 lr 0.000371	time 0.8257 (0.7508)	loss 5.0992 (5.3267)	grad_norm 1.9784 (2.0670)	mem 23874MB
[2022-11-11 01:41:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][550/1251]	eta 0:08:46 lr 0.000373	time 0.7426 (0.7504)	loss 5.3320 (5.3279)	grad_norm 1.9086 (2.0651)	mem 23874MB
[2022-11-11 01:41:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][600/1251]	eta 0:08:08 lr 0.000375	time 0.7365 (0.7501)	loss 4.9418 (5.3188)	grad_norm 1.5963 (2.0574)	mem 23874MB
[2022-11-11 01:42:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][650/1251]	eta 0:07:30 lr 0.000377	time 0.7359 (0.7498)	loss 5.8459 (5.3212)	grad_norm 2.0179 (2.0523)	mem 23874MB
[2022-11-11 01:42:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][700/1251]	eta 0:06:53 lr 0.000379	time 0.7431 (0.7496)	loss 5.1847 (5.3252)	grad_norm 2.3294 (2.0535)	mem 23874MB
[2022-11-11 01:43:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][750/1251]	eta 0:06:15 lr 0.000381	time 0.7514 (0.7494)	loss 4.7705 (5.3227)	grad_norm 1.9481 (2.0534)	mem 23874MB
[2022-11-11 01:44:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][800/1251]	eta 0:05:37 lr 0.000383	time 0.7371 (0.7494)	loss 5.8921 (5.3194)	grad_norm 1.6423 (2.0476)	mem 23874MB
[2022-11-11 01:44:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][850/1251]	eta 0:05:00 lr 0.000385	time 0.7384 (0.7493)	loss 5.3343 (5.3104)	grad_norm 1.6557 (2.0412)	mem 23874MB
[2022-11-11 01:45:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][900/1251]	eta 0:04:22 lr 0.000387	time 0.7423 (0.7491)	loss 4.3518 (5.3066)	grad_norm 1.6829 (2.0370)	mem 23874MB
[2022-11-11 01:46:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][950/1251]	eta 0:03:45 lr 0.000389	time 0.7269 (0.7491)	loss 5.1808 (5.3136)	grad_norm 1.6873 (2.0326)	mem 23874MB
[2022-11-11 01:46:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][1000/1251]	eta 0:03:07 lr 0.000391	time 0.7372 (0.7490)	loss 5.7448 (5.3101)	grad_norm 2.1133 (2.0257)	mem 23874MB
[2022-11-11 01:47:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][1050/1251]	eta 0:02:30 lr 0.000393	time 0.7386 (0.7489)	loss 5.2108 (5.3059)	grad_norm 2.1461 (2.0252)	mem 23874MB
[2022-11-11 01:47:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][1100/1251]	eta 0:01:53 lr 0.000395	time 0.7462 (0.7488)	loss 5.3279 (5.3002)	grad_norm 2.0902 (2.0210)	mem 23874MB
[2022-11-11 01:48:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][1150/1251]	eta 0:01:15 lr 0.000397	time 0.7385 (0.7488)	loss 5.1163 (5.2964)	grad_norm 1.8031 (2.0181)	mem 23874MB
[2022-11-11 01:49:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][1200/1251]	eta 0:00:38 lr 0.000399	time 0.7364 (0.7487)	loss 4.1134 (5.2937)	grad_norm 1.6030 (2.0185)	mem 23874MB
[2022-11-11 01:49:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [7/300][1250/1251]	eta 0:00:00 lr 0.000401	time 0.7273 (0.7484)	loss 4.9641 (5.2888)	grad_norm 2.3008 (inf)	mem 23874MB
[2022-11-11 01:49:50 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 7 training takes 0:15:36
[2022-11-11 01:49:51 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_7.pth saving......
[2022-11-11 01:49:52 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_7.pth saved !!!
[2022-11-11 01:49:53 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.684 (1.684)	Loss 2.6276 (2.6276)	Acc@1 43.066 (43.066)	Acc@5 69.531 (69.531)	Mem 23874MB
[2022-11-11 01:50:04 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 43.248 Acc@5 68.588
[2022-11-11 01:50:04 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 43.2%
[2022-11-11 01:50:06 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.944 (1.944)	Loss 6.7869 (6.7869)	Acc@1 0.781 (0.781)	Acc@5 2.246 (2.246)	Mem 23874MB
[2022-11-11 01:50:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.576 Acc@5 2.236
[2022-11-11 01:50:17 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.6%
[2022-11-11 01:50:17 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 43.25% at 7 epoch
[2022-11-11 01:50:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][0/1251]	eta 0:48:12 lr 0.000401	time 2.3118 (2.3118)	loss 5.5942 (5.5942)	grad_norm 1.9718 (1.9718)	mem 23874MB
[2022-11-11 01:50:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][50/1251]	eta 0:15:35 lr 0.000403	time 0.7385 (0.7788)	loss 5.9102 (5.3575)	grad_norm 1.5385 (2.0292)	mem 23874MB
[2022-11-11 01:51:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][100/1251]	eta 0:14:38 lr 0.000405	time 0.7366 (0.7629)	loss 4.6305 (5.3009)	grad_norm 1.6719 (1.9930)	mem 23874MB
[2022-11-11 01:52:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][150/1251]	eta 0:13:54 lr 0.000407	time 0.7384 (0.7583)	loss 5.0192 (5.2688)	grad_norm 1.5812 (2.0321)	mem 23874MB
[2022-11-11 01:52:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][200/1251]	eta 0:13:14 lr 0.000409	time 0.7402 (0.7556)	loss 4.5275 (5.2747)	grad_norm 1.5519 (2.0016)	mem 23874MB
[2022-11-11 01:53:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][250/1251]	eta 0:12:34 lr 0.000411	time 0.7463 (0.7541)	loss 5.5176 (5.2680)	grad_norm 1.8206 (1.9849)	mem 23874MB
[2022-11-11 01:54:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][300/1251]	eta 0:11:55 lr 0.000413	time 0.7376 (0.7527)	loss 4.5301 (5.2624)	grad_norm 2.0960 (1.9802)	mem 23874MB
[2022-11-11 01:54:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][350/1251]	eta 0:11:17 lr 0.000415	time 0.7390 (0.7519)	loss 5.4695 (5.2428)	grad_norm 2.1859 (1.9842)	mem 23874MB
[2022-11-11 01:55:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][400/1251]	eta 0:10:39 lr 0.000417	time 0.7371 (0.7513)	loss 5.5127 (5.2204)	grad_norm 1.6222 (1.9743)	mem 23874MB
[2022-11-11 01:55:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][450/1251]	eta 0:10:01 lr 0.000419	time 0.7357 (0.7508)	loss 5.0190 (5.2065)	grad_norm 2.1981 (1.9770)	mem 23874MB
[2022-11-11 01:56:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][500/1251]	eta 0:09:23 lr 0.000421	time 0.8205 (0.7504)	loss 5.3542 (5.2038)	grad_norm 1.8105 (1.9635)	mem 23874MB
[2022-11-11 01:57:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][550/1251]	eta 0:08:45 lr 0.000423	time 0.7412 (0.7499)	loss 4.2683 (5.1965)	grad_norm 1.7365 (1.9628)	mem 23874MB
[2022-11-11 01:57:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][600/1251]	eta 0:08:08 lr 0.000425	time 0.7423 (0.7499)	loss 5.4696 (5.1785)	grad_norm 1.8167 (1.9627)	mem 23874MB
[2022-11-11 01:58:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][650/1251]	eta 0:07:30 lr 0.000427	time 0.7434 (0.7495)	loss 5.2428 (5.1735)	grad_norm 1.8311 (1.9531)	mem 23874MB
[2022-11-11 01:59:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][700/1251]	eta 0:06:52 lr 0.000429	time 0.7414 (0.7494)	loss 4.4251 (5.1753)	grad_norm 2.6667 (1.9427)	mem 23874MB
[2022-11-11 01:59:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][750/1251]	eta 0:06:15 lr 0.000431	time 0.7425 (0.7492)	loss 4.4435 (5.1699)	grad_norm 1.8908 (1.9453)	mem 23874MB
[2022-11-11 02:00:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][800/1251]	eta 0:05:37 lr 0.000433	time 0.7388 (0.7490)	loss 5.4605 (5.1637)	grad_norm 1.6350 (1.9467)	mem 23874MB
[2022-11-11 02:00:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][850/1251]	eta 0:05:00 lr 0.000435	time 0.7411 (0.7488)	loss 5.3071 (5.1628)	grad_norm 2.0391 (1.9435)	mem 23874MB
[2022-11-11 02:01:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][900/1251]	eta 0:04:22 lr 0.000437	time 0.7440 (0.7487)	loss 5.0488 (5.1599)	grad_norm 2.0696 (1.9433)	mem 23874MB
[2022-11-11 02:02:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][950/1251]	eta 0:03:45 lr 0.000439	time 0.7603 (0.7486)	loss 3.9149 (5.1535)	grad_norm 1.9561 (1.9370)	mem 23874MB
[2022-11-11 02:02:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][1000/1251]	eta 0:03:07 lr 0.000441	time 0.8131 (0.7486)	loss 5.4898 (5.1559)	grad_norm 1.6149 (1.9315)	mem 23874MB
[2022-11-11 02:03:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][1050/1251]	eta 0:02:30 lr 0.000443	time 0.7376 (0.7485)	loss 5.5923 (5.1548)	grad_norm 1.5777 (1.9285)	mem 23874MB
[2022-11-11 02:04:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][1100/1251]	eta 0:01:52 lr 0.000445	time 0.7371 (0.7483)	loss 5.4020 (5.1521)	grad_norm 1.7148 (1.9234)	mem 23874MB
[2022-11-11 02:04:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][1150/1251]	eta 0:01:15 lr 0.000447	time 0.7417 (0.7483)	loss 5.0631 (5.1526)	grad_norm 1.8517 (1.9229)	mem 23874MB
[2022-11-11 02:05:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][1200/1251]	eta 0:00:38 lr 0.000449	time 0.8384 (0.7483)	loss 4.4798 (5.1468)	grad_norm 1.6839 (1.9224)	mem 23874MB
[2022-11-11 02:05:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [8/300][1250/1251]	eta 0:00:00 lr 0.000451	time 0.7301 (0.7481)	loss 5.5661 (5.1438)	grad_norm 1.7677 (1.9170)	mem 23874MB
[2022-11-11 02:05:53 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 8 training takes 0:15:36
[2022-11-11 02:05:53 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_8.pth saving......
[2022-11-11 02:05:54 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_8.pth saved !!!
[2022-11-11 02:05:56 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.587 (1.587)	Loss 2.4400 (2.4400)	Acc@1 46.777 (46.777)	Acc@5 73.438 (73.438)	Mem 23874MB
[2022-11-11 02:06:06 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 47.068 Acc@5 72.476
[2022-11-11 02:06:06 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 47.1%
[2022-11-11 02:06:08 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.786 (1.786)	Loss 6.6901 (6.6901)	Acc@1 0.586 (0.586)	Acc@5 2.930 (2.930)	Mem 23874MB
[2022-11-11 02:06:19 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 0.712 Acc@5 3.004
[2022-11-11 02:06:19 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 0.7%
[2022-11-11 02:06:19 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 47.07% at 8 epoch
[2022-11-11 02:06:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][0/1251]	eta 0:51:22 lr 0.000451	time 2.4642 (2.4642)	loss 4.4056 (4.4056)	grad_norm 2.5278 (2.5278)	mem 23874MB
[2022-11-11 02:06:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][50/1251]	eta 0:15:39 lr 0.000453	time 0.8120 (0.7825)	loss 4.3084 (5.0502)	grad_norm 2.0224 (1.9022)	mem 23874MB
[2022-11-11 02:07:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][100/1251]	eta 0:14:39 lr 0.000455	time 0.7758 (0.7639)	loss 5.3614 (5.1016)	grad_norm 1.5278 (1.9599)	mem 23874MB
[2022-11-11 02:08:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][150/1251]	eta 0:13:54 lr 0.000457	time 0.7392 (0.7581)	loss 3.9877 (5.0700)	grad_norm 1.7104 (1.8957)	mem 23874MB
[2022-11-11 02:08:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][200/1251]	eta 0:13:13 lr 0.000459	time 0.7440 (0.7553)	loss 5.1765 (5.0883)	grad_norm 1.9918 (1.8735)	mem 23874MB
[2022-11-11 02:09:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][250/1251]	eta 0:12:34 lr 0.000461	time 0.7292 (0.7536)	loss 5.0656 (5.1223)	grad_norm 1.7141 (1.8583)	mem 23874MB
[2022-11-11 02:10:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][300/1251]	eta 0:11:55 lr 0.000463	time 0.7378 (0.7524)	loss 4.1980 (5.0977)	grad_norm 1.6781 (1.8597)	mem 23874MB
[2022-11-11 02:10:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][350/1251]	eta 0:11:17 lr 0.000465	time 0.7428 (0.7518)	loss 5.2579 (5.0809)	grad_norm 1.9337 (1.8604)	mem 23874MB
[2022-11-11 02:11:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][400/1251]	eta 0:10:38 lr 0.000467	time 0.7390 (0.7509)	loss 4.1368 (5.0639)	grad_norm 1.6556 (1.8606)	mem 23874MB
[2022-11-11 02:11:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][450/1251]	eta 0:10:01 lr 0.000469	time 0.7368 (0.7507)	loss 5.0677 (5.0609)	grad_norm 2.1335 (1.8704)	mem 23874MB
[2022-11-11 02:12:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][500/1251]	eta 0:09:23 lr 0.000471	time 0.7349 (0.7499)	loss 5.5280 (5.0408)	grad_norm 1.9202 (1.8682)	mem 23874MB
[2022-11-11 02:13:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][550/1251]	eta 0:08:45 lr 0.000473	time 0.7427 (0.7498)	loss 5.5730 (5.0409)	grad_norm 1.6167 (1.8665)	mem 23874MB
[2022-11-11 02:13:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][600/1251]	eta 0:08:07 lr 0.000475	time 0.7439 (0.7496)	loss 4.6469 (5.0362)	grad_norm 1.5377 (1.8583)	mem 23874MB
[2022-11-11 02:14:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][650/1251]	eta 0:07:30 lr 0.000477	time 0.7409 (0.7493)	loss 4.8506 (5.0366)	grad_norm 1.7401 (1.8505)	mem 23874MB
[2022-11-11 02:15:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][700/1251]	eta 0:06:52 lr 0.000478	time 0.7437 (0.7490)	loss 5.5337 (5.0320)	grad_norm 2.0529 (1.8546)	mem 23874MB
[2022-11-11 02:15:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][750/1251]	eta 0:06:15 lr 0.000480	time 0.8246 (0.7490)	loss 4.8315 (5.0310)	grad_norm 1.8616 (1.8531)	mem 23874MB
[2022-11-11 02:16:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][800/1251]	eta 0:05:37 lr 0.000482	time 0.7419 (0.7487)	loss 4.0296 (5.0310)	grad_norm 2.0576 (1.8489)	mem 23874MB
[2022-11-11 02:16:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][850/1251]	eta 0:05:00 lr 0.000484	time 0.7467 (0.7485)	loss 5.1120 (5.0232)	grad_norm 1.4437 (1.8508)	mem 23874MB
[2022-11-11 02:17:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][900/1251]	eta 0:04:22 lr 0.000486	time 0.7397 (0.7484)	loss 5.8153 (5.0141)	grad_norm 1.7392 (1.8469)	mem 23874MB
[2022-11-11 02:18:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][950/1251]	eta 0:03:45 lr 0.000488	time 0.7420 (0.7484)	loss 5.3297 (5.0030)	grad_norm 1.8215 (1.8433)	mem 23874MB
[2022-11-11 02:18:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][1000/1251]	eta 0:03:07 lr 0.000490	time 0.7381 (0.7483)	loss 5.5656 (5.0024)	grad_norm 1.7861 (1.8382)	mem 23874MB
[2022-11-11 02:19:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][1050/1251]	eta 0:02:30 lr 0.000492	time 0.7431 (0.7482)	loss 4.8051 (5.0011)	grad_norm 1.6942 (1.8375)	mem 23874MB
[2022-11-11 02:20:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][1100/1251]	eta 0:01:52 lr 0.000494	time 0.7348 (0.7482)	loss 4.7539 (5.0017)	grad_norm 1.5323 (1.8393)	mem 23874MB
[2022-11-11 02:20:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][1150/1251]	eta 0:01:15 lr 0.000496	time 0.7441 (0.7480)	loss 5.6132 (4.9950)	grad_norm 1.5853 (1.8325)	mem 23874MB
[2022-11-11 02:21:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][1200/1251]	eta 0:00:38 lr 0.000498	time 0.7397 (0.7480)	loss 4.2636 (4.9877)	grad_norm 1.5699 (1.8316)	mem 23874MB
[2022-11-11 02:21:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [9/300][1250/1251]	eta 0:00:00 lr 0.000500	time 0.7288 (0.7478)	loss 5.1908 (4.9883)	grad_norm 3.1206 (1.8287)	mem 23874MB
[2022-11-11 02:21:54 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 9 training takes 0:15:35
[2022-11-11 02:21:54 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_9.pth saving......
[2022-11-11 02:21:56 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_9.pth saved !!!
[2022-11-11 02:21:57 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.597 (1.597)	Loss 2.3576 (2.3576)	Acc@1 47.461 (47.461)	Acc@5 74.219 (74.219)	Mem 23874MB
[2022-11-11 02:22:08 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 50.628 Acc@5 75.722
[2022-11-11 02:22:08 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 50.6%
[2022-11-11 02:22:10 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.918 (1.918)	Loss 6.4916 (6.4916)	Acc@1 0.879 (0.879)	Acc@5 4.395 (4.395)	Mem 23874MB
[2022-11-11 02:22:21 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 1.080 Acc@5 4.094
[2022-11-11 02:22:21 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 1.1%
[2022-11-11 02:22:21 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 50.63% at 9 epoch
[2022-11-11 02:22:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][0/1251]	eta 0:52:22 lr 0.000501	time 2.5119 (2.5119)	loss 5.5546 (5.5546)	grad_norm 1.4479 (1.4479)	mem 23874MB
[2022-11-11 02:23:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][50/1251]	eta 0:15:42 lr 0.000502	time 0.7367 (0.7850)	loss 4.3086 (5.0126)	grad_norm 3.4714 (1.7877)	mem 23874MB
[2022-11-11 02:23:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][100/1251]	eta 0:14:40 lr 0.000504	time 0.7386 (0.7649)	loss 4.8070 (4.9874)	grad_norm 1.5603 (1.8194)	mem 23874MB
[2022-11-11 02:24:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][150/1251]	eta 0:13:56 lr 0.000506	time 0.7383 (0.7601)	loss 5.4359 (4.9772)	grad_norm 1.6502 (1.8011)	mem 23874MB
[2022-11-11 02:24:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][200/1251]	eta 0:13:15 lr 0.000508	time 0.7388 (0.7568)	loss 4.9444 (4.9874)	grad_norm 1.7040 (1.7761)	mem 23874MB
[2022-11-11 02:25:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][250/1251]	eta 0:12:35 lr 0.000510	time 0.7462 (0.7549)	loss 4.0818 (4.9807)	grad_norm 1.3990 (1.7722)	mem 23874MB
[2022-11-11 02:26:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][300/1251]	eta 0:11:56 lr 0.000512	time 0.7375 (0.7535)	loss 5.2240 (4.9590)	grad_norm 1.6066 (1.7559)	mem 23874MB
[2022-11-11 02:26:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][350/1251]	eta 0:11:17 lr 0.000514	time 0.7384 (0.7523)	loss 4.2799 (4.9551)	grad_norm 1.8081 (1.7655)	mem 23874MB
[2022-11-11 02:27:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][400/1251]	eta 0:10:39 lr 0.000516	time 0.7400 (0.7520)	loss 3.9207 (4.9649)	grad_norm 1.9519 (1.7669)	mem 23874MB
[2022-11-11 02:27:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][450/1251]	eta 0:10:01 lr 0.000518	time 0.7447 (0.7515)	loss 5.0586 (4.9594)	grad_norm 1.9033 (1.7647)	mem 23874MB
[2022-11-11 02:28:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][500/1251]	eta 0:09:23 lr 0.000520	time 0.7351 (0.7509)	loss 5.4594 (4.9486)	grad_norm 1.6594 (1.7668)	mem 23874MB
[2022-11-11 02:29:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][550/1251]	eta 0:08:46 lr 0.000522	time 0.7432 (0.7505)	loss 3.8623 (4.9542)	grad_norm 2.0151 (1.7762)	mem 23874MB
[2022-11-11 02:29:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][600/1251]	eta 0:08:08 lr 0.000524	time 0.7598 (0.7503)	loss 4.8491 (4.9489)	grad_norm 1.8142 (1.7727)	mem 23874MB
[2022-11-11 02:30:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][650/1251]	eta 0:07:30 lr 0.000526	time 0.7366 (0.7501)	loss 4.8031 (4.9461)	grad_norm 1.5631 (1.7675)	mem 23874MB
[2022-11-11 02:31:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][700/1251]	eta 0:06:53 lr 0.000528	time 0.7414 (0.7501)	loss 5.4269 (4.9450)	grad_norm 1.7157 (1.7630)	mem 23874MB
[2022-11-11 02:31:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][750/1251]	eta 0:06:15 lr 0.000530	time 0.8021 (0.7497)	loss 4.5637 (4.9381)	grad_norm 1.5492 (1.7622)	mem 23874MB
[2022-11-11 02:32:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][800/1251]	eta 0:05:38 lr 0.000532	time 0.7415 (0.7495)	loss 3.3024 (4.9270)	grad_norm 1.7833 (1.7600)	mem 23874MB
[2022-11-11 02:32:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][850/1251]	eta 0:05:00 lr 0.000534	time 0.7423 (0.7497)	loss 3.7296 (4.9225)	grad_norm 1.4514 (1.7587)	mem 23874MB
[2022-11-11 02:33:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][900/1251]	eta 0:04:23 lr 0.000536	time 0.7404 (0.7494)	loss 3.6445 (4.9166)	grad_norm 1.4754 (1.7582)	mem 23874MB
[2022-11-11 02:34:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][950/1251]	eta 0:03:45 lr 0.000538	time 0.7433 (0.7495)	loss 4.3229 (4.9208)	grad_norm 1.6091 (1.7553)	mem 23874MB
[2022-11-11 02:34:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][1000/1251]	eta 0:03:08 lr 0.000540	time 0.7443 (0.7494)	loss 5.5087 (4.9197)	grad_norm 1.6256 (1.7560)	mem 23874MB
[2022-11-11 02:35:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][1050/1251]	eta 0:02:30 lr 0.000542	time 0.7392 (0.7493)	loss 5.0786 (4.9102)	grad_norm 2.4792 (1.7563)	mem 23874MB
[2022-11-11 02:36:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][1100/1251]	eta 0:01:53 lr 0.000544	time 0.7391 (0.7492)	loss 5.2718 (4.9009)	grad_norm 2.0134 (1.7541)	mem 23874MB
[2022-11-11 02:36:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][1150/1251]	eta 0:01:15 lr 0.000546	time 0.7397 (0.7491)	loss 4.1897 (4.8907)	grad_norm 1.6400 (1.7512)	mem 23874MB
[2022-11-11 02:37:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][1200/1251]	eta 0:00:38 lr 0.000548	time 0.7492 (0.7491)	loss 5.4341 (4.8899)	grad_norm 1.3302 (inf)	mem 23874MB
[2022-11-11 02:37:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [10/300][1250/1251]	eta 0:00:00 lr 0.000550	time 0.7425 (0.7490)	loss 4.9206 (4.8888)	grad_norm 1.7013 (inf)	mem 23874MB
[2022-11-11 02:37:58 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 10 training takes 0:15:37
[2022-11-11 02:37:58 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_10.pth saving......
[2022-11-11 02:37:59 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_10.pth saved !!!
[2022-11-11 02:38:01 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.727 (1.727)	Loss 2.2683 (2.2683)	Acc@1 51.562 (51.562)	Acc@5 76.758 (76.758)	Mem 23874MB
[2022-11-11 02:38:11 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 52.436 Acc@5 77.500
[2022-11-11 02:38:11 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 52.4%
[2022-11-11 02:38:13 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.882 (1.882)	Loss 6.3580 (6.3580)	Acc@1 1.758 (1.758)	Acc@5 5.859 (5.859)	Mem 23874MB
[2022-11-11 02:38:24 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 1.562 Acc@5 5.988
[2022-11-11 02:38:24 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 1.6%
[2022-11-11 02:38:24 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 52.44% at 10 epoch
[2022-11-11 02:38:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][0/1251]	eta 0:49:33 lr 0.000550	time 2.3766 (2.3766)	loss 5.2318 (5.2318)	grad_norm 1.8957 (1.8957)	mem 23874MB
[2022-11-11 02:39:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][50/1251]	eta 0:15:38 lr 0.000552	time 0.7569 (0.7813)	loss 4.7449 (4.8106)	grad_norm 1.5108 (1.6778)	mem 23874MB
[2022-11-11 02:39:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][100/1251]	eta 0:14:41 lr 0.000554	time 0.7297 (0.7662)	loss 4.7101 (4.8076)	grad_norm 1.5991 (1.7242)	mem 23874MB
[2022-11-11 02:40:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][150/1251]	eta 0:13:55 lr 0.000556	time 0.7379 (0.7590)	loss 4.9302 (4.8134)	grad_norm 1.5877 (1.7296)	mem 23874MB
[2022-11-11 02:40:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][200/1251]	eta 0:13:14 lr 0.000558	time 0.7382 (0.7557)	loss 4.3620 (4.7774)	grad_norm 1.7790 (1.7064)	mem 23874MB
[2022-11-11 02:41:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][250/1251]	eta 0:12:35 lr 0.000560	time 0.7502 (0.7543)	loss 5.0088 (4.8167)	grad_norm 1.9144 (1.7127)	mem 23874MB
[2022-11-11 02:42:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][300/1251]	eta 0:11:55 lr 0.000562	time 0.7434 (0.7528)	loss 5.1848 (4.8110)	grad_norm 1.5550 (1.7183)	mem 23874MB
[2022-11-11 02:42:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][350/1251]	eta 0:11:17 lr 0.000564	time 0.7465 (0.7524)	loss 3.6288 (4.8258)	grad_norm 1.7389 (1.7027)	mem 23874MB
[2022-11-11 02:43:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][400/1251]	eta 0:10:39 lr 0.000566	time 0.7373 (0.7518)	loss 5.4014 (4.8368)	grad_norm 1.6571 (1.6894)	mem 23874MB
[2022-11-11 02:44:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][450/1251]	eta 0:10:01 lr 0.000568	time 0.7391 (0.7512)	loss 4.4846 (4.8291)	grad_norm 1.5790 (1.6867)	mem 23874MB
[2022-11-11 02:44:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][500/1251]	eta 0:09:23 lr 0.000570	time 0.7346 (0.7509)	loss 5.2212 (4.8237)	grad_norm 1.6366 (1.6844)	mem 23874MB
[2022-11-11 02:45:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][550/1251]	eta 0:08:46 lr 0.000572	time 0.7420 (0.7505)	loss 4.9497 (4.8150)	grad_norm 1.4370 (1.6859)	mem 23874MB
[2022-11-11 02:45:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][600/1251]	eta 0:08:08 lr 0.000574	time 0.7355 (0.7502)	loss 5.3562 (4.8144)	grad_norm 1.5772 (1.6827)	mem 23874MB
[2022-11-11 02:46:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][650/1251]	eta 0:07:30 lr 0.000576	time 0.7414 (0.7499)	loss 5.3689 (4.8252)	grad_norm 1.6818 (1.6827)	mem 23874MB
[2022-11-11 02:47:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][700/1251]	eta 0:06:53 lr 0.000578	time 0.7426 (0.7496)	loss 5.2472 (4.8285)	grad_norm 1.6446 (1.6833)	mem 23874MB
[2022-11-11 02:47:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][750/1251]	eta 0:06:15 lr 0.000580	time 0.7387 (0.7495)	loss 5.1201 (4.8320)	grad_norm 1.5719 (1.6823)	mem 23874MB
[2022-11-11 02:48:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][800/1251]	eta 0:05:37 lr 0.000582	time 0.7392 (0.7494)	loss 4.8705 (4.8364)	grad_norm 1.5111 (1.6809)	mem 23874MB
[2022-11-11 02:49:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][850/1251]	eta 0:05:00 lr 0.000584	time 0.7427 (0.7491)	loss 5.1530 (4.8376)	grad_norm 1.7095 (1.6774)	mem 23874MB
[2022-11-11 02:49:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][900/1251]	eta 0:04:22 lr 0.000586	time 0.7380 (0.7491)	loss 3.7429 (4.8336)	grad_norm 1.4415 (1.6809)	mem 23874MB
[2022-11-11 02:50:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][950/1251]	eta 0:03:45 lr 0.000588	time 0.8441 (0.7490)	loss 4.6707 (4.8190)	grad_norm 1.4698 (1.6790)	mem 23874MB
[2022-11-11 02:50:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][1000/1251]	eta 0:03:07 lr 0.000590	time 0.7494 (0.7488)	loss 5.0199 (4.8128)	grad_norm 1.5468 (1.6762)	mem 23874MB
[2022-11-11 02:51:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][1050/1251]	eta 0:02:30 lr 0.000592	time 0.7437 (0.7488)	loss 4.2554 (4.8085)	grad_norm 1.3007 (1.6758)	mem 23874MB
[2022-11-11 02:52:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][1100/1251]	eta 0:01:53 lr 0.000594	time 0.7398 (0.7486)	loss 5.5865 (4.8069)	grad_norm 1.4512 (1.6800)	mem 23874MB
[2022-11-11 02:52:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][1150/1251]	eta 0:01:15 lr 0.000596	time 0.7408 (0.7487)	loss 5.1962 (4.8055)	grad_norm 1.5263 (1.6800)	mem 23874MB
[2022-11-11 02:53:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][1200/1251]	eta 0:00:38 lr 0.000598	time 0.7298 (0.7487)	loss 4.6331 (4.8037)	grad_norm 2.1618 (1.6782)	mem 23874MB
[2022-11-11 02:54:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [11/300][1250/1251]	eta 0:00:00 lr 0.000600	time 0.7317 (0.7484)	loss 5.0477 (4.8025)	grad_norm 1.3126 (1.6777)	mem 23874MB
[2022-11-11 02:54:01 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 11 training takes 0:15:36
[2022-11-11 02:54:01 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_11.pth saving......
[2022-11-11 02:54:02 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_11.pth saved !!!
[2022-11-11 02:54:03 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.634 (1.634)	Loss 1.9646 (1.9646)	Acc@1 56.738 (56.738)	Acc@5 81.934 (81.934)	Mem 23874MB
[2022-11-11 02:54:14 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 55.108 Acc@5 79.516
[2022-11-11 02:54:14 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 55.1%
[2022-11-11 02:54:16 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.808 (1.808)	Loss 6.1435 (6.1435)	Acc@1 2.344 (2.344)	Acc@5 8.887 (8.887)	Mem 23874MB
[2022-11-11 02:54:27 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 2.518 Acc@5 8.946
[2022-11-11 02:54:27 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 2.5%
[2022-11-11 02:54:27 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 55.11% at 11 epoch
[2022-11-11 02:54:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][0/1251]	eta 0:51:50 lr 0.000600	time 2.4867 (2.4867)	loss 5.6532 (5.6532)	grad_norm 1.4963 (1.4963)	mem 23874MB
[2022-11-11 02:55:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][50/1251]	eta 0:15:39 lr 0.000602	time 0.7376 (0.7819)	loss 4.8914 (4.6329)	grad_norm 2.2284 (1.6742)	mem 23874MB
[2022-11-11 02:55:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][100/1251]	eta 0:14:37 lr 0.000604	time 0.7389 (0.7627)	loss 4.6232 (4.6586)	grad_norm 1.6734 (1.6507)	mem 23874MB
[2022-11-11 02:56:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][150/1251]	eta 0:13:54 lr 0.000606	time 0.7417 (0.7575)	loss 4.6045 (4.6994)	grad_norm 1.6341 (1.6260)	mem 23874MB
[2022-11-11 02:56:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][200/1251]	eta 0:13:13 lr 0.000608	time 0.8155 (0.7548)	loss 5.1168 (4.7120)	grad_norm 2.1695 (1.6409)	mem 23874MB
[2022-11-11 02:57:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][250/1251]	eta 0:12:33 lr 0.000610	time 0.7417 (0.7532)	loss 5.1916 (4.7095)	grad_norm 1.6915 (1.6421)	mem 23874MB
[2022-11-11 02:58:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][300/1251]	eta 0:11:55 lr 0.000612	time 0.7412 (0.7519)	loss 5.3424 (4.7311)	grad_norm 1.4384 (1.6377)	mem 23874MB
[2022-11-11 02:58:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][350/1251]	eta 0:11:16 lr 0.000614	time 0.7443 (0.7510)	loss 4.5495 (4.7172)	grad_norm 1.4748 (1.6342)	mem 23874MB
[2022-11-11 02:59:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][400/1251]	eta 0:10:38 lr 0.000616	time 0.7551 (0.7503)	loss 5.0392 (4.7120)	grad_norm 2.1747 (1.6401)	mem 23874MB
[2022-11-11 03:00:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][450/1251]	eta 0:10:00 lr 0.000618	time 0.7416 (0.7502)	loss 4.1416 (4.7063)	grad_norm 1.8129 (1.6407)	mem 23874MB
[2022-11-11 03:00:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][500/1251]	eta 0:09:23 lr 0.000620	time 0.7446 (0.7497)	loss 5.2774 (4.7078)	grad_norm 1.6640 (1.6464)	mem 23874MB
[2022-11-11 03:01:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][550/1251]	eta 0:08:45 lr 0.000622	time 0.7420 (0.7494)	loss 4.7152 (4.7149)	grad_norm 1.4054 (1.6499)	mem 23874MB
[2022-11-11 03:01:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][600/1251]	eta 0:08:07 lr 0.000624	time 0.8275 (0.7490)	loss 5.1202 (4.7135)	grad_norm 1.3870 (1.6408)	mem 23874MB
[2022-11-11 03:02:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][650/1251]	eta 0:07:29 lr 0.000626	time 0.7386 (0.7486)	loss 3.8348 (4.7141)	grad_norm 2.8361 (1.6460)	mem 23874MB
[2022-11-11 03:03:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][700/1251]	eta 0:06:52 lr 0.000628	time 0.7438 (0.7485)	loss 5.0000 (4.7100)	grad_norm 2.0854 (1.6471)	mem 23874MB
[2022-11-11 03:03:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][750/1251]	eta 0:06:14 lr 0.000630	time 0.7399 (0.7483)	loss 4.7454 (4.7102)	grad_norm 1.4724 (1.6434)	mem 23874MB
[2022-11-11 03:04:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][800/1251]	eta 0:05:37 lr 0.000632	time 0.7322 (0.7482)	loss 4.1049 (4.7043)	grad_norm 1.3570 (1.6423)	mem 23874MB
[2022-11-11 03:05:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][850/1251]	eta 0:05:00 lr 0.000634	time 0.7455 (0.7482)	loss 4.6955 (4.7141)	grad_norm 1.3985 (1.6391)	mem 23874MB
[2022-11-11 03:05:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][900/1251]	eta 0:04:22 lr 0.000636	time 0.7364 (0.7479)	loss 4.9228 (4.7170)	grad_norm 1.7332 (1.6388)	mem 23874MB
[2022-11-11 03:06:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][950/1251]	eta 0:03:45 lr 0.000638	time 0.7385 (0.7478)	loss 5.5368 (4.7072)	grad_norm 1.3668 (1.6396)	mem 23874MB
[2022-11-11 03:06:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][1000/1251]	eta 0:03:07 lr 0.000640	time 0.8161 (0.7477)	loss 4.5720 (4.7020)	grad_norm 1.2885 (1.6385)	mem 23874MB
[2022-11-11 03:07:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][1050/1251]	eta 0:02:30 lr 0.000642	time 0.7365 (0.7476)	loss 4.9350 (4.6981)	grad_norm 1.5557 (1.6343)	mem 23874MB
[2022-11-11 03:08:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][1100/1251]	eta 0:01:52 lr 0.000644	time 0.7447 (0.7475)	loss 4.5587 (4.6957)	grad_norm 1.4106 (1.6328)	mem 23874MB
[2022-11-11 03:08:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][1150/1251]	eta 0:01:15 lr 0.000646	time 0.7385 (0.7474)	loss 3.6469 (4.6977)	grad_norm 1.6600 (1.6289)	mem 23874MB
[2022-11-11 03:09:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][1200/1251]	eta 0:00:38 lr 0.000648	time 0.7402 (0.7473)	loss 4.0867 (4.6963)	grad_norm 1.6374 (1.6332)	mem 23874MB
[2022-11-11 03:10:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [12/300][1250/1251]	eta 0:00:00 lr 0.000650	time 0.7349 (0.7471)	loss 5.3465 (4.6935)	grad_norm 1.4777 (1.6324)	mem 23874MB
[2022-11-11 03:10:02 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 12 training takes 0:15:34
[2022-11-11 03:10:02 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_12.pth saving......
[2022-11-11 03:10:03 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_12.pth saved !!!
[2022-11-11 03:10:04 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.651 (1.651)	Loss 1.9394 (1.9394)	Acc@1 58.691 (58.691)	Acc@5 83.105 (83.105)	Mem 23874MB
[2022-11-11 03:10:15 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 57.092 Acc@5 81.308
[2022-11-11 03:10:15 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 57.1%
[2022-11-11 03:10:17 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.818 (1.818)	Loss 5.7672 (5.7672)	Acc@1 4.297 (4.297)	Acc@5 13.184 (13.184)	Mem 23874MB
[2022-11-11 03:10:28 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 3.996 Acc@5 13.112
[2022-11-11 03:10:28 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 4.0%
[2022-11-11 03:10:28 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 57.09% at 12 epoch
[2022-11-11 03:10:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][0/1251]	eta 0:51:08 lr 0.000650	time 2.4530 (2.4530)	loss 3.8820 (3.8820)	grad_norm 1.3319 (1.3319)	mem 23874MB
[2022-11-11 03:11:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][50/1251]	eta 0:15:37 lr 0.000652	time 0.7444 (0.7802)	loss 5.0565 (4.5889)	grad_norm 1.8088 (1.5128)	mem 23874MB
[2022-11-11 03:11:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][100/1251]	eta 0:14:39 lr 0.000654	time 0.7475 (0.7640)	loss 3.8326 (4.5813)	grad_norm 1.5478 (1.5387)	mem 23874MB
[2022-11-11 03:12:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][150/1251]	eta 0:13:54 lr 0.000656	time 0.8153 (0.7579)	loss 4.8952 (4.5893)	grad_norm 1.5091 (1.5357)	mem 23874MB
[2022-11-11 03:12:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][200/1251]	eta 0:13:13 lr 0.000658	time 0.8038 (0.7552)	loss 4.0295 (4.5901)	grad_norm 1.5932 (1.5633)	mem 23874MB
[2022-11-11 03:13:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][250/1251]	eta 0:12:33 lr 0.000660	time 0.7420 (0.7529)	loss 5.0128 (4.6202)	grad_norm 1.3325 (1.5590)	mem 23874MB
[2022-11-11 03:14:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][300/1251]	eta 0:11:54 lr 0.000662	time 0.7408 (0.7516)	loss 4.8026 (4.5926)	grad_norm 1.4679 (1.5551)	mem 23874MB
[2022-11-11 03:14:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][350/1251]	eta 0:11:16 lr 0.000664	time 0.7443 (0.7507)	loss 4.1565 (4.5945)	grad_norm 1.5513 (1.5558)	mem 23874MB
[2022-11-11 03:15:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][400/1251]	eta 0:10:38 lr 0.000666	time 0.7402 (0.7503)	loss 5.1721 (4.5983)	grad_norm 1.3382 (1.5602)	mem 23874MB
[2022-11-11 03:16:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][450/1251]	eta 0:10:00 lr 0.000668	time 0.7389 (0.7498)	loss 5.1542 (4.6097)	grad_norm 1.4621 (1.5621)	mem 23874MB
[2022-11-11 03:16:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][500/1251]	eta 0:09:22 lr 0.000670	time 0.7408 (0.7492)	loss 5.4930 (4.6124)	grad_norm 1.5756 (1.5629)	mem 23874MB
[2022-11-11 03:17:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][550/1251]	eta 0:08:45 lr 0.000672	time 0.8207 (0.7490)	loss 4.8952 (4.6049)	grad_norm 1.6278 (1.5715)	mem 23874MB
[2022-11-11 03:17:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][600/1251]	eta 0:08:07 lr 0.000674	time 0.8175 (0.7487)	loss 5.4196 (4.6107)	grad_norm 1.5436 (1.5653)	mem 23874MB
[2022-11-11 03:18:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][650/1251]	eta 0:07:29 lr 0.000676	time 0.7415 (0.7484)	loss 4.8969 (4.6086)	grad_norm 1.4216 (1.5654)	mem 23874MB
[2022-11-11 03:19:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][700/1251]	eta 0:06:52 lr 0.000678	time 0.7426 (0.7481)	loss 5.5490 (4.6123)	grad_norm 1.4303 (1.5625)	mem 23874MB
[2022-11-11 03:19:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][750/1251]	eta 0:06:14 lr 0.000680	time 0.7389 (0.7480)	loss 4.2251 (4.6096)	grad_norm 1.6884 (1.5617)	mem 23874MB
[2022-11-11 03:20:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][800/1251]	eta 0:05:37 lr 0.000682	time 0.7335 (0.7480)	loss 4.1880 (4.6024)	grad_norm 1.5469 (1.5614)	mem 23874MB
[2022-11-11 03:21:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][850/1251]	eta 0:04:59 lr 0.000684	time 0.7355 (0.7477)	loss 4.8472 (4.6050)	grad_norm 1.9788 (1.5672)	mem 23874MB
[2022-11-11 03:21:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][900/1251]	eta 0:04:22 lr 0.000686	time 0.7417 (0.7475)	loss 3.9101 (4.6090)	grad_norm 1.4349 (1.5658)	mem 23874MB
[2022-11-11 03:22:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][950/1251]	eta 0:03:45 lr 0.000688	time 0.7269 (0.7475)	loss 4.5573 (4.6023)	grad_norm 1.2922 (1.5648)	mem 23874MB
[2022-11-11 03:22:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][1000/1251]	eta 0:03:07 lr 0.000690	time 0.8156 (0.7474)	loss 4.8020 (4.6033)	grad_norm 1.5460 (1.5638)	mem 23874MB
[2022-11-11 03:23:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][1050/1251]	eta 0:02:30 lr 0.000692	time 0.7412 (0.7473)	loss 4.2907 (4.6062)	grad_norm 1.8168 (1.5655)	mem 23874MB
[2022-11-11 03:24:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][1100/1251]	eta 0:01:52 lr 0.000694	time 0.7386 (0.7471)	loss 5.1641 (4.6038)	grad_norm 1.7685 (1.5685)	mem 23874MB
[2022-11-11 03:24:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][1150/1251]	eta 0:01:15 lr 0.000696	time 0.7388 (0.7472)	loss 4.9413 (4.6019)	grad_norm 1.5270 (1.5649)	mem 23874MB
[2022-11-11 03:25:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][1200/1251]	eta 0:00:38 lr 0.000698	time 0.7377 (0.7471)	loss 4.4559 (4.6000)	grad_norm 1.5034 (1.5677)	mem 23874MB
[2022-11-11 03:26:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [13/300][1250/1251]	eta 0:00:00 lr 0.000700	time 0.7266 (0.7468)	loss 4.7389 (4.5985)	grad_norm 1.4383 (1.5656)	mem 23874MB
[2022-11-11 03:26:02 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 13 training takes 0:15:34
[2022-11-11 03:26:02 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_13.pth saving......
[2022-11-11 03:26:03 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_13.pth saved !!!
[2022-11-11 03:26:05 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.689 (1.689)	Loss 1.8851 (1.8851)	Acc@1 57.617 (57.617)	Acc@5 81.348 (81.348)	Mem 23874MB
[2022-11-11 03:26:16 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 58.654 Acc@5 82.394
[2022-11-11 03:26:16 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 58.7%
[2022-11-11 03:26:17 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.866 (1.866)	Loss 5.6127 (5.6127)	Acc@1 6.055 (6.055)	Acc@5 16.699 (16.699)	Mem 23874MB
[2022-11-11 03:26:28 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 6.194 Acc@5 18.440
[2022-11-11 03:26:28 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 6.2%
[2022-11-11 03:26:28 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 58.65% at 13 epoch
[2022-11-11 03:26:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][0/1251]	eta 0:49:50 lr 0.000700	time 2.3901 (2.3901)	loss 5.0396 (5.0396)	grad_norm 1.5427 (1.5427)	mem 23874MB
[2022-11-11 03:27:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][50/1251]	eta 0:15:40 lr 0.000702	time 0.7353 (0.7828)	loss 4.8604 (4.5516)	grad_norm 1.8894 (1.5425)	mem 23874MB
[2022-11-11 03:27:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][100/1251]	eta 0:14:41 lr 0.000704	time 0.7356 (0.7661)	loss 4.2186 (4.5950)	grad_norm 1.3212 (1.5228)	mem 23874MB
[2022-11-11 03:28:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][150/1251]	eta 0:13:57 lr 0.000706	time 0.7408 (0.7606)	loss 4.2628 (4.5741)	grad_norm 1.5719 (1.5498)	mem 23874MB
[2022-11-11 03:29:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][200/1251]	eta 0:13:15 lr 0.000708	time 0.7409 (0.7572)	loss 5.1727 (4.5540)	grad_norm 1.6754 (1.5519)	mem 23874MB
[2022-11-11 03:29:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][250/1251]	eta 0:12:35 lr 0.000710	time 0.7381 (0.7552)	loss 5.4981 (4.5568)	grad_norm 2.3535 (inf)	mem 23874MB
[2022-11-11 03:30:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][300/1251]	eta 0:11:56 lr 0.000712	time 0.7362 (0.7535)	loss 4.2051 (4.5552)	grad_norm 1.4161 (inf)	mem 23874MB
[2022-11-11 03:30:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][350/1251]	eta 0:11:18 lr 0.000714	time 0.7421 (0.7527)	loss 3.8650 (4.5428)	grad_norm 1.3940 (inf)	mem 23874MB
[2022-11-11 03:31:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][400/1251]	eta 0:10:40 lr 0.000716	time 0.7363 (0.7523)	loss 4.7582 (4.5476)	grad_norm 1.6182 (inf)	mem 23874MB
[2022-11-11 03:32:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][450/1251]	eta 0:10:02 lr 0.000718	time 0.7425 (0.7516)	loss 4.4680 (4.5414)	grad_norm 1.7746 (inf)	mem 23874MB
[2022-11-11 03:32:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][500/1251]	eta 0:09:24 lr 0.000720	time 0.7346 (0.7511)	loss 5.4097 (4.5388)	grad_norm 1.3543 (inf)	mem 23874MB
[2022-11-11 03:33:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][550/1251]	eta 0:08:46 lr 0.000722	time 0.7354 (0.7506)	loss 3.4158 (4.5338)	grad_norm 1.2534 (inf)	mem 23874MB
[2022-11-11 03:33:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][600/1251]	eta 0:08:08 lr 0.000724	time 0.7338 (0.7500)	loss 4.8644 (4.5508)	grad_norm 1.4016 (inf)	mem 23874MB
[2022-11-11 03:34:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][650/1251]	eta 0:07:30 lr 0.000726	time 0.7412 (0.7499)	loss 3.3347 (4.5457)	grad_norm 1.3126 (inf)	mem 23874MB
[2022-11-11 03:35:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][700/1251]	eta 0:06:52 lr 0.000728	time 0.7401 (0.7494)	loss 4.4052 (4.5527)	grad_norm 1.6729 (inf)	mem 23874MB
[2022-11-11 03:35:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][750/1251]	eta 0:06:15 lr 0.000730	time 0.8146 (0.7493)	loss 4.5792 (4.5668)	grad_norm 1.5029 (inf)	mem 23874MB
[2022-11-11 03:36:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][800/1251]	eta 0:05:37 lr 0.000732	time 0.7345 (0.7489)	loss 3.2741 (4.5694)	grad_norm 1.3996 (inf)	mem 23874MB
[2022-11-11 03:37:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][850/1251]	eta 0:05:00 lr 0.000734	time 0.7354 (0.7487)	loss 4.5547 (4.5647)	grad_norm 1.3324 (inf)	mem 23874MB
[2022-11-11 03:37:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][900/1251]	eta 0:04:22 lr 0.000736	time 0.7357 (0.7486)	loss 3.6987 (4.5692)	grad_norm 1.7069 (inf)	mem 23874MB
[2022-11-11 03:38:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][950/1251]	eta 0:03:45 lr 0.000738	time 0.7311 (0.7485)	loss 4.0519 (4.5590)	grad_norm 1.3603 (inf)	mem 23874MB
[2022-11-11 03:38:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][1000/1251]	eta 0:03:07 lr 0.000740	time 0.8367 (0.7483)	loss 4.1064 (4.5624)	grad_norm 1.4964 (inf)	mem 23874MB
[2022-11-11 03:39:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][1050/1251]	eta 0:02:30 lr 0.000742	time 0.7413 (0.7483)	loss 3.4705 (4.5586)	grad_norm 1.5268 (inf)	mem 23874MB
[2022-11-11 03:40:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][1100/1251]	eta 0:01:52 lr 0.000744	time 0.7413 (0.7482)	loss 4.9476 (4.5575)	grad_norm 1.5105 (inf)	mem 23874MB
[2022-11-11 03:40:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][1150/1251]	eta 0:01:15 lr 0.000746	time 0.7416 (0.7481)	loss 4.8165 (4.5522)	grad_norm 1.6978 (inf)	mem 23874MB
[2022-11-11 03:41:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][1200/1251]	eta 0:00:38 lr 0.000748	time 0.7380 (0.7481)	loss 4.5166 (4.5481)	grad_norm 1.5723 (inf)	mem 23874MB
[2022-11-11 03:42:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [14/300][1250/1251]	eta 0:00:00 lr 0.000750	time 0.7249 (0.7478)	loss 4.5787 (4.5422)	grad_norm 1.3614 (inf)	mem 23874MB
[2022-11-11 03:42:04 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 14 training takes 0:15:35
[2022-11-11 03:42:04 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_14.pth saving......
[2022-11-11 03:42:05 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_14.pth saved !!!
[2022-11-11 03:42:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.729 (1.729)	Loss 1.8447 (1.8447)	Acc@1 60.449 (60.449)	Acc@5 83.008 (83.008)	Mem 23874MB
[2022-11-11 03:42:18 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 59.930 Acc@5 83.416
[2022-11-11 03:42:18 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 59.9%
[2022-11-11 03:42:19 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.931 (1.931)	Loss 5.2014 (5.2014)	Acc@1 7.520 (7.520)	Acc@5 24.609 (24.609)	Mem 23874MB
[2022-11-11 03:42:30 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 9.410 Acc@5 24.872
[2022-11-11 03:42:30 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 9.4%
[2022-11-11 03:42:30 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 59.93% at 14 epoch
[2022-11-11 03:42:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][0/1251]	eta 0:48:32 lr 0.000750	time 2.3282 (2.3282)	loss 4.4825 (4.4825)	grad_norm 1.3234 (1.3234)	mem 23874MB
[2022-11-11 03:43:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][50/1251]	eta 0:15:32 lr 0.000752	time 0.8410 (0.7762)	loss 5.0942 (4.6084)	grad_norm 1.4515 (1.4599)	mem 23874MB
[2022-11-11 03:43:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][100/1251]	eta 0:14:35 lr 0.000754	time 0.7374 (0.7608)	loss 4.9766 (4.4867)	grad_norm 1.5581 (1.5098)	mem 23874MB
[2022-11-11 03:44:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][150/1251]	eta 0:13:53 lr 0.000756	time 0.7413 (0.7568)	loss 4.5164 (4.4984)	grad_norm 1.4601 (1.5139)	mem 23874MB
[2022-11-11 03:45:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][200/1251]	eta 0:13:12 lr 0.000758	time 0.7488 (0.7542)	loss 5.2445 (4.4701)	grad_norm 1.8122 (1.5132)	mem 23874MB
[2022-11-11 03:45:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][250/1251]	eta 0:12:33 lr 0.000760	time 0.7396 (0.7525)	loss 4.2660 (4.4871)	grad_norm 1.4136 (1.5171)	mem 23874MB
[2022-11-11 03:46:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][300/1251]	eta 0:11:54 lr 0.000762	time 0.7436 (0.7508)	loss 5.0035 (4.4851)	grad_norm 1.4501 (1.5180)	mem 23874MB
[2022-11-11 03:46:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][350/1251]	eta 0:11:15 lr 0.000764	time 0.7401 (0.7501)	loss 3.3186 (4.5041)	grad_norm 1.6423 (1.5182)	mem 23874MB
[2022-11-11 03:47:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][400/1251]	eta 0:10:37 lr 0.000766	time 0.7415 (0.7495)	loss 3.8349 (4.4885)	grad_norm 1.4125 (1.5193)	mem 23874MB
[2022-11-11 03:48:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][450/1251]	eta 0:09:59 lr 0.000768	time 0.7363 (0.7488)	loss 5.1679 (4.5089)	grad_norm 1.4003 (1.5202)	mem 23874MB
[2022-11-11 03:48:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][500/1251]	eta 0:09:22 lr 0.000770	time 0.7449 (0.7485)	loss 4.0709 (4.5069)	grad_norm 1.5484 (1.5274)	mem 23874MB
[2022-11-11 03:49:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][550/1251]	eta 0:08:44 lr 0.000772	time 0.7528 (0.7484)	loss 3.3956 (4.4941)	grad_norm 1.7311 (1.5233)	mem 23874MB
[2022-11-11 03:50:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][600/1251]	eta 0:08:07 lr 0.000774	time 0.7383 (0.7481)	loss 3.8920 (4.4971)	grad_norm 1.3140 (1.5191)	mem 23874MB
[2022-11-11 03:50:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][650/1251]	eta 0:07:29 lr 0.000776	time 0.7412 (0.7478)	loss 5.4283 (4.4935)	grad_norm 1.7437 (1.5168)	mem 23874MB
[2022-11-11 03:51:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][700/1251]	eta 0:06:51 lr 0.000778	time 0.7367 (0.7477)	loss 4.9975 (4.5045)	grad_norm 1.3767 (1.5152)	mem 23874MB
[2022-11-11 03:51:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][750/1251]	eta 0:06:14 lr 0.000780	time 0.7460 (0.7476)	loss 4.7753 (4.5041)	grad_norm 1.3976 (1.5106)	mem 23874MB
[2022-11-11 03:52:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][800/1251]	eta 0:05:37 lr 0.000782	time 0.7336 (0.7473)	loss 4.5774 (4.5104)	grad_norm 1.3304 (1.5139)	mem 23874MB
[2022-11-11 03:53:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][850/1251]	eta 0:04:59 lr 0.000784	time 0.7418 (0.7472)	loss 3.7740 (4.5137)	grad_norm 1.5708 (1.5109)	mem 23874MB
[2022-11-11 03:53:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][900/1251]	eta 0:04:22 lr 0.000786	time 0.7483 (0.7470)	loss 4.2901 (4.5073)	grad_norm 1.4425 (1.5087)	mem 23874MB
[2022-11-11 03:54:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][950/1251]	eta 0:03:44 lr 0.000788	time 0.7400 (0.7470)	loss 3.5685 (4.5035)	grad_norm 1.5410 (1.5045)	mem 23874MB
[2022-11-11 03:54:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][1000/1251]	eta 0:03:07 lr 0.000790	time 0.8273 (0.7469)	loss 5.1248 (4.5077)	grad_norm 1.3130 (1.5052)	mem 23874MB
[2022-11-11 03:55:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][1050/1251]	eta 0:02:30 lr 0.000792	time 0.7349 (0.7467)	loss 3.8871 (4.4998)	grad_norm 1.5225 (inf)	mem 23874MB
[2022-11-11 03:56:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][1100/1251]	eta 0:01:52 lr 0.000794	time 0.7378 (0.7466)	loss 4.9111 (4.4983)	grad_norm 1.7122 (inf)	mem 23874MB
[2022-11-11 03:56:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][1150/1251]	eta 0:01:15 lr 0.000796	time 0.7409 (0.7466)	loss 5.3936 (4.5000)	grad_norm 1.3155 (inf)	mem 23874MB
[2022-11-11 03:57:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][1200/1251]	eta 0:00:38 lr 0.000798	time 0.7341 (0.7465)	loss 5.3259 (4.5002)	grad_norm 1.3028 (inf)	mem 23874MB
[2022-11-11 03:58:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [15/300][1250/1251]	eta 0:00:00 lr 0.000800	time 0.7471 (0.7463)	loss 4.9123 (4.4961)	grad_norm 1.4534 (inf)	mem 23874MB
[2022-11-11 03:58:04 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 15 training takes 0:15:33
[2022-11-11 03:58:04 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_15.pth saving......
[2022-11-11 03:58:05 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_15.pth saved !!!
[2022-11-11 03:58:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.636 (1.636)	Loss 1.8453 (1.8453)	Acc@1 59.766 (59.766)	Acc@5 82.812 (82.812)	Mem 23874MB
[2022-11-11 03:58:18 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 61.050 Acc@5 84.020
[2022-11-11 03:58:18 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 61.1%
[2022-11-11 03:58:19 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.917 (1.917)	Loss 4.7325 (4.7325)	Acc@1 12.500 (12.500)	Acc@5 33.008 (33.008)	Mem 23874MB
[2022-11-11 03:58:30 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 13.838 Acc@5 32.744
[2022-11-11 03:58:30 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 13.8%
[2022-11-11 03:58:30 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 61.05% at 15 epoch
[2022-11-11 03:58:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][0/1251]	eta 0:52:39 lr 0.000800	time 2.5254 (2.5254)	loss 4.5832 (4.5832)	grad_norm 1.2398 (1.2398)	mem 23874MB
[2022-11-11 03:59:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][50/1251]	eta 0:15:42 lr 0.000802	time 0.7470 (0.7844)	loss 3.5702 (4.5703)	grad_norm 1.4198 (1.4707)	mem 23874MB
[2022-11-11 03:59:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][100/1251]	eta 0:14:42 lr 0.000804	time 0.7392 (0.7664)	loss 4.5612 (4.5353)	grad_norm 1.4233 (1.4489)	mem 23874MB
[2022-11-11 04:00:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][150/1251]	eta 0:13:57 lr 0.000806	time 0.7461 (0.7603)	loss 4.9280 (4.4943)	grad_norm 1.7557 (1.4554)	mem 23874MB
[2022-11-11 04:01:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][200/1251]	eta 0:13:15 lr 0.000808	time 0.7389 (0.7571)	loss 4.8601 (4.4562)	grad_norm 1.7676 (1.4681)	mem 23874MB
[2022-11-11 04:01:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][250/1251]	eta 0:12:35 lr 0.000810	time 0.7457 (0.7547)	loss 4.6396 (4.4484)	grad_norm 1.5492 (1.4779)	mem 23874MB
[2022-11-11 04:02:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][300/1251]	eta 0:11:56 lr 0.000812	time 0.7439 (0.7533)	loss 4.4365 (4.4581)	grad_norm 1.3222 (1.4698)	mem 23874MB
[2022-11-11 04:02:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][350/1251]	eta 0:11:18 lr 0.000814	time 0.7415 (0.7526)	loss 4.5751 (4.4455)	grad_norm 1.5967 (1.4634)	mem 23874MB
[2022-11-11 04:03:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][400/1251]	eta 0:10:39 lr 0.000816	time 0.8237 (0.7520)	loss 5.0332 (4.4460)	grad_norm 1.6589 (1.4653)	mem 23874MB
[2022-11-11 04:04:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][450/1251]	eta 0:10:01 lr 0.000818	time 0.7389 (0.7515)	loss 4.0681 (4.4511)	grad_norm 1.5390 (1.4599)	mem 23874MB
[2022-11-11 04:04:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][500/1251]	eta 0:09:23 lr 0.000820	time 0.7504 (0.7509)	loss 4.5834 (4.4544)	grad_norm 1.2704 (1.4606)	mem 23874MB
[2022-11-11 04:05:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][550/1251]	eta 0:08:46 lr 0.000822	time 0.7435 (0.7505)	loss 5.0214 (4.4605)	grad_norm 1.4717 (1.4597)	mem 23874MB
[2022-11-11 04:06:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][600/1251]	eta 0:08:08 lr 0.000824	time 0.7345 (0.7502)	loss 4.8331 (4.4738)	grad_norm 1.3308 (1.4620)	mem 23874MB
[2022-11-11 04:06:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][650/1251]	eta 0:07:30 lr 0.000826	time 0.7411 (0.7500)	loss 3.4517 (4.4726)	grad_norm 1.3295 (1.4623)	mem 23874MB
[2022-11-11 04:07:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][700/1251]	eta 0:06:53 lr 0.000828	time 0.7381 (0.7497)	loss 4.7544 (4.4723)	grad_norm 1.2484 (1.4617)	mem 23874MB
[2022-11-11 04:07:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][750/1251]	eta 0:06:15 lr 0.000830	time 0.8251 (0.7496)	loss 3.9190 (4.4694)	grad_norm 1.2731 (1.4599)	mem 23874MB
[2022-11-11 04:08:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][800/1251]	eta 0:05:37 lr 0.000832	time 0.7428 (0.7492)	loss 4.8696 (4.4697)	grad_norm 1.6402 (1.4635)	mem 23874MB
[2022-11-11 04:09:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][850/1251]	eta 0:05:00 lr 0.000834	time 0.7376 (0.7493)	loss 3.2624 (4.4674)	grad_norm 1.4192 (1.4626)	mem 23874MB
[2022-11-11 04:09:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][900/1251]	eta 0:04:22 lr 0.000836	time 0.7427 (0.7490)	loss 4.9297 (4.4659)	grad_norm 1.2998 (1.4618)	mem 23874MB
[2022-11-11 04:10:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][950/1251]	eta 0:03:45 lr 0.000838	time 0.7415 (0.7488)	loss 5.0218 (4.4619)	grad_norm 1.4603 (1.4590)	mem 23874MB
[2022-11-11 04:11:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][1000/1251]	eta 0:03:07 lr 0.000840	time 0.7408 (0.7488)	loss 4.6155 (4.4576)	grad_norm 1.5060 (1.4583)	mem 23874MB
[2022-11-11 04:11:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][1050/1251]	eta 0:02:30 lr 0.000842	time 0.7404 (0.7487)	loss 4.9344 (4.4538)	grad_norm 1.3442 (1.4564)	mem 23874MB
[2022-11-11 04:12:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][1100/1251]	eta 0:01:53 lr 0.000844	time 0.7359 (0.7485)	loss 4.5000 (4.4477)	grad_norm 1.4160 (1.4547)	mem 23874MB
[2022-11-11 04:12:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][1150/1251]	eta 0:01:15 lr 0.000846	time 0.7456 (0.7485)	loss 4.1274 (4.4433)	grad_norm 1.3247 (1.4551)	mem 23874MB
[2022-11-11 04:13:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][1200/1251]	eta 0:00:38 lr 0.000848	time 0.7394 (0.7484)	loss 4.5668 (4.4470)	grad_norm 1.3879 (1.4531)	mem 23874MB
[2022-11-11 04:14:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [16/300][1250/1251]	eta 0:00:00 lr 0.000850	time 0.7251 (0.7482)	loss 4.7263 (4.4457)	grad_norm 1.4629 (1.4538)	mem 23874MB
[2022-11-11 04:14:06 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 16 training takes 0:15:36
[2022-11-11 04:14:07 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_16.pth saving......
[2022-11-11 04:14:08 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_16.pth saved !!!
[2022-11-11 04:14:09 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.620 (1.620)	Loss 1.7185 (1.7185)	Acc@1 60.547 (60.547)	Acc@5 85.645 (85.645)	Mem 23874MB
[2022-11-11 04:14:20 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 61.728 Acc@5 84.844
[2022-11-11 04:14:20 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 61.7%
[2022-11-11 04:14:22 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.859 (1.859)	Loss 4.2210 (4.2210)	Acc@1 17.480 (17.480)	Acc@5 41.699 (41.699)	Mem 23874MB
[2022-11-11 04:14:32 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 19.046 Acc@5 41.196
[2022-11-11 04:14:32 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 19.0%
[2022-11-11 04:14:32 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 61.73% at 16 epoch
[2022-11-11 04:14:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][0/1251]	eta 0:55:05 lr 0.000850	time 2.6425 (2.6425)	loss 4.2914 (4.2914)	grad_norm 1.2693 (1.2693)	mem 23874MB
[2022-11-11 04:15:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][50/1251]	eta 0:15:44 lr 0.000852	time 0.7378 (0.7861)	loss 4.7222 (4.2494)	grad_norm 1.3195 (1.4612)	mem 23874MB
[2022-11-11 04:15:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][100/1251]	eta 0:14:41 lr 0.000854	time 0.7351 (0.7661)	loss 4.6009 (4.4011)	grad_norm 1.3157 (1.4635)	mem 23874MB
[2022-11-11 04:16:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][150/1251]	eta 0:13:55 lr 0.000856	time 0.7365 (0.7592)	loss 3.7629 (4.3740)	grad_norm 1.6071 (1.4406)	mem 23874MB
[2022-11-11 04:17:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][200/1251]	eta 0:13:14 lr 0.000858	time 0.7334 (0.7560)	loss 4.8759 (4.3516)	grad_norm 1.3733 (1.4376)	mem 23874MB
[2022-11-11 04:17:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][250/1251]	eta 0:12:34 lr 0.000860	time 0.7382 (0.7534)	loss 3.3070 (4.3306)	grad_norm 1.6141 (1.4434)	mem 23874MB
[2022-11-11 04:18:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][300/1251]	eta 0:11:55 lr 0.000862	time 0.7392 (0.7522)	loss 4.4480 (4.3290)	grad_norm 1.4693 (1.4353)	mem 23874MB
[2022-11-11 04:18:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][350/1251]	eta 0:11:16 lr 0.000864	time 0.7337 (0.7512)	loss 4.7880 (4.3167)	grad_norm 2.3842 (1.4332)	mem 23874MB
[2022-11-11 04:19:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][400/1251]	eta 0:10:38 lr 0.000866	time 0.8018 (0.7507)	loss 4.9398 (4.3340)	grad_norm 1.3985 (1.4315)	mem 23874MB
[2022-11-11 04:20:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][450/1251]	eta 0:10:00 lr 0.000868	time 0.7378 (0.7501)	loss 4.5125 (4.3515)	grad_norm 1.5437 (1.4288)	mem 23874MB
[2022-11-11 04:20:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][500/1251]	eta 0:09:23 lr 0.000870	time 0.8341 (0.7497)	loss 3.5624 (4.3618)	grad_norm 1.1843 (1.4309)	mem 23874MB
[2022-11-11 04:21:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][550/1251]	eta 0:08:45 lr 0.000872	time 0.7337 (0.7490)	loss 4.2184 (4.3652)	grad_norm 1.5648 (1.4283)	mem 23874MB
[2022-11-11 04:22:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][600/1251]	eta 0:08:07 lr 0.000874	time 0.8270 (0.7490)	loss 5.4367 (4.3688)	grad_norm 1.3449 (1.4310)	mem 23874MB
[2022-11-11 04:22:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][650/1251]	eta 0:07:29 lr 0.000876	time 0.7379 (0.7486)	loss 4.1959 (4.3722)	grad_norm 1.5085 (1.4328)	mem 23874MB
[2022-11-11 04:23:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][700/1251]	eta 0:06:52 lr 0.000878	time 0.7372 (0.7484)	loss 4.6697 (4.3726)	grad_norm 1.5361 (1.4323)	mem 23874MB
[2022-11-11 04:23:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][750/1251]	eta 0:06:14 lr 0.000880	time 0.7294 (0.7483)	loss 3.2218 (4.3818)	grad_norm 1.1702 (1.4320)	mem 23874MB
[2022-11-11 04:24:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][800/1251]	eta 0:05:37 lr 0.000882	time 0.7407 (0.7480)	loss 4.7694 (4.3780)	grad_norm 1.3630 (1.4342)	mem 23874MB
[2022-11-11 04:25:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][850/1251]	eta 0:04:59 lr 0.000884	time 0.7370 (0.7477)	loss 4.8123 (4.3761)	grad_norm 1.3618 (1.4378)	mem 23874MB
[2022-11-11 04:25:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][900/1251]	eta 0:04:22 lr 0.000886	time 0.7377 (0.7478)	loss 4.4645 (4.3748)	grad_norm 1.2344 (1.4358)	mem 23874MB
[2022-11-11 04:26:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][950/1251]	eta 0:03:45 lr 0.000888	time 0.7524 (0.7476)	loss 4.1967 (4.3769)	grad_norm 1.5629 (1.4347)	mem 23874MB
[2022-11-11 04:27:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][1000/1251]	eta 0:03:07 lr 0.000890	time 0.7378 (0.7476)	loss 4.8818 (4.3827)	grad_norm 1.4598 (nan)	mem 23874MB
[2022-11-11 04:27:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][1050/1251]	eta 0:02:30 lr 0.000892	time 0.7410 (0.7473)	loss 4.3676 (4.3831)	grad_norm 1.3091 (nan)	mem 23874MB
[2022-11-11 04:28:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][1100/1251]	eta 0:01:52 lr 0.000894	time 0.7364 (0.7473)	loss 4.8466 (4.3902)	grad_norm 1.6049 (nan)	mem 23874MB
[2022-11-11 04:28:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][1150/1251]	eta 0:01:15 lr 0.000896	time 0.7385 (0.7473)	loss 4.5800 (4.3928)	grad_norm 1.3886 (nan)	mem 23874MB
[2022-11-11 04:29:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][1200/1251]	eta 0:00:38 lr 0.000898	time 0.7385 (0.7473)	loss 4.8444 (4.3975)	grad_norm 1.6171 (nan)	mem 23874MB
[2022-11-11 04:30:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [17/300][1250/1251]	eta 0:00:00 lr 0.000900	time 0.7267 (0.7470)	loss 4.8144 (4.3987)	grad_norm 1.5489 (nan)	mem 23874MB
[2022-11-11 04:30:07 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 17 training takes 0:15:34
[2022-11-11 04:30:07 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_17.pth saving......
[2022-11-11 04:30:08 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_17.pth saved !!!
[2022-11-11 04:30:10 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.741 (1.741)	Loss 1.7794 (1.7794)	Acc@1 61.621 (61.621)	Acc@5 84.473 (84.473)	Mem 23874MB
[2022-11-11 04:30:21 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 62.710 Acc@5 85.146
[2022-11-11 04:30:21 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 62.7%
[2022-11-11 04:30:23 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.916 (1.916)	Loss 3.8916 (3.8916)	Acc@1 23.926 (23.926)	Acc@5 48.047 (48.047)	Mem 23874MB
[2022-11-11 04:30:33 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 25.212 Acc@5 49.260
[2022-11-11 04:30:33 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 25.2%
[2022-11-11 04:30:33 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 62.71% at 17 epoch
[2022-11-11 04:30:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][0/1251]	eta 0:51:18 lr 0.000900	time 2.4608 (2.4608)	loss 4.2674 (4.2674)	grad_norm 1.2072 (1.2072)	mem 23874MB
[2022-11-11 04:31:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][50/1251]	eta 0:15:35 lr 0.000902	time 0.7363 (0.7790)	loss 3.9338 (4.2318)	grad_norm 1.6039 (1.4192)	mem 23874MB
[2022-11-11 04:31:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][100/1251]	eta 0:14:40 lr 0.000904	time 0.7408 (0.7651)	loss 4.0231 (4.2934)	grad_norm 1.4426 (1.4160)	mem 23874MB
[2022-11-11 04:32:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][150/1251]	eta 0:13:56 lr 0.000906	time 0.7444 (0.7595)	loss 4.7295 (4.3674)	grad_norm 1.2959 (1.4217)	mem 23874MB
[2022-11-11 04:33:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][200/1251]	eta 0:13:14 lr 0.000908	time 0.8183 (0.7557)	loss 4.9957 (4.3181)	grad_norm 1.4926 (1.4177)	mem 23874MB
[2022-11-11 04:33:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][250/1251]	eta 0:12:34 lr 0.000910	time 0.7418 (0.7540)	loss 3.6992 (4.3380)	grad_norm 1.2761 (1.4187)	mem 23874MB
[2022-11-11 04:34:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][300/1251]	eta 0:11:55 lr 0.000912	time 0.8147 (0.7527)	loss 4.6245 (4.3462)	grad_norm 1.5482 (1.4137)	mem 23874MB
[2022-11-11 04:34:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][350/1251]	eta 0:11:17 lr 0.000914	time 0.7414 (0.7515)	loss 5.0722 (4.3407)	grad_norm 1.5996 (1.4103)	mem 23874MB
[2022-11-11 04:35:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][400/1251]	eta 0:10:39 lr 0.000916	time 0.7358 (0.7511)	loss 4.5816 (4.3287)	grad_norm 1.2268 (1.4198)	mem 23874MB
[2022-11-11 04:36:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][450/1251]	eta 0:10:00 lr 0.000918	time 0.7374 (0.7502)	loss 4.6365 (4.3350)	grad_norm 1.2969 (1.4157)	mem 23874MB
[2022-11-11 04:36:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][500/1251]	eta 0:09:23 lr 0.000920	time 0.7496 (0.7500)	loss 4.5287 (4.3430)	grad_norm 1.5384 (1.4120)	mem 23874MB
[2022-11-11 04:37:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][550/1251]	eta 0:08:45 lr 0.000922	time 0.7425 (0.7499)	loss 4.8124 (4.3291)	grad_norm 1.3337 (1.4080)	mem 23874MB
[2022-11-11 04:38:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][600/1251]	eta 0:08:07 lr 0.000924	time 0.7355 (0.7493)	loss 5.3270 (4.3268)	grad_norm 1.1846 (1.4038)	mem 23874MB
[2022-11-11 04:38:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][650/1251]	eta 0:07:30 lr 0.000926	time 0.7384 (0.7493)	loss 3.3702 (4.3260)	grad_norm 1.6465 (1.4079)	mem 23874MB
[2022-11-11 04:39:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][700/1251]	eta 0:06:52 lr 0.000928	time 0.8130 (0.7491)	loss 4.8921 (4.3266)	grad_norm 1.4225 (1.4045)	mem 23874MB
[2022-11-11 04:39:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][750/1251]	eta 0:06:15 lr 0.000930	time 0.7355 (0.7487)	loss 3.7192 (4.3355)	grad_norm 1.3801 (1.4050)	mem 23874MB
[2022-11-11 04:40:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][800/1251]	eta 0:05:37 lr 0.000932	time 0.7380 (0.7486)	loss 4.1126 (4.3372)	grad_norm 1.6025 (1.4031)	mem 23874MB
[2022-11-11 04:41:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][850/1251]	eta 0:05:00 lr 0.000934	time 0.7495 (0.7483)	loss 4.7954 (4.3370)	grad_norm 1.2731 (1.4012)	mem 23874MB
[2022-11-11 04:41:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][900/1251]	eta 0:04:22 lr 0.000936	time 0.7388 (0.7484)	loss 4.8454 (4.3267)	grad_norm 1.1743 (1.3994)	mem 23874MB
[2022-11-11 04:42:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][950/1251]	eta 0:03:45 lr 0.000938	time 0.7446 (0.7483)	loss 4.7858 (4.3302)	grad_norm 1.1876 (1.3958)	mem 23874MB
[2022-11-11 04:43:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][1000/1251]	eta 0:03:07 lr 0.000940	time 0.7531 (0.7481)	loss 4.7439 (4.3304)	grad_norm 1.3952 (1.3970)	mem 23874MB
[2022-11-11 04:43:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][1050/1251]	eta 0:02:30 lr 0.000942	time 0.7392 (0.7480)	loss 4.3357 (4.3323)	grad_norm 1.4778 (1.3972)	mem 23874MB
[2022-11-11 04:44:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][1100/1251]	eta 0:01:52 lr 0.000944	time 0.8066 (0.7480)	loss 4.8596 (4.3396)	grad_norm 1.4552 (1.3956)	mem 23874MB
[2022-11-11 04:44:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][1150/1251]	eta 0:01:15 lr 0.000946	time 0.7378 (0.7480)	loss 4.0803 (4.3371)	grad_norm 1.2869 (1.3957)	mem 23874MB
[2022-11-11 04:45:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][1200/1251]	eta 0:00:38 lr 0.000948	time 0.7415 (0.7479)	loss 5.4386 (4.3403)	grad_norm 1.4576 (1.3961)	mem 23874MB
[2022-11-11 04:46:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [18/300][1250/1251]	eta 0:00:00 lr 0.000950	time 0.7331 (0.7476)	loss 3.8317 (4.3384)	grad_norm 1.2781 (1.3942)	mem 23874MB
[2022-11-11 04:46:09 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 18 training takes 0:15:35
[2022-11-11 04:46:09 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_18.pth saving......
[2022-11-11 04:46:10 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_18.pth saved !!!
[2022-11-11 04:46:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.691 (1.691)	Loss 1.6641 (1.6641)	Acc@1 62.891 (62.891)	Acc@5 85.547 (85.547)	Mem 23874MB
[2022-11-11 04:46:23 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 63.362 Acc@5 85.750
[2022-11-11 04:46:23 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 63.4%
[2022-11-11 04:46:25 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.906 (1.906)	Loss 3.4490 (3.4490)	Acc@1 29.883 (29.883)	Acc@5 54.004 (54.004)	Mem 23874MB
[2022-11-11 04:46:35 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 31.398 Acc@5 56.842
[2022-11-11 04:46:35 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 31.4%
[2022-11-11 04:46:35 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 63.36% at 18 epoch
[2022-11-11 04:46:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][0/1251]	eta 0:51:16 lr 0.000950	time 2.4596 (2.4596)	loss 4.2303 (4.2303)	grad_norm 1.2944 (1.2944)	mem 23874MB
[2022-11-11 04:47:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][50/1251]	eta 0:15:37 lr 0.000952	time 0.7376 (0.7802)	loss 4.3729 (4.1563)	grad_norm 1.5781 (1.4001)	mem 23874MB
[2022-11-11 04:47:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][100/1251]	eta 0:14:38 lr 0.000954	time 0.7373 (0.7630)	loss 4.2160 (4.2571)	grad_norm 1.1883 (1.3923)	mem 23874MB
[2022-11-11 04:48:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][150/1251]	eta 0:13:53 lr 0.000956	time 0.7346 (0.7570)	loss 4.2326 (4.2809)	grad_norm 1.5019 (1.3894)	mem 23874MB
[2022-11-11 04:49:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][200/1251]	eta 0:13:12 lr 0.000958	time 0.7375 (0.7537)	loss 3.3038 (4.3058)	grad_norm 1.7171 (1.3968)	mem 23874MB
[2022-11-11 04:49:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][250/1251]	eta 0:12:32 lr 0.000960	time 0.7391 (0.7521)	loss 3.4928 (4.3232)	grad_norm 1.3143 (1.3886)	mem 23874MB
[2022-11-11 04:50:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][300/1251]	eta 0:11:54 lr 0.000962	time 0.7363 (0.7508)	loss 3.3189 (4.3081)	grad_norm 1.3385 (1.3827)	mem 23874MB
[2022-11-11 04:50:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][350/1251]	eta 0:11:15 lr 0.000964	time 0.7398 (0.7499)	loss 4.2763 (4.3184)	grad_norm 1.2509 (1.3801)	mem 23874MB
[2022-11-11 04:51:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][400/1251]	eta 0:10:37 lr 0.000966	time 0.7404 (0.7493)	loss 3.4780 (4.3327)	grad_norm 1.3240 (1.3779)	mem 23874MB
[2022-11-11 04:52:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][450/1251]	eta 0:09:59 lr 0.000968	time 0.7367 (0.7488)	loss 4.8240 (4.3268)	grad_norm 1.6526 (1.3713)	mem 23874MB
[2022-11-11 04:52:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][500/1251]	eta 0:09:22 lr 0.000970	time 0.7366 (0.7486)	loss 4.6535 (4.3284)	grad_norm 1.3413 (1.3746)	mem 23874MB
[2022-11-11 04:53:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][550/1251]	eta 0:08:44 lr 0.000972	time 0.7387 (0.7482)	loss 4.0846 (4.3332)	grad_norm 1.5050 (1.3773)	mem 23874MB
[2022-11-11 04:54:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][600/1251]	eta 0:08:06 lr 0.000974	time 0.7420 (0.7480)	loss 4.0228 (4.3302)	grad_norm 1.4322 (1.3760)	mem 23874MB
[2022-11-11 04:54:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][650/1251]	eta 0:07:29 lr 0.000976	time 0.7454 (0.7479)	loss 4.8636 (4.3317)	grad_norm 1.3178 (1.3742)	mem 23874MB
[2022-11-11 04:55:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][700/1251]	eta 0:06:52 lr 0.000978	time 0.7391 (0.7477)	loss 4.8111 (4.3174)	grad_norm 1.1202 (1.3711)	mem 23874MB
[2022-11-11 04:55:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][750/1251]	eta 0:06:14 lr 0.000980	time 0.7356 (0.7477)	loss 4.2196 (4.3184)	grad_norm 1.3980 (1.3686)	mem 23874MB
[2022-11-11 04:56:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][800/1251]	eta 0:05:37 lr 0.000982	time 0.7385 (0.7476)	loss 4.4370 (4.3228)	grad_norm 1.2991 (1.3722)	mem 23874MB
[2022-11-11 04:57:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][850/1251]	eta 0:04:59 lr 0.000984	time 0.7374 (0.7475)	loss 5.2858 (4.3239)	grad_norm 1.2514 (1.3731)	mem 23874MB
[2022-11-11 04:57:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][900/1251]	eta 0:04:22 lr 0.000986	time 0.7403 (0.7473)	loss 4.1621 (4.3314)	grad_norm 1.3708 (1.3729)	mem 23874MB
[2022-11-11 04:58:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][950/1251]	eta 0:03:44 lr 0.000988	time 0.7429 (0.7473)	loss 5.1012 (4.3334)	grad_norm 1.1806 (1.3716)	mem 23874MB
[2022-11-11 04:59:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][1000/1251]	eta 0:03:07 lr 0.000990	time 0.7406 (0.7472)	loss 4.3241 (4.3428)	grad_norm 1.3383 (1.3696)	mem 23874MB
[2022-11-11 04:59:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][1050/1251]	eta 0:02:30 lr 0.000992	time 0.7363 (0.7471)	loss 5.0570 (4.3358)	grad_norm 1.2280 (1.3677)	mem 23874MB
[2022-11-11 05:00:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][1100/1251]	eta 0:01:52 lr 0.000994	time 0.7399 (0.7470)	loss 4.9373 (4.3356)	grad_norm 1.2640 (1.3678)	mem 23874MB
[2022-11-11 05:00:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][1150/1251]	eta 0:01:15 lr 0.000996	time 0.7336 (0.7468)	loss 4.2385 (4.3310)	grad_norm 1.3401 (1.3657)	mem 23874MB
[2022-11-11 05:01:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][1200/1251]	eta 0:00:38 lr 0.000998	time 0.7407 (0.7468)	loss 4.7943 (4.3329)	grad_norm 1.2782 (1.3648)	mem 23874MB
[2022-11-11 05:02:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [19/300][1250/1251]	eta 0:00:00 lr 0.001000	time 0.7262 (0.7466)	loss 4.7461 (4.3357)	grad_norm 1.6178 (1.3656)	mem 23874MB
[2022-11-11 05:02:10 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 19 training takes 0:15:34
[2022-11-11 05:02:10 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_19.pth saving......
[2022-11-11 05:02:11 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_19.pth saved !!!
[2022-11-11 05:02:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.701 (1.701)	Loss 1.5874 (1.5874)	Acc@1 64.355 (64.355)	Acc@5 86.621 (86.621)	Mem 23874MB
[2022-11-11 05:02:23 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 64.586 Acc@5 86.324
[2022-11-11 05:02:23 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 64.6%
[2022-11-11 05:02:25 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.777 (1.777)	Loss 3.0542 (3.0542)	Acc@1 36.719 (36.719)	Acc@5 64.160 (64.160)	Mem 23874MB
[2022-11-11 05:02:35 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 37.806 Acc@5 63.686
[2022-11-11 05:02:35 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 37.8%
[2022-11-11 05:02:35 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 64.59% at 19 epoch
[2022-11-11 05:02:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][0/1251]	eta 0:50:44 lr 0.000989	time 2.4340 (2.4340)	loss 3.9213 (3.9213)	grad_norm 1.5632 (1.5632)	mem 23874MB
[2022-11-11 05:03:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][50/1251]	eta 0:15:41 lr 0.000989	time 0.7363 (0.7841)	loss 3.1077 (4.1910)	grad_norm 1.1364 (1.4046)	mem 23874MB
[2022-11-11 05:03:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][100/1251]	eta 0:14:38 lr 0.000989	time 0.7404 (0.7634)	loss 4.0182 (4.1790)	grad_norm 1.2541 (1.3942)	mem 23874MB
[2022-11-11 05:04:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][150/1251]	eta 0:13:55 lr 0.000989	time 0.7423 (0.7588)	loss 3.7962 (4.2016)	grad_norm 1.4175 (1.3781)	mem 23874MB
[2022-11-11 05:05:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][200/1251]	eta 0:13:13 lr 0.000989	time 0.7418 (0.7550)	loss 3.9514 (4.2498)	grad_norm 1.3066 (1.3807)	mem 23874MB
[2022-11-11 05:05:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][250/1251]	eta 0:12:34 lr 0.000989	time 0.7400 (0.7534)	loss 4.4306 (4.2677)	grad_norm 2.0231 (1.3628)	mem 23874MB
[2022-11-11 05:06:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][300/1251]	eta 0:11:54 lr 0.000989	time 0.7463 (0.7517)	loss 3.1229 (4.2728)	grad_norm 1.3513 (nan)	mem 23874MB
[2022-11-11 05:06:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][350/1251]	eta 0:11:16 lr 0.000989	time 0.7380 (0.7512)	loss 4.7428 (4.2798)	grad_norm 1.2669 (nan)	mem 23874MB
[2022-11-11 05:07:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][400/1251]	eta 0:10:38 lr 0.000989	time 0.7450 (0.7506)	loss 4.7229 (4.2664)	grad_norm 1.4573 (nan)	mem 23874MB
[2022-11-11 05:08:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][450/1251]	eta 0:10:00 lr 0.000989	time 0.7395 (0.7502)	loss 5.0391 (4.2640)	grad_norm 1.3350 (nan)	mem 23874MB
[2022-11-11 05:08:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][500/1251]	eta 0:09:22 lr 0.000989	time 0.7400 (0.7496)	loss 4.5387 (4.2545)	grad_norm 1.2346 (nan)	mem 23874MB
[2022-11-11 05:09:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][550/1251]	eta 0:08:45 lr 0.000989	time 0.7369 (0.7495)	loss 3.9770 (4.2449)	grad_norm 1.1946 (nan)	mem 23874MB
[2022-11-11 05:10:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][600/1251]	eta 0:08:07 lr 0.000989	time 0.7371 (0.7491)	loss 4.3660 (4.2518)	grad_norm 1.4072 (nan)	mem 23874MB
[2022-11-11 05:10:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][650/1251]	eta 0:07:30 lr 0.000989	time 0.7440 (0.7490)	loss 5.2725 (4.2490)	grad_norm 1.0460 (nan)	mem 23874MB
[2022-11-11 05:11:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][700/1251]	eta 0:06:52 lr 0.000989	time 0.7388 (0.7488)	loss 4.9585 (4.2490)	grad_norm 1.2202 (nan)	mem 23874MB
[2022-11-11 05:11:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][750/1251]	eta 0:06:15 lr 0.000989	time 0.7362 (0.7487)	loss 5.0736 (4.2497)	grad_norm 1.2520 (nan)	mem 23874MB
[2022-11-11 05:12:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][800/1251]	eta 0:05:37 lr 0.000988	time 0.7339 (0.7485)	loss 3.9092 (4.2506)	grad_norm 1.1929 (nan)	mem 23874MB
[2022-11-11 05:13:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][850/1251]	eta 0:05:00 lr 0.000988	time 0.7392 (0.7484)	loss 4.6298 (4.2581)	grad_norm 1.3897 (nan)	mem 23874MB
[2022-11-11 05:13:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][900/1251]	eta 0:04:22 lr 0.000988	time 0.7352 (0.7482)	loss 4.6769 (4.2455)	grad_norm 1.2334 (nan)	mem 23874MB
[2022-11-11 05:14:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][950/1251]	eta 0:03:45 lr 0.000988	time 0.7361 (0.7482)	loss 3.9940 (4.2422)	grad_norm 1.2947 (nan)	mem 23874MB
[2022-11-11 05:15:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][1000/1251]	eta 0:03:07 lr 0.000988	time 0.7495 (0.7480)	loss 4.7062 (4.2341)	grad_norm 1.5177 (nan)	mem 23874MB
[2022-11-11 05:15:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][1050/1251]	eta 0:02:30 lr 0.000988	time 0.7352 (0.7480)	loss 4.4359 (4.2325)	grad_norm 1.5135 (nan)	mem 23874MB
[2022-11-11 05:16:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][1100/1251]	eta 0:01:52 lr 0.000988	time 0.7389 (0.7478)	loss 4.4078 (4.2347)	grad_norm 1.3754 (nan)	mem 23874MB
[2022-11-11 05:16:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][1150/1251]	eta 0:01:15 lr 0.000988	time 0.7342 (0.7478)	loss 5.0458 (4.2345)	grad_norm 1.2413 (nan)	mem 23874MB
[2022-11-11 05:17:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][1200/1251]	eta 0:00:38 lr 0.000988	time 0.7381 (0.7478)	loss 4.1277 (4.2362)	grad_norm 1.1068 (nan)	mem 23874MB
[2022-11-11 05:18:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [20/300][1250/1251]	eta 0:00:00 lr 0.000988	time 0.7268 (0.7475)	loss 4.6653 (4.2407)	grad_norm 1.2535 (nan)	mem 23874MB
[2022-11-11 05:18:11 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 20 training takes 0:15:35
[2022-11-11 05:18:11 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_20.pth saving......
[2022-11-11 05:18:12 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_20.pth saved !!!
[2022-11-11 05:18:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.674 (1.674)	Loss 1.5431 (1.5431)	Acc@1 65.723 (65.723)	Acc@5 87.988 (87.988)	Mem 23874MB
[2022-11-11 05:18:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 64.910 Acc@5 86.954
[2022-11-11 05:18:25 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 64.9%
[2022-11-11 05:18:27 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.861 (1.861)	Loss 2.6473 (2.6473)	Acc@1 44.727 (44.727)	Acc@5 70.703 (70.703)	Mem 23874MB
[2022-11-11 05:18:37 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 43.442 Acc@5 69.136
[2022-11-11 05:18:37 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 43.4%
[2022-11-11 05:18:37 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 64.91% at 20 epoch
[2022-11-11 05:18:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][0/1251]	eta 0:52:01 lr 0.000988	time 2.4949 (2.4949)	loss 3.8827 (3.8827)	grad_norm 1.2914 (1.2914)	mem 23874MB
[2022-11-11 05:19:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][50/1251]	eta 0:15:38 lr 0.000988	time 0.7321 (0.7818)	loss 4.5730 (4.3062)	grad_norm 1.3638 (1.3118)	mem 23874MB
[2022-11-11 05:19:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][100/1251]	eta 0:14:38 lr 0.000988	time 0.7347 (0.7632)	loss 4.3327 (4.3020)	grad_norm 1.2750 (1.3258)	mem 23874MB
[2022-11-11 05:20:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][150/1251]	eta 0:13:54 lr 0.000988	time 0.7371 (0.7582)	loss 4.5867 (4.3127)	grad_norm 1.2429 (1.3150)	mem 23874MB
[2022-11-11 05:21:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][200/1251]	eta 0:13:13 lr 0.000988	time 0.7493 (0.7551)	loss 4.2300 (4.3010)	grad_norm 1.2619 (1.3179)	mem 23874MB
[2022-11-11 05:21:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][250/1251]	eta 0:12:34 lr 0.000988	time 0.7511 (0.7539)	loss 4.1635 (4.2639)	grad_norm 1.3518 (1.3211)	mem 23874MB
[2022-11-11 05:22:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][300/1251]	eta 0:11:55 lr 0.000988	time 0.7410 (0.7525)	loss 4.1478 (4.2838)	grad_norm 1.1879 (1.3216)	mem 23874MB
[2022-11-11 05:23:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][350/1251]	eta 0:11:16 lr 0.000988	time 0.7358 (0.7514)	loss 3.9491 (4.2677)	grad_norm 1.2982 (1.3295)	mem 23874MB
[2022-11-11 05:23:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][400/1251]	eta 0:10:39 lr 0.000988	time 0.7363 (0.7511)	loss 3.6849 (4.2655)	grad_norm 1.2633 (1.3326)	mem 23874MB
[2022-11-11 05:24:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][450/1251]	eta 0:10:01 lr 0.000988	time 0.7384 (0.7506)	loss 4.7530 (4.2737)	grad_norm 1.3488 (1.3290)	mem 23874MB
[2022-11-11 05:24:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][500/1251]	eta 0:09:23 lr 0.000988	time 0.7447 (0.7504)	loss 4.8237 (4.2606)	grad_norm 1.1774 (1.3295)	mem 23874MB
[2022-11-11 05:25:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][550/1251]	eta 0:08:45 lr 0.000988	time 0.7382 (0.7499)	loss 3.7884 (4.2615)	grad_norm 1.3355 (1.3240)	mem 23874MB
[2022-11-11 05:26:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][600/1251]	eta 0:08:08 lr 0.000988	time 0.7415 (0.7496)	loss 4.9390 (4.2650)	grad_norm 1.4728 (1.3212)	mem 23874MB
[2022-11-11 05:26:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][650/1251]	eta 0:07:30 lr 0.000987	time 0.7354 (0.7493)	loss 4.8023 (4.2651)	grad_norm 1.2002 (1.3238)	mem 23874MB
[2022-11-11 05:27:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][700/1251]	eta 0:06:52 lr 0.000987	time 0.7340 (0.7491)	loss 3.7809 (4.2621)	grad_norm 1.3183 (1.3248)	mem 23874MB
[2022-11-11 05:28:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][750/1251]	eta 0:06:15 lr 0.000987	time 0.7376 (0.7488)	loss 4.6251 (4.2543)	grad_norm 1.1974 (1.3243)	mem 23874MB
[2022-11-11 05:28:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][800/1251]	eta 0:05:37 lr 0.000987	time 0.7359 (0.7486)	loss 3.1954 (4.2512)	grad_norm 1.2810 (1.3257)	mem 23874MB
[2022-11-11 05:29:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][850/1251]	eta 0:05:00 lr 0.000987	time 0.7376 (0.7486)	loss 4.1031 (4.2571)	grad_norm 1.4745 (1.3242)	mem 23874MB
[2022-11-11 05:29:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][900/1251]	eta 0:04:22 lr 0.000987	time 0.8225 (0.7484)	loss 4.0693 (4.2577)	grad_norm 1.1937 (1.3232)	mem 23874MB
[2022-11-11 05:30:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][950/1251]	eta 0:03:45 lr 0.000987	time 0.7395 (0.7484)	loss 2.9471 (4.2511)	grad_norm 1.1473 (1.3210)	mem 23874MB
[2022-11-11 05:31:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][1000/1251]	eta 0:03:07 lr 0.000987	time 0.7350 (0.7483)	loss 4.3783 (4.2485)	grad_norm 1.2661 (1.3222)	mem 23874MB
[2022-11-11 05:31:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][1050/1251]	eta 0:02:30 lr 0.000987	time 0.7358 (0.7482)	loss 5.0177 (4.2459)	grad_norm 1.4704 (1.3209)	mem 23874MB
[2022-11-11 05:32:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][1100/1251]	eta 0:01:52 lr 0.000987	time 0.7389 (0.7481)	loss 5.2649 (4.2425)	grad_norm 1.1456 (nan)	mem 23874MB
[2022-11-11 05:32:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][1150/1251]	eta 0:01:15 lr 0.000987	time 0.7463 (0.7481)	loss 3.7174 (4.2425)	grad_norm 1.1892 (nan)	mem 23874MB
[2022-11-11 05:33:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][1200/1251]	eta 0:00:38 lr 0.000987	time 0.7402 (0.7480)	loss 4.3994 (4.2375)	grad_norm 1.4289 (nan)	mem 23874MB
[2022-11-11 05:34:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [21/300][1250/1251]	eta 0:00:00 lr 0.000987	time 0.7274 (0.7478)	loss 3.6603 (4.2362)	grad_norm 1.2689 (nan)	mem 23874MB
[2022-11-11 05:34:13 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 21 training takes 0:15:35
[2022-11-11 05:34:13 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_21.pth saving......
[2022-11-11 05:34:14 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_21.pth saved !!!
[2022-11-11 05:34:16 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.626 (1.626)	Loss 1.3813 (1.3813)	Acc@1 67.773 (67.773)	Acc@5 88.477 (88.477)	Mem 23874MB
[2022-11-11 05:34:27 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 66.202 Acc@5 87.518
[2022-11-11 05:34:27 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 66.2%
[2022-11-11 05:34:28 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.888 (1.888)	Loss 2.4864 (2.4864)	Acc@1 48.535 (48.535)	Acc@5 73.926 (73.926)	Mem 23874MB
[2022-11-11 05:34:39 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 48.350 Acc@5 73.686
[2022-11-11 05:34:39 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 48.4%
[2022-11-11 05:34:39 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 66.20% at 21 epoch
[2022-11-11 05:34:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][0/1251]	eta 0:51:37 lr 0.000987	time 2.4758 (2.4758)	loss 4.9228 (4.9228)	grad_norm 1.3711 (1.3711)	mem 23874MB
[2022-11-11 05:35:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][50/1251]	eta 0:15:40 lr 0.000987	time 0.7410 (0.7831)	loss 4.0095 (4.2760)	grad_norm 1.6280 (1.3514)	mem 23874MB
[2022-11-11 05:35:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][100/1251]	eta 0:14:40 lr 0.000987	time 0.7339 (0.7652)	loss 4.3829 (4.1698)	grad_norm 1.2239 (1.3167)	mem 23874MB
[2022-11-11 05:36:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][150/1251]	eta 0:13:55 lr 0.000987	time 0.8058 (0.7586)	loss 3.2538 (4.1801)	grad_norm 1.4682 (1.3051)	mem 23874MB
[2022-11-11 05:37:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][200/1251]	eta 0:13:14 lr 0.000987	time 0.7482 (0.7555)	loss 4.2960 (4.1682)	grad_norm 1.2971 (1.3055)	mem 23874MB
[2022-11-11 05:37:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][250/1251]	eta 0:12:34 lr 0.000987	time 0.7415 (0.7537)	loss 4.3292 (4.1620)	grad_norm 1.2242 (1.3020)	mem 23874MB
[2022-11-11 05:38:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][300/1251]	eta 0:11:55 lr 0.000987	time 0.7416 (0.7526)	loss 4.0282 (4.1743)	grad_norm 1.2884 (1.3106)	mem 23874MB
[2022-11-11 05:39:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][350/1251]	eta 0:11:17 lr 0.000987	time 0.7345 (0.7519)	loss 3.6245 (4.1944)	grad_norm 1.2321 (1.3089)	mem 23874MB
[2022-11-11 05:39:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][400/1251]	eta 0:10:39 lr 0.000987	time 0.7350 (0.7511)	loss 4.4818 (4.1809)	grad_norm 1.3673 (1.3064)	mem 23874MB
[2022-11-11 05:40:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][450/1251]	eta 0:10:01 lr 0.000986	time 0.7374 (0.7504)	loss 4.1163 (4.1818)	grad_norm 1.4099 (1.3106)	mem 23874MB
[2022-11-11 05:40:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][500/1251]	eta 0:09:23 lr 0.000986	time 0.7378 (0.7500)	loss 4.4521 (4.1809)	grad_norm 1.2818 (1.3068)	mem 23874MB
[2022-11-11 05:41:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][550/1251]	eta 0:08:45 lr 0.000986	time 0.7420 (0.7495)	loss 3.2253 (4.1753)	grad_norm 1.3270 (1.3061)	mem 23874MB
[2022-11-11 05:42:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][600/1251]	eta 0:08:07 lr 0.000986	time 0.7364 (0.7494)	loss 3.9395 (4.1798)	grad_norm 1.3022 (1.3044)	mem 23874MB
[2022-11-11 05:42:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][650/1251]	eta 0:07:30 lr 0.000986	time 0.7375 (0.7493)	loss 4.9466 (4.1763)	grad_norm 1.3608 (1.3059)	mem 23874MB
[2022-11-11 05:43:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][700/1251]	eta 0:06:52 lr 0.000986	time 0.7461 (0.7489)	loss 4.4186 (4.1712)	grad_norm 1.4489 (1.3055)	mem 23874MB
[2022-11-11 05:44:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][750/1251]	eta 0:06:15 lr 0.000986	time 0.7446 (0.7490)	loss 2.8621 (4.1759)	grad_norm 1.3839 (1.3045)	mem 23874MB
[2022-11-11 05:44:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][800/1251]	eta 0:05:37 lr 0.000986	time 0.7366 (0.7486)	loss 4.6805 (4.1784)	grad_norm 1.1515 (1.3028)	mem 23874MB
[2022-11-11 05:45:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][850/1251]	eta 0:05:00 lr 0.000986	time 0.7469 (0.7486)	loss 3.9844 (4.1748)	grad_norm 1.4049 (1.3056)	mem 23874MB
[2022-11-11 05:45:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][900/1251]	eta 0:04:22 lr 0.000986	time 0.7376 (0.7484)	loss 4.6347 (4.1679)	grad_norm 1.5105 (1.3040)	mem 23874MB
[2022-11-11 05:46:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][950/1251]	eta 0:03:45 lr 0.000986	time 0.7386 (0.7483)	loss 4.3831 (4.1650)	grad_norm 1.2880 (1.3033)	mem 23874MB
[2022-11-11 05:47:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][1000/1251]	eta 0:03:07 lr 0.000986	time 0.7411 (0.7483)	loss 4.4965 (4.1670)	grad_norm 1.1839 (1.3033)	mem 23874MB
[2022-11-11 05:47:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][1050/1251]	eta 0:02:30 lr 0.000986	time 0.7435 (0.7481)	loss 4.6711 (4.1690)	grad_norm 1.4978 (1.3027)	mem 23874MB
[2022-11-11 05:48:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][1100/1251]	eta 0:01:52 lr 0.000986	time 0.7418 (0.7480)	loss 4.7815 (4.1766)	grad_norm 1.2735 (1.3017)	mem 23874MB
[2022-11-11 05:49:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][1150/1251]	eta 0:01:15 lr 0.000986	time 0.7379 (0.7480)	loss 4.0419 (4.1770)	grad_norm 1.3561 (1.3014)	mem 23874MB
[2022-11-11 05:49:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][1200/1251]	eta 0:00:38 lr 0.000986	time 0.7459 (0.7478)	loss 4.3113 (4.1823)	grad_norm 1.1560 (1.3005)	mem 23874MB
[2022-11-11 05:50:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [22/300][1250/1251]	eta 0:00:00 lr 0.000986	time 0.7237 (0.7475)	loss 4.6675 (4.1804)	grad_norm 1.2520 (1.3013)	mem 23874MB
[2022-11-11 05:50:14 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 22 training takes 0:15:35
[2022-11-11 05:50:15 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_22.pth saving......
[2022-11-11 05:50:16 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_22.pth saved !!!
[2022-11-11 05:50:17 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.596 (1.596)	Loss 1.4055 (1.4055)	Acc@1 67.285 (67.285)	Acc@5 89.453 (89.453)	Mem 23874MB
[2022-11-11 05:50:28 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 66.558 Acc@5 87.884
[2022-11-11 05:50:28 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 66.6%
[2022-11-11 05:50:30 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.883 (1.883)	Loss 2.3067 (2.3067)	Acc@1 50.391 (50.391)	Acc@5 75.488 (75.488)	Mem 23874MB
[2022-11-11 05:50:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 52.308 Acc@5 77.096
[2022-11-11 05:50:40 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 52.3%
[2022-11-11 05:50:40 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 66.56% at 22 epoch
[2022-11-11 05:50:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][0/1251]	eta 0:51:59 lr 0.000986	time 2.4939 (2.4939)	loss 3.2760 (3.2760)	grad_norm 1.2107 (1.2107)	mem 23874MB
[2022-11-11 05:51:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][50/1251]	eta 0:15:39 lr 0.000986	time 0.7428 (0.7821)	loss 4.8199 (4.0772)	grad_norm 1.3281 (1.2747)	mem 23874MB
[2022-11-11 05:51:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][100/1251]	eta 0:14:41 lr 0.000986	time 0.8356 (0.7659)	loss 3.9989 (4.1356)	grad_norm 1.1540 (1.2840)	mem 23874MB
[2022-11-11 05:52:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][150/1251]	eta 0:13:57 lr 0.000986	time 0.7389 (0.7603)	loss 3.3209 (4.0860)	grad_norm 1.2484 (1.2719)	mem 23874MB
[2022-11-11 05:53:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][200/1251]	eta 0:13:16 lr 0.000986	time 0.8167 (0.7574)	loss 4.2820 (4.0973)	grad_norm 1.2966 (1.2729)	mem 23874MB
[2022-11-11 05:53:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][250/1251]	eta 0:12:35 lr 0.000985	time 0.7399 (0.7550)	loss 4.3737 (4.1028)	grad_norm 1.3669 (1.2761)	mem 23874MB
[2022-11-11 05:54:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][300/1251]	eta 0:11:57 lr 0.000985	time 0.8101 (0.7540)	loss 3.8091 (4.1124)	grad_norm 1.3159 (1.2855)	mem 23874MB
[2022-11-11 05:55:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][350/1251]	eta 0:11:18 lr 0.000985	time 0.7381 (0.7532)	loss 3.0914 (4.1208)	grad_norm 1.2250 (1.2996)	mem 23874MB
[2022-11-11 05:55:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][400/1251]	eta 0:10:40 lr 0.000985	time 0.7428 (0.7526)	loss 4.6792 (4.1265)	grad_norm 1.5994 (1.2971)	mem 23874MB
[2022-11-11 05:56:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][450/1251]	eta 0:10:02 lr 0.000985	time 0.7376 (0.7524)	loss 3.4347 (4.1161)	grad_norm 1.2296 (1.2935)	mem 23874MB
[2022-11-11 05:56:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][500/1251]	eta 0:09:24 lr 0.000985	time 0.7413 (0.7518)	loss 4.1430 (4.1171)	grad_norm 1.3158 (1.2923)	mem 23874MB
[2022-11-11 05:57:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][550/1251]	eta 0:08:46 lr 0.000985	time 0.7343 (0.7513)	loss 3.7405 (4.1311)	grad_norm 1.6019 (1.2927)	mem 23874MB
[2022-11-11 05:58:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][600/1251]	eta 0:08:08 lr 0.000985	time 0.8200 (0.7510)	loss 4.9583 (4.1427)	grad_norm 1.4618 (1.2912)	mem 23874MB
[2022-11-11 05:58:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][650/1251]	eta 0:07:31 lr 0.000985	time 0.7391 (0.7505)	loss 4.3497 (4.1411)	grad_norm 1.2404 (1.2898)	mem 23874MB
[2022-11-11 05:59:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][700/1251]	eta 0:06:53 lr 0.000985	time 0.8140 (0.7503)	loss 4.5459 (4.1375)	grad_norm 1.2243 (1.2874)	mem 23874MB
[2022-11-11 06:00:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][750/1251]	eta 0:06:15 lr 0.000985	time 0.7375 (0.7501)	loss 4.1875 (4.1424)	grad_norm 1.3889 (1.2853)	mem 23874MB
[2022-11-11 06:00:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][800/1251]	eta 0:05:38 lr 0.000985	time 0.7359 (0.7499)	loss 4.6076 (4.1414)	grad_norm 1.4079 (1.2817)	mem 23874MB
[2022-11-11 06:01:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][850/1251]	eta 0:05:00 lr 0.000985	time 0.7327 (0.7497)	loss 2.9513 (4.1315)	grad_norm 1.1250 (1.2817)	mem 23874MB
[2022-11-11 06:01:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][900/1251]	eta 0:04:23 lr 0.000985	time 0.7391 (0.7496)	loss 4.2175 (4.1359)	grad_norm 1.3236 (1.2811)	mem 23874MB
[2022-11-11 06:02:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][950/1251]	eta 0:03:45 lr 0.000985	time 0.7379 (0.7496)	loss 4.8102 (4.1417)	grad_norm 1.1873 (1.2819)	mem 23874MB
[2022-11-11 06:03:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][1000/1251]	eta 0:03:08 lr 0.000985	time 0.7386 (0.7494)	loss 2.9607 (4.1343)	grad_norm 1.1726 (1.2826)	mem 23874MB
[2022-11-11 06:03:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][1050/1251]	eta 0:02:30 lr 0.000985	time 0.7361 (0.7494)	loss 4.9746 (4.1366)	grad_norm 1.3510 (1.2825)	mem 23874MB
[2022-11-11 06:04:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][1100/1251]	eta 0:01:53 lr 0.000985	time 0.8044 (0.7492)	loss 5.0676 (4.1376)	grad_norm 1.3671 (1.2818)	mem 23874MB
[2022-11-11 06:05:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][1150/1251]	eta 0:01:15 lr 0.000985	time 0.7384 (0.7492)	loss 3.9640 (4.1320)	grad_norm 1.1548 (1.2819)	mem 23874MB
[2022-11-11 06:05:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][1200/1251]	eta 0:00:38 lr 0.000985	time 0.7446 (0.7490)	loss 3.3434 (4.1369)	grad_norm 1.2865 (nan)	mem 23874MB
[2022-11-11 06:06:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [23/300][1250/1251]	eta 0:00:00 lr 0.000984	time 0.7277 (0.7487)	loss 3.4843 (4.1364)	grad_norm 1.1717 (nan)	mem 23874MB
[2022-11-11 06:06:17 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 23 training takes 0:15:36
[2022-11-11 06:06:17 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_23.pth saving......
[2022-11-11 06:06:19 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_23.pth saved !!!
[2022-11-11 06:06:20 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.618 (1.618)	Loss 1.5268 (1.5268)	Acc@1 65.723 (65.723)	Acc@5 87.012 (87.012)	Mem 23874MB
[2022-11-11 06:06:31 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 67.486 Acc@5 88.574
[2022-11-11 06:06:31 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 67.5%
[2022-11-11 06:06:33 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.914 (1.914)	Loss 1.8698 (1.8698)	Acc@1 60.059 (60.059)	Acc@5 83.496 (83.496)	Mem 23874MB
[2022-11-11 06:06:44 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 55.744 Acc@5 79.774
[2022-11-11 06:06:44 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 55.7%
[2022-11-11 06:06:44 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 67.49% at 23 epoch
[2022-11-11 06:06:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][0/1251]	eta 0:51:22 lr 0.000984	time 2.4637 (2.4637)	loss 4.5304 (4.5304)	grad_norm 1.1315 (1.1315)	mem 23874MB
[2022-11-11 06:07:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][50/1251]	eta 0:15:39 lr 0.000984	time 0.7377 (0.7827)	loss 4.7688 (4.1259)	grad_norm 1.3868 (1.2826)	mem 23874MB
[2022-11-11 06:08:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][100/1251]	eta 0:14:41 lr 0.000984	time 0.7492 (0.7661)	loss 4.3798 (4.1422)	grad_norm 1.2093 (1.2871)	mem 23874MB
[2022-11-11 06:08:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][150/1251]	eta 0:13:57 lr 0.000984	time 0.7338 (0.7603)	loss 4.4719 (4.1677)	grad_norm 1.1780 (1.2759)	mem 23874MB
[2022-11-11 06:09:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][200/1251]	eta 0:13:16 lr 0.000984	time 0.8087 (0.7574)	loss 2.6519 (4.1439)	grad_norm 1.3062 (1.2921)	mem 23874MB
[2022-11-11 06:09:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][250/1251]	eta 0:12:36 lr 0.000984	time 0.7509 (0.7555)	loss 3.5762 (4.1539)	grad_norm 1.4997 (1.2916)	mem 23874MB
[2022-11-11 06:10:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][300/1251]	eta 0:11:57 lr 0.000984	time 0.7346 (0.7542)	loss 4.0428 (4.1453)	grad_norm 1.2274 (1.2885)	mem 23874MB
[2022-11-11 06:11:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][350/1251]	eta 0:11:18 lr 0.000984	time 0.7383 (0.7534)	loss 3.1027 (4.1415)	grad_norm 1.4470 (1.2859)	mem 23874MB
[2022-11-11 06:11:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][400/1251]	eta 0:10:40 lr 0.000984	time 0.7443 (0.7529)	loss 4.1001 (4.1336)	grad_norm 1.1608 (1.2890)	mem 23874MB
[2022-11-11 06:12:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][450/1251]	eta 0:10:02 lr 0.000984	time 0.7371 (0.7521)	loss 4.3382 (4.1338)	grad_norm 1.4665 (1.2839)	mem 23874MB
[2022-11-11 06:13:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][500/1251]	eta 0:09:24 lr 0.000984	time 0.7350 (0.7515)	loss 2.7585 (4.1239)	grad_norm 1.2735 (1.2849)	mem 23874MB
[2022-11-11 06:13:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][550/1251]	eta 0:08:46 lr 0.000984	time 0.7389 (0.7510)	loss 4.3091 (4.1075)	grad_norm 1.2559 (1.2840)	mem 23874MB
[2022-11-11 06:14:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][600/1251]	eta 0:08:08 lr 0.000984	time 0.7451 (0.7508)	loss 3.9776 (4.1098)	grad_norm 1.3151 (1.2849)	mem 23874MB
[2022-11-11 06:14:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][650/1251]	eta 0:07:31 lr 0.000984	time 0.7400 (0.7505)	loss 4.7492 (4.1096)	grad_norm 1.3140 (1.2840)	mem 23874MB
[2022-11-11 06:15:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][700/1251]	eta 0:06:53 lr 0.000984	time 0.7357 (0.7503)	loss 4.7060 (4.1044)	grad_norm 1.1112 (1.2843)	mem 23874MB
[2022-11-11 06:16:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][750/1251]	eta 0:06:15 lr 0.000984	time 0.7383 (0.7499)	loss 4.2228 (4.1121)	grad_norm 1.1474 (1.2833)	mem 23874MB
[2022-11-11 06:16:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][800/1251]	eta 0:05:38 lr 0.000984	time 0.7359 (0.7497)	loss 4.4955 (4.1241)	grad_norm 1.4426 (1.2823)	mem 23874MB
[2022-11-11 06:17:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][850/1251]	eta 0:05:00 lr 0.000984	time 0.7408 (0.7496)	loss 4.0406 (4.1297)	grad_norm 1.3027 (1.2818)	mem 23874MB
[2022-11-11 06:17:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][900/1251]	eta 0:04:23 lr 0.000984	time 0.7443 (0.7495)	loss 2.9780 (4.1277)	grad_norm 1.2473 (1.2806)	mem 23874MB
[2022-11-11 06:18:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][950/1251]	eta 0:03:45 lr 0.000983	time 0.7437 (0.7495)	loss 4.1110 (4.1216)	grad_norm 1.2536 (1.2823)	mem 23874MB
[2022-11-11 06:19:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][1000/1251]	eta 0:03:08 lr 0.000983	time 0.7386 (0.7494)	loss 4.9281 (4.1182)	grad_norm 1.1346 (1.2838)	mem 23874MB
[2022-11-11 06:19:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][1050/1251]	eta 0:02:30 lr 0.000983	time 0.7435 (0.7494)	loss 3.2100 (4.1243)	grad_norm 1.3502 (1.2814)	mem 23874MB
[2022-11-11 06:20:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][1100/1251]	eta 0:01:53 lr 0.000983	time 0.7463 (0.7493)	loss 4.3948 (4.1308)	grad_norm 1.1728 (1.2807)	mem 23874MB
[2022-11-11 06:21:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][1150/1251]	eta 0:01:15 lr 0.000983	time 0.7426 (0.7493)	loss 4.8649 (4.1343)	grad_norm 1.3552 (1.2804)	mem 23874MB
[2022-11-11 06:21:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][1200/1251]	eta 0:00:38 lr 0.000983	time 0.7388 (0.7492)	loss 4.3677 (4.1258)	grad_norm 1.1677 (1.2794)	mem 23874MB
[2022-11-11 06:22:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [24/300][1250/1251]	eta 0:00:00 lr 0.000983	time 0.7262 (0.7490)	loss 3.6998 (4.1271)	grad_norm 1.2491 (1.2795)	mem 23874MB
[2022-11-11 06:22:21 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 24 training takes 0:15:37
[2022-11-11 06:22:21 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_24.pth saving......
[2022-11-11 06:22:22 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_24.pth saved !!!
[2022-11-11 06:22:24 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.690 (1.690)	Loss 1.3920 (1.3920)	Acc@1 69.922 (69.922)	Acc@5 89.258 (89.258)	Mem 23874MB
[2022-11-11 06:22:34 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 68.068 Acc@5 88.670
[2022-11-11 06:22:34 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 68.1%
[2022-11-11 06:22:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.822 (1.822)	Loss 1.9376 (1.9376)	Acc@1 57.812 (57.812)	Acc@5 82.715 (82.715)	Mem 23874MB
[2022-11-11 06:22:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 58.534 Acc@5 81.940
[2022-11-11 06:22:47 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 58.5%
[2022-11-11 06:22:47 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 68.07% at 24 epoch
[2022-11-11 06:22:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][0/1251]	eta 0:51:42 lr 0.000983	time 2.4803 (2.4803)	loss 4.7904 (4.7904)	grad_norm 1.1657 (1.1657)	mem 23874MB
[2022-11-11 06:23:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][50/1251]	eta 0:15:37 lr 0.000983	time 0.7368 (0.7807)	loss 3.3134 (3.9613)	grad_norm 1.3354 (1.3031)	mem 23874MB
[2022-11-11 06:24:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][100/1251]	eta 0:14:40 lr 0.000983	time 0.7390 (0.7651)	loss 4.9711 (4.0926)	grad_norm 1.2903 (1.2993)	mem 23874MB
[2022-11-11 06:24:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][150/1251]	eta 0:13:56 lr 0.000983	time 0.7347 (0.7600)	loss 4.5887 (4.0869)	grad_norm 1.1291 (1.2935)	mem 23874MB
[2022-11-11 06:25:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][200/1251]	eta 0:13:16 lr 0.000983	time 0.7493 (0.7578)	loss 4.2977 (4.1041)	grad_norm 1.2986 (1.2817)	mem 23874MB
[2022-11-11 06:25:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][250/1251]	eta 0:12:36 lr 0.000983	time 0.7445 (0.7559)	loss 4.1313 (4.1094)	grad_norm 1.1607 (1.2870)	mem 23874MB
[2022-11-11 06:26:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][300/1251]	eta 0:11:58 lr 0.000983	time 0.7359 (0.7551)	loss 4.6763 (4.0982)	grad_norm 1.1229 (1.2834)	mem 23874MB
[2022-11-11 06:27:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][350/1251]	eta 0:11:19 lr 0.000983	time 0.7360 (0.7545)	loss 4.4418 (4.0946)	grad_norm 1.2829 (1.2813)	mem 23874MB
[2022-11-11 06:27:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][400/1251]	eta 0:10:41 lr 0.000983	time 0.7437 (0.7538)	loss 4.8680 (4.0990)	grad_norm 1.1959 (1.2814)	mem 23874MB
[2022-11-11 06:28:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][450/1251]	eta 0:10:03 lr 0.000983	time 0.7452 (0.7533)	loss 4.4053 (4.1043)	grad_norm 1.4146 (1.2811)	mem 23874MB
[2022-11-11 06:29:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][500/1251]	eta 0:09:25 lr 0.000983	time 0.7409 (0.7529)	loss 4.4491 (4.1051)	grad_norm 1.2058 (1.2771)	mem 23874MB
[2022-11-11 06:29:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][550/1251]	eta 0:08:47 lr 0.000983	time 0.7427 (0.7524)	loss 3.8715 (4.1079)	grad_norm 1.2254 (1.2731)	mem 23874MB
[2022-11-11 06:30:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][600/1251]	eta 0:08:09 lr 0.000982	time 0.7424 (0.7525)	loss 4.4649 (4.1051)	grad_norm 1.2444 (1.2758)	mem 23874MB
[2022-11-11 06:30:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][650/1251]	eta 0:07:31 lr 0.000982	time 0.7436 (0.7520)	loss 4.2293 (4.1010)	grad_norm 1.1550 (1.2785)	mem 23874MB
[2022-11-11 06:31:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][700/1251]	eta 0:06:54 lr 0.000982	time 0.7384 (0.7518)	loss 3.0392 (4.0971)	grad_norm 1.1590 (1.2806)	mem 23874MB
[2022-11-11 06:32:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][750/1251]	eta 0:06:16 lr 0.000982	time 0.7413 (0.7516)	loss 3.4452 (4.0899)	grad_norm 1.3714 (1.2820)	mem 23874MB
[2022-11-11 06:32:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][800/1251]	eta 0:05:38 lr 0.000982	time 0.7401 (0.7516)	loss 4.4195 (4.0884)	grad_norm 1.1784 (1.2815)	mem 23874MB
[2022-11-11 06:33:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][850/1251]	eta 0:05:01 lr 0.000982	time 0.7537 (0.7515)	loss 3.2185 (4.0930)	grad_norm 1.3398 (1.2827)	mem 23874MB
[2022-11-11 06:34:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][900/1251]	eta 0:04:23 lr 0.000982	time 0.7583 (0.7514)	loss 4.1601 (4.0988)	grad_norm 1.1763 (1.2809)	mem 23874MB
[2022-11-11 06:34:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][950/1251]	eta 0:03:46 lr 0.000982	time 0.7350 (0.7512)	loss 3.8846 (4.1010)	grad_norm 1.1908 (1.2796)	mem 23874MB
[2022-11-11 06:35:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][1000/1251]	eta 0:03:08 lr 0.000982	time 0.8016 (0.7512)	loss 3.6621 (4.0981)	grad_norm 1.2099 (1.2809)	mem 23874MB
[2022-11-11 06:35:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][1050/1251]	eta 0:02:30 lr 0.000982	time 0.7414 (0.7510)	loss 3.0240 (4.0997)	grad_norm 1.2062 (nan)	mem 23874MB
[2022-11-11 06:36:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][1100/1251]	eta 0:01:53 lr 0.000982	time 0.7370 (0.7509)	loss 3.4735 (4.1001)	grad_norm 1.3009 (nan)	mem 23874MB
[2022-11-11 06:37:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][1150/1251]	eta 0:01:15 lr 0.000982	time 0.7420 (0.7509)	loss 3.3732 (4.1003)	grad_norm 1.3458 (nan)	mem 23874MB
[2022-11-11 06:37:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][1200/1251]	eta 0:00:38 lr 0.000982	time 0.7355 (0.7507)	loss 4.7987 (4.1007)	grad_norm 1.2698 (nan)	mem 23874MB
[2022-11-11 06:38:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [25/300][1250/1251]	eta 0:00:00 lr 0.000982	time 0.7342 (0.7505)	loss 4.5505 (4.0964)	grad_norm 1.2244 (nan)	mem 23874MB
[2022-11-11 06:38:26 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 25 training takes 0:15:39
[2022-11-11 06:38:26 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_25.pth saving......
[2022-11-11 06:38:27 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_25.pth saved !!!
[2022-11-11 06:38:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.702 (1.702)	Loss 1.3760 (1.3760)	Acc@1 68.066 (68.066)	Acc@5 88.867 (88.867)	Mem 23874MB
[2022-11-11 06:38:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 68.328 Acc@5 88.978
[2022-11-11 06:38:40 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 68.3%
[2022-11-11 06:38:41 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.843 (1.843)	Loss 1.7199 (1.7199)	Acc@1 60.840 (60.840)	Acc@5 85.449 (85.449)	Mem 23874MB
[2022-11-11 06:38:52 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 60.880 Acc@5 83.752
[2022-11-11 06:38:52 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 60.9%
[2022-11-11 06:38:52 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 68.33% at 25 epoch
[2022-11-11 06:38:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][0/1251]	eta 0:48:37 lr 0.000982	time 2.3322 (2.3322)	loss 3.9044 (3.9044)	grad_norm 1.2523 (1.2523)	mem 23874MB
[2022-11-11 06:39:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][50/1251]	eta 0:15:36 lr 0.000982	time 0.8268 (0.7796)	loss 4.5366 (4.0380)	grad_norm 1.2171 (1.2755)	mem 23874MB
[2022-11-11 06:40:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][100/1251]	eta 0:14:41 lr 0.000982	time 0.7659 (0.7656)	loss 4.8130 (4.0035)	grad_norm 1.2253 (1.2783)	mem 23874MB
[2022-11-11 06:40:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][150/1251]	eta 0:13:55 lr 0.000982	time 0.7424 (0.7588)	loss 2.6782 (4.0144)	grad_norm 1.1967 (1.2780)	mem 23874MB
[2022-11-11 06:41:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][200/1251]	eta 0:13:15 lr 0.000982	time 0.7347 (0.7565)	loss 4.4515 (4.0166)	grad_norm 1.0990 (1.2835)	mem 23874MB
[2022-11-11 06:42:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][250/1251]	eta 0:12:34 lr 0.000981	time 0.7409 (0.7541)	loss 4.3269 (4.0233)	grad_norm 1.2709 (1.2795)	mem 23874MB
[2022-11-11 06:42:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][300/1251]	eta 0:11:55 lr 0.000981	time 0.7290 (0.7527)	loss 5.0216 (4.0465)	grad_norm 1.2162 (1.2869)	mem 23874MB
[2022-11-11 06:43:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][350/1251]	eta 0:11:17 lr 0.000981	time 0.7389 (0.7521)	loss 3.1843 (4.0590)	grad_norm 1.4093 (1.2824)	mem 23874MB
[2022-11-11 06:43:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][400/1251]	eta 0:10:39 lr 0.000981	time 0.7354 (0.7511)	loss 4.4532 (4.0437)	grad_norm 1.3396 (1.2836)	mem 23874MB
[2022-11-11 06:44:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][450/1251]	eta 0:10:01 lr 0.000981	time 0.7440 (0.7508)	loss 3.9787 (4.0415)	grad_norm 1.2481 (1.2829)	mem 23874MB
[2022-11-11 06:45:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][500/1251]	eta 0:09:23 lr 0.000981	time 0.7354 (0.7504)	loss 4.5846 (4.0401)	grad_norm 1.2906 (1.2824)	mem 23874MB
[2022-11-11 06:45:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][550/1251]	eta 0:08:45 lr 0.000981	time 0.7396 (0.7502)	loss 4.2405 (4.0491)	grad_norm 1.2328 (1.2823)	mem 23874MB
[2022-11-11 06:46:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][600/1251]	eta 0:08:08 lr 0.000981	time 0.7429 (0.7499)	loss 4.2960 (4.0565)	grad_norm 1.2793 (nan)	mem 23874MB
[2022-11-11 06:47:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][650/1251]	eta 0:07:30 lr 0.000981	time 0.7412 (0.7498)	loss 4.2824 (4.0455)	grad_norm 1.1561 (nan)	mem 23874MB
[2022-11-11 06:47:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][700/1251]	eta 0:06:53 lr 0.000981	time 0.7421 (0.7496)	loss 4.8228 (4.0438)	grad_norm 1.2865 (nan)	mem 23874MB
[2022-11-11 06:48:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][750/1251]	eta 0:06:15 lr 0.000981	time 0.8352 (0.7495)	loss 3.1959 (4.0472)	grad_norm 1.1582 (nan)	mem 23874MB
[2022-11-11 06:48:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][800/1251]	eta 0:05:37 lr 0.000981	time 0.7361 (0.7493)	loss 4.7983 (4.0534)	grad_norm 1.3765 (nan)	mem 23874MB
[2022-11-11 06:49:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][850/1251]	eta 0:05:00 lr 0.000981	time 0.8229 (0.7492)	loss 4.2751 (4.0544)	grad_norm 1.1041 (nan)	mem 23874MB
[2022-11-11 06:50:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][900/1251]	eta 0:04:22 lr 0.000981	time 0.7456 (0.7492)	loss 4.8372 (4.0504)	grad_norm 1.0887 (nan)	mem 23874MB
[2022-11-11 06:50:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][950/1251]	eta 0:03:45 lr 0.000981	time 0.7397 (0.7491)	loss 4.4976 (4.0517)	grad_norm 1.5156 (nan)	mem 23874MB
[2022-11-11 06:51:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][1000/1251]	eta 0:03:08 lr 0.000981	time 0.7435 (0.7491)	loss 4.3345 (4.0572)	grad_norm 1.3292 (nan)	mem 23874MB
[2022-11-11 06:51:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][1050/1251]	eta 0:02:30 lr 0.000981	time 0.7427 (0.7490)	loss 3.5473 (4.0605)	grad_norm 1.2131 (nan)	mem 23874MB
[2022-11-11 06:52:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][1100/1251]	eta 0:01:53 lr 0.000981	time 0.7355 (0.7489)	loss 4.4172 (4.0654)	grad_norm 1.1711 (nan)	mem 23874MB
[2022-11-11 06:53:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][1150/1251]	eta 0:01:15 lr 0.000980	time 0.7380 (0.7489)	loss 2.8793 (4.0568)	grad_norm 1.3344 (nan)	mem 23874MB
[2022-11-11 06:53:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][1200/1251]	eta 0:00:38 lr 0.000980	time 0.7352 (0.7490)	loss 4.1720 (4.0504)	grad_norm 1.4820 (nan)	mem 23874MB
[2022-11-11 06:54:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [26/300][1250/1251]	eta 0:00:00 lr 0.000980	time 0.7485 (0.7487)	loss 4.4479 (4.0478)	grad_norm 1.2146 (nan)	mem 23874MB
[2022-11-11 06:54:29 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 26 training takes 0:15:36
[2022-11-11 06:54:29 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_26.pth saving......
[2022-11-11 06:54:30 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_26.pth saved !!!
[2022-11-11 06:54:32 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.604 (1.604)	Loss 1.3048 (1.3048)	Acc@1 70.410 (70.410)	Acc@5 89.355 (89.355)	Mem 23874MB
[2022-11-11 06:54:43 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 68.790 Acc@5 89.536
[2022-11-11 06:54:43 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 68.8%
[2022-11-11 06:54:44 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.804 (1.804)	Loss 1.7552 (1.7552)	Acc@1 61.621 (61.621)	Acc@5 85.254 (85.254)	Mem 23874MB
[2022-11-11 06:54:55 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 62.840 Acc@5 85.176
[2022-11-11 06:54:55 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 62.8%
[2022-11-11 06:54:55 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 68.79% at 26 epoch
[2022-11-11 06:54:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][0/1251]	eta 0:50:39 lr 0.000980	time 2.4296 (2.4296)	loss 4.4371 (4.4371)	grad_norm 1.1824 (1.1824)	mem 23874MB
[2022-11-11 06:55:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][50/1251]	eta 0:15:42 lr 0.000980	time 0.7443 (0.7849)	loss 3.6782 (4.0272)	grad_norm 1.2297 (1.2391)	mem 23874MB
[2022-11-11 06:56:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][100/1251]	eta 0:14:42 lr 0.000980	time 0.7414 (0.7670)	loss 4.4785 (4.1087)	grad_norm 1.2282 (1.2367)	mem 23874MB
[2022-11-11 06:56:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][150/1251]	eta 0:13:57 lr 0.000980	time 0.7367 (0.7609)	loss 4.3384 (4.0314)	grad_norm 1.1555 (1.2495)	mem 23874MB
[2022-11-11 06:57:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][200/1251]	eta 0:13:16 lr 0.000980	time 0.7419 (0.7581)	loss 4.3774 (4.0082)	grad_norm 1.2612 (1.2664)	mem 23874MB
[2022-11-11 06:58:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][250/1251]	eta 0:12:36 lr 0.000980	time 0.7494 (0.7561)	loss 3.3123 (3.9995)	grad_norm 1.4614 (1.2625)	mem 23874MB
[2022-11-11 06:58:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][300/1251]	eta 0:11:57 lr 0.000980	time 0.7386 (0.7548)	loss 3.9975 (4.0169)	grad_norm 1.1696 (1.2611)	mem 23874MB
[2022-11-11 06:59:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][350/1251]	eta 0:11:19 lr 0.000980	time 0.7376 (0.7536)	loss 3.0512 (4.0018)	grad_norm 1.1214 (1.2644)	mem 23874MB
[2022-11-11 06:59:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][400/1251]	eta 0:10:40 lr 0.000980	time 0.7392 (0.7527)	loss 3.6263 (4.0011)	grad_norm 1.2586 (1.2665)	mem 23874MB
[2022-11-11 07:00:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][450/1251]	eta 0:10:02 lr 0.000980	time 0.7428 (0.7523)	loss 3.7917 (3.9829)	grad_norm 1.1834 (1.2638)	mem 23874MB
[2022-11-11 07:01:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][500/1251]	eta 0:09:24 lr 0.000980	time 0.8307 (0.7521)	loss 4.0737 (3.9842)	grad_norm 1.2630 (1.2700)	mem 23874MB
[2022-11-11 07:01:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][550/1251]	eta 0:08:46 lr 0.000980	time 0.7388 (0.7513)	loss 4.1626 (3.9973)	grad_norm 1.3106 (1.2691)	mem 23874MB
[2022-11-11 07:02:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][600/1251]	eta 0:08:09 lr 0.000980	time 0.7344 (0.7513)	loss 4.1570 (3.9982)	grad_norm 1.2472 (1.2662)	mem 23874MB
[2022-11-11 07:03:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][650/1251]	eta 0:07:31 lr 0.000980	time 0.7352 (0.7506)	loss 3.7343 (3.9971)	grad_norm 1.2220 (1.2654)	mem 23874MB
[2022-11-11 07:03:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][700/1251]	eta 0:06:53 lr 0.000980	time 0.7327 (0.7506)	loss 3.5974 (3.9960)	grad_norm 1.2092 (1.2652)	mem 23874MB
[2022-11-11 07:04:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][750/1251]	eta 0:06:15 lr 0.000979	time 0.7431 (0.7504)	loss 3.8980 (4.0011)	grad_norm 1.2980 (1.2649)	mem 23874MB
[2022-11-11 07:04:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][800/1251]	eta 0:05:38 lr 0.000979	time 0.7353 (0.7502)	loss 3.7735 (4.0014)	grad_norm 1.4750 (1.2662)	mem 23874MB
[2022-11-11 07:05:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][850/1251]	eta 0:05:00 lr 0.000979	time 0.8228 (0.7502)	loss 4.9720 (3.9938)	grad_norm 1.3164 (1.2668)	mem 23874MB
[2022-11-11 07:06:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][900/1251]	eta 0:04:23 lr 0.000979	time 0.7411 (0.7500)	loss 3.2902 (3.9901)	grad_norm 1.2418 (1.2677)	mem 23874MB
[2022-11-11 07:06:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][950/1251]	eta 0:03:45 lr 0.000979	time 0.7398 (0.7498)	loss 3.8374 (4.0036)	grad_norm 1.3546 (1.2674)	mem 23874MB
[2022-11-11 07:07:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][1000/1251]	eta 0:03:08 lr 0.000979	time 0.7480 (0.7498)	loss 4.7391 (4.0062)	grad_norm 1.2555 (1.2691)	mem 23874MB
[2022-11-11 07:08:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][1050/1251]	eta 0:02:30 lr 0.000979	time 0.7357 (0.7497)	loss 4.3859 (4.0023)	grad_norm 1.0725 (1.2673)	mem 23874MB
[2022-11-11 07:08:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][1100/1251]	eta 0:01:53 lr 0.000979	time 0.7372 (0.7497)	loss 3.4353 (4.0076)	grad_norm 1.2119 (1.2664)	mem 23874MB
[2022-11-11 07:09:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][1150/1251]	eta 0:01:15 lr 0.000979	time 0.7454 (0.7495)	loss 3.7546 (4.0069)	grad_norm 1.2211 (1.2657)	mem 23874MB
[2022-11-11 07:09:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][1200/1251]	eta 0:00:38 lr 0.000979	time 0.7405 (0.7494)	loss 3.3642 (4.0124)	grad_norm 1.1355 (1.2687)	mem 23874MB
[2022-11-11 07:10:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [27/300][1250/1251]	eta 0:00:00 lr 0.000979	time 0.7284 (0.7494)	loss 2.7557 (4.0059)	grad_norm 1.1648 (1.2684)	mem 23874MB
[2022-11-11 07:10:33 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 27 training takes 0:15:37
[2022-11-11 07:10:33 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_27.pth saving......
[2022-11-11 07:10:34 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_27.pth saved !!!
[2022-11-11 07:10:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.738 (1.738)	Loss 1.4017 (1.4017)	Acc@1 69.043 (69.043)	Acc@5 89.453 (89.453)	Mem 23874MB
[2022-11-11 07:10:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 69.310 Acc@5 89.500
[2022-11-11 07:10:47 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 69.3%
[2022-11-11 07:10:48 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.844 (1.844)	Loss 1.6519 (1.6519)	Acc@1 63.965 (63.965)	Acc@5 85.352 (85.352)	Mem 23874MB
[2022-11-11 07:10:59 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 64.548 Acc@5 86.282
[2022-11-11 07:10:59 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 64.5%
[2022-11-11 07:10:59 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 69.31% at 27 epoch
[2022-11-11 07:11:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][0/1251]	eta 0:50:56 lr 0.000979	time 2.4433 (2.4433)	loss 4.3936 (4.3936)	grad_norm 1.1421 (1.1421)	mem 23874MB
[2022-11-11 07:11:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][50/1251]	eta 0:15:41 lr 0.000979	time 0.7390 (0.7836)	loss 2.5912 (3.8898)	grad_norm 1.4166 (1.2765)	mem 23874MB
[2022-11-11 07:12:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][100/1251]	eta 0:14:42 lr 0.000979	time 0.7387 (0.7670)	loss 4.9818 (3.9873)	grad_norm 1.3103 (1.2824)	mem 23874MB
[2022-11-11 07:12:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][150/1251]	eta 0:13:57 lr 0.000979	time 0.7414 (0.7606)	loss 4.0018 (4.0359)	grad_norm 1.2999 (1.2773)	mem 23874MB
[2022-11-11 07:13:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][200/1251]	eta 0:13:16 lr 0.000979	time 0.7491 (0.7582)	loss 3.4896 (4.0608)	grad_norm 1.1217 (1.2684)	mem 23874MB
[2022-11-11 07:14:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][250/1251]	eta 0:12:37 lr 0.000979	time 0.7411 (0.7566)	loss 3.7906 (4.0584)	grad_norm 1.1640 (1.2618)	mem 23874MB
[2022-11-11 07:14:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][300/1251]	eta 0:11:58 lr 0.000979	time 0.7521 (0.7550)	loss 3.2789 (4.0325)	grad_norm 1.2839 (1.2656)	mem 23874MB
[2022-11-11 07:15:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][350/1251]	eta 0:11:19 lr 0.000978	time 0.7402 (0.7543)	loss 2.6860 (4.0072)	grad_norm 1.2135 (1.2614)	mem 23874MB
[2022-11-11 07:16:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][400/1251]	eta 0:10:41 lr 0.000978	time 0.7373 (0.7538)	loss 3.2187 (3.9945)	grad_norm 1.3918 (1.2595)	mem 23874MB
[2022-11-11 07:16:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][450/1251]	eta 0:10:03 lr 0.000978	time 0.7384 (0.7533)	loss 3.8744 (3.9956)	grad_norm 1.1081 (1.2585)	mem 23874MB
[2022-11-11 07:17:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][500/1251]	eta 0:09:25 lr 0.000978	time 0.7440 (0.7528)	loss 3.6824 (3.9978)	grad_norm 1.1940 (1.2615)	mem 23874MB
[2022-11-11 07:17:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][550/1251]	eta 0:08:47 lr 0.000978	time 0.7452 (0.7523)	loss 4.1206 (3.9959)	grad_norm 1.3041 (1.2629)	mem 23874MB
[2022-11-11 07:18:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][600/1251]	eta 0:08:09 lr 0.000978	time 0.7341 (0.7522)	loss 4.3472 (4.0104)	grad_norm 1.1811 (1.2615)	mem 23874MB
[2022-11-11 07:19:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][650/1251]	eta 0:07:31 lr 0.000978	time 0.7398 (0.7520)	loss 3.1371 (3.9962)	grad_norm 1.2966 (1.2629)	mem 23874MB
[2022-11-11 07:19:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][700/1251]	eta 0:06:54 lr 0.000978	time 0.7421 (0.7516)	loss 3.9703 (3.9883)	grad_norm 1.3680 (1.2638)	mem 23874MB
[2022-11-11 07:20:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][750/1251]	eta 0:06:16 lr 0.000978	time 0.7361 (0.7516)	loss 3.7704 (3.9817)	grad_norm 1.6208 (1.2644)	mem 23874MB
[2022-11-11 07:21:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][800/1251]	eta 0:05:38 lr 0.000978	time 0.7375 (0.7515)	loss 4.1936 (3.9862)	grad_norm 1.1641 (1.2620)	mem 23874MB
[2022-11-11 07:21:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][850/1251]	eta 0:05:01 lr 0.000978	time 0.7360 (0.7512)	loss 3.0564 (3.9814)	grad_norm 1.4293 (1.2615)	mem 23874MB
[2022-11-11 07:22:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][900/1251]	eta 0:04:23 lr 0.000978	time 0.7401 (0.7513)	loss 4.2853 (3.9746)	grad_norm 1.1991 (1.2606)	mem 23874MB
[2022-11-11 07:22:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][950/1251]	eta 0:03:46 lr 0.000978	time 0.7391 (0.7510)	loss 3.1885 (3.9725)	grad_norm 1.1241 (1.2604)	mem 23874MB
[2022-11-11 07:23:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][1000/1251]	eta 0:03:08 lr 0.000978	time 0.7435 (0.7511)	loss 3.5896 (3.9682)	grad_norm 1.4451 (1.2587)	mem 23874MB
[2022-11-11 07:24:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][1050/1251]	eta 0:02:30 lr 0.000978	time 0.7437 (0.7510)	loss 4.5166 (3.9691)	grad_norm 1.1493 (1.2582)	mem 23874MB
[2022-11-11 07:24:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][1100/1251]	eta 0:01:53 lr 0.000978	time 0.7386 (0.7509)	loss 4.0288 (3.9635)	grad_norm 1.2433 (1.2579)	mem 23874MB
[2022-11-11 07:25:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][1150/1251]	eta 0:01:15 lr 0.000977	time 0.8302 (0.7509)	loss 3.7834 (3.9657)	grad_norm 1.1560 (1.2571)	mem 23874MB
[2022-11-11 07:26:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][1200/1251]	eta 0:00:38 lr 0.000977	time 0.7406 (0.7508)	loss 4.4077 (3.9643)	grad_norm 1.1391 (1.2565)	mem 23874MB
[2022-11-11 07:26:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [28/300][1250/1251]	eta 0:00:00 lr 0.000977	time 0.7279 (0.7505)	loss 4.0503 (3.9687)	grad_norm 1.3091 (1.2567)	mem 23874MB
[2022-11-11 07:26:38 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 28 training takes 0:15:38
[2022-11-11 07:26:38 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_28.pth saving......
[2022-11-11 07:26:40 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_28.pth saved !!!
[2022-11-11 07:26:41 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.646 (1.646)	Loss 1.2264 (1.2264)	Acc@1 71.582 (71.582)	Acc@5 89.844 (89.844)	Mem 23874MB
[2022-11-11 07:26:52 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 69.768 Acc@5 89.914
[2022-11-11 07:26:52 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 69.8%
[2022-11-11 07:26:54 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.892 (1.892)	Loss 1.4661 (1.4661)	Acc@1 67.871 (67.871)	Acc@5 89.258 (89.258)	Mem 23874MB
[2022-11-11 07:27:05 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 66.020 Acc@5 87.182
[2022-11-11 07:27:05 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 66.0%
[2022-11-11 07:27:05 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 69.77% at 28 epoch
[2022-11-11 07:27:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][0/1251]	eta 0:50:30 lr 0.000977	time 2.4224 (2.4224)	loss 4.2747 (4.2747)	grad_norm 1.1557 (1.1557)	mem 23874MB
[2022-11-11 07:27:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][50/1251]	eta 0:15:32 lr 0.000977	time 0.7461 (0.7766)	loss 2.9677 (3.9201)	grad_norm 1.2840 (1.2237)	mem 23874MB
[2022-11-11 07:28:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][100/1251]	eta 0:14:38 lr 0.000977	time 0.7459 (0.7632)	loss 4.6794 (3.9008)	grad_norm 1.1530 (1.2493)	mem 23874MB
[2022-11-11 07:28:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][150/1251]	eta 0:13:55 lr 0.000977	time 0.7419 (0.7589)	loss 3.6334 (3.8988)	grad_norm 1.2856 (1.2478)	mem 23874MB
[2022-11-11 07:29:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][200/1251]	eta 0:13:14 lr 0.000977	time 0.7287 (0.7558)	loss 2.9915 (3.9092)	grad_norm 1.2384 (1.2453)	mem 23874MB
[2022-11-11 07:30:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][250/1251]	eta 0:12:35 lr 0.000977	time 0.7417 (0.7550)	loss 3.8581 (3.9373)	grad_norm 1.1679 (1.2455)	mem 23874MB
[2022-11-11 07:30:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][300/1251]	eta 0:11:56 lr 0.000977	time 0.7425 (0.7534)	loss 3.9098 (3.9271)	grad_norm 1.3021 (1.2416)	mem 23874MB
[2022-11-11 07:31:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][350/1251]	eta 0:11:18 lr 0.000977	time 0.7352 (0.7525)	loss 4.2759 (3.9293)	grad_norm 1.0917 (1.2393)	mem 23874MB
[2022-11-11 07:32:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][400/1251]	eta 0:10:39 lr 0.000977	time 0.7418 (0.7519)	loss 3.7156 (3.9371)	grad_norm 1.2905 (1.2409)	mem 23874MB
[2022-11-11 07:32:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][450/1251]	eta 0:10:01 lr 0.000977	time 0.7370 (0.7514)	loss 2.8793 (3.9284)	grad_norm 1.1410 (1.2450)	mem 23874MB
[2022-11-11 07:33:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][500/1251]	eta 0:09:24 lr 0.000977	time 0.7427 (0.7512)	loss 3.2286 (3.9373)	grad_norm 1.6380 (1.2472)	mem 23874MB
[2022-11-11 07:33:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][550/1251]	eta 0:08:46 lr 0.000977	time 0.7376 (0.7508)	loss 3.0674 (3.9314)	grad_norm 1.1976 (1.2459)	mem 23874MB
[2022-11-11 07:34:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][600/1251]	eta 0:08:08 lr 0.000977	time 0.7389 (0.7505)	loss 3.8304 (3.9326)	grad_norm 1.2275 (1.2455)	mem 23874MB
[2022-11-11 07:35:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][650/1251]	eta 0:07:31 lr 0.000977	time 0.7393 (0.7504)	loss 5.0271 (3.9392)	grad_norm 1.3285 (1.2460)	mem 23874MB
[2022-11-11 07:35:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][700/1251]	eta 0:06:53 lr 0.000976	time 0.7387 (0.7502)	loss 4.7081 (3.9447)	grad_norm 1.1500 (1.2459)	mem 23874MB
[2022-11-11 07:36:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][750/1251]	eta 0:06:15 lr 0.000976	time 0.7306 (0.7500)	loss 4.0536 (3.9450)	grad_norm 1.5096 (1.2479)	mem 23874MB
[2022-11-11 07:37:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][800/1251]	eta 0:05:38 lr 0.000976	time 0.7386 (0.7500)	loss 4.3504 (3.9465)	grad_norm 1.2137 (1.2509)	mem 23874MB
[2022-11-11 07:37:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][850/1251]	eta 0:05:00 lr 0.000976	time 0.7345 (0.7499)	loss 4.4480 (3.9434)	grad_norm 1.3134 (1.2509)	mem 23874MB
[2022-11-11 07:38:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][900/1251]	eta 0:04:23 lr 0.000976	time 0.7432 (0.7497)	loss 4.3665 (3.9502)	grad_norm 1.3369 (nan)	mem 23874MB
[2022-11-11 07:38:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][950/1251]	eta 0:03:45 lr 0.000976	time 0.7412 (0.7497)	loss 2.9348 (3.9428)	grad_norm 1.1745 (nan)	mem 23874MB
[2022-11-11 07:39:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][1000/1251]	eta 0:03:08 lr 0.000976	time 0.7419 (0.7495)	loss 4.7700 (3.9458)	grad_norm 1.1445 (nan)	mem 23874MB
[2022-11-11 07:40:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][1050/1251]	eta 0:02:30 lr 0.000976	time 0.7409 (0.7495)	loss 4.7094 (3.9525)	grad_norm 1.2996 (nan)	mem 23874MB
[2022-11-11 07:40:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][1100/1251]	eta 0:01:53 lr 0.000976	time 0.8194 (0.7495)	loss 3.3879 (3.9497)	grad_norm 1.1857 (nan)	mem 23874MB
[2022-11-11 07:41:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][1150/1251]	eta 0:01:15 lr 0.000976	time 0.7436 (0.7495)	loss 3.6078 (3.9526)	grad_norm 1.1035 (nan)	mem 23874MB
[2022-11-11 07:42:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][1200/1251]	eta 0:00:38 lr 0.000976	time 0.7428 (0.7493)	loss 4.5436 (3.9508)	grad_norm 1.3333 (nan)	mem 23874MB
[2022-11-11 07:42:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [29/300][1250/1251]	eta 0:00:00 lr 0.000976	time 0.7250 (0.7490)	loss 4.3779 (3.9463)	grad_norm 1.4008 (nan)	mem 23874MB
[2022-11-11 07:42:42 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 29 training takes 0:15:37
[2022-11-11 07:42:42 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_29.pth saving......
[2022-11-11 07:42:43 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_29.pth saved !!!
[2022-11-11 07:42:45 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.618 (1.618)	Loss 1.3876 (1.3876)	Acc@1 69.043 (69.043)	Acc@5 90.527 (90.527)	Mem 23874MB
[2022-11-11 07:42:55 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 69.714 Acc@5 89.894
[2022-11-11 07:42:55 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 69.7%
[2022-11-11 07:42:57 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.858 (1.858)	Loss 1.5581 (1.5581)	Acc@1 68.262 (68.262)	Acc@5 86.719 (86.719)	Mem 23874MB
[2022-11-11 07:43:08 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 67.200 Acc@5 87.912
[2022-11-11 07:43:08 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 67.2%
[2022-11-11 07:43:08 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 69.77% at 28 epoch
[2022-11-11 07:43:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][0/1251]	eta 0:49:24 lr 0.000976	time 2.3695 (2.3695)	loss 4.4808 (4.4808)	grad_norm 1.2223 (1.2223)	mem 23874MB
[2022-11-11 07:43:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][50/1251]	eta 0:15:36 lr 0.000976	time 0.7401 (0.7802)	loss 4.2104 (3.9537)	grad_norm 1.1424 (1.2620)	mem 23874MB
[2022-11-11 07:44:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][100/1251]	eta 0:14:38 lr 0.000976	time 0.7377 (0.7636)	loss 3.1691 (3.9070)	grad_norm 1.2078 (1.2521)	mem 23874MB
[2022-11-11 07:45:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][150/1251]	eta 0:13:54 lr 0.000976	time 0.7375 (0.7575)	loss 3.2975 (3.9465)	grad_norm 1.1205 (1.2444)	mem 23874MB
[2022-11-11 07:45:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][200/1251]	eta 0:13:13 lr 0.000976	time 0.8149 (0.7553)	loss 4.5538 (3.9640)	grad_norm 1.1351 (1.2443)	mem 23874MB
[2022-11-11 07:46:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][250/1251]	eta 0:12:34 lr 0.000975	time 0.7382 (0.7539)	loss 3.4008 (3.9314)	grad_norm 1.1233 (1.2497)	mem 23874MB
[2022-11-11 07:46:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][300/1251]	eta 0:11:56 lr 0.000975	time 0.7381 (0.7531)	loss 3.8495 (3.9199)	grad_norm 1.0728 (1.2540)	mem 23874MB
[2022-11-11 07:47:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][350/1251]	eta 0:11:17 lr 0.000975	time 0.7361 (0.7523)	loss 3.5893 (3.9301)	grad_norm 1.2645 (1.2507)	mem 23874MB
[2022-11-11 07:48:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][400/1251]	eta 0:10:39 lr 0.000975	time 0.7421 (0.7515)	loss 4.0026 (3.9313)	grad_norm 1.4248 (1.2504)	mem 23874MB
[2022-11-11 07:48:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][450/1251]	eta 0:10:01 lr 0.000975	time 0.7392 (0.7512)	loss 2.8141 (3.9214)	grad_norm 1.2827 (1.2506)	mem 23874MB
[2022-11-11 07:49:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][500/1251]	eta 0:09:23 lr 0.000975	time 0.7298 (0.7504)	loss 2.5092 (3.9122)	grad_norm 1.1536 (1.2520)	mem 23874MB
[2022-11-11 07:50:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][550/1251]	eta 0:08:45 lr 0.000975	time 0.7482 (0.7501)	loss 2.7358 (3.9050)	grad_norm 1.6014 (1.2538)	mem 23874MB
[2022-11-11 07:50:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][600/1251]	eta 0:08:08 lr 0.000975	time 0.8155 (0.7498)	loss 2.7361 (3.9150)	grad_norm 1.1344 (1.2517)	mem 23874MB
[2022-11-11 07:51:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][650/1251]	eta 0:07:30 lr 0.000975	time 0.7410 (0.7496)	loss 2.6810 (3.9176)	grad_norm 1.3931 (1.2535)	mem 23874MB
[2022-11-11 07:51:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][700/1251]	eta 0:06:52 lr 0.000975	time 0.7406 (0.7493)	loss 3.8159 (3.9233)	grad_norm 1.2384 (1.2520)	mem 23874MB
[2022-11-11 07:52:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][750/1251]	eta 0:06:15 lr 0.000975	time 0.7375 (0.7491)	loss 3.6224 (3.9336)	grad_norm 1.1843 (1.2522)	mem 23874MB
[2022-11-11 07:53:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][800/1251]	eta 0:05:37 lr 0.000975	time 0.7393 (0.7490)	loss 4.1066 (3.9263)	grad_norm 1.1563 (1.2534)	mem 23874MB
[2022-11-11 07:53:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][850/1251]	eta 0:05:00 lr 0.000975	time 0.7392 (0.7491)	loss 2.8297 (3.9247)	grad_norm 1.2393 (1.2543)	mem 23874MB
[2022-11-11 07:54:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][900/1251]	eta 0:04:22 lr 0.000975	time 0.7396 (0.7487)	loss 3.2037 (3.9241)	grad_norm 1.4399 (1.2529)	mem 23874MB
[2022-11-11 07:55:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][950/1251]	eta 0:03:45 lr 0.000975	time 0.7418 (0.7488)	loss 3.2991 (3.9214)	grad_norm 1.2649 (1.2528)	mem 23874MB
[2022-11-11 07:55:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][1000/1251]	eta 0:03:07 lr 0.000974	time 0.8191 (0.7486)	loss 4.1245 (3.9238)	grad_norm 1.1055 (1.2517)	mem 23874MB
[2022-11-11 07:56:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][1050/1251]	eta 0:02:30 lr 0.000974	time 0.7398 (0.7486)	loss 4.6116 (3.9197)	grad_norm 1.4039 (1.2506)	mem 23874MB
[2022-11-11 07:56:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][1100/1251]	eta 0:01:53 lr 0.000974	time 0.7398 (0.7485)	loss 3.9053 (3.9173)	grad_norm 1.2593 (1.2516)	mem 23874MB
[2022-11-11 07:57:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][1150/1251]	eta 0:01:15 lr 0.000974	time 0.7401 (0.7485)	loss 4.1302 (3.9160)	grad_norm 1.2350 (1.2511)	mem 23874MB
[2022-11-11 07:58:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][1200/1251]	eta 0:00:38 lr 0.000974	time 0.7367 (0.7484)	loss 4.5647 (3.9178)	grad_norm 1.2104 (1.2500)	mem 23874MB
[2022-11-11 07:58:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [30/300][1250/1251]	eta 0:00:00 lr 0.000974	time 0.7250 (0.7484)	loss 3.8558 (3.9170)	grad_norm 1.1417 (1.2483)	mem 23874MB
[2022-11-11 07:58:44 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 30 training takes 0:15:36
[2022-11-11 07:58:45 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_30.pth saving......
[2022-11-11 07:58:46 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_30.pth saved !!!
[2022-11-11 07:58:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.754 (1.754)	Loss 1.3158 (1.3158)	Acc@1 70.020 (70.020)	Acc@5 89.551 (89.551)	Mem 23874MB
[2022-11-11 07:58:58 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 70.310 Acc@5 90.320
[2022-11-11 07:58:58 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 70.3%
[2022-11-11 07:59:00 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.818 (1.818)	Loss 1.4515 (1.4515)	Acc@1 68.262 (68.262)	Acc@5 87.793 (87.793)	Mem 23874MB
[2022-11-11 07:59:11 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 68.230 Acc@5 88.530
[2022-11-11 07:59:11 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 68.2%
[2022-11-11 07:59:11 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 70.31% at 30 epoch
[2022-11-11 07:59:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][0/1251]	eta 0:49:32 lr 0.000974	time 2.3761 (2.3761)	loss 3.3129 (3.3129)	grad_norm 1.2238 (1.2238)	mem 23874MB
[2022-11-11 07:59:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][50/1251]	eta 0:15:40 lr 0.000974	time 0.7387 (0.7828)	loss 4.1118 (3.7677)	grad_norm 1.0830 (1.2734)	mem 23874MB
[2022-11-11 08:00:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][100/1251]	eta 0:14:42 lr 0.000974	time 0.7510 (0.7663)	loss 4.2247 (3.8186)	grad_norm 1.2765 (1.2689)	mem 23874MB
[2022-11-11 08:01:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][150/1251]	eta 0:13:56 lr 0.000974	time 0.7435 (0.7596)	loss 3.3945 (3.8735)	grad_norm 1.1112 (1.2592)	mem 23874MB
[2022-11-11 08:01:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][200/1251]	eta 0:13:15 lr 0.000974	time 0.8039 (0.7573)	loss 3.9386 (3.8966)	grad_norm 1.1962 (1.2503)	mem 23874MB
[2022-11-11 08:02:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][250/1251]	eta 0:12:35 lr 0.000974	time 0.7394 (0.7544)	loss 4.4361 (3.8866)	grad_norm 1.1343 (1.2514)	mem 23874MB
[2022-11-11 08:02:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][300/1251]	eta 0:11:56 lr 0.000974	time 0.7371 (0.7539)	loss 3.6077 (3.9169)	grad_norm 1.1884 (1.2545)	mem 23874MB
[2022-11-11 08:03:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][350/1251]	eta 0:11:17 lr 0.000974	time 0.7349 (0.7522)	loss 3.6243 (3.9245)	grad_norm 1.2590 (1.2514)	mem 23874MB
[2022-11-11 08:04:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][400/1251]	eta 0:10:39 lr 0.000974	time 0.7408 (0.7520)	loss 3.7487 (3.9192)	grad_norm 1.2066 (1.2517)	mem 23874MB
[2022-11-11 08:04:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][450/1251]	eta 0:10:01 lr 0.000974	time 0.7502 (0.7513)	loss 4.3572 (3.9220)	grad_norm 1.3928 (nan)	mem 23874MB
[2022-11-11 08:05:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][500/1251]	eta 0:09:23 lr 0.000973	time 0.7359 (0.7508)	loss 3.5145 (3.9207)	grad_norm 1.2203 (nan)	mem 23874MB
[2022-11-11 08:06:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][550/1251]	eta 0:08:46 lr 0.000973	time 0.7330 (0.7504)	loss 3.1407 (3.9123)	grad_norm 1.2760 (nan)	mem 23874MB
[2022-11-11 08:06:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][600/1251]	eta 0:08:08 lr 0.000973	time 0.8195 (0.7504)	loss 4.1742 (3.9120)	grad_norm 1.2158 (nan)	mem 23874MB
[2022-11-11 08:07:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][650/1251]	eta 0:07:30 lr 0.000973	time 0.7392 (0.7501)	loss 2.8486 (3.9203)	grad_norm 1.1214 (nan)	mem 23874MB
[2022-11-11 08:07:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][700/1251]	eta 0:06:53 lr 0.000973	time 0.7339 (0.7498)	loss 4.5913 (3.9254)	grad_norm 1.2351 (nan)	mem 23874MB
[2022-11-11 08:08:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][750/1251]	eta 0:06:15 lr 0.000973	time 0.7436 (0.7496)	loss 4.6203 (3.9331)	grad_norm 1.1516 (nan)	mem 23874MB
[2022-11-11 08:09:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][800/1251]	eta 0:05:38 lr 0.000973	time 0.7506 (0.7495)	loss 3.8349 (3.9282)	grad_norm 1.2113 (nan)	mem 23874MB
[2022-11-11 08:09:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][850/1251]	eta 0:05:00 lr 0.000973	time 0.7349 (0.7495)	loss 3.0963 (3.9340)	grad_norm 1.1140 (nan)	mem 23874MB
[2022-11-11 08:10:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][900/1251]	eta 0:04:23 lr 0.000973	time 0.7348 (0.7493)	loss 4.2980 (3.9349)	grad_norm 1.1869 (nan)	mem 23874MB
[2022-11-11 08:11:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][950/1251]	eta 0:03:45 lr 0.000973	time 0.7281 (0.7494)	loss 3.5124 (3.9363)	grad_norm 1.1699 (nan)	mem 23874MB
[2022-11-11 08:11:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][1000/1251]	eta 0:03:08 lr 0.000973	time 0.8171 (0.7493)	loss 4.3863 (3.9359)	grad_norm 1.3773 (nan)	mem 23874MB
[2022-11-11 08:12:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][1050/1251]	eta 0:02:30 lr 0.000973	time 0.7369 (0.7493)	loss 4.0201 (3.9343)	grad_norm 1.1609 (nan)	mem 23874MB
[2022-11-11 08:12:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][1100/1251]	eta 0:01:53 lr 0.000973	time 0.7417 (0.7490)	loss 2.4634 (3.9316)	grad_norm 1.1841 (nan)	mem 23874MB
[2022-11-11 08:13:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][1150/1251]	eta 0:01:15 lr 0.000973	time 0.7386 (0.7490)	loss 4.2539 (3.9288)	grad_norm 1.1664 (nan)	mem 23874MB
[2022-11-11 08:14:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][1200/1251]	eta 0:00:38 lr 0.000973	time 0.7347 (0.7491)	loss 4.3078 (3.9304)	grad_norm 1.0796 (nan)	mem 23874MB
[2022-11-11 08:14:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [31/300][1250/1251]	eta 0:00:00 lr 0.000972	time 0.7313 (0.7489)	loss 3.5768 (3.9286)	grad_norm 1.1103 (nan)	mem 23874MB
[2022-11-11 08:14:48 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 31 training takes 0:15:37
[2022-11-11 08:14:48 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_31.pth saving......
[2022-11-11 08:14:49 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_31.pth saved !!!
[2022-11-11 08:14:51 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.551 (1.551)	Loss 1.2964 (1.2964)	Acc@1 70.605 (70.605)	Acc@5 89.160 (89.160)	Mem 23874MB
[2022-11-11 08:15:01 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 70.746 Acc@5 90.490
[2022-11-11 08:15:01 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 70.7%
[2022-11-11 08:15:03 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.825 (1.825)	Loss 1.4024 (1.4024)	Acc@1 69.922 (69.922)	Acc@5 89.355 (89.355)	Mem 23874MB
[2022-11-11 08:15:14 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 69.076 Acc@5 89.168
[2022-11-11 08:15:14 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 69.1%
[2022-11-11 08:15:14 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 70.75% at 31 epoch
[2022-11-11 08:15:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][0/1251]	eta 0:49:55 lr 0.000972	time 2.3943 (2.3943)	loss 3.7312 (3.7312)	grad_norm 1.2771 (1.2771)	mem 23874MB
[2022-11-11 08:15:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][50/1251]	eta 0:15:40 lr 0.000972	time 0.7352 (0.7829)	loss 4.0037 (3.8654)	grad_norm 1.3377 (1.2560)	mem 23874MB
[2022-11-11 08:16:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][100/1251]	eta 0:14:40 lr 0.000972	time 0.7417 (0.7654)	loss 3.7293 (3.8523)	grad_norm 1.2499 (1.2515)	mem 23874MB
[2022-11-11 08:17:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][150/1251]	eta 0:13:55 lr 0.000972	time 0.7358 (0.7591)	loss 4.7411 (3.8823)	grad_norm 1.1971 (1.2404)	mem 23874MB
[2022-11-11 08:17:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][200/1251]	eta 0:13:13 lr 0.000972	time 0.7329 (0.7552)	loss 3.8071 (3.8758)	grad_norm 1.1904 (1.2458)	mem 23874MB
[2022-11-11 08:18:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][250/1251]	eta 0:12:34 lr 0.000972	time 0.7397 (0.7540)	loss 4.0217 (3.8850)	grad_norm 1.1197 (1.2553)	mem 23874MB
[2022-11-11 08:19:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][300/1251]	eta 0:11:55 lr 0.000972	time 0.7361 (0.7525)	loss 3.7794 (3.9011)	grad_norm 1.1887 (1.2506)	mem 23874MB
[2022-11-11 08:19:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][350/1251]	eta 0:11:17 lr 0.000972	time 0.7412 (0.7521)	loss 4.7202 (3.8974)	grad_norm 1.3013 (1.2562)	mem 23874MB
[2022-11-11 08:20:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][400/1251]	eta 0:10:39 lr 0.000972	time 0.7414 (0.7518)	loss 3.0369 (3.8884)	grad_norm 1.2957 (1.2516)	mem 23874MB
[2022-11-11 08:20:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][450/1251]	eta 0:10:01 lr 0.000972	time 0.7352 (0.7512)	loss 4.8425 (3.9094)	grad_norm 1.2151 (1.2495)	mem 23874MB
[2022-11-11 08:21:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][500/1251]	eta 0:09:23 lr 0.000972	time 0.7413 (0.7508)	loss 3.9249 (3.9173)	grad_norm 1.4371 (1.2494)	mem 23874MB
[2022-11-11 08:22:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][550/1251]	eta 0:08:46 lr 0.000972	time 0.8153 (0.7509)	loss 2.6884 (3.9067)	grad_norm 1.1869 (1.2475)	mem 23874MB
[2022-11-11 08:22:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][600/1251]	eta 0:08:08 lr 0.000972	time 0.7383 (0.7503)	loss 2.7193 (3.8997)	grad_norm 1.1323 (1.2489)	mem 23874MB
[2022-11-11 08:23:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][650/1251]	eta 0:07:30 lr 0.000972	time 0.8266 (0.7500)	loss 2.8121 (3.9050)	grad_norm 1.4287 (1.2510)	mem 23874MB
[2022-11-11 08:23:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][700/1251]	eta 0:06:53 lr 0.000972	time 0.7418 (0.7499)	loss 3.8076 (3.9047)	grad_norm 1.1945 (1.2504)	mem 23874MB
[2022-11-11 08:24:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][750/1251]	eta 0:06:15 lr 0.000971	time 0.7364 (0.7496)	loss 4.5581 (3.9001)	grad_norm 1.0981 (1.2510)	mem 23874MB
[2022-11-11 08:25:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][800/1251]	eta 0:05:38 lr 0.000971	time 0.7472 (0.7495)	loss 3.9962 (3.8911)	grad_norm 1.2448 (1.2510)	mem 23874MB
[2022-11-11 08:25:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][850/1251]	eta 0:05:00 lr 0.000971	time 0.7425 (0.7492)	loss 4.1403 (3.8898)	grad_norm 1.1954 (1.2500)	mem 23874MB
[2022-11-11 08:26:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][900/1251]	eta 0:04:22 lr 0.000971	time 0.7402 (0.7491)	loss 4.3874 (3.8857)	grad_norm 1.3191 (1.2495)	mem 23874MB
[2022-11-11 08:27:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][950/1251]	eta 0:03:45 lr 0.000971	time 0.7434 (0.7491)	loss 4.6675 (3.8835)	grad_norm 1.2451 (1.2494)	mem 23874MB
[2022-11-11 08:27:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][1000/1251]	eta 0:03:08 lr 0.000971	time 0.7360 (0.7490)	loss 3.9245 (3.8875)	grad_norm 1.0645 (1.2491)	mem 23874MB
[2022-11-11 08:28:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][1050/1251]	eta 0:02:30 lr 0.000971	time 0.7542 (0.7490)	loss 2.9415 (3.8891)	grad_norm 1.1773 (1.2480)	mem 23874MB
[2022-11-11 08:28:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][1100/1251]	eta 0:01:53 lr 0.000971	time 0.7398 (0.7489)	loss 3.9963 (3.8954)	grad_norm 1.2442 (1.2486)	mem 23874MB
[2022-11-11 08:29:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][1150/1251]	eta 0:01:15 lr 0.000971	time 0.7406 (0.7488)	loss 3.9561 (3.8997)	grad_norm 1.2402 (1.2494)	mem 23874MB
[2022-11-11 08:30:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][1200/1251]	eta 0:00:38 lr 0.000971	time 0.7350 (0.7489)	loss 4.4842 (3.8967)	grad_norm 1.4350 (1.2500)	mem 23874MB
[2022-11-11 08:30:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [32/300][1250/1251]	eta 0:00:00 lr 0.000971	time 0.7291 (0.7486)	loss 4.6982 (3.8960)	grad_norm 1.3741 (1.2495)	mem 23874MB
[2022-11-11 08:30:50 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 32 training takes 0:15:36
[2022-11-11 08:30:51 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_32.pth saving......
[2022-11-11 08:30:52 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_32.pth saved !!!
[2022-11-11 08:30:53 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.729 (1.729)	Loss 1.2827 (1.2827)	Acc@1 68.457 (68.457)	Acc@5 90.820 (90.820)	Mem 23874MB
[2022-11-11 08:31:04 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 70.998 Acc@5 90.738
[2022-11-11 08:31:04 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 71.0%
[2022-11-11 08:31:06 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.808 (1.808)	Loss 1.2381 (1.2381)	Acc@1 73.145 (73.145)	Acc@5 91.211 (91.211)	Mem 23874MB
[2022-11-11 08:31:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 69.930 Acc@5 89.654
[2022-11-11 08:31:17 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 69.9%
[2022-11-11 08:31:17 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 71.00% at 32 epoch
[2022-11-11 08:31:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][0/1251]	eta 0:48:23 lr 0.000971	time 2.3207 (2.3207)	loss 4.3318 (4.3318)	grad_norm 1.1988 (1.1988)	mem 23874MB
[2022-11-11 08:31:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][50/1251]	eta 0:15:34 lr 0.000971	time 0.7384 (0.7780)	loss 3.2662 (3.8495)	grad_norm 1.2193 (1.2005)	mem 23874MB
[2022-11-11 08:32:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][100/1251]	eta 0:14:39 lr 0.000971	time 0.7404 (0.7645)	loss 4.1920 (3.8798)	grad_norm 1.2348 (nan)	mem 23874MB
[2022-11-11 08:33:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][150/1251]	eta 0:13:55 lr 0.000971	time 0.7401 (0.7592)	loss 3.5244 (3.8655)	grad_norm 1.0780 (nan)	mem 23874MB
[2022-11-11 08:33:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][200/1251]	eta 0:13:15 lr 0.000970	time 0.7385 (0.7568)	loss 2.7454 (3.8435)	grad_norm 1.1567 (nan)	mem 23874MB
[2022-11-11 08:34:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][250/1251]	eta 0:12:36 lr 0.000970	time 0.7388 (0.7553)	loss 4.3052 (3.8322)	grad_norm 1.2759 (nan)	mem 23874MB
[2022-11-11 08:35:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][300/1251]	eta 0:11:56 lr 0.000970	time 0.7523 (0.7534)	loss 4.1217 (3.8559)	grad_norm 1.1441 (nan)	mem 23874MB
[2022-11-11 08:35:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][350/1251]	eta 0:11:18 lr 0.000970	time 0.7437 (0.7525)	loss 3.0044 (3.8430)	grad_norm 1.3554 (nan)	mem 23874MB
[2022-11-11 08:36:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][400/1251]	eta 0:10:40 lr 0.000970	time 0.7360 (0.7523)	loss 3.3719 (3.8344)	grad_norm 1.2151 (nan)	mem 23874MB
[2022-11-11 08:36:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][450/1251]	eta 0:10:01 lr 0.000970	time 0.7468 (0.7514)	loss 4.2907 (3.8285)	grad_norm 1.3675 (nan)	mem 23874MB
[2022-11-11 08:37:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][500/1251]	eta 0:09:23 lr 0.000970	time 0.7359 (0.7510)	loss 4.2083 (3.8264)	grad_norm 1.1727 (nan)	mem 23874MB
[2022-11-11 08:38:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][550/1251]	eta 0:08:46 lr 0.000970	time 0.8022 (0.7506)	loss 4.7456 (3.8254)	grad_norm 1.3906 (nan)	mem 23874MB
[2022-11-11 08:38:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][600/1251]	eta 0:08:08 lr 0.000970	time 0.7232 (0.7501)	loss 3.2159 (3.8201)	grad_norm 1.0697 (nan)	mem 23874MB
[2022-11-11 08:39:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][650/1251]	eta 0:07:30 lr 0.000970	time 0.8199 (0.7500)	loss 3.8236 (3.8243)	grad_norm 1.4361 (nan)	mem 23874MB
[2022-11-11 08:40:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][700/1251]	eta 0:06:53 lr 0.000970	time 0.7465 (0.7496)	loss 4.6732 (3.8367)	grad_norm 1.0653 (nan)	mem 23874MB
[2022-11-11 08:40:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][750/1251]	eta 0:06:15 lr 0.000970	time 0.7378 (0.7494)	loss 4.3627 (3.8419)	grad_norm 1.2773 (nan)	mem 23874MB
[2022-11-11 08:41:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][800/1251]	eta 0:05:37 lr 0.000970	time 0.7343 (0.7493)	loss 3.2767 (3.8473)	grad_norm 1.1154 (nan)	mem 23874MB
[2022-11-11 08:41:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][850/1251]	eta 0:05:00 lr 0.000970	time 0.7407 (0.7490)	loss 3.9576 (3.8514)	grad_norm 1.3270 (nan)	mem 23874MB
[2022-11-11 08:42:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][900/1251]	eta 0:04:22 lr 0.000969	time 0.7436 (0.7489)	loss 3.1242 (3.8521)	grad_norm 1.1836 (nan)	mem 23874MB
[2022-11-11 08:43:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][950/1251]	eta 0:03:45 lr 0.000969	time 0.7388 (0.7488)	loss 4.5949 (3.8585)	grad_norm 1.2012 (nan)	mem 23874MB
[2022-11-11 08:43:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][1000/1251]	eta 0:03:07 lr 0.000969	time 0.7340 (0.7487)	loss 4.1238 (3.8561)	grad_norm 1.2059 (nan)	mem 23874MB
[2022-11-11 08:44:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][1050/1251]	eta 0:02:30 lr 0.000969	time 0.7371 (0.7487)	loss 4.1313 (3.8604)	grad_norm 1.3274 (nan)	mem 23874MB
[2022-11-11 08:45:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][1100/1251]	eta 0:01:53 lr 0.000969	time 0.7368 (0.7485)	loss 3.6539 (3.8574)	grad_norm 1.2248 (nan)	mem 23874MB
[2022-11-11 08:45:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][1150/1251]	eta 0:01:15 lr 0.000969	time 0.7478 (0.7484)	loss 2.6299 (3.8600)	grad_norm 1.2723 (nan)	mem 23874MB
[2022-11-11 08:46:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][1200/1251]	eta 0:00:38 lr 0.000969	time 0.7360 (0.7483)	loss 4.4202 (3.8612)	grad_norm 1.1834 (nan)	mem 23874MB
[2022-11-11 08:46:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [33/300][1250/1251]	eta 0:00:00 lr 0.000969	time 0.7246 (0.7481)	loss 3.9387 (3.8648)	grad_norm 1.1891 (nan)	mem 23874MB
[2022-11-11 08:46:53 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 33 training takes 0:15:36
[2022-11-11 08:46:53 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_33.pth saving......
[2022-11-11 08:46:54 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_33.pth saved !!!
[2022-11-11 08:46:56 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.599 (1.599)	Loss 1.1232 (1.1232)	Acc@1 73.535 (73.535)	Acc@5 92.090 (92.090)	Mem 23874MB
[2022-11-11 08:47:06 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.416 Acc@5 90.828
[2022-11-11 08:47:06 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 71.4%
[2022-11-11 08:47:08 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.961 (1.961)	Loss 1.3035 (1.3035)	Acc@1 72.754 (72.754)	Acc@5 89.941 (89.941)	Mem 23874MB
[2022-11-11 08:47:19 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 70.586 Acc@5 90.126
[2022-11-11 08:47:19 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 70.6%
[2022-11-11 08:47:19 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 71.42% at 33 epoch
[2022-11-11 08:47:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][0/1251]	eta 0:51:40 lr 0.000969	time 2.4784 (2.4784)	loss 4.0698 (4.0698)	grad_norm 1.2541 (1.2541)	mem 23874MB
[2022-11-11 08:47:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][50/1251]	eta 0:15:41 lr 0.000969	time 0.7434 (0.7843)	loss 3.9769 (3.7842)	grad_norm 1.1088 (1.2344)	mem 23874MB
[2022-11-11 08:48:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][100/1251]	eta 0:14:43 lr 0.000969	time 0.8226 (0.7674)	loss 4.2861 (3.7845)	grad_norm 1.1586 (1.2550)	mem 23874MB
[2022-11-11 08:49:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][150/1251]	eta 0:13:58 lr 0.000969	time 0.7390 (0.7613)	loss 4.9149 (3.8005)	grad_norm 1.3556 (1.2470)	mem 23874MB
[2022-11-11 08:49:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][200/1251]	eta 0:13:16 lr 0.000969	time 0.7387 (0.7579)	loss 4.1314 (3.8078)	grad_norm 1.2674 (1.2404)	mem 23874MB
[2022-11-11 08:50:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][250/1251]	eta 0:12:36 lr 0.000969	time 0.7426 (0.7558)	loss 4.3424 (3.8114)	grad_norm 1.2420 (1.2456)	mem 23874MB
[2022-11-11 08:51:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][300/1251]	eta 0:11:57 lr 0.000969	time 0.7353 (0.7540)	loss 4.0470 (3.8244)	grad_norm 1.1959 (1.2400)	mem 23874MB
[2022-11-11 08:51:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][350/1251]	eta 0:11:18 lr 0.000968	time 0.7389 (0.7535)	loss 4.0494 (3.8350)	grad_norm 1.3902 (1.2411)	mem 23874MB
[2022-11-11 08:52:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][400/1251]	eta 0:10:40 lr 0.000968	time 0.7418 (0.7524)	loss 3.4276 (3.8252)	grad_norm 1.3443 (1.2448)	mem 23874MB
[2022-11-11 08:52:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][450/1251]	eta 0:10:02 lr 0.000968	time 0.7347 (0.7521)	loss 3.3107 (3.8188)	grad_norm 1.2087 (1.2438)	mem 23874MB
[2022-11-11 08:53:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][500/1251]	eta 0:09:24 lr 0.000968	time 0.7362 (0.7515)	loss 4.0674 (3.8221)	grad_norm 1.2842 (1.2417)	mem 23874MB
[2022-11-11 08:54:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][550/1251]	eta 0:08:46 lr 0.000968	time 0.7363 (0.7512)	loss 4.3795 (3.8334)	grad_norm 1.1290 (1.2451)	mem 23874MB
[2022-11-11 08:54:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][600/1251]	eta 0:08:08 lr 0.000968	time 0.7371 (0.7509)	loss 3.2672 (3.8320)	grad_norm 1.3196 (1.2481)	mem 23874MB
[2022-11-11 08:55:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][650/1251]	eta 0:07:31 lr 0.000968	time 0.7392 (0.7506)	loss 4.0500 (3.8389)	grad_norm 1.1878 (1.2487)	mem 23874MB
[2022-11-11 08:56:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][700/1251]	eta 0:06:53 lr 0.000968	time 0.7447 (0.7503)	loss 4.1349 (3.8463)	grad_norm 1.3883 (1.2497)	mem 23874MB
[2022-11-11 08:56:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][750/1251]	eta 0:06:15 lr 0.000968	time 0.7401 (0.7503)	loss 3.6611 (3.8545)	grad_norm 1.2388 (1.2503)	mem 23874MB
[2022-11-11 08:57:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][800/1251]	eta 0:05:38 lr 0.000968	time 0.7344 (0.7500)	loss 3.7823 (3.8540)	grad_norm 1.1827 (1.2490)	mem 23874MB
[2022-11-11 08:57:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][850/1251]	eta 0:05:00 lr 0.000968	time 0.7445 (0.7498)	loss 4.4380 (3.8555)	grad_norm 1.2660 (1.2494)	mem 23874MB
[2022-11-11 08:58:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][900/1251]	eta 0:04:23 lr 0.000968	time 0.7371 (0.7496)	loss 4.1683 (3.8582)	grad_norm 1.2220 (1.2498)	mem 23874MB
[2022-11-11 08:59:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][950/1251]	eta 0:03:45 lr 0.000968	time 0.7418 (0.7496)	loss 4.0233 (3.8572)	grad_norm 1.2305 (1.2494)	mem 23874MB
[2022-11-11 08:59:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][1000/1251]	eta 0:03:08 lr 0.000967	time 0.7335 (0.7493)	loss 4.3403 (3.8515)	grad_norm 1.2357 (1.2490)	mem 23874MB
[2022-11-11 09:00:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][1050/1251]	eta 0:02:30 lr 0.000967	time 0.7356 (0.7493)	loss 2.6660 (3.8461)	grad_norm 1.3063 (1.2495)	mem 23874MB
[2022-11-11 09:01:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][1100/1251]	eta 0:01:53 lr 0.000967	time 0.7369 (0.7490)	loss 3.5379 (3.8457)	grad_norm 1.2246 (1.2488)	mem 23874MB
[2022-11-11 09:01:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][1150/1251]	eta 0:01:15 lr 0.000967	time 0.7406 (0.7492)	loss 3.7697 (3.8479)	grad_norm 1.3095 (1.2481)	mem 23874MB
[2022-11-11 09:02:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][1200/1251]	eta 0:00:38 lr 0.000967	time 0.8365 (0.7491)	loss 4.5125 (3.8475)	grad_norm 1.2884 (1.2488)	mem 23874MB
[2022-11-11 09:02:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [34/300][1250/1251]	eta 0:00:00 lr 0.000967	time 0.7261 (0.7490)	loss 4.3214 (3.8473)	grad_norm 1.4089 (1.2490)	mem 23874MB
[2022-11-11 09:02:56 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 34 training takes 0:15:37
[2022-11-11 09:02:56 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_34.pth saving......
[2022-11-11 09:02:58 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_34.pth saved !!!
[2022-11-11 09:02:59 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.735 (1.735)	Loss 1.1659 (1.1659)	Acc@1 73.145 (73.145)	Acc@5 90.332 (90.332)	Mem 23874MB
[2022-11-11 09:03:10 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.486 Acc@5 90.818
[2022-11-11 09:03:10 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 71.5%
[2022-11-11 09:03:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.868 (1.868)	Loss 1.3315 (1.3315)	Acc@1 69.922 (69.922)	Acc@5 89.746 (89.746)	Mem 23874MB
[2022-11-11 09:03:23 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.162 Acc@5 90.464
[2022-11-11 09:03:23 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 71.2%
[2022-11-11 09:03:23 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 71.49% at 34 epoch
[2022-11-11 09:03:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][0/1251]	eta 0:49:14 lr 0.000967	time 2.3620 (2.3620)	loss 4.3310 (4.3310)	grad_norm 1.1478 (1.1478)	mem 23874MB
[2022-11-11 09:04:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][50/1251]	eta 0:15:38 lr 0.000967	time 0.7395 (0.7811)	loss 2.8270 (3.8080)	grad_norm 1.4395 (nan)	mem 23874MB
[2022-11-11 09:04:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][100/1251]	eta 0:14:39 lr 0.000967	time 0.7343 (0.7640)	loss 2.8448 (3.8138)	grad_norm 1.2751 (nan)	mem 23874MB
[2022-11-11 09:05:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][150/1251]	eta 0:13:55 lr 0.000967	time 0.7350 (0.7588)	loss 4.1097 (3.8343)	grad_norm 1.4814 (nan)	mem 23874MB
[2022-11-11 09:05:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][200/1251]	eta 0:13:14 lr 0.000967	time 0.8369 (0.7560)	loss 2.6751 (3.8241)	grad_norm 1.4650 (nan)	mem 23874MB
[2022-11-11 09:06:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][250/1251]	eta 0:12:34 lr 0.000967	time 0.7268 (0.7539)	loss 3.9467 (3.8310)	grad_norm 1.3655 (nan)	mem 23874MB
[2022-11-11 09:07:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][300/1251]	eta 0:11:55 lr 0.000967	time 0.7332 (0.7527)	loss 4.4261 (3.8420)	grad_norm 1.2254 (nan)	mem 23874MB
[2022-11-11 09:07:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][350/1251]	eta 0:11:17 lr 0.000967	time 0.7343 (0.7517)	loss 2.8348 (3.8518)	grad_norm 1.2689 (nan)	mem 23874MB
[2022-11-11 09:08:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][400/1251]	eta 0:10:39 lr 0.000967	time 0.7409 (0.7510)	loss 4.2492 (3.8630)	grad_norm 1.2040 (nan)	mem 23874MB
[2022-11-11 09:09:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][450/1251]	eta 0:10:01 lr 0.000966	time 0.7398 (0.7508)	loss 3.5602 (3.8669)	grad_norm 1.2666 (nan)	mem 23874MB
[2022-11-11 09:09:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][500/1251]	eta 0:09:23 lr 0.000966	time 0.7495 (0.7501)	loss 3.9075 (3.8637)	grad_norm 1.2125 (nan)	mem 23874MB
[2022-11-11 09:10:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][550/1251]	eta 0:08:45 lr 0.000966	time 0.7443 (0.7499)	loss 3.1737 (3.8583)	grad_norm 1.1031 (nan)	mem 23874MB
[2022-11-11 09:10:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][600/1251]	eta 0:08:08 lr 0.000966	time 0.7406 (0.7498)	loss 3.6378 (3.8661)	grad_norm 1.3260 (nan)	mem 23874MB
[2022-11-11 09:11:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][650/1251]	eta 0:07:30 lr 0.000966	time 0.7395 (0.7495)	loss 3.8428 (3.8571)	grad_norm 1.2928 (nan)	mem 23874MB
[2022-11-11 09:12:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][700/1251]	eta 0:06:52 lr 0.000966	time 0.7424 (0.7495)	loss 4.2727 (3.8495)	grad_norm 1.2178 (nan)	mem 23874MB
[2022-11-11 09:12:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][750/1251]	eta 0:06:15 lr 0.000966	time 0.8140 (0.7492)	loss 2.7225 (3.8405)	grad_norm 1.3605 (nan)	mem 23874MB
[2022-11-11 09:13:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][800/1251]	eta 0:05:37 lr 0.000966	time 0.7370 (0.7489)	loss 4.2946 (3.8442)	grad_norm 1.1915 (nan)	mem 23874MB
[2022-11-11 09:14:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][850/1251]	eta 0:05:00 lr 0.000966	time 0.7335 (0.7488)	loss 4.3186 (3.8445)	grad_norm 1.2304 (nan)	mem 23874MB
[2022-11-11 09:14:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][900/1251]	eta 0:04:22 lr 0.000966	time 0.7349 (0.7487)	loss 3.3391 (3.8480)	grad_norm 1.2264 (nan)	mem 23874MB
[2022-11-11 09:15:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][950/1251]	eta 0:03:45 lr 0.000966	time 0.7332 (0.7487)	loss 4.2343 (3.8524)	grad_norm 1.1559 (nan)	mem 23874MB
[2022-11-11 09:15:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][1000/1251]	eta 0:03:07 lr 0.000966	time 0.7412 (0.7487)	loss 3.6543 (3.8516)	grad_norm 1.2910 (nan)	mem 23874MB
[2022-11-11 09:16:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][1050/1251]	eta 0:02:30 lr 0.000966	time 0.7345 (0.7484)	loss 3.6189 (3.8495)	grad_norm 1.4004 (nan)	mem 23874MB
[2022-11-11 09:17:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][1100/1251]	eta 0:01:53 lr 0.000965	time 0.8162 (0.7485)	loss 4.8999 (3.8428)	grad_norm 1.2768 (nan)	mem 23874MB
[2022-11-11 09:17:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][1150/1251]	eta 0:01:15 lr 0.000965	time 0.7396 (0.7484)	loss 4.1615 (3.8407)	grad_norm 1.1222 (nan)	mem 23874MB
[2022-11-11 09:18:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][1200/1251]	eta 0:00:38 lr 0.000965	time 0.7420 (0.7484)	loss 2.9013 (3.8420)	grad_norm 1.1468 (nan)	mem 23874MB
[2022-11-11 09:18:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [35/300][1250/1251]	eta 0:00:00 lr 0.000965	time 0.7298 (0.7482)	loss 3.8176 (3.8461)	grad_norm 1.1630 (nan)	mem 23874MB
[2022-11-11 09:18:59 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 35 training takes 0:15:36
[2022-11-11 09:18:59 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_35.pth saving......
[2022-11-11 09:19:00 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_35.pth saved !!!
[2022-11-11 09:19:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.582 (1.582)	Loss 1.1826 (1.1826)	Acc@1 73.145 (73.145)	Acc@5 90.820 (90.820)	Mem 23874MB
[2022-11-11 09:19:12 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.664 Acc@5 90.968
[2022-11-11 09:19:12 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 71.7%
[2022-11-11 09:19:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.789 (1.789)	Loss 1.3195 (1.3195)	Acc@1 70.605 (70.605)	Acc@5 88.770 (88.770)	Mem 23874MB
[2022-11-11 09:19:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.758 Acc@5 90.784
[2022-11-11 09:19:25 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 71.8%
[2022-11-11 09:19:25 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 71.76% at 35 epoch
[2022-11-11 09:19:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][0/1251]	eta 0:51:08 lr 0.000965	time 2.4529 (2.4529)	loss 3.9409 (3.9409)	grad_norm 1.1323 (1.1323)	mem 23874MB
[2022-11-11 09:20:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][50/1251]	eta 0:15:41 lr 0.000965	time 0.7354 (0.7836)	loss 4.0577 (3.9365)	grad_norm 1.2196 (1.2463)	mem 23874MB
[2022-11-11 09:20:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][100/1251]	eta 0:14:38 lr 0.000965	time 0.7392 (0.7636)	loss 4.0366 (3.9260)	grad_norm 1.1129 (1.2321)	mem 23874MB
[2022-11-11 09:21:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][150/1251]	eta 0:13:54 lr 0.000965	time 0.7419 (0.7583)	loss 3.5293 (3.8976)	grad_norm 1.0704 (1.2354)	mem 23874MB
[2022-11-11 09:21:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][200/1251]	eta 0:13:14 lr 0.000965	time 0.8219 (0.7555)	loss 4.5877 (3.8691)	grad_norm 1.2509 (1.2366)	mem 23874MB
[2022-11-11 09:22:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][250/1251]	eta 0:12:34 lr 0.000965	time 0.7277 (0.7541)	loss 3.8130 (3.8612)	grad_norm 1.2378 (1.2379)	mem 23874MB
[2022-11-11 09:23:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][300/1251]	eta 0:11:55 lr 0.000965	time 0.7407 (0.7526)	loss 4.0627 (3.8596)	grad_norm 1.2238 (1.2376)	mem 23874MB
[2022-11-11 09:23:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][350/1251]	eta 0:11:17 lr 0.000965	time 0.7415 (0.7518)	loss 3.8538 (3.8445)	grad_norm 1.2692 (1.2356)	mem 23874MB
[2022-11-11 09:24:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][400/1251]	eta 0:10:39 lr 0.000965	time 0.7369 (0.7510)	loss 4.1829 (3.8475)	grad_norm 1.1915 (1.2375)	mem 23874MB
[2022-11-11 09:25:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][450/1251]	eta 0:10:01 lr 0.000965	time 0.7404 (0.7505)	loss 4.4774 (3.8514)	grad_norm 1.1968 (1.2410)	mem 23874MB
[2022-11-11 09:25:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][500/1251]	eta 0:09:23 lr 0.000964	time 0.7264 (0.7502)	loss 3.5354 (3.8313)	grad_norm 1.3981 (1.2424)	mem 23874MB
[2022-11-11 09:26:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][550/1251]	eta 0:08:45 lr 0.000964	time 0.7424 (0.7495)	loss 4.1398 (3.8280)	grad_norm 1.3486 (1.2470)	mem 23874MB
[2022-11-11 09:26:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][600/1251]	eta 0:08:07 lr 0.000964	time 0.7395 (0.7495)	loss 4.1983 (3.8260)	grad_norm 1.3258 (1.2453)	mem 23874MB
[2022-11-11 09:27:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][650/1251]	eta 0:07:30 lr 0.000964	time 0.7367 (0.7492)	loss 3.8481 (3.8277)	grad_norm 1.2055 (1.2451)	mem 23874MB
[2022-11-11 09:28:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][700/1251]	eta 0:06:52 lr 0.000964	time 0.7501 (0.7491)	loss 3.8595 (3.8280)	grad_norm 1.1978 (1.2436)	mem 23874MB
[2022-11-11 09:28:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][750/1251]	eta 0:06:15 lr 0.000964	time 0.7365 (0.7488)	loss 4.1338 (3.8274)	grad_norm 1.1856 (1.2433)	mem 23874MB
[2022-11-11 09:29:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][800/1251]	eta 0:05:37 lr 0.000964	time 0.7418 (0.7488)	loss 3.3445 (3.8245)	grad_norm 1.2687 (1.2409)	mem 23874MB
[2022-11-11 09:30:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][850/1251]	eta 0:05:00 lr 0.000964	time 0.7437 (0.7486)	loss 3.9908 (3.8115)	grad_norm 1.2834 (1.2416)	mem 23874MB
[2022-11-11 09:30:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][900/1251]	eta 0:04:22 lr 0.000964	time 0.7389 (0.7486)	loss 3.3029 (3.8119)	grad_norm 1.1488 (nan)	mem 23874MB
[2022-11-11 09:31:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][950/1251]	eta 0:03:45 lr 0.000964	time 0.7351 (0.7484)	loss 4.0418 (3.8195)	grad_norm 1.2137 (nan)	mem 23874MB
[2022-11-11 09:31:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][1000/1251]	eta 0:03:07 lr 0.000964	time 0.7398 (0.7484)	loss 3.1092 (3.8213)	grad_norm 1.2650 (nan)	mem 23874MB
[2022-11-11 09:32:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][1050/1251]	eta 0:02:30 lr 0.000964	time 0.7445 (0.7482)	loss 3.5439 (3.8200)	grad_norm 1.2987 (nan)	mem 23874MB
[2022-11-11 09:33:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][1100/1251]	eta 0:01:52 lr 0.000964	time 0.7362 (0.7482)	loss 3.6938 (3.8212)	grad_norm 1.2119 (nan)	mem 23874MB
[2022-11-11 09:33:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][1150/1251]	eta 0:01:15 lr 0.000963	time 0.7382 (0.7482)	loss 4.5753 (3.8247)	grad_norm 1.2121 (nan)	mem 23874MB
[2022-11-11 09:34:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][1200/1251]	eta 0:00:38 lr 0.000963	time 0.7377 (0.7482)	loss 3.9998 (3.8272)	grad_norm 1.3275 (nan)	mem 23874MB
[2022-11-11 09:35:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [36/300][1250/1251]	eta 0:00:00 lr 0.000963	time 0.7289 (0.7479)	loss 3.9691 (3.8267)	grad_norm 1.2677 (nan)	mem 23874MB
[2022-11-11 09:35:01 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 36 training takes 0:15:35
[2022-11-11 09:35:01 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_36.pth saving......
[2022-11-11 09:35:02 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_36.pth saved !!!
[2022-11-11 09:35:04 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.805 (1.805)	Loss 1.2079 (1.2079)	Acc@1 71.094 (71.094)	Acc@5 92.090 (92.090)	Mem 23874MB
[2022-11-11 09:35:14 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.054 Acc@5 91.310
[2022-11-11 09:35:14 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.1%
[2022-11-11 09:35:16 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.804 (1.804)	Loss 1.1850 (1.1850)	Acc@1 71.582 (71.582)	Acc@5 91.895 (91.895)	Mem 23874MB
[2022-11-11 09:35:27 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.214 Acc@5 91.068
[2022-11-11 09:35:27 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 72.2%
[2022-11-11 09:35:27 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 72.21% at 36 epoch
[2022-11-11 09:35:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][0/1251]	eta 0:50:40 lr 0.000963	time 2.4304 (2.4304)	loss 3.8081 (3.8081)	grad_norm 1.1222 (1.1222)	mem 23874MB
[2022-11-11 09:36:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][50/1251]	eta 0:15:35 lr 0.000963	time 0.7350 (0.7785)	loss 3.9631 (3.8729)	grad_norm 1.2518 (1.2376)	mem 23874MB
[2022-11-11 09:36:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][100/1251]	eta 0:14:38 lr 0.000963	time 0.7368 (0.7632)	loss 4.4035 (3.8622)	grad_norm 1.3344 (1.2546)	mem 23874MB
[2022-11-11 09:37:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][150/1251]	eta 0:13:56 lr 0.000963	time 0.7343 (0.7595)	loss 3.8319 (3.8500)	grad_norm 1.4822 (1.2564)	mem 23874MB
[2022-11-11 09:37:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][200/1251]	eta 0:13:15 lr 0.000963	time 0.8097 (0.7567)	loss 3.9759 (3.8309)	grad_norm 1.3543 (1.2465)	mem 23874MB
[2022-11-11 09:38:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][250/1251]	eta 0:12:35 lr 0.000963	time 0.7393 (0.7545)	loss 2.8074 (3.8130)	grad_norm 1.1336 (1.2438)	mem 23874MB
[2022-11-11 09:39:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][300/1251]	eta 0:11:56 lr 0.000963	time 0.7362 (0.7530)	loss 2.9976 (3.8117)	grad_norm 1.3677 (1.2418)	mem 23874MB
[2022-11-11 09:39:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][350/1251]	eta 0:11:18 lr 0.000963	time 0.7417 (0.7527)	loss 4.5040 (3.8155)	grad_norm 1.1549 (1.2405)	mem 23874MB
[2022-11-11 09:40:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][400/1251]	eta 0:10:40 lr 0.000963	time 0.7343 (0.7521)	loss 3.7201 (3.8089)	grad_norm 1.3568 (1.2448)	mem 23874MB
[2022-11-11 09:41:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][450/1251]	eta 0:10:01 lr 0.000963	time 0.7399 (0.7515)	loss 3.3754 (3.7940)	grad_norm 1.0732 (1.2422)	mem 23874MB
[2022-11-11 09:41:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][500/1251]	eta 0:09:24 lr 0.000963	time 0.8384 (0.7514)	loss 3.1303 (3.7998)	grad_norm 1.3050 (1.2410)	mem 23874MB
[2022-11-11 09:42:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][550/1251]	eta 0:08:46 lr 0.000962	time 0.7438 (0.7507)	loss 4.3020 (3.8124)	grad_norm 1.1656 (1.2395)	mem 23874MB
[2022-11-11 09:42:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][600/1251]	eta 0:08:08 lr 0.000962	time 0.7435 (0.7506)	loss 4.5124 (3.8105)	grad_norm 1.3250 (1.2386)	mem 23874MB
[2022-11-11 09:43:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][650/1251]	eta 0:07:30 lr 0.000962	time 0.7416 (0.7502)	loss 4.1449 (3.8086)	grad_norm 1.3229 (1.2412)	mem 23874MB
[2022-11-11 09:44:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][700/1251]	eta 0:06:53 lr 0.000962	time 0.7374 (0.7503)	loss 2.6622 (3.8144)	grad_norm 1.1320 (1.2442)	mem 23874MB
[2022-11-11 09:44:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][750/1251]	eta 0:06:15 lr 0.000962	time 0.7396 (0.7500)	loss 3.6532 (3.8236)	grad_norm 1.2330 (1.2433)	mem 23874MB
[2022-11-11 09:45:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][800/1251]	eta 0:05:38 lr 0.000962	time 0.7427 (0.7500)	loss 3.5058 (3.8207)	grad_norm 1.2254 (1.2416)	mem 23874MB
[2022-11-11 09:46:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][850/1251]	eta 0:05:00 lr 0.000962	time 0.7383 (0.7497)	loss 3.8658 (3.8158)	grad_norm 1.1313 (1.2417)	mem 23874MB
[2022-11-11 09:46:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][900/1251]	eta 0:04:23 lr 0.000962	time 0.7442 (0.7497)	loss 3.7003 (3.8175)	grad_norm 1.1964 (1.2430)	mem 23874MB
[2022-11-11 09:47:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][950/1251]	eta 0:03:45 lr 0.000962	time 0.7391 (0.7495)	loss 4.3016 (3.8167)	grad_norm 1.2649 (1.2424)	mem 23874MB
[2022-11-11 09:47:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][1000/1251]	eta 0:03:08 lr 0.000962	time 0.7433 (0.7493)	loss 4.2973 (3.8185)	grad_norm 1.3167 (1.2426)	mem 23874MB
[2022-11-11 09:48:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][1050/1251]	eta 0:02:30 lr 0.000962	time 0.8026 (0.7492)	loss 4.3301 (3.8157)	grad_norm 1.4185 (1.2432)	mem 23874MB
[2022-11-11 09:49:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][1100/1251]	eta 0:01:53 lr 0.000962	time 0.7400 (0.7489)	loss 4.2890 (3.8189)	grad_norm 1.2839 (1.2451)	mem 23874MB
[2022-11-11 09:49:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][1150/1251]	eta 0:01:15 lr 0.000961	time 0.7355 (0.7489)	loss 4.4712 (3.8237)	grad_norm 1.1184 (1.2441)	mem 23874MB
[2022-11-11 09:50:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][1200/1251]	eta 0:00:38 lr 0.000961	time 0.7352 (0.7489)	loss 4.5155 (3.8258)	grad_norm 1.1812 (1.2435)	mem 23874MB
[2022-11-11 09:51:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [37/300][1250/1251]	eta 0:00:00 lr 0.000961	time 0.7259 (0.7486)	loss 3.8434 (3.8247)	grad_norm 1.2781 (1.2441)	mem 23874MB
[2022-11-11 09:51:04 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 37 training takes 0:15:36
[2022-11-11 09:51:04 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_37.pth saving......
[2022-11-11 09:51:05 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_37.pth saved !!!
[2022-11-11 09:51:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.575 (1.575)	Loss 1.2562 (1.2562)	Acc@1 73.242 (73.242)	Acc@5 90.820 (90.820)	Mem 23874MB
[2022-11-11 09:51:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.156 Acc@5 91.280
[2022-11-11 09:51:17 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.2%
[2022-11-11 09:51:19 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.790 (1.790)	Loss 1.1435 (1.1435)	Acc@1 74.219 (74.219)	Acc@5 90.918 (90.918)	Mem 23874MB
[2022-11-11 09:51:30 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.644 Acc@5 91.306
[2022-11-11 09:51:30 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 72.6%
[2022-11-11 09:51:30 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 72.64% at 37 epoch
[2022-11-11 09:51:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][0/1251]	eta 0:51:27 lr 0.000961	time 2.4681 (2.4681)	loss 4.2976 (4.2976)	grad_norm 1.2046 (1.2046)	mem 23874MB
[2022-11-11 09:52:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][50/1251]	eta 0:15:41 lr 0.000961	time 0.7397 (0.7843)	loss 3.9956 (3.7443)	grad_norm 1.2947 (1.2641)	mem 23874MB
[2022-11-11 09:52:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][100/1251]	eta 0:14:40 lr 0.000961	time 0.7391 (0.7654)	loss 2.6893 (3.7959)	grad_norm 1.1978 (1.2553)	mem 23874MB
[2022-11-11 09:53:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][150/1251]	eta 0:13:57 lr 0.000961	time 0.7434 (0.7606)	loss 3.9033 (3.8149)	grad_norm 1.1573 (1.2414)	mem 23874MB
[2022-11-11 09:54:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][200/1251]	eta 0:13:15 lr 0.000961	time 0.7385 (0.7567)	loss 4.2364 (3.8276)	grad_norm 1.2442 (1.2453)	mem 23874MB
[2022-11-11 09:54:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][250/1251]	eta 0:12:35 lr 0.000961	time 0.7394 (0.7548)	loss 4.5908 (3.8568)	grad_norm 1.1970 (1.2475)	mem 23874MB
[2022-11-11 09:55:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][300/1251]	eta 0:11:56 lr 0.000961	time 0.7435 (0.7534)	loss 3.7893 (3.8524)	grad_norm 1.2384 (1.2472)	mem 23874MB
[2022-11-11 09:55:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][350/1251]	eta 0:11:18 lr 0.000961	time 0.7377 (0.7527)	loss 2.9135 (3.8505)	grad_norm 1.1578 (1.2448)	mem 23874MB
[2022-11-11 09:56:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][400/1251]	eta 0:10:39 lr 0.000961	time 0.7353 (0.7517)	loss 4.6993 (3.8513)	grad_norm 1.3077 (1.2430)	mem 23874MB
[2022-11-11 09:57:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][450/1251]	eta 0:10:01 lr 0.000961	time 0.7377 (0.7514)	loss 4.0482 (3.8400)	grad_norm 1.3457 (1.2438)	mem 23874MB
[2022-11-11 09:57:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][500/1251]	eta 0:09:23 lr 0.000961	time 0.7362 (0.7505)	loss 4.0491 (3.8456)	grad_norm 1.1353 (nan)	mem 23874MB
[2022-11-11 09:58:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][550/1251]	eta 0:08:45 lr 0.000960	time 0.7369 (0.7502)	loss 2.7703 (3.8416)	grad_norm 1.2540 (nan)	mem 23874MB
[2022-11-11 09:59:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][600/1251]	eta 0:08:08 lr 0.000960	time 0.8327 (0.7500)	loss 4.5347 (3.8320)	grad_norm 1.3235 (nan)	mem 23874MB
[2022-11-11 09:59:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][650/1251]	eta 0:07:30 lr 0.000960	time 0.7460 (0.7498)	loss 4.2670 (3.8322)	grad_norm 1.3664 (nan)	mem 23874MB
[2022-11-11 10:00:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][700/1251]	eta 0:06:52 lr 0.000960	time 0.7240 (0.7495)	loss 4.5004 (3.8352)	grad_norm 1.4036 (nan)	mem 23874MB
[2022-11-11 10:00:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][750/1251]	eta 0:06:15 lr 0.000960	time 0.7354 (0.7493)	loss 4.5799 (3.8396)	grad_norm 1.2481 (nan)	mem 23874MB
[2022-11-11 10:01:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][800/1251]	eta 0:05:37 lr 0.000960	time 0.7394 (0.7490)	loss 3.7545 (3.8321)	grad_norm 1.1997 (nan)	mem 23874MB
[2022-11-11 10:02:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][850/1251]	eta 0:05:00 lr 0.000960	time 0.8275 (0.7490)	loss 4.2247 (3.8321)	grad_norm 1.1147 (nan)	mem 23874MB
[2022-11-11 10:02:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][900/1251]	eta 0:04:22 lr 0.000960	time 0.7354 (0.7488)	loss 3.8597 (3.8402)	grad_norm 1.4180 (nan)	mem 23874MB
[2022-11-11 10:03:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][950/1251]	eta 0:03:45 lr 0.000960	time 0.7529 (0.7487)	loss 4.3555 (3.8394)	grad_norm 1.3148 (nan)	mem 23874MB
[2022-11-11 10:03:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][1000/1251]	eta 0:03:07 lr 0.000960	time 0.7418 (0.7486)	loss 4.5260 (3.8354)	grad_norm 1.2473 (nan)	mem 23874MB
[2022-11-11 10:04:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][1050/1251]	eta 0:02:30 lr 0.000960	time 0.7366 (0.7485)	loss 4.1737 (3.8328)	grad_norm 1.2570 (nan)	mem 23874MB
[2022-11-11 10:05:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][1100/1251]	eta 0:01:53 lr 0.000960	time 0.7399 (0.7485)	loss 2.5830 (3.8312)	grad_norm 1.2600 (nan)	mem 23874MB
[2022-11-11 10:05:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][1150/1251]	eta 0:01:15 lr 0.000959	time 0.7393 (0.7484)	loss 4.6143 (3.8298)	grad_norm 1.2515 (nan)	mem 23874MB
[2022-11-11 10:06:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][1200/1251]	eta 0:00:38 lr 0.000959	time 0.7379 (0.7484)	loss 3.5783 (3.8240)	grad_norm 1.1136 (nan)	mem 23874MB
[2022-11-11 10:07:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [38/300][1250/1251]	eta 0:00:00 lr 0.000959	time 0.7302 (0.7482)	loss 4.5359 (3.8226)	grad_norm 1.1115 (nan)	mem 23874MB
[2022-11-11 10:07:06 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 38 training takes 0:15:36
[2022-11-11 10:07:06 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_38.pth saving......
[2022-11-11 10:07:07 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_38.pth saved !!!
[2022-11-11 10:07:09 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.667 (1.667)	Loss 1.1837 (1.1837)	Acc@1 72.754 (72.754)	Acc@5 91.406 (91.406)	Mem 23874MB
[2022-11-11 10:07:20 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.700 Acc@5 91.590
[2022-11-11 10:07:20 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.7%
[2022-11-11 10:07:22 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.836 (1.836)	Loss 1.2122 (1.2122)	Acc@1 69.824 (69.824)	Acc@5 90.918 (90.918)	Mem 23874MB
[2022-11-11 10:07:32 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.066 Acc@5 91.570
[2022-11-11 10:07:32 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 73.1%
[2022-11-11 10:07:32 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 73.07% at 38 epoch
[2022-11-11 10:07:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][0/1251]	eta 0:50:02 lr 0.000959	time 2.3998 (2.3998)	loss 3.5828 (3.5828)	grad_norm 1.1761 (1.1761)	mem 23874MB
[2022-11-11 10:08:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][50/1251]	eta 0:15:41 lr 0.000959	time 0.7347 (0.7843)	loss 3.8674 (3.7634)	grad_norm 1.3650 (1.2519)	mem 23874MB
[2022-11-11 10:08:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][100/1251]	eta 0:14:39 lr 0.000959	time 0.7449 (0.7638)	loss 3.2527 (3.7932)	grad_norm 1.2412 (1.2609)	mem 23874MB
[2022-11-11 10:09:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][150/1251]	eta 0:13:55 lr 0.000959	time 0.7431 (0.7589)	loss 4.0852 (3.8499)	grad_norm 1.2605 (1.2608)	mem 23874MB
[2022-11-11 10:10:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][200/1251]	eta 0:13:14 lr 0.000959	time 0.7367 (0.7557)	loss 2.6009 (3.8461)	grad_norm 1.4939 (1.2610)	mem 23874MB
[2022-11-11 10:10:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][250/1251]	eta 0:12:34 lr 0.000959	time 0.7361 (0.7542)	loss 3.4674 (3.8242)	grad_norm 1.2882 (1.2598)	mem 23874MB
[2022-11-11 10:11:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][300/1251]	eta 0:11:55 lr 0.000959	time 0.7419 (0.7529)	loss 4.4183 (3.8225)	grad_norm 1.2031 (1.2548)	mem 23874MB
[2022-11-11 10:11:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][350/1251]	eta 0:11:17 lr 0.000959	time 0.7419 (0.7520)	loss 4.1902 (3.8196)	grad_norm 1.1211 (1.2531)	mem 23874MB
[2022-11-11 10:12:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][400/1251]	eta 0:10:39 lr 0.000959	time 0.7455 (0.7511)	loss 4.1869 (3.8204)	grad_norm 1.3413 (1.2537)	mem 23874MB
[2022-11-11 10:13:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][450/1251]	eta 0:10:01 lr 0.000959	time 0.7492 (0.7510)	loss 3.5509 (3.8225)	grad_norm 1.2403 (1.2527)	mem 23874MB
[2022-11-11 10:13:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][500/1251]	eta 0:09:23 lr 0.000958	time 0.7333 (0.7504)	loss 4.1274 (3.8188)	grad_norm 1.1521 (1.2526)	mem 23874MB
[2022-11-11 10:14:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][550/1251]	eta 0:08:45 lr 0.000958	time 0.7452 (0.7501)	loss 3.9329 (3.8168)	grad_norm 1.1906 (1.2496)	mem 23874MB
[2022-11-11 10:15:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][600/1251]	eta 0:08:07 lr 0.000958	time 0.7358 (0.7496)	loss 4.0922 (3.8279)	grad_norm 1.1852 (1.2484)	mem 23874MB
[2022-11-11 10:15:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][650/1251]	eta 0:07:30 lr 0.000958	time 0.7342 (0.7494)	loss 3.8491 (3.8294)	grad_norm 1.4261 (1.2493)	mem 23874MB
[2022-11-11 10:16:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][700/1251]	eta 0:06:52 lr 0.000958	time 0.7385 (0.7494)	loss 2.9509 (3.8199)	grad_norm 1.3410 (1.2500)	mem 23874MB
[2022-11-11 10:16:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][750/1251]	eta 0:06:15 lr 0.000958	time 0.7369 (0.7491)	loss 4.3413 (3.8163)	grad_norm 1.2093 (1.2497)	mem 23874MB
[2022-11-11 10:17:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][800/1251]	eta 0:05:37 lr 0.000958	time 0.7460 (0.7489)	loss 4.3721 (3.8127)	grad_norm 1.2524 (1.2476)	mem 23874MB
[2022-11-11 10:18:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][850/1251]	eta 0:05:00 lr 0.000958	time 0.7372 (0.7489)	loss 2.9599 (3.8075)	grad_norm 1.3503 (1.2465)	mem 23874MB
[2022-11-11 10:18:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][900/1251]	eta 0:04:22 lr 0.000958	time 0.7384 (0.7485)	loss 3.6735 (3.8093)	grad_norm 1.2721 (1.2476)	mem 23874MB
[2022-11-11 10:19:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][950/1251]	eta 0:03:45 lr 0.000958	time 0.7344 (0.7484)	loss 3.9560 (3.8064)	grad_norm 1.2504 (1.2477)	mem 23874MB
[2022-11-11 10:20:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][1000/1251]	eta 0:03:07 lr 0.000958	time 0.7379 (0.7482)	loss 3.6857 (3.8046)	grad_norm 1.3635 (1.2478)	mem 23874MB
[2022-11-11 10:20:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][1050/1251]	eta 0:02:30 lr 0.000958	time 0.7374 (0.7480)	loss 2.9500 (3.8074)	grad_norm 1.2491 (1.2479)	mem 23874MB
[2022-11-11 10:21:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][1100/1251]	eta 0:01:52 lr 0.000957	time 0.7403 (0.7479)	loss 3.7001 (3.8100)	grad_norm 1.1250 (1.2488)	mem 23874MB
[2022-11-11 10:21:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][1150/1251]	eta 0:01:15 lr 0.000957	time 0.7329 (0.7481)	loss 4.1050 (3.8161)	grad_norm 1.2205 (1.2479)	mem 23874MB
[2022-11-11 10:22:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][1200/1251]	eta 0:00:38 lr 0.000957	time 0.7404 (0.7479)	loss 3.9281 (3.8181)	grad_norm 1.2243 (1.2468)	mem 23874MB
[2022-11-11 10:23:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [39/300][1250/1251]	eta 0:00:00 lr 0.000957	time 0.7265 (0.7479)	loss 2.9091 (3.8079)	grad_norm 1.2222 (nan)	mem 23874MB
[2022-11-11 10:23:08 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 39 training takes 0:15:35
[2022-11-11 10:23:08 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_39.pth saving......
[2022-11-11 10:23:10 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_39.pth saved !!!
[2022-11-11 10:23:11 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.700 (1.700)	Loss 1.2854 (1.2854)	Acc@1 71.289 (71.289)	Acc@5 90.820 (90.820)	Mem 23874MB
[2022-11-11 10:23:22 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 71.950 Acc@5 91.290
[2022-11-11 10:23:22 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 71.9%
[2022-11-11 10:23:24 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.849 (1.849)	Loss 1.1845 (1.1845)	Acc@1 71.484 (71.484)	Acc@5 91.504 (91.504)	Mem 23874MB
[2022-11-11 10:23:35 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.414 Acc@5 91.784
[2022-11-11 10:23:35 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 73.4%
[2022-11-11 10:23:35 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 73.41% at 39 epoch
[2022-11-11 10:23:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][0/1251]	eta 0:48:07 lr 0.000957	time 2.3080 (2.3080)	loss 2.6575 (2.6575)	grad_norm 1.3006 (1.3006)	mem 23874MB
[2022-11-11 10:24:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][50/1251]	eta 0:15:41 lr 0.000957	time 0.7425 (0.7840)	loss 3.0959 (3.8172)	grad_norm 1.0483 (1.2386)	mem 23874MB
[2022-11-11 10:24:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][100/1251]	eta 0:14:43 lr 0.000957	time 0.7408 (0.7675)	loss 4.4031 (3.7461)	grad_norm 1.3157 (1.2424)	mem 23874MB
[2022-11-11 10:25:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][150/1251]	eta 0:13:57 lr 0.000957	time 0.7402 (0.7607)	loss 3.8163 (3.7491)	grad_norm 1.2560 (1.2522)	mem 23874MB
[2022-11-11 10:26:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][200/1251]	eta 0:13:16 lr 0.000957	time 0.7327 (0.7582)	loss 3.8179 (3.7400)	grad_norm 1.0980 (1.2547)	mem 23874MB
[2022-11-11 10:26:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][250/1251]	eta 0:12:36 lr 0.000957	time 0.7399 (0.7555)	loss 4.2467 (3.7588)	grad_norm 1.5729 (1.2536)	mem 23874MB
[2022-11-11 10:27:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][300/1251]	eta 0:11:58 lr 0.000957	time 0.7421 (0.7550)	loss 4.3018 (3.7516)	grad_norm 1.2273 (1.2542)	mem 23874MB
[2022-11-11 10:27:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][350/1251]	eta 0:11:19 lr 0.000957	time 0.7405 (0.7539)	loss 3.3672 (3.7545)	grad_norm 1.2738 (1.2544)	mem 23874MB
[2022-11-11 10:28:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][400/1251]	eta 0:10:41 lr 0.000957	time 0.7392 (0.7533)	loss 4.5772 (3.7958)	grad_norm 1.4226 (1.2555)	mem 23874MB
[2022-11-11 10:29:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][450/1251]	eta 0:10:02 lr 0.000956	time 0.7417 (0.7526)	loss 3.0907 (3.7954)	grad_norm 1.1774 (1.2519)	mem 23874MB
[2022-11-11 10:29:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][500/1251]	eta 0:09:24 lr 0.000956	time 0.7378 (0.7522)	loss 4.4778 (3.8026)	grad_norm 1.4080 (1.2535)	mem 23874MB
[2022-11-11 10:30:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][550/1251]	eta 0:08:46 lr 0.000956	time 0.7341 (0.7517)	loss 4.1818 (3.8089)	grad_norm 1.1537 (1.2535)	mem 23874MB
[2022-11-11 10:31:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][600/1251]	eta 0:08:09 lr 0.000956	time 0.7411 (0.7515)	loss 3.7555 (3.7930)	grad_norm 1.3287 (1.2523)	mem 23874MB
[2022-11-11 10:31:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][650/1251]	eta 0:07:31 lr 0.000956	time 0.7371 (0.7513)	loss 3.9576 (3.7933)	grad_norm 1.2603 (1.2500)	mem 23874MB
[2022-11-11 10:32:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][700/1251]	eta 0:06:53 lr 0.000956	time 0.7393 (0.7512)	loss 3.7615 (3.8036)	grad_norm 1.0683 (1.2497)	mem 23874MB
[2022-11-11 10:32:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][750/1251]	eta 0:06:16 lr 0.000956	time 0.7353 (0.7508)	loss 3.7142 (3.7907)	grad_norm 1.2072 (1.2465)	mem 23874MB
[2022-11-11 10:33:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][800/1251]	eta 0:05:38 lr 0.000956	time 0.7392 (0.7507)	loss 3.1769 (3.7905)	grad_norm 1.3645 (1.2473)	mem 23874MB
[2022-11-11 10:34:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][850/1251]	eta 0:05:00 lr 0.000956	time 0.7393 (0.7506)	loss 2.8526 (3.7985)	grad_norm 1.4066 (1.2468)	mem 23874MB
[2022-11-11 10:34:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][900/1251]	eta 0:04:23 lr 0.000956	time 0.7392 (0.7505)	loss 4.1709 (3.8052)	grad_norm 1.1721 (1.2477)	mem 23874MB
[2022-11-11 10:35:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][950/1251]	eta 0:03:45 lr 0.000956	time 0.8281 (0.7502)	loss 3.8252 (3.8018)	grad_norm 1.2108 (1.2475)	mem 23874MB
[2022-11-11 10:36:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][1000/1251]	eta 0:03:08 lr 0.000956	time 0.7373 (0.7501)	loss 3.8407 (3.8026)	grad_norm 1.2390 (1.2463)	mem 23874MB
[2022-11-11 10:36:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][1050/1251]	eta 0:02:30 lr 0.000955	time 0.8418 (0.7500)	loss 4.1310 (3.8035)	grad_norm 1.2355 (1.2470)	mem 23874MB
[2022-11-11 10:37:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][1100/1251]	eta 0:01:53 lr 0.000955	time 0.7433 (0.7498)	loss 3.8193 (3.7980)	grad_norm 1.1227 (1.2474)	mem 23874MB
[2022-11-11 10:37:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][1150/1251]	eta 0:01:15 lr 0.000955	time 0.7383 (0.7496)	loss 3.6191 (3.7998)	grad_norm 1.2087 (1.2472)	mem 23874MB
[2022-11-11 10:38:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][1200/1251]	eta 0:00:38 lr 0.000955	time 0.8237 (0.7495)	loss 4.4809 (3.7923)	grad_norm 1.1760 (1.2476)	mem 23874MB
[2022-11-11 10:39:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [40/300][1250/1251]	eta 0:00:00 lr 0.000955	time 0.7283 (0.7492)	loss 4.0255 (3.7897)	grad_norm 1.3110 (1.2472)	mem 23874MB
[2022-11-11 10:39:12 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 40 training takes 0:15:37
[2022-11-11 10:39:12 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_40.pth saving......
[2022-11-11 10:39:13 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_40.pth saved !!!
[2022-11-11 10:39:15 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.586 (1.586)	Loss 1.2462 (1.2462)	Acc@1 70.605 (70.605)	Acc@5 90.137 (90.137)	Mem 23874MB
[2022-11-11 10:39:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.878 Acc@5 91.906
[2022-11-11 10:39:25 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.9%
[2022-11-11 10:39:27 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.848 (1.848)	Loss 1.0867 (1.0867)	Acc@1 74.512 (74.512)	Acc@5 92.578 (92.578)	Mem 23874MB
[2022-11-11 10:39:38 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.730 Acc@5 92.018
[2022-11-11 10:39:38 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 73.7%
[2022-11-11 10:39:38 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 73.73% at 40 epoch
[2022-11-11 10:39:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][0/1251]	eta 0:48:33 lr 0.000955	time 2.3290 (2.3290)	loss 3.8140 (3.8140)	grad_norm 1.0604 (1.0604)	mem 23874MB
[2022-11-11 10:40:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][50/1251]	eta 0:15:37 lr 0.000955	time 0.7361 (0.7805)	loss 3.8935 (3.6898)	grad_norm 1.0746 (1.2410)	mem 23874MB
[2022-11-11 10:40:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][100/1251]	eta 0:14:41 lr 0.000955	time 0.7502 (0.7657)	loss 3.6194 (3.7360)	grad_norm 1.2890 (1.2612)	mem 23874MB
[2022-11-11 10:41:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][150/1251]	eta 0:13:55 lr 0.000955	time 0.7350 (0.7591)	loss 3.8811 (3.7722)	grad_norm 1.2501 (1.2487)	mem 23874MB
[2022-11-11 10:42:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][200/1251]	eta 0:13:15 lr 0.000955	time 0.7337 (0.7565)	loss 3.6920 (3.7388)	grad_norm 1.2495 (1.2562)	mem 23874MB
[2022-11-11 10:42:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][250/1251]	eta 0:12:35 lr 0.000955	time 0.7206 (0.7550)	loss 4.4002 (3.7778)	grad_norm 1.3329 (1.2520)	mem 23874MB
[2022-11-11 10:43:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][300/1251]	eta 0:11:56 lr 0.000955	time 0.7364 (0.7534)	loss 4.4489 (3.7676)	grad_norm 1.3009 (1.2553)	mem 23874MB
[2022-11-11 10:44:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][350/1251]	eta 0:11:18 lr 0.000954	time 0.7421 (0.7527)	loss 4.5305 (3.7864)	grad_norm 1.2030 (nan)	mem 23874MB
[2022-11-11 10:44:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][400/1251]	eta 0:10:39 lr 0.000954	time 0.7446 (0.7516)	loss 4.1971 (3.7789)	grad_norm 1.2246 (nan)	mem 23874MB
[2022-11-11 10:45:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][450/1251]	eta 0:10:01 lr 0.000954	time 0.7395 (0.7513)	loss 3.4738 (3.7661)	grad_norm 1.2121 (nan)	mem 23874MB
[2022-11-11 10:45:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][500/1251]	eta 0:09:23 lr 0.000954	time 0.7401 (0.7506)	loss 2.4899 (3.7569)	grad_norm 1.2076 (nan)	mem 23874MB
[2022-11-11 10:46:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][550/1251]	eta 0:08:45 lr 0.000954	time 0.7426 (0.7503)	loss 3.0421 (3.7535)	grad_norm 1.1474 (nan)	mem 23874MB
[2022-11-11 10:47:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][600/1251]	eta 0:08:08 lr 0.000954	time 0.7363 (0.7498)	loss 4.3010 (3.7579)	grad_norm 1.3195 (nan)	mem 23874MB
[2022-11-11 10:47:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][650/1251]	eta 0:07:30 lr 0.000954	time 0.7377 (0.7498)	loss 3.9872 (3.7688)	grad_norm 1.2989 (nan)	mem 23874MB
[2022-11-11 10:48:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][700/1251]	eta 0:06:52 lr 0.000954	time 0.7381 (0.7495)	loss 3.4919 (3.7704)	grad_norm 1.5276 (nan)	mem 23874MB
[2022-11-11 10:49:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][750/1251]	eta 0:06:15 lr 0.000954	time 0.7387 (0.7491)	loss 4.2704 (3.7723)	grad_norm 1.0647 (nan)	mem 23874MB
[2022-11-11 10:49:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][800/1251]	eta 0:05:37 lr 0.000954	time 0.7376 (0.7491)	loss 2.7815 (3.7653)	grad_norm 1.2493 (nan)	mem 23874MB
[2022-11-11 10:50:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][850/1251]	eta 0:05:00 lr 0.000954	time 0.7396 (0.7488)	loss 3.8608 (3.7763)	grad_norm 1.3512 (nan)	mem 23874MB
[2022-11-11 10:50:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][900/1251]	eta 0:04:22 lr 0.000954	time 0.7416 (0.7489)	loss 4.0230 (3.7803)	grad_norm 1.1171 (nan)	mem 23874MB
[2022-11-11 10:51:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][950/1251]	eta 0:03:45 lr 0.000953	time 0.8272 (0.7487)	loss 4.2293 (3.7821)	grad_norm 1.3063 (nan)	mem 23874MB
[2022-11-11 10:52:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][1000/1251]	eta 0:03:07 lr 0.000953	time 0.7373 (0.7486)	loss 3.4192 (3.7814)	grad_norm 1.3325 (nan)	mem 23874MB
[2022-11-11 10:52:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][1050/1251]	eta 0:02:30 lr 0.000953	time 0.7389 (0.7485)	loss 2.6009 (3.7823)	grad_norm 1.1894 (nan)	mem 23874MB
[2022-11-11 10:53:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][1100/1251]	eta 0:01:53 lr 0.000953	time 0.7405 (0.7483)	loss 2.9584 (3.7822)	grad_norm 1.1740 (nan)	mem 23874MB
[2022-11-11 10:53:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][1150/1251]	eta 0:01:15 lr 0.000953	time 0.7376 (0.7483)	loss 3.8404 (3.7838)	grad_norm 1.2655 (nan)	mem 23874MB
[2022-11-11 10:54:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][1200/1251]	eta 0:00:38 lr 0.000953	time 0.7390 (0.7482)	loss 3.1388 (3.7826)	grad_norm 1.1173 (nan)	mem 23874MB
[2022-11-11 10:55:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [41/300][1250/1251]	eta 0:00:00 lr 0.000953	time 0.7248 (0.7479)	loss 4.1549 (3.7824)	grad_norm 1.2909 (nan)	mem 23874MB
[2022-11-11 10:55:14 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 41 training takes 0:15:35
[2022-11-11 10:55:14 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_41.pth saving......
[2022-11-11 10:55:15 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_41.pth saved !!!
[2022-11-11 10:55:17 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.569 (1.569)	Loss 1.2355 (1.2355)	Acc@1 73.242 (73.242)	Acc@5 91.211 (91.211)	Mem 23874MB
[2022-11-11 10:55:27 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.934 Acc@5 91.800
[2022-11-11 10:55:27 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.9%
[2022-11-11 10:55:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.977 (1.977)	Loss 1.0267 (1.0267)	Acc@1 75.879 (75.879)	Acc@5 92.383 (92.383)	Mem 23874MB
[2022-11-11 10:55:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.110 Acc@5 92.236
[2022-11-11 10:55:40 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 74.1%
[2022-11-11 10:55:40 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 74.11% at 41 epoch
[2022-11-11 10:55:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][0/1251]	eta 0:51:23 lr 0.000953	time 2.4647 (2.4647)	loss 3.8614 (3.8614)	grad_norm 1.2283 (1.2283)	mem 23874MB
[2022-11-11 10:56:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][50/1251]	eta 0:15:37 lr 0.000953	time 0.7411 (0.7805)	loss 3.5312 (3.9159)	grad_norm 1.2447 (1.2221)	mem 23874MB
[2022-11-11 10:56:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][100/1251]	eta 0:14:39 lr 0.000953	time 0.7356 (0.7639)	loss 4.4770 (3.8206)	grad_norm 1.1950 (1.2190)	mem 23874MB
[2022-11-11 10:57:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][150/1251]	eta 0:13:54 lr 0.000953	time 0.7425 (0.7580)	loss 3.9583 (3.7333)	grad_norm 1.2973 (1.2236)	mem 23874MB
[2022-11-11 10:58:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][200/1251]	eta 0:13:14 lr 0.000953	time 0.7374 (0.7559)	loss 2.7522 (3.7492)	grad_norm 1.1329 (1.2231)	mem 23874MB
[2022-11-11 10:58:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][250/1251]	eta 0:12:34 lr 0.000952	time 0.8115 (0.7539)	loss 3.8215 (3.7369)	grad_norm 1.2607 (1.2300)	mem 23874MB
[2022-11-11 10:59:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][300/1251]	eta 0:11:55 lr 0.000952	time 0.7482 (0.7525)	loss 3.8823 (3.7224)	grad_norm 1.2667 (1.2322)	mem 23874MB
[2022-11-11 11:00:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][350/1251]	eta 0:11:17 lr 0.000952	time 0.7363 (0.7514)	loss 3.5859 (3.7293)	grad_norm 1.1545 (1.2320)	mem 23874MB
[2022-11-11 11:00:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][400/1251]	eta 0:10:39 lr 0.000952	time 0.7344 (0.7509)	loss 3.7424 (3.7229)	grad_norm 1.2988 (1.2348)	mem 23874MB
[2022-11-11 11:01:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][450/1251]	eta 0:10:00 lr 0.000952	time 0.7350 (0.7500)	loss 4.1954 (3.7204)	grad_norm 1.2377 (1.2329)	mem 23874MB
[2022-11-11 11:01:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][500/1251]	eta 0:09:23 lr 0.000952	time 0.7366 (0.7499)	loss 3.6425 (3.7304)	grad_norm 1.3131 (1.2339)	mem 23874MB
[2022-11-11 11:02:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][550/1251]	eta 0:08:45 lr 0.000952	time 0.7370 (0.7495)	loss 2.8146 (3.7276)	grad_norm 1.3036 (1.2352)	mem 23874MB
[2022-11-11 11:03:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][600/1251]	eta 0:08:07 lr 0.000952	time 0.7393 (0.7490)	loss 3.4372 (3.7308)	grad_norm 1.1398 (1.2346)	mem 23874MB
[2022-11-11 11:03:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][650/1251]	eta 0:07:29 lr 0.000952	time 0.7435 (0.7487)	loss 4.2204 (3.7335)	grad_norm 1.3585 (1.2342)	mem 23874MB
[2022-11-11 11:04:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][700/1251]	eta 0:06:52 lr 0.000952	time 0.8153 (0.7484)	loss 3.1945 (3.7249)	grad_norm 1.2039 (1.2370)	mem 23874MB
[2022-11-11 11:05:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][750/1251]	eta 0:06:14 lr 0.000952	time 0.7383 (0.7482)	loss 3.2454 (3.7223)	grad_norm 1.1087 (1.2363)	mem 23874MB
[2022-11-11 11:05:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][800/1251]	eta 0:05:37 lr 0.000951	time 0.7358 (0.7480)	loss 3.1864 (3.7193)	grad_norm 1.1765 (1.2367)	mem 23874MB
[2022-11-11 11:06:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][850/1251]	eta 0:04:59 lr 0.000951	time 0.7369 (0.7480)	loss 3.7457 (3.7172)	grad_norm 1.2076 (1.2366)	mem 23874MB
[2022-11-11 11:06:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][900/1251]	eta 0:04:22 lr 0.000951	time 0.7358 (0.7477)	loss 4.3815 (3.7187)	grad_norm 1.3163 (1.2378)	mem 23874MB
[2022-11-11 11:07:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][950/1251]	eta 0:03:45 lr 0.000951	time 0.7393 (0.7477)	loss 4.5658 (3.7191)	grad_norm 1.2978 (1.2372)	mem 23874MB
[2022-11-11 11:08:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][1000/1251]	eta 0:03:07 lr 0.000951	time 0.7412 (0.7476)	loss 3.0292 (3.7225)	grad_norm 1.3156 (1.2366)	mem 23874MB
[2022-11-11 11:08:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][1050/1251]	eta 0:02:30 lr 0.000951	time 0.7345 (0.7476)	loss 3.9130 (3.7282)	grad_norm 1.1013 (1.2383)	mem 23874MB
[2022-11-11 11:09:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][1100/1251]	eta 0:01:52 lr 0.000951	time 0.7333 (0.7474)	loss 3.9619 (3.7249)	grad_norm 1.1567 (1.2387)	mem 23874MB
[2022-11-11 11:10:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][1150/1251]	eta 0:01:15 lr 0.000951	time 0.7428 (0.7474)	loss 3.9554 (3.7249)	grad_norm 1.1703 (1.2390)	mem 23874MB
[2022-11-11 11:10:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][1200/1251]	eta 0:00:38 lr 0.000951	time 0.7394 (0.7473)	loss 4.3002 (3.7285)	grad_norm 1.1756 (1.2388)	mem 23874MB
[2022-11-11 11:11:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [42/300][1250/1251]	eta 0:00:00 lr 0.000951	time 0.7248 (0.7472)	loss 2.5214 (3.7276)	grad_norm 1.3624 (1.2380)	mem 23874MB
[2022-11-11 11:11:15 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 42 training takes 0:15:34
[2022-11-11 11:11:15 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_42.pth saving......
[2022-11-11 11:11:16 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_42.pth saved !!!
[2022-11-11 11:11:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.725 (1.725)	Loss 1.1861 (1.1861)	Acc@1 70.996 (70.996)	Acc@5 91.504 (91.504)	Mem 23874MB
[2022-11-11 11:11:28 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.168 Acc@5 91.894
[2022-11-11 11:11:28 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.2%
[2022-11-11 11:11:30 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.817 (1.817)	Loss 1.0635 (1.0635)	Acc@1 73.047 (73.047)	Acc@5 92.871 (92.871)	Mem 23874MB
[2022-11-11 11:11:41 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.352 Acc@5 92.348
[2022-11-11 11:11:41 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 74.4%
[2022-11-11 11:11:41 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 74.35% at 42 epoch
[2022-11-11 11:11:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][0/1251]	eta 0:49:14 lr 0.000951	time 2.3617 (2.3617)	loss 4.0802 (4.0802)	grad_norm 1.2314 (1.2314)	mem 23874MB
[2022-11-11 11:12:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][50/1251]	eta 0:15:41 lr 0.000951	time 0.7408 (0.7840)	loss 3.1980 (3.8412)	grad_norm 1.2503 (1.2543)	mem 23874MB
[2022-11-11 11:12:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][100/1251]	eta 0:14:39 lr 0.000950	time 0.7353 (0.7644)	loss 3.5961 (3.8019)	grad_norm 1.2445 (1.2588)	mem 23874MB
[2022-11-11 11:13:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][150/1251]	eta 0:13:57 lr 0.000950	time 0.7438 (0.7603)	loss 3.8979 (3.7906)	grad_norm 1.4367 (1.2568)	mem 23874MB
[2022-11-11 11:14:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][200/1251]	eta 0:13:16 lr 0.000950	time 0.7383 (0.7576)	loss 4.1388 (3.7634)	grad_norm 1.1018 (1.2463)	mem 23874MB
[2022-11-11 11:14:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][250/1251]	eta 0:12:35 lr 0.000950	time 0.7357 (0.7552)	loss 3.1543 (3.7594)	grad_norm 1.2675 (1.2415)	mem 23874MB
[2022-11-11 11:15:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][300/1251]	eta 0:11:57 lr 0.000950	time 0.7419 (0.7542)	loss 3.6822 (3.7768)	grad_norm 1.2343 (1.2410)	mem 23874MB
[2022-11-11 11:16:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][350/1251]	eta 0:11:18 lr 0.000950	time 0.7417 (0.7534)	loss 3.0955 (3.7482)	grad_norm 1.4628 (1.2422)	mem 23874MB
[2022-11-11 11:16:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][400/1251]	eta 0:10:40 lr 0.000950	time 0.7412 (0.7523)	loss 2.7561 (3.7415)	grad_norm 1.2820 (1.2445)	mem 23874MB
[2022-11-11 11:17:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][450/1251]	eta 0:10:02 lr 0.000950	time 0.7376 (0.7517)	loss 4.5802 (3.7489)	grad_norm 1.2465 (1.2439)	mem 23874MB
[2022-11-11 11:17:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][500/1251]	eta 0:09:24 lr 0.000950	time 0.7363 (0.7511)	loss 4.0129 (3.7567)	grad_norm 1.1848 (1.2450)	mem 23874MB
[2022-11-11 11:18:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][550/1251]	eta 0:08:46 lr 0.000950	time 0.7378 (0.7507)	loss 4.2034 (3.7565)	grad_norm 1.1705 (1.2475)	mem 23874MB
[2022-11-11 11:19:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][600/1251]	eta 0:08:08 lr 0.000950	time 0.7341 (0.7506)	loss 3.7124 (3.7485)	grad_norm 1.1501 (1.2454)	mem 23874MB
[2022-11-11 11:19:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][650/1251]	eta 0:07:30 lr 0.000949	time 0.7373 (0.7502)	loss 3.8862 (3.7389)	grad_norm 1.2506 (1.2452)	mem 23874MB
[2022-11-11 11:20:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][700/1251]	eta 0:06:53 lr 0.000949	time 0.7375 (0.7501)	loss 4.5886 (3.7380)	grad_norm 1.2043 (1.2459)	mem 23874MB
[2022-11-11 11:21:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][750/1251]	eta 0:06:15 lr 0.000949	time 0.7332 (0.7499)	loss 4.3285 (3.7358)	grad_norm 1.1873 (1.2461)	mem 23874MB
[2022-11-11 11:21:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][800/1251]	eta 0:05:37 lr 0.000949	time 0.7383 (0.7494)	loss 4.3807 (3.7466)	grad_norm 1.3493 (1.2470)	mem 23874MB
[2022-11-11 11:22:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][850/1251]	eta 0:05:00 lr 0.000949	time 0.7497 (0.7495)	loss 4.1456 (3.7445)	grad_norm 1.3838 (1.2467)	mem 23874MB
[2022-11-11 11:22:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][900/1251]	eta 0:04:22 lr 0.000949	time 0.7427 (0.7492)	loss 3.5408 (3.7401)	grad_norm 1.2454 (1.2476)	mem 23874MB
[2022-11-11 11:23:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][950/1251]	eta 0:03:45 lr 0.000949	time 0.7377 (0.7490)	loss 4.2160 (3.7284)	grad_norm 1.2231 (1.2488)	mem 23874MB
[2022-11-11 11:24:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][1000/1251]	eta 0:03:07 lr 0.000949	time 0.7375 (0.7489)	loss 3.2518 (3.7257)	grad_norm 1.5026 (1.2486)	mem 23874MB
[2022-11-11 11:24:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][1050/1251]	eta 0:02:30 lr 0.000949	time 0.7366 (0.7487)	loss 3.8909 (3.7241)	grad_norm 1.1575 (1.2501)	mem 23874MB
[2022-11-11 11:25:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][1100/1251]	eta 0:01:53 lr 0.000949	time 0.7343 (0.7487)	loss 3.9197 (3.7282)	grad_norm 1.1611 (1.2506)	mem 23874MB
[2022-11-11 11:26:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][1150/1251]	eta 0:01:15 lr 0.000949	time 0.7350 (0.7486)	loss 4.5518 (3.7330)	grad_norm 1.1910 (1.2499)	mem 23874MB
[2022-11-11 11:26:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][1200/1251]	eta 0:00:38 lr 0.000948	time 0.7273 (0.7484)	loss 2.7455 (3.7281)	grad_norm 1.2221 (1.2495)	mem 23874MB
[2022-11-11 11:27:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [43/300][1250/1251]	eta 0:00:00 lr 0.000948	time 0.7249 (0.7482)	loss 4.3008 (3.7276)	grad_norm 1.2151 (1.2500)	mem 23874MB
[2022-11-11 11:27:17 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 43 training takes 0:15:36
[2022-11-11 11:27:17 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_43.pth saving......
[2022-11-11 11:27:18 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_43.pth saved !!!
[2022-11-11 11:27:20 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.549 (1.549)	Loss 1.3182 (1.3182)	Acc@1 69.629 (69.629)	Acc@5 90.430 (90.430)	Mem 23874MB
[2022-11-11 11:27:31 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 72.922 Acc@5 91.862
[2022-11-11 11:27:31 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 72.9%
[2022-11-11 11:27:32 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.918 (1.918)	Loss 1.0204 (1.0204)	Acc@1 76.562 (76.562)	Acc@5 92.871 (92.871)	Mem 23874MB
[2022-11-11 11:27:43 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.624 Acc@5 92.494
[2022-11-11 11:27:43 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 74.6%
[2022-11-11 11:27:43 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 74.62% at 43 epoch
[2022-11-11 11:27:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][0/1251]	eta 0:51:19 lr 0.000948	time 2.4613 (2.4613)	loss 3.5894 (3.5894)	grad_norm 1.2821 (1.2821)	mem 23874MB
[2022-11-11 11:28:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][50/1251]	eta 0:15:41 lr 0.000948	time 0.7351 (0.7840)	loss 2.9107 (3.7786)	grad_norm 1.3496 (1.2719)	mem 23874MB
[2022-11-11 11:29:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][100/1251]	eta 0:14:42 lr 0.000948	time 0.8262 (0.7669)	loss 3.6412 (3.7543)	grad_norm 1.1912 (1.2769)	mem 23874MB
[2022-11-11 11:29:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][150/1251]	eta 0:13:56 lr 0.000948	time 0.7391 (0.7601)	loss 3.9572 (3.7344)	grad_norm 1.1247 (1.2681)	mem 23874MB
[2022-11-11 11:30:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][200/1251]	eta 0:13:16 lr 0.000948	time 0.8154 (0.7579)	loss 2.5568 (3.7244)	grad_norm 1.2403 (1.2620)	mem 23874MB
[2022-11-11 11:30:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][250/1251]	eta 0:12:35 lr 0.000948	time 0.7358 (0.7551)	loss 3.2282 (3.7116)	grad_norm 1.2687 (1.2595)	mem 23874MB
[2022-11-11 11:31:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][300/1251]	eta 0:11:57 lr 0.000948	time 0.7372 (0.7539)	loss 3.7537 (3.7151)	grad_norm 1.1485 (nan)	mem 23874MB
[2022-11-11 11:32:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][350/1251]	eta 0:11:18 lr 0.000948	time 0.7364 (0.7533)	loss 3.4597 (3.7262)	grad_norm 1.1816 (nan)	mem 23874MB
[2022-11-11 11:32:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][400/1251]	eta 0:10:40 lr 0.000948	time 0.8137 (0.7524)	loss 3.6118 (3.7336)	grad_norm 1.1635 (nan)	mem 23874MB
[2022-11-11 11:33:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][450/1251]	eta 0:10:02 lr 0.000948	time 0.7488 (0.7520)	loss 3.1984 (3.7414)	grad_norm 1.1560 (nan)	mem 23874MB
[2022-11-11 11:33:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][500/1251]	eta 0:09:23 lr 0.000947	time 0.7350 (0.7509)	loss 2.8792 (3.7364)	grad_norm 1.3014 (nan)	mem 23874MB
[2022-11-11 11:34:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][550/1251]	eta 0:08:46 lr 0.000947	time 0.7375 (0.7506)	loss 3.2260 (3.7391)	grad_norm 1.1414 (nan)	mem 23874MB
[2022-11-11 11:35:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][600/1251]	eta 0:08:08 lr 0.000947	time 0.7358 (0.7502)	loss 3.8072 (3.7420)	grad_norm 1.2324 (nan)	mem 23874MB
[2022-11-11 11:35:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][650/1251]	eta 0:07:30 lr 0.000947	time 0.7397 (0.7503)	loss 4.2539 (3.7367)	grad_norm 1.3486 (nan)	mem 23874MB
[2022-11-11 11:36:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][700/1251]	eta 0:06:53 lr 0.000947	time 0.7426 (0.7500)	loss 2.6137 (3.7320)	grad_norm 1.3185 (nan)	mem 23874MB
[2022-11-11 11:37:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][750/1251]	eta 0:06:15 lr 0.000947	time 0.8300 (0.7499)	loss 2.7096 (3.7295)	grad_norm 1.1411 (nan)	mem 23874MB
[2022-11-11 11:37:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][800/1251]	eta 0:05:37 lr 0.000947	time 0.7310 (0.7494)	loss 3.3897 (3.7245)	grad_norm 1.2941 (nan)	mem 23874MB
[2022-11-11 11:38:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][850/1251]	eta 0:05:00 lr 0.000947	time 0.7534 (0.7494)	loss 3.9357 (3.7219)	grad_norm 1.3154 (nan)	mem 23874MB
[2022-11-11 11:38:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][900/1251]	eta 0:04:22 lr 0.000947	time 0.7388 (0.7492)	loss 3.4152 (3.7229)	grad_norm 1.2154 (nan)	mem 23874MB
[2022-11-11 11:39:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][950/1251]	eta 0:03:45 lr 0.000947	time 0.7393 (0.7492)	loss 4.0324 (3.7288)	grad_norm 1.3694 (nan)	mem 23874MB
[2022-11-11 11:40:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][1000/1251]	eta 0:03:08 lr 0.000947	time 0.7409 (0.7492)	loss 4.0860 (3.7286)	grad_norm 1.4255 (nan)	mem 23874MB
[2022-11-11 11:40:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][1050/1251]	eta 0:02:30 lr 0.000946	time 0.7370 (0.7489)	loss 3.8442 (3.7278)	grad_norm 1.2801 (nan)	mem 23874MB
[2022-11-11 11:41:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][1100/1251]	eta 0:01:53 lr 0.000946	time 0.7330 (0.7488)	loss 4.1264 (3.7261)	grad_norm 1.2391 (nan)	mem 23874MB
[2022-11-11 11:42:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][1150/1251]	eta 0:01:15 lr 0.000946	time 0.7432 (0.7488)	loss 4.0561 (3.7236)	grad_norm 1.2293 (nan)	mem 23874MB
[2022-11-11 11:42:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][1200/1251]	eta 0:00:38 lr 0.000946	time 0.7401 (0.7488)	loss 3.6080 (3.7245)	grad_norm 1.3749 (nan)	mem 23874MB
[2022-11-11 11:43:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [44/300][1250/1251]	eta 0:00:00 lr 0.000946	time 0.7433 (0.7487)	loss 3.1558 (3.7246)	grad_norm 1.2020 (nan)	mem 23874MB
[2022-11-11 11:43:20 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 44 training takes 0:15:36
[2022-11-11 11:43:20 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_44.pth saving......
[2022-11-11 11:43:21 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_44.pth saved !!!
[2022-11-11 11:43:23 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.715 (1.715)	Loss 1.1265 (1.1265)	Acc@1 74.805 (74.805)	Acc@5 93.359 (93.359)	Mem 23874MB
[2022-11-11 11:43:34 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.346 Acc@5 91.978
[2022-11-11 11:43:34 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.3%
[2022-11-11 11:43:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.917 (1.917)	Loss 0.9722 (0.9722)	Acc@1 75.684 (75.684)	Acc@5 94.043 (94.043)	Mem 23874MB
[2022-11-11 11:43:46 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.832 Acc@5 92.642
[2022-11-11 11:43:46 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 74.8%
[2022-11-11 11:43:46 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 74.83% at 44 epoch
[2022-11-11 11:43:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][0/1251]	eta 0:50:05 lr 0.000946	time 2.4027 (2.4027)	loss 4.4342 (4.4342)	grad_norm 1.2907 (1.2907)	mem 23874MB
[2022-11-11 11:44:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][50/1251]	eta 0:15:38 lr 0.000946	time 0.7431 (0.7812)	loss 4.0391 (3.8702)	grad_norm 1.3598 (1.2778)	mem 23874MB
[2022-11-11 11:45:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][100/1251]	eta 0:14:37 lr 0.000946	time 0.7406 (0.7625)	loss 3.6398 (3.8562)	grad_norm 1.1036 (1.2505)	mem 23874MB
[2022-11-11 11:45:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][150/1251]	eta 0:13:55 lr 0.000946	time 0.7344 (0.7588)	loss 3.6283 (3.8264)	grad_norm 1.1685 (1.2356)	mem 23874MB
[2022-11-11 11:46:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][200/1251]	eta 0:13:14 lr 0.000946	time 0.8105 (0.7555)	loss 4.2247 (3.7909)	grad_norm 1.3239 (1.2297)	mem 23874MB
[2022-11-11 11:46:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][250/1251]	eta 0:12:34 lr 0.000946	time 0.8180 (0.7539)	loss 4.2710 (3.7773)	grad_norm 1.2408 (1.2365)	mem 23874MB
[2022-11-11 11:47:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][300/1251]	eta 0:11:55 lr 0.000945	time 0.7376 (0.7521)	loss 3.1652 (3.7576)	grad_norm 1.1994 (1.2400)	mem 23874MB
[2022-11-11 11:48:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][350/1251]	eta 0:11:16 lr 0.000945	time 0.7326 (0.7509)	loss 2.7779 (3.7528)	grad_norm 1.2838 (1.2457)	mem 23874MB
[2022-11-11 11:48:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][400/1251]	eta 0:10:38 lr 0.000945	time 0.7372 (0.7503)	loss 3.8834 (3.7528)	grad_norm 1.4366 (1.2473)	mem 23874MB
[2022-11-11 11:49:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][450/1251]	eta 0:10:00 lr 0.000945	time 0.7384 (0.7495)	loss 3.6836 (3.7455)	grad_norm 1.2020 (1.2484)	mem 23874MB
[2022-11-11 11:50:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][500/1251]	eta 0:09:22 lr 0.000945	time 0.7499 (0.7493)	loss 2.7198 (3.7398)	grad_norm 1.2412 (1.2468)	mem 23874MB
[2022-11-11 11:50:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][550/1251]	eta 0:08:44 lr 0.000945	time 0.7456 (0.7488)	loss 3.6894 (3.7441)	grad_norm 1.1357 (1.2453)	mem 23874MB
[2022-11-11 11:51:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][600/1251]	eta 0:08:07 lr 0.000945	time 0.8136 (0.7489)	loss 3.9010 (3.7365)	grad_norm 1.1690 (1.2456)	mem 23874MB
[2022-11-11 11:51:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][650/1251]	eta 0:07:29 lr 0.000945	time 0.7377 (0.7485)	loss 4.2440 (3.7351)	grad_norm 1.0699 (1.2462)	mem 23874MB
[2022-11-11 11:52:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][700/1251]	eta 0:06:52 lr 0.000945	time 0.7380 (0.7484)	loss 3.9602 (3.7351)	grad_norm 1.1609 (1.2465)	mem 23874MB
[2022-11-11 11:53:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][750/1251]	eta 0:06:14 lr 0.000945	time 0.7434 (0.7479)	loss 4.5408 (3.7281)	grad_norm 1.2261 (1.2497)	mem 23874MB
[2022-11-11 11:53:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][800/1251]	eta 0:05:37 lr 0.000945	time 0.7398 (0.7480)	loss 4.1784 (3.7298)	grad_norm 1.2425 (1.2512)	mem 23874MB
[2022-11-11 11:54:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][850/1251]	eta 0:04:59 lr 0.000944	time 0.7342 (0.7477)	loss 4.1579 (3.7285)	grad_norm 1.2160 (1.2506)	mem 23874MB
[2022-11-11 11:55:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][900/1251]	eta 0:04:22 lr 0.000944	time 0.7347 (0.7476)	loss 3.8548 (3.7230)	grad_norm 1.1934 (1.2523)	mem 23874MB
[2022-11-11 11:55:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][950/1251]	eta 0:03:44 lr 0.000944	time 0.7344 (0.7475)	loss 4.2205 (3.7177)	grad_norm 1.2454 (1.2509)	mem 23874MB
[2022-11-11 11:56:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][1000/1251]	eta 0:03:07 lr 0.000944	time 0.8091 (0.7476)	loss 4.4453 (3.7177)	grad_norm 1.2862 (1.2503)	mem 23874MB
[2022-11-11 11:56:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][1050/1251]	eta 0:02:30 lr 0.000944	time 0.7368 (0.7473)	loss 3.4940 (3.7228)	grad_norm 1.2136 (1.2496)	mem 23874MB
[2022-11-11 11:57:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][1100/1251]	eta 0:01:52 lr 0.000944	time 0.7373 (0.7472)	loss 2.5772 (3.7189)	grad_norm 1.2315 (1.2483)	mem 23874MB
[2022-11-11 11:58:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][1150/1251]	eta 0:01:15 lr 0.000944	time 0.7423 (0.7470)	loss 3.6893 (3.7178)	grad_norm 1.2839 (1.2479)	mem 23874MB
[2022-11-11 11:58:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][1200/1251]	eta 0:00:38 lr 0.000944	time 0.7369 (0.7471)	loss 4.2649 (3.7127)	grad_norm 1.3021 (1.2484)	mem 23874MB
[2022-11-11 11:59:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [45/300][1250/1251]	eta 0:00:00 lr 0.000944	time 0.7258 (0.7468)	loss 3.9447 (3.7149)	grad_norm 1.1819 (1.2487)	mem 23874MB
[2022-11-11 11:59:21 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 45 training takes 0:15:34
[2022-11-11 11:59:21 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_45.pth saving......
[2022-11-11 11:59:22 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_45.pth saved !!!
[2022-11-11 11:59:24 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.663 (1.663)	Loss 1.1050 (1.1050)	Acc@1 73.926 (73.926)	Acc@5 92.285 (92.285)	Mem 23874MB
[2022-11-11 11:59:34 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.456 Acc@5 92.162
[2022-11-11 11:59:34 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.5%
[2022-11-11 11:59:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.850 (1.850)	Loss 0.9860 (0.9860)	Acc@1 75.977 (75.977)	Acc@5 93.066 (93.066)	Mem 23874MB
[2022-11-11 11:59:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.110 Acc@5 92.758
[2022-11-11 11:59:47 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 75.1%
[2022-11-11 11:59:47 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 75.11% at 45 epoch
[2022-11-11 11:59:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][0/1251]	eta 0:50:07 lr 0.000944	time 2.4044 (2.4044)	loss 2.8362 (2.8362)	grad_norm 1.2405 (1.2405)	mem 23874MB
[2022-11-11 12:00:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][50/1251]	eta 0:15:34 lr 0.000944	time 0.7395 (0.7779)	loss 3.2859 (3.7041)	grad_norm 1.1768 (1.2503)	mem 23874MB
[2022-11-11 12:01:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][100/1251]	eta 0:14:39 lr 0.000943	time 0.7384 (0.7638)	loss 4.0977 (3.8000)	grad_norm 1.3930 (1.2684)	mem 23874MB
[2022-11-11 12:01:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][150/1251]	eta 0:13:55 lr 0.000943	time 0.7465 (0.7584)	loss 3.8454 (3.7652)	grad_norm 1.1458 (1.2643)	mem 23874MB
[2022-11-11 12:02:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][200/1251]	eta 0:13:14 lr 0.000943	time 0.7390 (0.7559)	loss 3.9986 (3.7396)	grad_norm 1.1864 (1.2636)	mem 23874MB
[2022-11-11 12:02:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][250/1251]	eta 0:12:34 lr 0.000943	time 0.7362 (0.7540)	loss 3.3119 (3.7433)	grad_norm 1.0914 (1.2571)	mem 23874MB
[2022-11-11 12:03:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][300/1251]	eta 0:11:55 lr 0.000943	time 0.7399 (0.7526)	loss 4.1895 (3.7280)	grad_norm 1.1058 (1.2554)	mem 23874MB
[2022-11-11 12:04:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][350/1251]	eta 0:11:17 lr 0.000943	time 0.7372 (0.7520)	loss 3.3074 (3.6995)	grad_norm 1.4713 (1.2542)	mem 23874MB
[2022-11-11 12:04:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][400/1251]	eta 0:10:39 lr 0.000943	time 0.7338 (0.7515)	loss 4.2227 (3.6947)	grad_norm 1.3085 (1.2528)	mem 23874MB
[2022-11-11 12:05:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][450/1251]	eta 0:10:01 lr 0.000943	time 0.7390 (0.7506)	loss 4.1122 (3.6917)	grad_norm 1.1630 (1.2538)	mem 23874MB
[2022-11-11 12:06:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][500/1251]	eta 0:09:23 lr 0.000943	time 0.7645 (0.7503)	loss 4.2366 (3.7020)	grad_norm 1.0810 (nan)	mem 23874MB
[2022-11-11 12:06:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][550/1251]	eta 0:08:45 lr 0.000943	time 0.7431 (0.7499)	loss 3.9695 (3.7066)	grad_norm 1.1611 (nan)	mem 23874MB
[2022-11-11 12:07:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][600/1251]	eta 0:08:08 lr 0.000943	time 0.7412 (0.7496)	loss 3.1153 (3.7056)	grad_norm 1.4001 (nan)	mem 23874MB
[2022-11-11 12:07:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][650/1251]	eta 0:07:30 lr 0.000942	time 0.7408 (0.7494)	loss 3.6015 (3.7000)	grad_norm 1.1824 (nan)	mem 23874MB
[2022-11-11 12:08:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][700/1251]	eta 0:06:52 lr 0.000942	time 0.7495 (0.7492)	loss 3.4747 (3.7003)	grad_norm 1.0523 (nan)	mem 23874MB
[2022-11-11 12:09:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][750/1251]	eta 0:06:15 lr 0.000942	time 0.7410 (0.7490)	loss 4.3312 (3.7002)	grad_norm 1.1475 (nan)	mem 23874MB
[2022-11-11 12:09:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][800/1251]	eta 0:05:37 lr 0.000942	time 0.7345 (0.7488)	loss 4.3310 (3.7063)	grad_norm 1.3063 (nan)	mem 23874MB
[2022-11-11 12:10:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][850/1251]	eta 0:05:00 lr 0.000942	time 0.7346 (0.7486)	loss 4.2417 (3.6988)	grad_norm 1.1285 (nan)	mem 23874MB
[2022-11-11 12:11:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][900/1251]	eta 0:04:22 lr 0.000942	time 0.7445 (0.7485)	loss 4.2738 (3.7015)	grad_norm 1.3857 (nan)	mem 23874MB
[2022-11-11 12:11:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][950/1251]	eta 0:03:45 lr 0.000942	time 0.7392 (0.7485)	loss 3.7079 (3.7072)	grad_norm 1.3024 (nan)	mem 23874MB
[2022-11-11 12:12:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][1000/1251]	eta 0:03:07 lr 0.000942	time 0.7352 (0.7483)	loss 2.8644 (3.7044)	grad_norm 1.2785 (nan)	mem 23874MB
[2022-11-11 12:12:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][1050/1251]	eta 0:02:30 lr 0.000942	time 0.7419 (0.7484)	loss 4.1699 (3.7010)	grad_norm 1.1711 (nan)	mem 23874MB
[2022-11-11 12:13:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][1100/1251]	eta 0:01:52 lr 0.000942	time 0.7366 (0.7482)	loss 2.4712 (3.6951)	grad_norm 1.1391 (nan)	mem 23874MB
[2022-11-11 12:14:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][1150/1251]	eta 0:01:15 lr 0.000941	time 0.7404 (0.7482)	loss 3.9402 (3.6992)	grad_norm 1.2421 (nan)	mem 23874MB
[2022-11-11 12:14:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][1200/1251]	eta 0:00:38 lr 0.000941	time 0.7388 (0.7481)	loss 4.1070 (3.6989)	grad_norm 1.3354 (nan)	mem 23874MB
[2022-11-11 12:15:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [46/300][1250/1251]	eta 0:00:00 lr 0.000941	time 0.7259 (0.7479)	loss 3.7935 (3.7025)	grad_norm 1.3032 (nan)	mem 23874MB
[2022-11-11 12:15:23 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 46 training takes 0:15:35
[2022-11-11 12:15:23 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_46.pth saving......
[2022-11-11 12:15:24 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_46.pth saved !!!
[2022-11-11 12:15:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.643 (1.643)	Loss 1.1456 (1.1456)	Acc@1 72.559 (72.559)	Acc@5 92.480 (92.480)	Mem 23874MB
[2022-11-11 12:15:36 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.660 Acc@5 92.210
[2022-11-11 12:15:36 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.7%
[2022-11-11 12:15:38 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.967 (1.967)	Loss 1.1400 (1.1400)	Acc@1 71.875 (71.875)	Acc@5 91.211 (91.211)	Mem 23874MB
[2022-11-11 12:15:49 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.378 Acc@5 92.904
[2022-11-11 12:15:49 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 75.4%
[2022-11-11 12:15:49 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 75.38% at 46 epoch
[2022-11-11 12:15:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][0/1251]	eta 0:48:08 lr 0.000941	time 2.3089 (2.3089)	loss 4.1005 (4.1005)	grad_norm 1.1815 (1.1815)	mem 23874MB
[2022-11-11 12:16:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][50/1251]	eta 0:15:38 lr 0.000941	time 0.7399 (0.7813)	loss 3.6951 (3.5980)	grad_norm 1.3355 (1.2609)	mem 23874MB
[2022-11-11 12:17:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][100/1251]	eta 0:14:38 lr 0.000941	time 0.7380 (0.7634)	loss 3.4704 (3.6709)	grad_norm 1.3865 (1.2495)	mem 23874MB
[2022-11-11 12:17:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][150/1251]	eta 0:13:53 lr 0.000941	time 0.7405 (0.7572)	loss 3.7453 (3.6410)	grad_norm 1.1630 (1.2465)	mem 23874MB
[2022-11-11 12:18:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][200/1251]	eta 0:13:12 lr 0.000941	time 0.7403 (0.7542)	loss 3.9387 (3.6743)	grad_norm 1.1742 (1.2442)	mem 23874MB
[2022-11-11 12:18:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][250/1251]	eta 0:12:33 lr 0.000941	time 0.7373 (0.7524)	loss 3.2642 (3.6720)	grad_norm 1.3700 (1.2481)	mem 23874MB
[2022-11-11 12:19:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][300/1251]	eta 0:11:54 lr 0.000941	time 0.7558 (0.7517)	loss 3.8563 (3.6757)	grad_norm 1.1853 (1.2468)	mem 23874MB
[2022-11-11 12:20:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][350/1251]	eta 0:11:16 lr 0.000941	time 0.7428 (0.7511)	loss 3.1684 (3.6830)	grad_norm 1.1467 (1.2467)	mem 23874MB
[2022-11-11 12:20:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][400/1251]	eta 0:10:38 lr 0.000940	time 0.7377 (0.7501)	loss 3.9797 (3.6841)	grad_norm 1.2695 (1.2453)	mem 23874MB
[2022-11-11 12:21:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][450/1251]	eta 0:10:00 lr 0.000940	time 0.7428 (0.7496)	loss 3.7833 (3.6951)	grad_norm 1.2697 (1.2519)	mem 23874MB
[2022-11-11 12:22:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][500/1251]	eta 0:09:22 lr 0.000940	time 0.7367 (0.7489)	loss 3.5843 (3.6799)	grad_norm 1.1377 (1.2561)	mem 23874MB
[2022-11-11 12:22:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][550/1251]	eta 0:08:44 lr 0.000940	time 0.7435 (0.7487)	loss 4.2043 (3.6906)	grad_norm 1.2920 (1.2555)	mem 23874MB
[2022-11-11 12:23:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][600/1251]	eta 0:08:07 lr 0.000940	time 0.7346 (0.7484)	loss 3.1009 (3.7024)	grad_norm 1.1082 (1.2548)	mem 23874MB
[2022-11-11 12:23:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][650/1251]	eta 0:07:29 lr 0.000940	time 0.7398 (0.7481)	loss 3.2341 (3.7068)	grad_norm 1.2144 (1.2539)	mem 23874MB
[2022-11-11 12:24:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][700/1251]	eta 0:06:52 lr 0.000940	time 0.7690 (0.7480)	loss 3.9742 (3.6958)	grad_norm 1.0712 (1.2542)	mem 23874MB
[2022-11-11 12:25:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][750/1251]	eta 0:06:14 lr 0.000940	time 0.7416 (0.7480)	loss 4.0420 (3.7026)	grad_norm 1.1733 (1.2551)	mem 23874MB
[2022-11-11 12:25:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][800/1251]	eta 0:05:37 lr 0.000940	time 0.7341 (0.7477)	loss 4.2038 (3.7075)	grad_norm 1.1572 (1.2548)	mem 23874MB
[2022-11-11 12:26:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][850/1251]	eta 0:04:59 lr 0.000940	time 0.8046 (0.7476)	loss 2.8315 (3.7091)	grad_norm 1.2573 (1.2540)	mem 23874MB
[2022-11-11 12:27:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][900/1251]	eta 0:04:22 lr 0.000939	time 0.7364 (0.7473)	loss 2.8237 (3.6958)	grad_norm 1.3095 (1.2516)	mem 23874MB
[2022-11-11 12:27:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][950/1251]	eta 0:03:44 lr 0.000939	time 0.7355 (0.7471)	loss 3.7644 (3.6952)	grad_norm 1.2405 (1.2543)	mem 23874MB
[2022-11-11 12:28:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][1000/1251]	eta 0:03:07 lr 0.000939	time 0.7410 (0.7472)	loss 4.4663 (3.6920)	grad_norm 1.4132 (1.2557)	mem 23874MB
[2022-11-11 12:28:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][1050/1251]	eta 0:02:30 lr 0.000939	time 0.7408 (0.7471)	loss 4.1826 (3.6930)	grad_norm 1.3635 (1.2549)	mem 23874MB
[2022-11-11 12:29:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][1100/1251]	eta 0:01:52 lr 0.000939	time 0.8350 (0.7471)	loss 3.7831 (3.6950)	grad_norm 1.2576 (1.2551)	mem 23874MB
[2022-11-11 12:30:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][1150/1251]	eta 0:01:15 lr 0.000939	time 0.7617 (0.7471)	loss 3.9735 (3.6938)	grad_norm 1.2072 (1.2552)	mem 23874MB
[2022-11-11 12:30:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][1200/1251]	eta 0:00:38 lr 0.000939	time 0.7374 (0.7470)	loss 3.7144 (3.6990)	grad_norm 1.2842 (1.2549)	mem 23874MB
[2022-11-11 12:31:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [47/300][1250/1251]	eta 0:00:00 lr 0.000939	time 0.7265 (0.7467)	loss 3.5163 (3.7001)	grad_norm 1.2003 (1.2553)	mem 23874MB
[2022-11-11 12:31:23 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 47 training takes 0:15:34
[2022-11-11 12:31:24 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_47.pth saving......
[2022-11-11 12:31:25 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_47.pth saved !!!
[2022-11-11 12:31:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.691 (1.691)	Loss 1.1674 (1.1674)	Acc@1 74.316 (74.316)	Acc@5 91.992 (91.992)	Mem 23874MB
[2022-11-11 12:31:37 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.918 Acc@5 92.228
[2022-11-11 12:31:37 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.9%
[2022-11-11 12:31:39 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.877 (1.877)	Loss 0.9952 (0.9952)	Acc@1 76.074 (76.074)	Acc@5 92.480 (92.480)	Mem 23874MB
[2022-11-11 12:31:50 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.538 Acc@5 92.992
[2022-11-11 12:31:50 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 75.5%
[2022-11-11 12:31:50 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 75.54% at 47 epoch
[2022-11-11 12:31:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][0/1251]	eta 0:49:33 lr 0.000939	time 2.3771 (2.3771)	loss 4.0056 (4.0056)	grad_norm 1.3814 (1.3814)	mem 23874MB
[2022-11-11 12:32:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][50/1251]	eta 0:15:38 lr 0.000939	time 0.7411 (0.7811)	loss 3.9972 (3.7389)	grad_norm 1.0771 (1.2832)	mem 23874MB
[2022-11-11 12:33:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][100/1251]	eta 0:14:40 lr 0.000939	time 0.7394 (0.7653)	loss 2.8653 (3.7442)	grad_norm 1.4251 (1.2797)	mem 23874MB
[2022-11-11 12:33:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][150/1251]	eta 0:13:55 lr 0.000938	time 0.7431 (0.7586)	loss 3.5809 (3.7209)	grad_norm 1.2269 (1.2703)	mem 23874MB
[2022-11-11 12:34:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][200/1251]	eta 0:13:15 lr 0.000938	time 0.7449 (0.7567)	loss 4.0799 (3.7283)	grad_norm 1.2697 (1.2616)	mem 23874MB
[2022-11-11 12:34:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][250/1251]	eta 0:12:35 lr 0.000938	time 0.7393 (0.7545)	loss 2.9387 (3.7010)	grad_norm 1.4866 (1.2656)	mem 23874MB
[2022-11-11 12:35:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][300/1251]	eta 0:11:56 lr 0.000938	time 0.7434 (0.7535)	loss 3.8923 (3.6955)	grad_norm 1.4391 (1.2659)	mem 23874MB
[2022-11-11 12:36:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][350/1251]	eta 0:11:18 lr 0.000938	time 0.7352 (0.7527)	loss 3.3277 (3.6991)	grad_norm 1.1954 (1.2670)	mem 23874MB
[2022-11-11 12:36:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][400/1251]	eta 0:10:39 lr 0.000938	time 0.7415 (0.7518)	loss 3.8375 (3.6948)	grad_norm 1.3075 (1.2681)	mem 23874MB
[2022-11-11 12:37:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][450/1251]	eta 0:10:01 lr 0.000938	time 0.7395 (0.7514)	loss 3.2819 (3.6835)	grad_norm 1.1911 (1.2662)	mem 23874MB
[2022-11-11 12:38:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][500/1251]	eta 0:09:24 lr 0.000938	time 0.8155 (0.7510)	loss 4.0917 (3.6981)	grad_norm 1.2694 (1.2645)	mem 23874MB
[2022-11-11 12:38:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][550/1251]	eta 0:08:46 lr 0.000938	time 0.7432 (0.7504)	loss 2.6255 (3.6986)	grad_norm 1.1828 (1.2644)	mem 23874MB
[2022-11-11 12:39:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][600/1251]	eta 0:08:08 lr 0.000938	time 0.8163 (0.7504)	loss 3.9614 (3.7009)	grad_norm 1.2029 (1.2624)	mem 23874MB
[2022-11-11 12:39:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][650/1251]	eta 0:07:30 lr 0.000937	time 0.7365 (0.7498)	loss 3.4584 (3.7051)	grad_norm 1.2080 (1.2611)	mem 23874MB
[2022-11-11 12:40:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][700/1251]	eta 0:06:53 lr 0.000937	time 0.7440 (0.7497)	loss 3.9877 (3.7079)	grad_norm 1.2520 (1.2603)	mem 23874MB
[2022-11-11 12:41:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][750/1251]	eta 0:06:15 lr 0.000937	time 0.8168 (0.7498)	loss 4.4416 (3.7066)	grad_norm 1.4859 (1.2623)	mem 23874MB
[2022-11-11 12:41:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][800/1251]	eta 0:05:37 lr 0.000937	time 0.7357 (0.7494)	loss 3.7407 (3.7139)	grad_norm 1.2335 (1.2619)	mem 23874MB
[2022-11-11 12:42:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][850/1251]	eta 0:05:00 lr 0.000937	time 0.7399 (0.7494)	loss 3.2396 (3.7056)	grad_norm 1.4026 (1.2620)	mem 23874MB
[2022-11-11 12:43:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][900/1251]	eta 0:04:22 lr 0.000937	time 0.7374 (0.7492)	loss 3.7338 (3.7045)	grad_norm 1.1270 (1.2614)	mem 23874MB
[2022-11-11 12:43:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][950/1251]	eta 0:03:45 lr 0.000937	time 0.7426 (0.7491)	loss 3.8775 (3.7065)	grad_norm 1.3647 (1.2611)	mem 23874MB
[2022-11-11 12:44:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][1000/1251]	eta 0:03:07 lr 0.000937	time 0.7468 (0.7490)	loss 3.4673 (3.7039)	grad_norm 1.1046 (1.2618)	mem 23874MB
[2022-11-11 12:44:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][1050/1251]	eta 0:02:30 lr 0.000937	time 0.7447 (0.7488)	loss 3.3288 (3.6990)	grad_norm 1.2999 (1.2623)	mem 23874MB
[2022-11-11 12:45:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][1100/1251]	eta 0:01:53 lr 0.000937	time 0.7409 (0.7487)	loss 3.9341 (3.7006)	grad_norm 1.2616 (1.2631)	mem 23874MB
[2022-11-11 12:46:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][1150/1251]	eta 0:01:15 lr 0.000936	time 0.7459 (0.7486)	loss 3.6401 (3.6947)	grad_norm 1.2426 (nan)	mem 23874MB
[2022-11-11 12:46:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][1200/1251]	eta 0:00:38 lr 0.000936	time 0.7329 (0.7484)	loss 3.2841 (3.6979)	grad_norm 1.2449 (nan)	mem 23874MB
[2022-11-11 12:47:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [48/300][1250/1251]	eta 0:00:00 lr 0.000936	time 0.7247 (0.7483)	loss 3.6432 (3.6965)	grad_norm 1.1504 (nan)	mem 23874MB
[2022-11-11 12:47:26 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 48 training takes 0:15:36
[2022-11-11 12:47:26 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_48.pth saving......
[2022-11-11 12:47:28 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_48.pth saved !!!
[2022-11-11 12:47:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.595 (1.595)	Loss 1.1026 (1.1026)	Acc@1 73.242 (73.242)	Acc@5 92.773 (92.773)	Mem 23874MB
[2022-11-11 12:47:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.912 Acc@5 92.334
[2022-11-11 12:47:40 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.9%
[2022-11-11 12:47:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.810 (1.810)	Loss 1.0400 (1.0400)	Acc@1 75.000 (75.000)	Acc@5 91.992 (91.992)	Mem 23874MB
[2022-11-11 12:47:52 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.738 Acc@5 93.102
[2022-11-11 12:47:52 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 75.7%
[2022-11-11 12:47:52 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 75.74% at 48 epoch
[2022-11-11 12:47:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][0/1251]	eta 0:48:38 lr 0.000936	time 2.3328 (2.3328)	loss 3.9319 (3.9319)	grad_norm 1.1871 (1.1871)	mem 23874MB
[2022-11-11 12:48:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][50/1251]	eta 0:15:37 lr 0.000936	time 0.7398 (0.7808)	loss 2.6803 (3.6885)	grad_norm 1.3522 (1.2762)	mem 23874MB
[2022-11-11 12:49:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][100/1251]	eta 0:14:37 lr 0.000936	time 0.7372 (0.7625)	loss 3.1495 (3.6208)	grad_norm 1.2472 (1.2665)	mem 23874MB
[2022-11-11 12:49:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][150/1251]	eta 0:13:54 lr 0.000936	time 0.7357 (0.7580)	loss 3.7395 (3.6242)	grad_norm 1.1373 (1.2536)	mem 23874MB
[2022-11-11 12:50:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][200/1251]	eta 0:13:13 lr 0.000936	time 0.7366 (0.7548)	loss 3.6416 (3.6165)	grad_norm 1.2370 (1.2571)	mem 23874MB
[2022-11-11 12:51:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][250/1251]	eta 0:12:34 lr 0.000936	time 0.7401 (0.7534)	loss 3.8764 (3.6178)	grad_norm 1.2993 (1.2554)	mem 23874MB
[2022-11-11 12:51:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][300/1251]	eta 0:11:55 lr 0.000936	time 0.7362 (0.7519)	loss 3.5344 (3.6219)	grad_norm 1.1946 (1.2498)	mem 23874MB
[2022-11-11 12:52:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][350/1251]	eta 0:11:16 lr 0.000936	time 0.7443 (0.7513)	loss 3.7799 (3.6201)	grad_norm 1.3199 (1.2530)	mem 23874MB
[2022-11-11 12:52:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][400/1251]	eta 0:10:38 lr 0.000935	time 0.7351 (0.7505)	loss 3.8905 (3.6345)	grad_norm 1.2573 (1.2503)	mem 23874MB
[2022-11-11 12:53:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][450/1251]	eta 0:10:00 lr 0.000935	time 0.7406 (0.7502)	loss 3.8993 (3.6455)	grad_norm 1.2302 (1.2467)	mem 23874MB
[2022-11-11 12:54:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][500/1251]	eta 0:09:23 lr 0.000935	time 0.7228 (0.7497)	loss 3.2286 (3.6400)	grad_norm 1.1108 (1.2471)	mem 23874MB
[2022-11-11 12:54:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][550/1251]	eta 0:08:45 lr 0.000935	time 0.7361 (0.7492)	loss 3.7812 (3.6400)	grad_norm 1.2350 (1.2456)	mem 23874MB
[2022-11-11 12:55:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][600/1251]	eta 0:08:07 lr 0.000935	time 0.7348 (0.7490)	loss 3.9319 (3.6467)	grad_norm 1.1638 (1.2459)	mem 23874MB
[2022-11-11 12:56:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][650/1251]	eta 0:07:30 lr 0.000935	time 0.7362 (0.7488)	loss 3.8018 (3.6585)	grad_norm 1.2839 (1.2463)	mem 23874MB
[2022-11-11 12:56:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][700/1251]	eta 0:06:52 lr 0.000935	time 0.7400 (0.7487)	loss 3.5509 (3.6538)	grad_norm 1.2669 (1.2468)	mem 23874MB
[2022-11-11 12:57:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][750/1251]	eta 0:06:15 lr 0.000935	time 0.7355 (0.7486)	loss 4.0153 (3.6566)	grad_norm 1.3189 (1.2477)	mem 23874MB
[2022-11-11 12:57:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][800/1251]	eta 0:05:37 lr 0.000935	time 0.7379 (0.7483)	loss 3.7684 (3.6539)	grad_norm 1.2582 (1.2481)	mem 23874MB
[2022-11-11 12:58:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][850/1251]	eta 0:04:59 lr 0.000935	time 0.7294 (0.7481)	loss 4.0957 (3.6474)	grad_norm 1.2641 (1.2491)	mem 23874MB
[2022-11-11 12:59:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][900/1251]	eta 0:04:22 lr 0.000934	time 0.7422 (0.7480)	loss 2.6289 (3.6521)	grad_norm 1.2691 (1.2503)	mem 23874MB
[2022-11-11 12:59:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][950/1251]	eta 0:03:45 lr 0.000934	time 0.7406 (0.7480)	loss 3.4453 (3.6516)	grad_norm 1.2637 (1.2506)	mem 23874MB
[2022-11-11 13:00:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][1000/1251]	eta 0:03:07 lr 0.000934	time 0.7352 (0.7479)	loss 4.2899 (3.6491)	grad_norm 1.2475 (1.2511)	mem 23874MB
[2022-11-11 13:00:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][1050/1251]	eta 0:02:30 lr 0.000934	time 0.7507 (0.7478)	loss 3.7953 (3.6493)	grad_norm 1.1516 (1.2522)	mem 23874MB
[2022-11-11 13:01:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][1100/1251]	eta 0:01:52 lr 0.000934	time 0.7356 (0.7476)	loss 3.5513 (3.6486)	grad_norm 1.3556 (1.2529)	mem 23874MB
[2022-11-11 13:02:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][1150/1251]	eta 0:01:15 lr 0.000934	time 0.7361 (0.7477)	loss 4.2190 (3.6488)	grad_norm 1.3704 (1.2530)	mem 23874MB
[2022-11-11 13:02:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][1200/1251]	eta 0:00:38 lr 0.000934	time 0.7377 (0.7477)	loss 4.7031 (3.6493)	grad_norm 1.3609 (1.2529)	mem 23874MB
[2022-11-11 13:03:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [49/300][1250/1251]	eta 0:00:00 lr 0.000934	time 0.7286 (0.7475)	loss 3.9784 (3.6533)	grad_norm 1.1856 (1.2530)	mem 23874MB
[2022-11-11 13:03:28 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 49 training takes 0:15:35
[2022-11-11 13:03:28 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_49.pth saving......
[2022-11-11 13:03:29 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_49.pth saved !!!
[2022-11-11 13:03:31 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.563 (1.563)	Loss 1.0876 (1.0876)	Acc@1 75.000 (75.000)	Acc@5 93.457 (93.457)	Mem 23874MB
[2022-11-11 13:03:41 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.514 Acc@5 92.268
[2022-11-11 13:03:41 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.5%
[2022-11-11 13:03:43 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.877 (1.877)	Loss 0.9602 (0.9602)	Acc@1 75.879 (75.879)	Acc@5 92.969 (92.969)	Mem 23874MB
[2022-11-11 13:03:54 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.982 Acc@5 93.200
[2022-11-11 13:03:54 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.0%
[2022-11-11 13:03:54 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 75.98% at 49 epoch
[2022-11-11 13:03:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][0/1251]	eta 0:49:46 lr 0.000934	time 2.3871 (2.3871)	loss 3.0839 (3.0839)	grad_norm 1.1726 (1.1726)	mem 23874MB
[2022-11-11 13:04:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][50/1251]	eta 0:15:36 lr 0.000934	time 0.7372 (0.7799)	loss 4.1750 (3.5694)	grad_norm 1.3086 (1.2451)	mem 23874MB
[2022-11-11 13:05:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][100/1251]	eta 0:14:39 lr 0.000933	time 0.7373 (0.7642)	loss 3.7098 (3.5808)	grad_norm 1.2862 (1.2608)	mem 23874MB
[2022-11-11 13:05:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][150/1251]	eta 0:13:56 lr 0.000933	time 0.8288 (0.7596)	loss 3.8801 (3.6517)	grad_norm 1.2406 (1.2611)	mem 23874MB
[2022-11-11 13:06:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][200/1251]	eta 0:13:14 lr 0.000933	time 0.7514 (0.7560)	loss 3.5820 (3.6595)	grad_norm 1.2942 (1.2661)	mem 23874MB
[2022-11-11 13:07:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][250/1251]	eta 0:12:35 lr 0.000933	time 0.7390 (0.7547)	loss 3.7521 (3.6627)	grad_norm 1.2500 (1.2648)	mem 23874MB
[2022-11-11 13:07:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][300/1251]	eta 0:11:56 lr 0.000933	time 0.7418 (0.7534)	loss 2.8203 (3.6499)	grad_norm 1.1697 (1.2613)	mem 23874MB
[2022-11-11 13:08:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][350/1251]	eta 0:11:17 lr 0.000933	time 0.7418 (0.7523)	loss 2.9790 (3.6827)	grad_norm 1.1752 (1.2628)	mem 23874MB
[2022-11-11 13:08:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][400/1251]	eta 0:10:39 lr 0.000933	time 0.7384 (0.7518)	loss 3.5236 (3.6791)	grad_norm 1.3550 (1.2615)	mem 23874MB
[2022-11-11 13:09:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][450/1251]	eta 0:10:02 lr 0.000933	time 0.7383 (0.7519)	loss 3.1409 (3.6793)	grad_norm 1.3094 (1.2623)	mem 23874MB
[2022-11-11 13:10:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][500/1251]	eta 0:09:24 lr 0.000933	time 0.7424 (0.7510)	loss 3.9802 (3.6731)	grad_norm 1.3532 (1.2635)	mem 23874MB
[2022-11-11 13:10:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][550/1251]	eta 0:08:46 lr 0.000933	time 0.8160 (0.7508)	loss 3.8317 (3.6775)	grad_norm 1.2300 (1.2636)	mem 23874MB
[2022-11-11 13:11:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][600/1251]	eta 0:08:08 lr 0.000932	time 0.8139 (0.7505)	loss 3.3704 (3.6739)	grad_norm 1.4575 (1.2654)	mem 23874MB
[2022-11-11 13:12:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][650/1251]	eta 0:07:30 lr 0.000932	time 0.7406 (0.7501)	loss 3.1231 (3.6675)	grad_norm 1.1351 (1.2630)	mem 23874MB
[2022-11-11 13:12:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][700/1251]	eta 0:06:53 lr 0.000932	time 0.7406 (0.7502)	loss 3.0899 (3.6676)	grad_norm 1.2496 (1.2630)	mem 23874MB
[2022-11-11 13:13:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][750/1251]	eta 0:06:15 lr 0.000932	time 0.7385 (0.7500)	loss 2.3401 (3.6694)	grad_norm 1.1639 (1.2642)	mem 23874MB
[2022-11-11 13:13:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][800/1251]	eta 0:05:38 lr 0.000932	time 0.7430 (0.7499)	loss 2.7249 (3.6807)	grad_norm 1.1797 (1.2646)	mem 23874MB
[2022-11-11 13:14:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][850/1251]	eta 0:05:00 lr 0.000932	time 0.7389 (0.7498)	loss 3.6239 (3.6822)	grad_norm 1.1726 (1.2641)	mem 23874MB
[2022-11-11 13:15:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][900/1251]	eta 0:04:23 lr 0.000932	time 0.7466 (0.7496)	loss 4.4060 (3.6844)	grad_norm 1.2959 (1.2648)	mem 23874MB
[2022-11-11 13:15:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][950/1251]	eta 0:03:45 lr 0.000932	time 0.7382 (0.7496)	loss 3.8093 (3.6809)	grad_norm 1.1484 (1.2648)	mem 23874MB
[2022-11-11 13:16:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][1000/1251]	eta 0:03:08 lr 0.000932	time 0.8246 (0.7495)	loss 3.2371 (3.6795)	grad_norm 1.1451 (nan)	mem 23874MB
[2022-11-11 13:17:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][1050/1251]	eta 0:02:30 lr 0.000931	time 0.7394 (0.7495)	loss 3.8219 (3.6773)	grad_norm 1.1766 (nan)	mem 23874MB
[2022-11-11 13:17:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][1100/1251]	eta 0:01:53 lr 0.000931	time 0.7384 (0.7494)	loss 3.6932 (3.6748)	grad_norm 1.3321 (nan)	mem 23874MB
[2022-11-11 13:18:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][1150/1251]	eta 0:01:15 lr 0.000931	time 0.7475 (0.7494)	loss 4.1160 (3.6787)	grad_norm 1.2494 (nan)	mem 23874MB
[2022-11-11 13:18:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][1200/1251]	eta 0:00:38 lr 0.000931	time 0.7366 (0.7493)	loss 3.4122 (3.6806)	grad_norm 1.3166 (nan)	mem 23874MB
[2022-11-11 13:19:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [50/300][1250/1251]	eta 0:00:00 lr 0.000931	time 0.7361 (0.7492)	loss 4.1999 (3.6822)	grad_norm 1.2869 (nan)	mem 23874MB
[2022-11-11 13:19:31 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 50 training takes 0:15:37
[2022-11-11 13:19:32 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_50.pth saving......
[2022-11-11 13:19:33 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_50.pth saved !!!
[2022-11-11 13:19:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.630 (1.630)	Loss 1.1990 (1.1990)	Acc@1 71.680 (71.680)	Acc@5 90.918 (90.918)	Mem 23874MB
[2022-11-11 13:19:45 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 73.930 Acc@5 92.276
[2022-11-11 13:19:45 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 73.9%
[2022-11-11 13:19:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.957 (1.957)	Loss 1.1101 (1.1101)	Acc@1 72.070 (72.070)	Acc@5 92.188 (92.188)	Mem 23874MB
[2022-11-11 13:19:58 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.206 Acc@5 93.242
[2022-11-11 13:19:58 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.2%
[2022-11-11 13:19:58 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.21% at 50 epoch
[2022-11-11 13:20:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][0/1251]	eta 0:49:17 lr 0.000931	time 2.3639 (2.3639)	loss 2.3183 (2.3183)	grad_norm 1.1776 (1.1776)	mem 23874MB
[2022-11-11 13:20:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][50/1251]	eta 0:15:43 lr 0.000931	time 0.8244 (0.7855)	loss 3.9352 (3.6465)	grad_norm 1.4074 (1.2259)	mem 23874MB
[2022-11-11 13:21:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][100/1251]	eta 0:14:40 lr 0.000931	time 0.7582 (0.7651)	loss 3.0530 (3.6755)	grad_norm 1.2422 (1.2440)	mem 23874MB
[2022-11-11 13:21:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][150/1251]	eta 0:13:56 lr 0.000931	time 0.7371 (0.7598)	loss 4.3111 (3.6575)	grad_norm 1.2756 (1.2552)	mem 23874MB
[2022-11-11 13:22:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][200/1251]	eta 0:13:15 lr 0.000931	time 0.8097 (0.7566)	loss 4.4791 (3.6753)	grad_norm 1.2730 (1.2554)	mem 23874MB
[2022-11-11 13:23:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][250/1251]	eta 0:12:35 lr 0.000931	time 0.7395 (0.7548)	loss 2.3235 (3.6617)	grad_norm 1.2151 (1.2545)	mem 23874MB
[2022-11-11 13:23:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][300/1251]	eta 0:11:56 lr 0.000930	time 0.7433 (0.7531)	loss 3.6836 (3.6701)	grad_norm 1.4569 (1.2572)	mem 23874MB
[2022-11-11 13:24:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][350/1251]	eta 0:11:17 lr 0.000930	time 0.7413 (0.7523)	loss 3.2210 (3.6493)	grad_norm 1.2489 (1.2542)	mem 23874MB
[2022-11-11 13:24:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][400/1251]	eta 0:10:39 lr 0.000930	time 0.7364 (0.7517)	loss 3.8195 (3.6448)	grad_norm 1.1581 (1.2544)	mem 23874MB
[2022-11-11 13:25:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][450/1251]	eta 0:10:01 lr 0.000930	time 0.7397 (0.7512)	loss 4.1654 (3.6513)	grad_norm 1.2543 (1.2526)	mem 23874MB
[2022-11-11 13:26:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][500/1251]	eta 0:09:23 lr 0.000930	time 0.7454 (0.7507)	loss 4.0887 (3.6521)	grad_norm 1.4162 (1.2541)	mem 23874MB
[2022-11-11 13:26:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][550/1251]	eta 0:08:46 lr 0.000930	time 0.8171 (0.7505)	loss 2.7833 (3.6526)	grad_norm 1.2958 (1.2566)	mem 23874MB
[2022-11-11 13:27:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][600/1251]	eta 0:08:08 lr 0.000930	time 0.8052 (0.7501)	loss 3.6779 (3.6530)	grad_norm 1.1234 (1.2577)	mem 23874MB
[2022-11-11 13:28:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][650/1251]	eta 0:07:30 lr 0.000930	time 0.7369 (0.7499)	loss 3.6101 (3.6530)	grad_norm 1.3663 (1.2567)	mem 23874MB
[2022-11-11 13:28:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][700/1251]	eta 0:06:53 lr 0.000930	time 0.7366 (0.7497)	loss 2.7158 (3.6561)	grad_norm 1.2325 (1.2568)	mem 23874MB
[2022-11-11 13:29:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][750/1251]	eta 0:06:15 lr 0.000929	time 0.7359 (0.7496)	loss 3.7317 (3.6493)	grad_norm 1.1861 (1.2572)	mem 23874MB
[2022-11-11 13:29:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][800/1251]	eta 0:05:38 lr 0.000929	time 0.7362 (0.7495)	loss 3.4006 (3.6502)	grad_norm 1.2539 (1.2570)	mem 23874MB
[2022-11-11 13:30:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][850/1251]	eta 0:05:00 lr 0.000929	time 0.7485 (0.7494)	loss 3.5188 (3.6534)	grad_norm 1.2279 (1.2569)	mem 23874MB
[2022-11-11 13:31:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][900/1251]	eta 0:04:23 lr 0.000929	time 0.7376 (0.7493)	loss 3.5925 (3.6500)	grad_norm 1.2544 (1.2576)	mem 23874MB
[2022-11-11 13:31:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][950/1251]	eta 0:03:45 lr 0.000929	time 0.8157 (0.7492)	loss 2.9461 (3.6500)	grad_norm 1.3724 (1.2569)	mem 23874MB
[2022-11-11 13:32:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][1000/1251]	eta 0:03:08 lr 0.000929	time 0.8125 (0.7492)	loss 3.0517 (3.6448)	grad_norm 1.3177 (1.2570)	mem 23874MB
[2022-11-11 13:33:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][1050/1251]	eta 0:02:30 lr 0.000929	time 0.7387 (0.7490)	loss 4.3174 (3.6414)	grad_norm 1.2677 (1.2575)	mem 23874MB
[2022-11-11 13:33:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][1100/1251]	eta 0:01:53 lr 0.000929	time 0.7341 (0.7490)	loss 4.0823 (3.6469)	grad_norm 1.2778 (1.2576)	mem 23874MB
[2022-11-11 13:34:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][1150/1251]	eta 0:01:15 lr 0.000929	time 0.7370 (0.7488)	loss 3.5276 (3.6482)	grad_norm 1.2263 (1.2581)	mem 23874MB
[2022-11-11 13:34:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][1200/1251]	eta 0:00:38 lr 0.000929	time 0.7362 (0.7489)	loss 3.2296 (3.6482)	grad_norm 1.2983 (1.2588)	mem 23874MB
[2022-11-11 13:35:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [51/300][1250/1251]	eta 0:00:00 lr 0.000928	time 0.7250 (0.7486)	loss 4.1566 (3.6485)	grad_norm 1.3359 (1.2602)	mem 23874MB
[2022-11-11 13:35:34 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 51 training takes 0:15:36
[2022-11-11 13:35:34 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_51.pth saving......
[2022-11-11 13:35:35 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_51.pth saved !!!
[2022-11-11 13:35:37 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.667 (1.667)	Loss 1.1169 (1.1169)	Acc@1 72.949 (72.949)	Acc@5 91.406 (91.406)	Mem 23874MB
[2022-11-11 13:35:48 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.304 Acc@5 92.432
[2022-11-11 13:35:48 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.3%
[2022-11-11 13:35:50 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.832 (1.832)	Loss 0.9981 (0.9981)	Acc@1 76.855 (76.855)	Acc@5 92.773 (92.773)	Mem 23874MB
[2022-11-11 13:36:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.354 Acc@5 93.356
[2022-11-11 13:36:00 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.4%
[2022-11-11 13:36:00 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.35% at 51 epoch
[2022-11-11 13:36:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][0/1251]	eta 0:50:23 lr 0.000928	time 2.4168 (2.4168)	loss 3.1433 (3.1433)	grad_norm 1.3186 (1.3186)	mem 23874MB
[2022-11-11 13:36:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][50/1251]	eta 0:15:40 lr 0.000928	time 0.7345 (0.7827)	loss 3.9757 (3.6228)	grad_norm 1.1147 (1.3051)	mem 23874MB
[2022-11-11 13:37:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][100/1251]	eta 0:14:40 lr 0.000928	time 0.7537 (0.7653)	loss 4.1011 (3.6036)	grad_norm 1.3745 (1.2875)	mem 23874MB
[2022-11-11 13:37:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][150/1251]	eta 0:13:55 lr 0.000928	time 0.7371 (0.7585)	loss 4.1028 (3.7061)	grad_norm 1.4737 (1.2872)	mem 23874MB
[2022-11-11 13:38:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][200/1251]	eta 0:13:14 lr 0.000928	time 0.7362 (0.7560)	loss 3.8977 (3.7035)	grad_norm 1.2072 (1.2758)	mem 23874MB
[2022-11-11 13:39:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][250/1251]	eta 0:12:35 lr 0.000928	time 0.7373 (0.7548)	loss 3.8666 (3.6901)	grad_norm 1.3935 (1.2714)	mem 23874MB
[2022-11-11 13:39:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][300/1251]	eta 0:11:55 lr 0.000928	time 0.7385 (0.7528)	loss 4.2525 (3.6769)	grad_norm 1.2671 (1.2780)	mem 23874MB
[2022-11-11 13:40:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][350/1251]	eta 0:11:17 lr 0.000928	time 0.7349 (0.7519)	loss 4.4652 (3.6697)	grad_norm 1.3658 (1.2764)	mem 23874MB
[2022-11-11 13:41:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][400/1251]	eta 0:10:39 lr 0.000928	time 0.7342 (0.7512)	loss 3.9986 (3.6880)	grad_norm 1.2443 (1.2721)	mem 23874MB
[2022-11-11 13:41:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][450/1251]	eta 0:10:01 lr 0.000927	time 0.7380 (0.7507)	loss 4.6850 (3.6901)	grad_norm 1.2494 (1.2682)	mem 23874MB
[2022-11-11 13:42:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][500/1251]	eta 0:09:23 lr 0.000927	time 0.7516 (0.7505)	loss 4.2894 (3.7035)	grad_norm 1.4333 (1.2684)	mem 23874MB
[2022-11-11 13:42:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][550/1251]	eta 0:08:45 lr 0.000927	time 0.7353 (0.7500)	loss 3.4445 (3.6968)	grad_norm 1.2429 (1.2691)	mem 23874MB
[2022-11-11 13:43:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][600/1251]	eta 0:08:07 lr 0.000927	time 0.8136 (0.7496)	loss 3.6557 (3.6905)	grad_norm 1.2994 (1.2684)	mem 23874MB
[2022-11-11 13:44:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][650/1251]	eta 0:07:30 lr 0.000927	time 0.7369 (0.7495)	loss 3.8848 (3.6839)	grad_norm 1.3328 (1.2680)	mem 23874MB
[2022-11-11 13:44:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][700/1251]	eta 0:06:52 lr 0.000927	time 0.7373 (0.7491)	loss 3.6792 (3.6752)	grad_norm 1.2483 (1.2673)	mem 23874MB
[2022-11-11 13:45:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][750/1251]	eta 0:06:15 lr 0.000927	time 0.7393 (0.7489)	loss 2.8124 (3.6766)	grad_norm 1.2096 (1.2666)	mem 23874MB
[2022-11-11 13:46:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][800/1251]	eta 0:05:37 lr 0.000927	time 0.7381 (0.7487)	loss 4.3832 (3.6673)	grad_norm 1.4258 (1.2654)	mem 23874MB
[2022-11-11 13:46:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][850/1251]	eta 0:05:00 lr 0.000927	time 0.7384 (0.7486)	loss 4.2622 (3.6654)	grad_norm 1.3996 (1.2655)	mem 23874MB
[2022-11-11 13:47:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][900/1251]	eta 0:04:22 lr 0.000926	time 0.7463 (0.7484)	loss 3.9637 (3.6691)	grad_norm 1.2026 (1.2670)	mem 23874MB
[2022-11-11 13:47:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][950/1251]	eta 0:03:45 lr 0.000926	time 0.7371 (0.7483)	loss 3.6057 (3.6638)	grad_norm 1.2397 (1.2670)	mem 23874MB
[2022-11-11 13:48:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][1000/1251]	eta 0:03:07 lr 0.000926	time 0.7376 (0.7482)	loss 3.8158 (3.6644)	grad_norm 1.2437 (1.2666)	mem 23874MB
[2022-11-11 13:49:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][1050/1251]	eta 0:02:30 lr 0.000926	time 0.7439 (0.7482)	loss 2.8896 (3.6670)	grad_norm 1.1820 (1.2656)	mem 23874MB
[2022-11-11 13:49:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][1100/1251]	eta 0:01:52 lr 0.000926	time 0.7408 (0.7479)	loss 3.7074 (3.6683)	grad_norm 1.2101 (1.2652)	mem 23874MB
[2022-11-11 13:50:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][1150/1251]	eta 0:01:15 lr 0.000926	time 0.7391 (0.7480)	loss 3.6718 (3.6705)	grad_norm 1.2448 (1.2658)	mem 23874MB
[2022-11-11 13:50:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][1200/1251]	eta 0:00:38 lr 0.000926	time 0.7445 (0.7480)	loss 3.5496 (3.6702)	grad_norm 1.1402 (1.2651)	mem 23874MB
[2022-11-11 13:51:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [52/300][1250/1251]	eta 0:00:00 lr 0.000926	time 0.7257 (0.7478)	loss 3.8386 (3.6677)	grad_norm 1.3478 (1.2656)	mem 23874MB
[2022-11-11 13:51:36 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 52 training takes 0:15:35
[2022-11-11 13:51:36 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_52.pth saving......
[2022-11-11 13:51:37 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_52.pth saved !!!
[2022-11-11 13:51:39 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.588 (1.588)	Loss 1.1454 (1.1454)	Acc@1 72.754 (72.754)	Acc@5 91.309 (91.309)	Mem 23874MB
[2022-11-11 13:51:50 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.314 Acc@5 92.642
[2022-11-11 13:51:50 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.3%
[2022-11-11 13:51:51 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.769 (1.769)	Loss 0.9815 (0.9815)	Acc@1 75.195 (75.195)	Acc@5 93.652 (93.652)	Mem 23874MB
[2022-11-11 13:52:02 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.494 Acc@5 93.474
[2022-11-11 13:52:02 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.5%
[2022-11-11 13:52:02 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.49% at 52 epoch
[2022-11-11 13:52:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][0/1251]	eta 0:50:11 lr 0.000926	time 2.4070 (2.4070)	loss 4.0578 (4.0578)	grad_norm 1.1615 (1.1615)	mem 23874MB
[2022-11-11 13:52:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][50/1251]	eta 0:15:36 lr 0.000926	time 0.7386 (0.7801)	loss 4.4157 (3.5404)	grad_norm 1.2944 (1.2586)	mem 23874MB
[2022-11-11 13:53:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][100/1251]	eta 0:14:40 lr 0.000925	time 0.7320 (0.7651)	loss 3.2526 (3.6345)	grad_norm 1.2485 (1.2443)	mem 23874MB
[2022-11-11 13:53:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][150/1251]	eta 0:13:55 lr 0.000925	time 0.7493 (0.7589)	loss 4.0516 (3.6107)	grad_norm 1.3450 (1.2482)	mem 23874MB
[2022-11-11 13:54:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][200/1251]	eta 0:13:15 lr 0.000925	time 0.7376 (0.7570)	loss 4.2403 (3.6074)	grad_norm 1.2997 (1.2532)	mem 23874MB
[2022-11-11 13:55:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][250/1251]	eta 0:12:35 lr 0.000925	time 0.7451 (0.7548)	loss 4.3787 (3.6220)	grad_norm 1.2924 (1.2563)	mem 23874MB
[2022-11-11 13:55:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][300/1251]	eta 0:11:56 lr 0.000925	time 0.7436 (0.7539)	loss 2.3076 (3.6325)	grad_norm 1.1785 (1.2605)	mem 23874MB
[2022-11-11 13:56:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][350/1251]	eta 0:11:18 lr 0.000925	time 0.7391 (0.7532)	loss 3.2140 (3.6198)	grad_norm 1.2245 (1.2581)	mem 23874MB
[2022-11-11 13:57:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][400/1251]	eta 0:10:40 lr 0.000925	time 0.8102 (0.7522)	loss 3.5537 (3.6263)	grad_norm 1.3310 (1.2598)	mem 23874MB
[2022-11-11 13:57:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][450/1251]	eta 0:10:02 lr 0.000925	time 0.7374 (0.7516)	loss 3.5920 (3.6303)	grad_norm 1.1495 (1.2614)	mem 23874MB
[2022-11-11 13:58:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][500/1251]	eta 0:09:23 lr 0.000925	time 0.7401 (0.7509)	loss 3.9619 (3.6324)	grad_norm 1.3636 (1.2610)	mem 23874MB
[2022-11-11 13:58:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][550/1251]	eta 0:08:46 lr 0.000924	time 0.7454 (0.7508)	loss 4.4044 (3.6534)	grad_norm 1.3588 (1.2629)	mem 23874MB
[2022-11-11 13:59:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][600/1251]	eta 0:08:08 lr 0.000924	time 0.8140 (0.7504)	loss 3.9134 (3.6417)	grad_norm 1.1671 (nan)	mem 23874MB
[2022-11-11 14:00:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][650/1251]	eta 0:07:30 lr 0.000924	time 0.7326 (0.7500)	loss 4.6100 (3.6469)	grad_norm 1.2577 (nan)	mem 23874MB
[2022-11-11 14:00:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][700/1251]	eta 0:06:53 lr 0.000924	time 0.7401 (0.7497)	loss 2.9828 (3.6438)	grad_norm 1.6015 (nan)	mem 23874MB
[2022-11-11 14:01:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][750/1251]	eta 0:06:15 lr 0.000924	time 0.7375 (0.7497)	loss 3.8589 (3.6427)	grad_norm 1.3379 (nan)	mem 23874MB
[2022-11-11 14:02:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][800/1251]	eta 0:05:37 lr 0.000924	time 0.7478 (0.7494)	loss 3.0197 (3.6405)	grad_norm 1.3517 (nan)	mem 23874MB
[2022-11-11 14:02:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][850/1251]	eta 0:05:00 lr 0.000924	time 0.7380 (0.7496)	loss 4.3809 (3.6351)	grad_norm 1.2325 (nan)	mem 23874MB
[2022-11-11 14:03:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][900/1251]	eta 0:04:23 lr 0.000924	time 0.7420 (0.7494)	loss 3.4675 (3.6356)	grad_norm 1.1760 (nan)	mem 23874MB
[2022-11-11 14:03:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][950/1251]	eta 0:03:45 lr 0.000924	time 0.7363 (0.7492)	loss 2.8843 (3.6316)	grad_norm 1.1825 (nan)	mem 23874MB
[2022-11-11 14:04:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][1000/1251]	eta 0:03:08 lr 0.000923	time 0.7472 (0.7491)	loss 2.5385 (3.6321)	grad_norm 1.1262 (nan)	mem 23874MB
[2022-11-11 14:05:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][1050/1251]	eta 0:02:30 lr 0.000923	time 0.8341 (0.7491)	loss 3.6128 (3.6296)	grad_norm 1.2206 (nan)	mem 23874MB
[2022-11-11 14:05:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][1100/1251]	eta 0:01:53 lr 0.000923	time 0.7387 (0.7490)	loss 3.2651 (3.6288)	grad_norm 1.0549 (nan)	mem 23874MB
[2022-11-11 14:06:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][1150/1251]	eta 0:01:15 lr 0.000923	time 0.7411 (0.7491)	loss 3.2911 (3.6284)	grad_norm 1.2110 (nan)	mem 23874MB
[2022-11-11 14:07:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][1200/1251]	eta 0:00:38 lr 0.000923	time 0.7344 (0.7487)	loss 4.1055 (3.6308)	grad_norm 1.3124 (nan)	mem 23874MB
[2022-11-11 14:07:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [53/300][1250/1251]	eta 0:00:00 lr 0.000923	time 0.7245 (0.7486)	loss 3.6365 (3.6239)	grad_norm 1.1392 (nan)	mem 23874MB
[2022-11-11 14:07:39 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 53 training takes 0:15:36
[2022-11-11 14:07:39 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_53.pth saving......
[2022-11-11 14:07:40 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_53.pth saved !!!
[2022-11-11 14:07:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.612 (1.612)	Loss 1.0464 (1.0464)	Acc@1 75.977 (75.977)	Acc@5 92.578 (92.578)	Mem 23874MB
[2022-11-11 14:07:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.234 Acc@5 92.560
[2022-11-11 14:07:53 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.2%
[2022-11-11 14:07:54 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.768 (1.768)	Loss 0.9442 (0.9442)	Acc@1 76.660 (76.660)	Acc@5 93.164 (93.164)	Mem 23874MB
[2022-11-11 14:08:05 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.586 Acc@5 93.548
[2022-11-11 14:08:05 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.6%
[2022-11-11 14:08:05 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.59% at 53 epoch
[2022-11-11 14:08:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][0/1251]	eta 0:49:29 lr 0.000923	time 2.3733 (2.3733)	loss 2.4513 (2.4513)	grad_norm 1.1744 (1.1744)	mem 23874MB
[2022-11-11 14:08:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][50/1251]	eta 0:15:38 lr 0.000923	time 0.7361 (0.7812)	loss 4.4090 (3.7007)	grad_norm 1.3548 (1.2887)	mem 23874MB
[2022-11-11 14:09:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][100/1251]	eta 0:14:39 lr 0.000923	time 0.7360 (0.7643)	loss 3.7281 (3.6676)	grad_norm 1.4532 (1.2781)	mem 23874MB
[2022-11-11 14:10:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][150/1251]	eta 0:13:55 lr 0.000923	time 0.7360 (0.7592)	loss 4.1933 (3.6881)	grad_norm 1.3382 (1.2726)	mem 23874MB
[2022-11-11 14:10:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][200/1251]	eta 0:13:15 lr 0.000922	time 0.8291 (0.7565)	loss 3.7882 (3.6592)	grad_norm 1.3724 (1.2844)	mem 23874MB
[2022-11-11 14:11:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][250/1251]	eta 0:12:34 lr 0.000922	time 0.7378 (0.7539)	loss 4.2182 (3.6557)	grad_norm 1.4143 (1.2905)	mem 23874MB
[2022-11-11 14:11:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][300/1251]	eta 0:11:56 lr 0.000922	time 0.7500 (0.7534)	loss 3.5610 (3.6338)	grad_norm 1.3451 (1.2874)	mem 23874MB
[2022-11-11 14:12:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][350/1251]	eta 0:11:17 lr 0.000922	time 0.7371 (0.7525)	loss 3.5637 (3.6412)	grad_norm 1.2399 (1.2880)	mem 23874MB
[2022-11-11 14:13:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][400/1251]	eta 0:10:39 lr 0.000922	time 0.7419 (0.7516)	loss 3.7781 (3.6315)	grad_norm 1.2707 (1.2856)	mem 23874MB
[2022-11-11 14:13:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][450/1251]	eta 0:10:01 lr 0.000922	time 0.7412 (0.7514)	loss 3.7722 (3.6304)	grad_norm 1.3506 (1.2882)	mem 23874MB
[2022-11-11 14:14:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][500/1251]	eta 0:09:23 lr 0.000922	time 0.7417 (0.7505)	loss 2.3359 (3.6250)	grad_norm 1.2736 (1.2872)	mem 23874MB
[2022-11-11 14:14:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][550/1251]	eta 0:08:46 lr 0.000922	time 0.7350 (0.7505)	loss 4.1898 (3.6257)	grad_norm 1.5013 (1.2864)	mem 23874MB
[2022-11-11 14:15:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][600/1251]	eta 0:08:08 lr 0.000922	time 0.8257 (0.7501)	loss 3.5630 (3.6267)	grad_norm 1.3679 (1.2847)	mem 23874MB
[2022-11-11 14:16:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][650/1251]	eta 0:07:30 lr 0.000921	time 0.7368 (0.7498)	loss 3.0404 (3.6350)	grad_norm 1.2588 (1.2826)	mem 23874MB
[2022-11-11 14:16:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][700/1251]	eta 0:06:53 lr 0.000921	time 0.7418 (0.7497)	loss 4.0545 (3.6423)	grad_norm 1.3509 (1.2848)	mem 23874MB
[2022-11-11 14:17:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][750/1251]	eta 0:06:15 lr 0.000921	time 0.7387 (0.7495)	loss 2.6802 (3.6450)	grad_norm 1.2490 (1.2845)	mem 23874MB
[2022-11-11 14:18:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][800/1251]	eta 0:05:37 lr 0.000921	time 0.7439 (0.7494)	loss 4.3390 (3.6527)	grad_norm 1.2010 (1.2834)	mem 23874MB
[2022-11-11 14:18:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][850/1251]	eta 0:05:00 lr 0.000921	time 0.7380 (0.7493)	loss 2.8710 (3.6565)	grad_norm 1.2222 (1.2811)	mem 23874MB
[2022-11-11 14:19:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][900/1251]	eta 0:04:22 lr 0.000921	time 0.7428 (0.7490)	loss 3.8854 (3.6594)	grad_norm 1.2140 (1.2792)	mem 23874MB
[2022-11-11 14:19:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][950/1251]	eta 0:03:45 lr 0.000921	time 0.8332 (0.7491)	loss 4.0148 (3.6588)	grad_norm 1.1486 (1.2774)	mem 23874MB
[2022-11-11 14:20:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][1000/1251]	eta 0:03:07 lr 0.000921	time 0.8118 (0.7489)	loss 3.7586 (3.6609)	grad_norm 1.1797 (1.2757)	mem 23874MB
[2022-11-11 14:21:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][1050/1251]	eta 0:02:30 lr 0.000921	time 0.7410 (0.7488)	loss 4.1800 (3.6607)	grad_norm 1.2081 (1.2765)	mem 23874MB
[2022-11-11 14:21:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][1100/1251]	eta 0:01:53 lr 0.000920	time 0.7417 (0.7487)	loss 4.2542 (3.6640)	grad_norm 1.1708 (1.2749)	mem 23874MB
[2022-11-11 14:22:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][1150/1251]	eta 0:01:15 lr 0.000920	time 0.7386 (0.7486)	loss 3.9012 (3.6599)	grad_norm 1.2311 (1.2748)	mem 23874MB
[2022-11-11 14:23:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][1200/1251]	eta 0:00:38 lr 0.000920	time 0.7379 (0.7485)	loss 4.1699 (3.6637)	grad_norm 1.2617 (1.2735)	mem 23874MB
[2022-11-11 14:23:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [54/300][1250/1251]	eta 0:00:00 lr 0.000920	time 0.7254 (0.7482)	loss 3.0008 (3.6572)	grad_norm 1.1189 (1.2728)	mem 23874MB
[2022-11-11 14:23:41 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 54 training takes 0:15:36
[2022-11-11 14:23:41 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_54.pth saving......
[2022-11-11 14:23:42 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_54.pth saved !!!
[2022-11-11 14:23:44 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.621 (1.621)	Loss 1.0621 (1.0621)	Acc@1 73.535 (73.535)	Acc@5 93.262 (93.262)	Mem 23874MB
[2022-11-11 14:23:55 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.248 Acc@5 92.542
[2022-11-11 14:23:55 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.2%
[2022-11-11 14:23:57 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.920 (1.920)	Loss 0.9167 (0.9167)	Acc@1 77.148 (77.148)	Acc@5 94.727 (94.727)	Mem 23874MB
[2022-11-11 14:24:07 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.758 Acc@5 93.648
[2022-11-11 14:24:07 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.8%
[2022-11-11 14:24:07 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.76% at 54 epoch
[2022-11-11 14:24:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][0/1251]	eta 0:50:26 lr 0.000920	time 2.4193 (2.4193)	loss 3.6195 (3.6195)	grad_norm 1.1315 (1.1315)	mem 23874MB
[2022-11-11 14:24:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][50/1251]	eta 0:15:40 lr 0.000920	time 0.7415 (0.7828)	loss 3.8220 (3.6297)	grad_norm 1.2523 (1.2737)	mem 23874MB
[2022-11-11 14:25:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][100/1251]	eta 0:14:42 lr 0.000920	time 0.7412 (0.7666)	loss 3.9370 (3.6109)	grad_norm 1.1620 (1.2722)	mem 23874MB
[2022-11-11 14:26:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][150/1251]	eta 0:13:55 lr 0.000920	time 0.7373 (0.7593)	loss 3.7014 (3.6402)	grad_norm 1.2588 (1.2743)	mem 23874MB
[2022-11-11 14:26:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][200/1251]	eta 0:13:15 lr 0.000920	time 0.7377 (0.7570)	loss 4.0396 (3.6576)	grad_norm 1.2356 (1.2815)	mem 23874MB
[2022-11-11 14:27:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][250/1251]	eta 0:12:35 lr 0.000920	time 0.7521 (0.7547)	loss 4.5358 (3.6538)	grad_norm 1.2352 (1.2781)	mem 23874MB
[2022-11-11 14:27:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][300/1251]	eta 0:11:57 lr 0.000919	time 0.7379 (0.7540)	loss 2.9424 (3.6259)	grad_norm 1.6235 (1.2772)	mem 23874MB
[2022-11-11 14:28:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][350/1251]	eta 0:11:18 lr 0.000919	time 0.7432 (0.7531)	loss 3.3528 (3.6174)	grad_norm 1.5432 (1.2815)	mem 23874MB
[2022-11-11 14:29:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][400/1251]	eta 0:10:40 lr 0.000919	time 0.7424 (0.7527)	loss 2.4800 (3.6147)	grad_norm 1.2717 (1.2843)	mem 23874MB
[2022-11-11 14:29:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][450/1251]	eta 0:10:02 lr 0.000919	time 0.7369 (0.7519)	loss 4.4655 (3.6136)	grad_norm 1.1346 (1.2825)	mem 23874MB
[2022-11-11 14:30:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][500/1251]	eta 0:09:24 lr 0.000919	time 0.7378 (0.7514)	loss 2.7490 (3.6092)	grad_norm 1.4494 (1.2834)	mem 23874MB
[2022-11-11 14:31:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][550/1251]	eta 0:08:46 lr 0.000919	time 0.7389 (0.7511)	loss 4.0548 (3.6169)	grad_norm 1.2681 (1.2846)	mem 23874MB
[2022-11-11 14:31:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][600/1251]	eta 0:08:08 lr 0.000919	time 0.7409 (0.7508)	loss 3.3883 (3.6165)	grad_norm 1.2848 (1.2818)	mem 23874MB
[2022-11-11 14:32:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][650/1251]	eta 0:07:31 lr 0.000919	time 0.7427 (0.7507)	loss 3.6014 (3.6277)	grad_norm 1.3482 (1.2816)	mem 23874MB
[2022-11-11 14:32:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][700/1251]	eta 0:06:53 lr 0.000919	time 0.7359 (0.7506)	loss 4.3678 (3.6247)	grad_norm 1.2262 (1.2807)	mem 23874MB
[2022-11-11 14:33:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][750/1251]	eta 0:06:15 lr 0.000918	time 0.8071 (0.7503)	loss 2.8214 (3.6244)	grad_norm 1.3822 (1.2797)	mem 23874MB
[2022-11-11 14:34:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][800/1251]	eta 0:05:38 lr 0.000918	time 0.7463 (0.7501)	loss 2.7881 (3.6171)	grad_norm 1.3017 (1.2804)	mem 23874MB
[2022-11-11 14:34:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][850/1251]	eta 0:05:00 lr 0.000918	time 0.7437 (0.7500)	loss 2.3584 (3.6119)	grad_norm 1.2337 (1.2806)	mem 23874MB
[2022-11-11 14:35:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][900/1251]	eta 0:04:23 lr 0.000918	time 0.7386 (0.7498)	loss 3.6100 (3.6146)	grad_norm 1.2767 (1.2819)	mem 23874MB
[2022-11-11 14:36:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][950/1251]	eta 0:03:45 lr 0.000918	time 0.7473 (0.7499)	loss 4.1349 (3.6214)	grad_norm 1.3009 (1.2810)	mem 23874MB
[2022-11-11 14:36:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][1000/1251]	eta 0:03:08 lr 0.000918	time 0.7993 (0.7498)	loss 4.1494 (3.6233)	grad_norm 1.3022 (1.2797)	mem 23874MB
[2022-11-11 14:37:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][1050/1251]	eta 0:02:30 lr 0.000918	time 0.7317 (0.7497)	loss 2.8476 (3.6215)	grad_norm 1.2605 (1.2785)	mem 23874MB
[2022-11-11 14:37:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][1100/1251]	eta 0:01:53 lr 0.000918	time 0.7426 (0.7497)	loss 3.8212 (3.6172)	grad_norm 1.1314 (1.2787)	mem 23874MB
[2022-11-11 14:38:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][1150/1251]	eta 0:01:15 lr 0.000918	time 0.7364 (0.7496)	loss 4.3515 (3.6211)	grad_norm 1.3355 (1.2781)	mem 23874MB
[2022-11-11 14:39:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][1200/1251]	eta 0:00:38 lr 0.000917	time 0.7322 (0.7497)	loss 2.7112 (3.6127)	grad_norm 1.2183 (1.2783)	mem 23874MB
[2022-11-11 14:39:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [55/300][1250/1251]	eta 0:00:00 lr 0.000917	time 0.7266 (0.7494)	loss 3.9089 (3.6111)	grad_norm 1.4272 (1.2777)	mem 23874MB
[2022-11-11 14:39:45 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 55 training takes 0:15:37
[2022-11-11 14:39:45 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_55.pth saving......
[2022-11-11 14:39:46 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_55.pth saved !!!
[2022-11-11 14:39:48 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.605 (1.605)	Loss 1.1173 (1.1173)	Acc@1 74.414 (74.414)	Acc@5 91.602 (91.602)	Mem 23874MB
[2022-11-11 14:39:59 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.680 Acc@5 92.736
[2022-11-11 14:39:59 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.7%
[2022-11-11 14:40:01 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.930 (1.930)	Loss 0.9854 (0.9854)	Acc@1 75.781 (75.781)	Acc@5 93.359 (93.359)	Mem 23874MB
[2022-11-11 14:40:11 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.858 Acc@5 93.730
[2022-11-11 14:40:11 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.9%
[2022-11-11 14:40:11 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.86% at 55 epoch
[2022-11-11 14:40:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][0/1251]	eta 0:49:48 lr 0.000917	time 2.3890 (2.3890)	loss 3.5267 (3.5267)	grad_norm 1.2372 (1.2372)	mem 23874MB
[2022-11-11 14:40:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][50/1251]	eta 0:15:40 lr 0.000917	time 0.8198 (0.7832)	loss 2.9754 (3.5570)	grad_norm 1.2286 (1.2878)	mem 23874MB
[2022-11-11 14:41:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][100/1251]	eta 0:14:42 lr 0.000917	time 0.7559 (0.7666)	loss 2.7107 (3.6029)	grad_norm 1.1582 (1.2728)	mem 23874MB
[2022-11-11 14:42:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][150/1251]	eta 0:13:57 lr 0.000917	time 0.7388 (0.7611)	loss 3.4134 (3.6227)	grad_norm 1.3174 (1.2731)	mem 23874MB
[2022-11-11 14:42:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][200/1251]	eta 0:13:16 lr 0.000917	time 0.7344 (0.7581)	loss 2.9993 (3.5946)	grad_norm 1.4071 (1.2708)	mem 23874MB
[2022-11-11 14:43:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][250/1251]	eta 0:12:36 lr 0.000917	time 0.7398 (0.7558)	loss 3.4534 (3.6053)	grad_norm 1.0475 (1.2681)	mem 23874MB
[2022-11-11 14:43:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][300/1251]	eta 0:11:57 lr 0.000917	time 0.7473 (0.7547)	loss 3.9180 (3.6214)	grad_norm 1.1822 (1.2702)	mem 23874MB
[2022-11-11 14:44:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][350/1251]	eta 0:11:19 lr 0.000916	time 0.7424 (0.7538)	loss 3.7338 (3.6339)	grad_norm 1.3508 (1.2697)	mem 23874MB
[2022-11-11 14:45:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][400/1251]	eta 0:10:40 lr 0.000916	time 0.7331 (0.7531)	loss 4.1810 (3.6356)	grad_norm 1.2366 (1.2707)	mem 23874MB
[2022-11-11 14:45:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][450/1251]	eta 0:10:02 lr 0.000916	time 0.7449 (0.7525)	loss 3.5678 (3.6255)	grad_norm 1.1348 (1.2708)	mem 23874MB
[2022-11-11 14:46:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][500/1251]	eta 0:09:24 lr 0.000916	time 0.7398 (0.7523)	loss 4.0920 (3.6327)	grad_norm 1.3340 (1.2734)	mem 23874MB
[2022-11-11 14:47:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][550/1251]	eta 0:08:47 lr 0.000916	time 0.7409 (0.7518)	loss 3.2317 (3.6351)	grad_norm 1.2393 (1.2753)	mem 23874MB
[2022-11-11 14:47:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][600/1251]	eta 0:08:09 lr 0.000916	time 0.7221 (0.7519)	loss 3.9834 (3.6315)	grad_norm 1.2786 (1.2758)	mem 23874MB
[2022-11-11 14:48:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][650/1251]	eta 0:07:31 lr 0.000916	time 0.7411 (0.7512)	loss 3.5488 (3.6428)	grad_norm 1.2332 (1.2762)	mem 23874MB
[2022-11-11 14:48:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][700/1251]	eta 0:06:53 lr 0.000916	time 0.7388 (0.7513)	loss 2.9153 (3.6386)	grad_norm 1.2122 (1.2779)	mem 23874MB
[2022-11-11 14:49:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][750/1251]	eta 0:06:16 lr 0.000916	time 0.7392 (0.7509)	loss 4.0556 (3.6293)	grad_norm 1.1989 (1.2776)	mem 23874MB
[2022-11-11 14:50:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][800/1251]	eta 0:05:38 lr 0.000915	time 0.7332 (0.7509)	loss 4.1044 (3.6286)	grad_norm 1.2100 (1.2771)	mem 23874MB
[2022-11-11 14:50:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][850/1251]	eta 0:05:00 lr 0.000915	time 0.7359 (0.7506)	loss 3.7972 (3.6258)	grad_norm 1.3198 (1.2769)	mem 23874MB
[2022-11-11 14:51:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][900/1251]	eta 0:04:23 lr 0.000915	time 0.7387 (0.7506)	loss 3.9137 (3.6309)	grad_norm 1.3605 (nan)	mem 23874MB
[2022-11-11 14:52:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][950/1251]	eta 0:03:45 lr 0.000915	time 0.7415 (0.7504)	loss 3.9235 (3.6340)	grad_norm 1.2419 (nan)	mem 23874MB
[2022-11-11 14:52:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][1000/1251]	eta 0:03:08 lr 0.000915	time 0.7488 (0.7503)	loss 4.1439 (3.6424)	grad_norm 1.3038 (nan)	mem 23874MB
[2022-11-11 14:53:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][1050/1251]	eta 0:02:30 lr 0.000915	time 0.7383 (0.7502)	loss 3.1274 (3.6342)	grad_norm 1.2005 (nan)	mem 23874MB
[2022-11-11 14:53:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][1100/1251]	eta 0:01:53 lr 0.000915	time 0.7435 (0.7501)	loss 2.8242 (3.6362)	grad_norm 1.2217 (nan)	mem 23874MB
[2022-11-11 14:54:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][1150/1251]	eta 0:01:15 lr 0.000915	time 0.7449 (0.7501)	loss 3.8214 (3.6353)	grad_norm 1.2752 (nan)	mem 23874MB
[2022-11-11 14:55:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][1200/1251]	eta 0:00:38 lr 0.000915	time 0.7392 (0.7500)	loss 3.0389 (3.6365)	grad_norm 1.2767 (nan)	mem 23874MB
[2022-11-11 14:55:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [56/300][1250/1251]	eta 0:00:00 lr 0.000914	time 0.7270 (0.7498)	loss 3.9464 (3.6355)	grad_norm 1.1840 (nan)	mem 23874MB
[2022-11-11 14:55:49 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 56 training takes 0:15:38
[2022-11-11 14:55:50 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_56.pth saving......
[2022-11-11 14:55:51 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_56.pth saved !!!
[2022-11-11 14:55:52 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.656 (1.656)	Loss 1.0554 (1.0554)	Acc@1 75.488 (75.488)	Acc@5 94.336 (94.336)	Mem 23874MB
[2022-11-11 14:56:03 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.652 Acc@5 92.840
[2022-11-11 14:56:03 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.7%
[2022-11-11 14:56:05 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.880 (1.880)	Loss 0.8248 (0.8248)	Acc@1 79.688 (79.688)	Acc@5 93.848 (93.848)	Mem 23874MB
[2022-11-11 14:56:16 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.940 Acc@5 93.822
[2022-11-11 14:56:16 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 76.9%
[2022-11-11 14:56:16 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 76.94% at 56 epoch
[2022-11-11 14:56:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][0/1251]	eta 0:48:59 lr 0.000914	time 2.3500 (2.3500)	loss 2.4298 (2.4298)	grad_norm 1.3209 (1.3209)	mem 23874MB
[2022-11-11 14:56:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][50/1251]	eta 0:15:37 lr 0.000914	time 0.7408 (0.7802)	loss 3.9727 (3.4808)	grad_norm 1.3774 (1.3139)	mem 23874MB
[2022-11-11 14:57:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][100/1251]	eta 0:14:39 lr 0.000914	time 0.7349 (0.7645)	loss 4.0177 (3.5719)	grad_norm 1.4542 (1.2883)	mem 23874MB
[2022-11-11 14:58:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][150/1251]	eta 0:13:55 lr 0.000914	time 0.7431 (0.7591)	loss 3.9616 (3.6012)	grad_norm 1.3348 (1.2790)	mem 23874MB
[2022-11-11 14:58:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][200/1251]	eta 0:13:14 lr 0.000914	time 0.7394 (0.7564)	loss 3.0134 (3.6129)	grad_norm 1.2491 (1.2820)	mem 23874MB
[2022-11-11 14:59:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][250/1251]	eta 0:12:35 lr 0.000914	time 0.7365 (0.7543)	loss 3.9949 (3.5855)	grad_norm 1.3647 (1.2785)	mem 23874MB
[2022-11-11 15:00:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][300/1251]	eta 0:11:56 lr 0.000914	time 0.7346 (0.7532)	loss 3.1053 (3.5840)	grad_norm 1.2901 (1.2723)	mem 23874MB
[2022-11-11 15:00:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][350/1251]	eta 0:11:18 lr 0.000914	time 0.8226 (0.7527)	loss 4.2457 (3.5881)	grad_norm 1.4180 (1.2720)	mem 23874MB
[2022-11-11 15:01:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][400/1251]	eta 0:10:39 lr 0.000913	time 0.7492 (0.7516)	loss 4.0816 (3.6134)	grad_norm 1.2295 (1.2724)	mem 23874MB
[2022-11-11 15:01:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][450/1251]	eta 0:10:01 lr 0.000913	time 0.7366 (0.7514)	loss 3.7983 (3.6185)	grad_norm 1.2323 (1.2740)	mem 23874MB
[2022-11-11 15:02:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][500/1251]	eta 0:09:23 lr 0.000913	time 0.7384 (0.7506)	loss 4.0422 (3.6096)	grad_norm 1.2898 (1.2750)	mem 23874MB
[2022-11-11 15:03:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][550/1251]	eta 0:08:46 lr 0.000913	time 0.7403 (0.7504)	loss 3.9753 (3.6111)	grad_norm 1.4195 (1.2755)	mem 23874MB
[2022-11-11 15:03:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][600/1251]	eta 0:08:08 lr 0.000913	time 0.7408 (0.7501)	loss 2.4988 (3.5983)	grad_norm 1.2115 (1.2745)	mem 23874MB
[2022-11-11 15:04:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][650/1251]	eta 0:07:30 lr 0.000913	time 0.8105 (0.7502)	loss 2.5632 (3.5968)	grad_norm 1.2978 (1.2747)	mem 23874MB
[2022-11-11 15:05:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][700/1251]	eta 0:06:53 lr 0.000913	time 0.7403 (0.7499)	loss 4.3443 (3.6045)	grad_norm 1.3555 (1.2756)	mem 23874MB
[2022-11-11 15:05:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][750/1251]	eta 0:06:15 lr 0.000913	time 0.8177 (0.7498)	loss 3.0724 (3.6044)	grad_norm 1.1879 (1.2762)	mem 23874MB
[2022-11-11 15:06:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][800/1251]	eta 0:05:38 lr 0.000913	time 0.7411 (0.7495)	loss 3.4143 (3.6095)	grad_norm 1.3170 (1.2765)	mem 23874MB
[2022-11-11 15:06:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][850/1251]	eta 0:05:00 lr 0.000912	time 0.7372 (0.7493)	loss 3.2253 (3.6078)	grad_norm 1.3814 (1.2776)	mem 23874MB
[2022-11-11 15:07:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][900/1251]	eta 0:04:22 lr 0.000912	time 0.7377 (0.7492)	loss 3.8980 (3.6013)	grad_norm 1.1888 (1.2776)	mem 23874MB
[2022-11-11 15:08:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][950/1251]	eta 0:03:45 lr 0.000912	time 0.7340 (0.7489)	loss 4.3425 (3.6090)	grad_norm 1.3387 (1.2780)	mem 23874MB
[2022-11-11 15:08:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][1000/1251]	eta 0:03:08 lr 0.000912	time 0.7469 (0.7491)	loss 3.3568 (3.6149)	grad_norm 1.2831 (1.2781)	mem 23874MB
[2022-11-11 15:09:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][1050/1251]	eta 0:02:30 lr 0.000912	time 0.7381 (0.7489)	loss 3.5778 (3.6110)	grad_norm 1.1991 (1.2778)	mem 23874MB
[2022-11-11 15:10:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][1100/1251]	eta 0:01:53 lr 0.000912	time 0.8401 (0.7488)	loss 3.8161 (3.6105)	grad_norm 1.3736 (1.2777)	mem 23874MB
[2022-11-11 15:10:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][1150/1251]	eta 0:01:15 lr 0.000912	time 0.7454 (0.7488)	loss 4.2026 (3.6096)	grad_norm 1.2838 (1.2776)	mem 23874MB
[2022-11-11 15:11:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][1200/1251]	eta 0:00:38 lr 0.000912	time 0.7352 (0.7488)	loss 3.7288 (3.6072)	grad_norm 1.1721 (1.2784)	mem 23874MB
[2022-11-11 15:11:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [57/300][1250/1251]	eta 0:00:00 lr 0.000911	time 0.7270 (0.7486)	loss 3.3876 (3.6020)	grad_norm 1.2503 (1.2781)	mem 23874MB
[2022-11-11 15:11:52 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 57 training takes 0:15:36
[2022-11-11 15:11:53 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_57.pth saving......
[2022-11-11 15:11:54 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_57.pth saved !!!
[2022-11-11 15:11:55 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.585 (1.585)	Loss 1.0592 (1.0592)	Acc@1 74.023 (74.023)	Acc@5 92.676 (92.676)	Mem 23874MB
[2022-11-11 15:12:06 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.752 Acc@5 92.790
[2022-11-11 15:12:06 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.8%
[2022-11-11 15:12:08 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.862 (1.862)	Loss 0.8963 (0.8963)	Acc@1 77.832 (77.832)	Acc@5 94.531 (94.531)	Mem 23874MB
[2022-11-11 15:12:18 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.066 Acc@5 93.904
[2022-11-11 15:12:18 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.1%
[2022-11-11 15:12:18 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.07% at 57 epoch
[2022-11-11 15:12:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][0/1251]	eta 0:54:13 lr 0.000911	time 2.6010 (2.6010)	loss 3.8006 (3.8006)	grad_norm 1.3967 (1.3967)	mem 23874MB
[2022-11-11 15:12:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][50/1251]	eta 0:15:45 lr 0.000911	time 0.7406 (0.7873)	loss 3.7742 (3.4571)	grad_norm 1.1800 (1.2649)	mem 23874MB
[2022-11-11 15:13:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][100/1251]	eta 0:14:44 lr 0.000911	time 0.7507 (0.7680)	loss 4.1032 (3.5726)	grad_norm 1.3624 (1.2781)	mem 23874MB
[2022-11-11 15:14:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][150/1251]	eta 0:13:58 lr 0.000911	time 0.7394 (0.7612)	loss 3.6055 (3.5690)	grad_norm 1.2576 (1.2871)	mem 23874MB
[2022-11-11 15:14:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][200/1251]	eta 0:13:16 lr 0.000911	time 0.7390 (0.7578)	loss 3.3045 (3.5845)	grad_norm 1.3126 (1.2860)	mem 23874MB
[2022-11-11 15:15:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][250/1251]	eta 0:12:36 lr 0.000911	time 0.7355 (0.7554)	loss 4.4484 (3.5833)	grad_norm 1.2819 (1.2885)	mem 23874MB
[2022-11-11 15:16:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][300/1251]	eta 0:11:56 lr 0.000911	time 0.7428 (0.7539)	loss 4.0134 (3.5946)	grad_norm 1.2368 (1.2837)	mem 23874MB
[2022-11-11 15:16:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][350/1251]	eta 0:11:18 lr 0.000911	time 0.8484 (0.7528)	loss 4.0618 (3.6115)	grad_norm 1.3313 (1.2799)	mem 23874MB
[2022-11-11 15:17:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][400/1251]	eta 0:10:40 lr 0.000911	time 0.7445 (0.7522)	loss 3.5270 (3.6149)	grad_norm 1.2626 (nan)	mem 23874MB
[2022-11-11 15:17:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][450/1251]	eta 0:10:02 lr 0.000910	time 0.7499 (0.7518)	loss 3.2321 (3.6297)	grad_norm 1.3813 (nan)	mem 23874MB
[2022-11-11 15:18:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][500/1251]	eta 0:09:24 lr 0.000910	time 0.7365 (0.7514)	loss 2.6391 (3.6231)	grad_norm 1.2582 (nan)	mem 23874MB
[2022-11-11 15:19:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][550/1251]	eta 0:08:46 lr 0.000910	time 0.7498 (0.7509)	loss 3.5539 (3.6362)	grad_norm 1.1164 (nan)	mem 23874MB
[2022-11-11 15:19:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][600/1251]	eta 0:08:08 lr 0.000910	time 0.8177 (0.7507)	loss 4.0171 (3.6418)	grad_norm 1.3107 (nan)	mem 23874MB
[2022-11-11 15:20:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][650/1251]	eta 0:07:31 lr 0.000910	time 0.7415 (0.7504)	loss 2.5724 (3.6439)	grad_norm 1.1671 (nan)	mem 23874MB
[2022-11-11 15:21:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][700/1251]	eta 0:06:53 lr 0.000910	time 0.7396 (0.7502)	loss 3.8334 (3.6430)	grad_norm 1.1738 (nan)	mem 23874MB
[2022-11-11 15:21:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][750/1251]	eta 0:06:15 lr 0.000910	time 0.7399 (0.7500)	loss 3.7366 (3.6465)	grad_norm 1.2166 (nan)	mem 23874MB
[2022-11-11 15:22:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][800/1251]	eta 0:05:38 lr 0.000910	time 0.7478 (0.7498)	loss 2.4626 (3.6398)	grad_norm 1.1827 (nan)	mem 23874MB
[2022-11-11 15:22:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][850/1251]	eta 0:05:00 lr 0.000909	time 0.7403 (0.7497)	loss 4.1167 (3.6395)	grad_norm 1.1708 (nan)	mem 23874MB
[2022-11-11 15:23:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][900/1251]	eta 0:04:23 lr 0.000909	time 0.7377 (0.7497)	loss 3.7678 (3.6381)	grad_norm 1.2047 (nan)	mem 23874MB
[2022-11-11 15:24:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][950/1251]	eta 0:03:45 lr 0.000909	time 0.7425 (0.7494)	loss 3.5077 (3.6363)	grad_norm 1.1480 (nan)	mem 23874MB
[2022-11-11 15:24:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][1000/1251]	eta 0:03:08 lr 0.000909	time 0.7384 (0.7494)	loss 3.2791 (3.6324)	grad_norm 1.1737 (nan)	mem 23874MB
[2022-11-11 15:25:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][1050/1251]	eta 0:02:30 lr 0.000909	time 0.7378 (0.7491)	loss 4.3497 (3.6324)	grad_norm 1.3526 (nan)	mem 23874MB
[2022-11-11 15:26:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][1100/1251]	eta 0:01:53 lr 0.000909	time 0.7364 (0.7490)	loss 3.8009 (3.6285)	grad_norm 1.2636 (nan)	mem 23874MB
[2022-11-11 15:26:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][1150/1251]	eta 0:01:15 lr 0.000909	time 0.7437 (0.7489)	loss 3.7955 (3.6249)	grad_norm 1.2754 (nan)	mem 23874MB
[2022-11-11 15:27:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][1200/1251]	eta 0:00:38 lr 0.000909	time 0.7389 (0.7489)	loss 3.8125 (3.6233)	grad_norm 1.3115 (nan)	mem 23874MB
[2022-11-11 15:27:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [58/300][1250/1251]	eta 0:00:00 lr 0.000908	time 0.7268 (0.7487)	loss 2.4180 (3.6224)	grad_norm 1.2509 (nan)	mem 23874MB
[2022-11-11 15:27:55 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 58 training takes 0:15:36
[2022-11-11 15:27:55 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_58.pth saving......
[2022-11-11 15:27:57 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_58.pth saved !!!
[2022-11-11 15:27:58 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.691 (1.691)	Loss 0.9962 (0.9962)	Acc@1 76.953 (76.953)	Acc@5 92.480 (92.480)	Mem 23874MB
[2022-11-11 15:28:09 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.896 Acc@5 92.788
[2022-11-11 15:28:09 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.9%
[2022-11-11 15:28:11 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.792 (1.792)	Loss 0.9604 (0.9604)	Acc@1 75.293 (75.293)	Acc@5 93.066 (93.066)	Mem 23874MB
[2022-11-11 15:28:21 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.178 Acc@5 93.968
[2022-11-11 15:28:21 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.2%
[2022-11-11 15:28:21 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.18% at 58 epoch
[2022-11-11 15:28:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][0/1251]	eta 0:48:02 lr 0.000908	time 2.3044 (2.3044)	loss 4.0732 (4.0732)	grad_norm 1.1280 (1.1280)	mem 23874MB
[2022-11-11 15:29:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][50/1251]	eta 0:15:39 lr 0.000908	time 0.7393 (0.7821)	loss 3.7793 (3.6097)	grad_norm 1.2318 (1.2458)	mem 23874MB
[2022-11-11 15:29:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][100/1251]	eta 0:14:42 lr 0.000908	time 0.7401 (0.7666)	loss 3.1849 (3.5714)	grad_norm 1.3130 (1.2526)	mem 23874MB
[2022-11-11 15:30:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][150/1251]	eta 0:13:57 lr 0.000908	time 0.7357 (0.7609)	loss 4.1174 (3.5496)	grad_norm 1.1828 (1.2511)	mem 23874MB
[2022-11-11 15:30:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][200/1251]	eta 0:13:16 lr 0.000908	time 0.7376 (0.7581)	loss 4.0567 (3.5764)	grad_norm 1.2930 (1.2544)	mem 23874MB
[2022-11-11 15:31:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][250/1251]	eta 0:12:36 lr 0.000908	time 0.7414 (0.7558)	loss 4.0978 (3.5767)	grad_norm 1.2982 (1.2652)	mem 23874MB
[2022-11-11 15:32:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][300/1251]	eta 0:11:57 lr 0.000908	time 0.8071 (0.7550)	loss 3.6847 (3.5831)	grad_norm 1.2638 (1.2689)	mem 23874MB
[2022-11-11 15:32:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][350/1251]	eta 0:11:19 lr 0.000908	time 0.7430 (0.7538)	loss 3.2725 (3.5938)	grad_norm 1.1090 (1.2738)	mem 23874MB
[2022-11-11 15:33:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][400/1251]	eta 0:10:40 lr 0.000908	time 0.7395 (0.7531)	loss 3.1940 (3.5877)	grad_norm 1.3189 (1.2740)	mem 23874MB
[2022-11-11 15:34:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][450/1251]	eta 0:10:02 lr 0.000907	time 0.7359 (0.7524)	loss 3.8877 (3.5716)	grad_norm 1.2049 (1.2788)	mem 23874MB
[2022-11-11 15:34:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][500/1251]	eta 0:09:25 lr 0.000907	time 0.7423 (0.7524)	loss 3.3179 (3.5822)	grad_norm 1.3512 (1.2810)	mem 23874MB
[2022-11-11 15:35:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][550/1251]	eta 0:08:47 lr 0.000907	time 0.7393 (0.7519)	loss 4.0804 (3.5984)	grad_norm 1.3917 (1.2821)	mem 23874MB
[2022-11-11 15:35:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][600/1251]	eta 0:08:09 lr 0.000907	time 0.7466 (0.7517)	loss 4.1815 (3.5971)	grad_norm 1.1901 (1.2823)	mem 23874MB
[2022-11-11 15:36:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][650/1251]	eta 0:07:31 lr 0.000907	time 0.7425 (0.7514)	loss 2.9251 (3.5931)	grad_norm 1.1202 (1.2825)	mem 23874MB
[2022-11-11 15:37:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][700/1251]	eta 0:06:53 lr 0.000907	time 0.8054 (0.7510)	loss 3.8578 (3.5964)	grad_norm 1.3400 (1.2819)	mem 23874MB
[2022-11-11 15:37:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][750/1251]	eta 0:06:16 lr 0.000907	time 0.7355 (0.7509)	loss 3.9413 (3.5915)	grad_norm 1.4629 (1.2820)	mem 23874MB
[2022-11-11 15:38:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][800/1251]	eta 0:05:38 lr 0.000907	time 0.7437 (0.7505)	loss 2.8563 (3.5970)	grad_norm 1.1846 (1.2806)	mem 23874MB
[2022-11-11 15:39:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][850/1251]	eta 0:05:00 lr 0.000906	time 0.7563 (0.7506)	loss 3.9974 (3.6000)	grad_norm 1.2156 (1.2806)	mem 23874MB
[2022-11-11 15:39:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][900/1251]	eta 0:04:23 lr 0.000906	time 0.7352 (0.7504)	loss 3.5845 (3.6039)	grad_norm 1.2456 (1.2808)	mem 23874MB
[2022-11-11 15:40:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][950/1251]	eta 0:03:45 lr 0.000906	time 0.7558 (0.7504)	loss 3.5913 (3.6127)	grad_norm 1.1155 (1.2799)	mem 23874MB
[2022-11-11 15:40:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][1000/1251]	eta 0:03:08 lr 0.000906	time 0.7402 (0.7501)	loss 2.7676 (3.6128)	grad_norm 1.2121 (1.2800)	mem 23874MB
[2022-11-11 15:41:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][1050/1251]	eta 0:02:30 lr 0.000906	time 0.7438 (0.7502)	loss 3.1302 (3.6114)	grad_norm 1.2810 (1.2816)	mem 23874MB
[2022-11-11 15:42:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][1100/1251]	eta 0:01:53 lr 0.000906	time 0.8151 (0.7500)	loss 3.7544 (3.6142)	grad_norm 1.1727 (1.2820)	mem 23874MB
[2022-11-11 15:42:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][1150/1251]	eta 0:01:15 lr 0.000906	time 0.7329 (0.7499)	loss 3.7974 (3.6122)	grad_norm 1.3092 (1.2814)	mem 23874MB
[2022-11-11 15:43:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][1200/1251]	eta 0:00:38 lr 0.000906	time 0.7426 (0.7498)	loss 3.9015 (3.6078)	grad_norm 1.3002 (1.2818)	mem 23874MB
[2022-11-11 15:43:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [59/300][1250/1251]	eta 0:00:00 lr 0.000905	time 0.7263 (0.7496)	loss 4.3978 (3.6094)	grad_norm 1.2627 (1.2818)	mem 23874MB
[2022-11-11 15:43:59 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 59 training takes 0:15:37
[2022-11-11 15:44:00 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_59.pth saving......
[2022-11-11 15:44:01 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_59.pth saved !!!
[2022-11-11 15:44:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.642 (1.642)	Loss 1.0304 (1.0304)	Acc@1 75.098 (75.098)	Acc@5 93.457 (93.457)	Mem 23874MB
[2022-11-11 15:44:13 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.810 Acc@5 92.842
[2022-11-11 15:44:13 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.8%
[2022-11-11 15:44:15 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.830 (1.830)	Loss 0.9389 (0.9389)	Acc@1 76.758 (76.758)	Acc@5 93.164 (93.164)	Mem 23874MB
[2022-11-11 15:44:26 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.312 Acc@5 94.002
[2022-11-11 15:44:26 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.3%
[2022-11-11 15:44:26 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.31% at 59 epoch
[2022-11-11 15:44:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][0/1251]	eta 0:50:32 lr 0.000905	time 2.4243 (2.4243)	loss 3.4103 (3.4103)	grad_norm 1.3445 (1.3445)	mem 23874MB
[2022-11-11 15:45:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][50/1251]	eta 0:15:38 lr 0.000905	time 0.7415 (0.7817)	loss 4.0314 (3.5817)	grad_norm 1.4295 (1.2594)	mem 23874MB
[2022-11-11 15:45:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][100/1251]	eta 0:14:40 lr 0.000905	time 0.7449 (0.7652)	loss 4.2790 (3.5669)	grad_norm 1.4852 (1.3019)	mem 23874MB
[2022-11-11 15:46:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][150/1251]	eta 0:13:55 lr 0.000905	time 0.7407 (0.7591)	loss 3.3098 (3.5939)	grad_norm 1.2361 (1.2952)	mem 23874MB
[2022-11-11 15:46:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][200/1251]	eta 0:13:14 lr 0.000905	time 0.7381 (0.7562)	loss 3.7303 (3.6046)	grad_norm 1.3797 (1.2873)	mem 23874MB
[2022-11-11 15:47:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][250/1251]	eta 0:12:35 lr 0.000905	time 0.7455 (0.7544)	loss 3.8761 (3.6035)	grad_norm 1.1846 (1.2866)	mem 23874MB
[2022-11-11 15:48:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][300/1251]	eta 0:11:56 lr 0.000905	time 0.7416 (0.7532)	loss 4.1832 (3.5979)	grad_norm 1.3393 (1.2806)	mem 23874MB
[2022-11-11 15:48:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][350/1251]	eta 0:11:18 lr 0.000905	time 0.7355 (0.7528)	loss 2.6306 (3.5939)	grad_norm 1.3675 (1.2845)	mem 23874MB
[2022-11-11 15:49:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][400/1251]	eta 0:10:40 lr 0.000904	time 0.7370 (0.7521)	loss 3.8593 (3.6035)	grad_norm 1.4187 (1.2847)	mem 23874MB
[2022-11-11 15:50:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][450/1251]	eta 0:10:02 lr 0.000904	time 0.7361 (0.7518)	loss 4.0127 (3.6016)	grad_norm 1.2416 (1.2829)	mem 23874MB
[2022-11-11 15:50:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][500/1251]	eta 0:09:24 lr 0.000904	time 0.7443 (0.7512)	loss 3.6044 (3.6029)	grad_norm 1.2462 (1.2804)	mem 23874MB
[2022-11-11 15:51:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][550/1251]	eta 0:08:46 lr 0.000904	time 0.8244 (0.7507)	loss 4.0597 (3.6157)	grad_norm 1.1272 (1.2816)	mem 23874MB
[2022-11-11 15:51:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][600/1251]	eta 0:08:08 lr 0.000904	time 0.7363 (0.7504)	loss 3.6939 (3.6205)	grad_norm 1.5026 (1.2829)	mem 23874MB
[2022-11-11 15:52:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][650/1251]	eta 0:07:30 lr 0.000904	time 0.7441 (0.7500)	loss 3.3118 (3.6295)	grad_norm 1.1624 (1.2843)	mem 23874MB
[2022-11-11 15:53:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][700/1251]	eta 0:06:53 lr 0.000904	time 0.7369 (0.7501)	loss 3.4502 (3.6307)	grad_norm 1.1967 (1.2854)	mem 23874MB
[2022-11-11 15:53:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][750/1251]	eta 0:06:15 lr 0.000904	time 0.7394 (0.7499)	loss 4.5786 (3.6275)	grad_norm 1.2744 (1.2844)	mem 23874MB
[2022-11-11 15:54:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][800/1251]	eta 0:05:38 lr 0.000904	time 0.7383 (0.7496)	loss 2.7403 (3.6230)	grad_norm 1.3903 (1.2849)	mem 23874MB
[2022-11-11 15:55:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][850/1251]	eta 0:05:00 lr 0.000903	time 0.7366 (0.7496)	loss 2.7480 (3.6191)	grad_norm 1.2166 (1.2859)	mem 23874MB
[2022-11-11 15:55:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][900/1251]	eta 0:04:23 lr 0.000903	time 0.7369 (0.7493)	loss 2.5660 (3.6136)	grad_norm 1.4782 (1.2847)	mem 23874MB
[2022-11-11 15:56:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][950/1251]	eta 0:03:45 lr 0.000903	time 0.7432 (0.7492)	loss 3.7497 (3.6080)	grad_norm 1.2787 (1.2867)	mem 23874MB
[2022-11-11 15:56:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][1000/1251]	eta 0:03:08 lr 0.000903	time 0.8208 (0.7492)	loss 2.6424 (3.6056)	grad_norm 1.1986 (1.2869)	mem 23874MB
[2022-11-11 15:57:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][1050/1251]	eta 0:02:30 lr 0.000903	time 0.7365 (0.7490)	loss 4.3815 (3.6113)	grad_norm 1.3394 (1.2868)	mem 23874MB
[2022-11-11 15:58:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][1100/1251]	eta 0:01:53 lr 0.000903	time 0.8139 (0.7490)	loss 2.7931 (3.6131)	grad_norm 1.2131 (1.2864)	mem 23874MB
[2022-11-11 15:58:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][1150/1251]	eta 0:01:15 lr 0.000903	time 0.7427 (0.7489)	loss 4.0523 (3.6100)	grad_norm 1.2192 (1.2872)	mem 23874MB
[2022-11-11 15:59:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][1200/1251]	eta 0:00:38 lr 0.000903	time 0.7356 (0.7488)	loss 4.2353 (3.6134)	grad_norm 1.1368 (1.2876)	mem 23874MB
[2022-11-11 16:00:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [60/300][1250/1251]	eta 0:00:00 lr 0.000902	time 0.7264 (0.7486)	loss 3.8271 (3.6099)	grad_norm 1.2624 (1.2881)	mem 23874MB
[2022-11-11 16:00:02 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 60 training takes 0:15:36
[2022-11-11 16:00:02 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_60.pth saving......
[2022-11-11 16:00:04 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_60.pth saved !!!
[2022-11-11 16:00:05 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.792 (1.792)	Loss 0.9816 (0.9816)	Acc@1 76.367 (76.367)	Acc@5 94.531 (94.531)	Mem 23874MB
[2022-11-11 16:00:16 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.992 Acc@5 92.928
[2022-11-11 16:00:16 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.0%
[2022-11-11 16:00:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.914 (1.914)	Loss 0.9676 (0.9676)	Acc@1 76.758 (76.758)	Acc@5 93.555 (93.555)	Mem 23874MB
[2022-11-11 16:00:29 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.424 Acc@5 94.046
[2022-11-11 16:00:29 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.4%
[2022-11-11 16:00:29 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.42% at 60 epoch
[2022-11-11 16:00:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][0/1251]	eta 0:50:11 lr 0.000902	time 2.4072 (2.4072)	loss 4.3970 (4.3970)	grad_norm 1.3702 (1.3702)	mem 23874MB
[2022-11-11 16:01:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][50/1251]	eta 0:15:40 lr 0.000902	time 0.8315 (0.7828)	loss 3.1012 (3.5227)	grad_norm 1.2660 (1.3162)	mem 23874MB
[2022-11-11 16:01:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][100/1251]	eta 0:14:41 lr 0.000902	time 0.7419 (0.7659)	loss 4.1352 (3.5568)	grad_norm 1.4435 (1.3029)	mem 23874MB
[2022-11-11 16:02:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][150/1251]	eta 0:13:56 lr 0.000902	time 0.7366 (0.7600)	loss 4.1852 (3.5635)	grad_norm 1.2744 (1.3042)	mem 23874MB
[2022-11-11 16:03:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][200/1251]	eta 0:13:15 lr 0.000902	time 0.8077 (0.7573)	loss 3.4361 (3.5593)	grad_norm 1.1773 (1.2972)	mem 23874MB
[2022-11-11 16:03:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][250/1251]	eta 0:12:35 lr 0.000902	time 0.7401 (0.7551)	loss 3.7812 (3.5753)	grad_norm 1.2962 (1.2954)	mem 23874MB
[2022-11-11 16:04:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][300/1251]	eta 0:11:56 lr 0.000902	time 0.7410 (0.7539)	loss 4.0219 (3.5833)	grad_norm 1.4114 (1.3018)	mem 23874MB
[2022-11-11 16:04:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][350/1251]	eta 0:11:18 lr 0.000902	time 0.7400 (0.7533)	loss 3.3395 (3.5869)	grad_norm 1.2114 (1.2980)	mem 23874MB
[2022-11-11 16:05:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][400/1251]	eta 0:10:40 lr 0.000901	time 0.7393 (0.7524)	loss 3.8685 (3.5949)	grad_norm 1.3010 (1.2974)	mem 23874MB
[2022-11-11 16:06:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][450/1251]	eta 0:10:02 lr 0.000901	time 0.7555 (0.7523)	loss 3.4952 (3.5913)	grad_norm 1.1403 (1.2990)	mem 23874MB
[2022-11-11 16:06:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][500/1251]	eta 0:09:24 lr 0.000901	time 0.7440 (0.7518)	loss 2.4143 (3.5911)	grad_norm 1.3602 (1.2961)	mem 23874MB
[2022-11-11 16:07:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][550/1251]	eta 0:08:46 lr 0.000901	time 0.8298 (0.7512)	loss 4.3979 (3.5989)	grad_norm 1.3885 (1.2972)	mem 23874MB
[2022-11-11 16:08:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][600/1251]	eta 0:08:08 lr 0.000901	time 0.7448 (0.7510)	loss 3.3373 (3.6035)	grad_norm 1.2991 (1.2953)	mem 23874MB
[2022-11-11 16:08:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][650/1251]	eta 0:07:31 lr 0.000901	time 0.7402 (0.7507)	loss 2.9259 (3.6033)	grad_norm 1.2065 (1.2966)	mem 23874MB
[2022-11-11 16:09:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][700/1251]	eta 0:06:53 lr 0.000901	time 0.7359 (0.7508)	loss 2.7868 (3.5991)	grad_norm 1.2758 (1.2955)	mem 23874MB
[2022-11-11 16:09:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][750/1251]	eta 0:06:15 lr 0.000901	time 0.7427 (0.7504)	loss 4.0911 (3.5975)	grad_norm 1.2470 (1.2955)	mem 23874MB
[2022-11-11 16:10:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][800/1251]	eta 0:05:38 lr 0.000900	time 0.7525 (0.7503)	loss 3.8336 (3.6033)	grad_norm 1.1920 (1.2958)	mem 23874MB
[2022-11-11 16:11:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][850/1251]	eta 0:05:00 lr 0.000900	time 0.7437 (0.7502)	loss 2.9931 (3.6053)	grad_norm 1.3542 (1.2958)	mem 23874MB
[2022-11-11 16:11:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][900/1251]	eta 0:04:23 lr 0.000900	time 0.7391 (0.7500)	loss 4.1994 (3.6135)	grad_norm 1.1704 (1.2934)	mem 23874MB
[2022-11-11 16:12:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][950/1251]	eta 0:03:45 lr 0.000900	time 0.7310 (0.7499)	loss 2.8704 (3.6152)	grad_norm 1.2905 (1.2924)	mem 23874MB
[2022-11-11 16:12:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][1000/1251]	eta 0:03:08 lr 0.000900	time 0.7390 (0.7499)	loss 3.7780 (3.6153)	grad_norm 1.4162 (1.2932)	mem 23874MB
[2022-11-11 16:13:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][1050/1251]	eta 0:02:30 lr 0.000900	time 0.7418 (0.7497)	loss 2.4674 (3.6181)	grad_norm 1.2996 (1.2931)	mem 23874MB
[2022-11-11 16:14:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][1100/1251]	eta 0:01:53 lr 0.000900	time 0.7366 (0.7497)	loss 3.6633 (3.6217)	grad_norm 1.2335 (1.2949)	mem 23874MB
[2022-11-11 16:14:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][1150/1251]	eta 0:01:15 lr 0.000900	time 0.7528 (0.7496)	loss 3.9113 (3.6181)	grad_norm 1.2280 (1.2961)	mem 23874MB
[2022-11-11 16:15:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][1200/1251]	eta 0:00:38 lr 0.000899	time 0.7397 (0.7496)	loss 2.5419 (3.6171)	grad_norm 1.4156 (1.2956)	mem 23874MB
[2022-11-11 16:16:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [61/300][1250/1251]	eta 0:00:00 lr 0.000899	time 0.7267 (0.7493)	loss 3.6346 (3.6121)	grad_norm 1.4549 (1.2955)	mem 23874MB
[2022-11-11 16:16:06 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 61 training takes 0:15:37
[2022-11-11 16:16:07 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_61.pth saving......
[2022-11-11 16:16:08 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_61.pth saved !!!
[2022-11-11 16:16:09 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.667 (1.667)	Loss 1.0008 (1.0008)	Acc@1 75.000 (75.000)	Acc@5 94.434 (94.434)	Mem 23874MB
[2022-11-11 16:16:20 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.268 Acc@5 92.956
[2022-11-11 16:16:20 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.3%
[2022-11-11 16:16:22 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.783 (1.783)	Loss 0.9365 (0.9365)	Acc@1 76.758 (76.758)	Acc@5 94.043 (94.043)	Mem 23874MB
[2022-11-11 16:16:33 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.546 Acc@5 94.124
[2022-11-11 16:16:33 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.5%
[2022-11-11 16:16:33 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.55% at 61 epoch
[2022-11-11 16:16:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][0/1251]	eta 0:49:40 lr 0.000899	time 2.3829 (2.3829)	loss 3.9301 (3.9301)	grad_norm 1.2021 (1.2021)	mem 23874MB
[2022-11-11 16:17:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][50/1251]	eta 0:15:40 lr 0.000899	time 0.7389 (0.7835)	loss 3.1620 (3.6065)	grad_norm 1.2017 (1.2972)	mem 23874MB
[2022-11-11 16:17:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][100/1251]	eta 0:14:42 lr 0.000899	time 0.7389 (0.7670)	loss 4.1407 (3.5509)	grad_norm 1.2386 (1.2799)	mem 23874MB
[2022-11-11 16:18:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][150/1251]	eta 0:13:57 lr 0.000899	time 0.7432 (0.7604)	loss 4.3207 (3.5285)	grad_norm 1.2523 (nan)	mem 23874MB
[2022-11-11 16:19:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][200/1251]	eta 0:13:16 lr 0.000899	time 0.7443 (0.7575)	loss 3.8912 (3.5231)	grad_norm 1.2960 (nan)	mem 23874MB
[2022-11-11 16:19:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][250/1251]	eta 0:12:36 lr 0.000899	time 0.7354 (0.7556)	loss 3.2788 (3.5371)	grad_norm 1.2210 (nan)	mem 23874MB
[2022-11-11 16:20:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][300/1251]	eta 0:11:57 lr 0.000899	time 0.7408 (0.7542)	loss 3.5708 (3.5337)	grad_norm 1.2515 (nan)	mem 23874MB
[2022-11-11 16:20:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][350/1251]	eta 0:11:19 lr 0.000898	time 0.7362 (0.7537)	loss 4.4381 (3.5435)	grad_norm 1.4875 (nan)	mem 23874MB
[2022-11-11 16:21:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][400/1251]	eta 0:10:40 lr 0.000898	time 0.7409 (0.7526)	loss 4.0298 (3.5600)	grad_norm 1.5181 (nan)	mem 23874MB
[2022-11-11 16:22:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][450/1251]	eta 0:10:02 lr 0.000898	time 0.7441 (0.7521)	loss 2.6884 (3.5472)	grad_norm 1.2574 (nan)	mem 23874MB
[2022-11-11 16:22:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][500/1251]	eta 0:09:24 lr 0.000898	time 0.7398 (0.7516)	loss 3.5008 (3.5579)	grad_norm 1.4013 (nan)	mem 23874MB
[2022-11-11 16:23:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][550/1251]	eta 0:08:46 lr 0.000898	time 0.7358 (0.7512)	loss 3.8191 (3.5623)	grad_norm 1.3290 (nan)	mem 23874MB
[2022-11-11 16:24:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][600/1251]	eta 0:08:08 lr 0.000898	time 0.7455 (0.7510)	loss 3.1690 (3.5694)	grad_norm 1.3365 (nan)	mem 23874MB
[2022-11-11 16:24:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][650/1251]	eta 0:07:31 lr 0.000898	time 0.7386 (0.7509)	loss 3.9910 (3.5718)	grad_norm 1.2427 (nan)	mem 23874MB
[2022-11-11 16:25:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][700/1251]	eta 0:06:53 lr 0.000898	time 0.7420 (0.7505)	loss 4.2126 (3.5723)	grad_norm 1.2495 (nan)	mem 23874MB
[2022-11-11 16:25:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][750/1251]	eta 0:06:15 lr 0.000897	time 0.7434 (0.7504)	loss 4.2162 (3.5695)	grad_norm 1.5097 (nan)	mem 23874MB
[2022-11-11 16:26:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][800/1251]	eta 0:05:38 lr 0.000897	time 0.7359 (0.7501)	loss 2.4713 (3.5692)	grad_norm 1.3895 (nan)	mem 23874MB
[2022-11-11 16:27:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][850/1251]	eta 0:05:00 lr 0.000897	time 0.7385 (0.7499)	loss 3.8258 (3.5699)	grad_norm 1.1194 (nan)	mem 23874MB
[2022-11-11 16:27:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][900/1251]	eta 0:04:23 lr 0.000897	time 0.7367 (0.7499)	loss 2.9794 (3.5627)	grad_norm 1.3228 (nan)	mem 23874MB
[2022-11-11 16:28:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][950/1251]	eta 0:03:45 lr 0.000897	time 0.7310 (0.7498)	loss 4.0123 (3.5622)	grad_norm 1.2468 (nan)	mem 23874MB
[2022-11-11 16:29:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][1000/1251]	eta 0:03:08 lr 0.000897	time 0.7399 (0.7497)	loss 3.4934 (3.5621)	grad_norm 1.2344 (nan)	mem 23874MB
[2022-11-11 16:29:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][1050/1251]	eta 0:02:30 lr 0.000897	time 0.7260 (0.7498)	loss 4.1961 (3.5663)	grad_norm 1.2668 (nan)	mem 23874MB
[2022-11-11 16:30:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][1100/1251]	eta 0:01:53 lr 0.000897	time 0.7421 (0.7497)	loss 3.7230 (3.5690)	grad_norm 1.3169 (nan)	mem 23874MB
[2022-11-11 16:30:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][1150/1251]	eta 0:01:15 lr 0.000896	time 0.7410 (0.7497)	loss 4.0138 (3.5720)	grad_norm 1.1640 (nan)	mem 23874MB
[2022-11-11 16:31:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][1200/1251]	eta 0:00:38 lr 0.000896	time 0.7426 (0.7497)	loss 4.1690 (3.5737)	grad_norm 1.3996 (nan)	mem 23874MB
[2022-11-11 16:32:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [62/300][1250/1251]	eta 0:00:00 lr 0.000896	time 0.7301 (0.7494)	loss 3.9960 (3.5728)	grad_norm 1.4796 (nan)	mem 23874MB
[2022-11-11 16:32:10 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 62 training takes 0:15:37
[2022-11-11 16:32:10 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_62.pth saving......
[2022-11-11 16:32:12 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_62.pth saved !!!
[2022-11-11 16:32:13 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.632 (1.632)	Loss 1.0447 (1.0447)	Acc@1 74.512 (74.512)	Acc@5 93.750 (93.750)	Mem 23874MB
[2022-11-11 16:32:24 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.096 Acc@5 92.956
[2022-11-11 16:32:24 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.1%
[2022-11-11 16:32:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.789 (1.789)	Loss 0.9523 (0.9523)	Acc@1 76.465 (76.465)	Acc@5 93.555 (93.555)	Mem 23874MB
[2022-11-11 16:32:37 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.640 Acc@5 94.168
[2022-11-11 16:32:37 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.6%
[2022-11-11 16:32:37 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.64% at 62 epoch
[2022-11-11 16:32:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][0/1251]	eta 0:49:27 lr 0.000896	time 2.3722 (2.3722)	loss 3.6128 (3.6128)	grad_norm 1.2579 (1.2579)	mem 23874MB
[2022-11-11 16:33:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][50/1251]	eta 0:15:39 lr 0.000896	time 0.7478 (0.7819)	loss 3.3429 (3.5350)	grad_norm 1.3650 (1.3227)	mem 23874MB
[2022-11-11 16:33:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][100/1251]	eta 0:14:41 lr 0.000896	time 0.7360 (0.7655)	loss 4.0956 (3.5430)	grad_norm 1.2216 (1.3066)	mem 23874MB
[2022-11-11 16:34:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][150/1251]	eta 0:13:56 lr 0.000896	time 0.7371 (0.7595)	loss 3.6573 (3.5464)	grad_norm 1.2885 (1.3036)	mem 23874MB
[2022-11-11 16:35:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][200/1251]	eta 0:13:14 lr 0.000896	time 0.7385 (0.7560)	loss 3.4543 (3.5360)	grad_norm 1.3547 (1.2999)	mem 23874MB
[2022-11-11 16:35:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][250/1251]	eta 0:12:35 lr 0.000895	time 0.7361 (0.7544)	loss 2.7202 (3.5440)	grad_norm 1.4179 (1.2995)	mem 23874MB
[2022-11-11 16:36:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][300/1251]	eta 0:11:55 lr 0.000895	time 0.7459 (0.7528)	loss 2.8032 (3.5284)	grad_norm 1.3990 (1.2986)	mem 23874MB
[2022-11-11 16:37:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][350/1251]	eta 0:11:17 lr 0.000895	time 0.7457 (0.7523)	loss 4.0897 (3.5402)	grad_norm 1.4369 (1.2980)	mem 23874MB
[2022-11-11 16:37:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][400/1251]	eta 0:10:39 lr 0.000895	time 0.7392 (0.7514)	loss 3.0224 (3.5544)	grad_norm 1.2966 (1.2957)	mem 23874MB
[2022-11-11 16:38:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][450/1251]	eta 0:10:01 lr 0.000895	time 0.7402 (0.7512)	loss 3.9612 (3.5784)	grad_norm 1.2888 (1.2941)	mem 23874MB
[2022-11-11 16:38:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][500/1251]	eta 0:09:23 lr 0.000895	time 0.7337 (0.7506)	loss 3.7535 (3.5742)	grad_norm 1.3387 (1.2911)	mem 23874MB
[2022-11-11 16:39:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][550/1251]	eta 0:08:46 lr 0.000895	time 0.8021 (0.7504)	loss 4.2281 (3.5846)	grad_norm 1.2350 (1.2910)	mem 23874MB
[2022-11-11 16:40:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][600/1251]	eta 0:08:08 lr 0.000895	time 0.8258 (0.7498)	loss 2.5719 (3.5746)	grad_norm 1.3584 (1.2918)	mem 23874MB
[2022-11-11 16:40:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][650/1251]	eta 0:07:30 lr 0.000894	time 0.7456 (0.7495)	loss 3.9532 (3.5894)	grad_norm 1.3033 (1.2945)	mem 23874MB
[2022-11-11 16:41:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][700/1251]	eta 0:06:52 lr 0.000894	time 0.7373 (0.7490)	loss 3.6937 (3.5859)	grad_norm 1.3007 (1.2941)	mem 23874MB
[2022-11-11 16:41:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][750/1251]	eta 0:06:15 lr 0.000894	time 0.7572 (0.7490)	loss 3.7332 (3.5762)	grad_norm 1.2623 (1.2966)	mem 23874MB
[2022-11-11 16:42:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][800/1251]	eta 0:05:37 lr 0.000894	time 0.7389 (0.7488)	loss 4.0488 (3.5710)	grad_norm 1.2539 (1.2967)	mem 23874MB
[2022-11-11 16:43:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][850/1251]	eta 0:05:00 lr 0.000894	time 0.7378 (0.7488)	loss 3.3329 (3.5690)	grad_norm 1.3663 (1.2970)	mem 23874MB
[2022-11-11 16:43:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][900/1251]	eta 0:04:22 lr 0.000894	time 0.6962 (0.7485)	loss 3.3519 (3.5664)	grad_norm nan (nan)	mem 23874MB
[2022-11-11 16:44:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][950/1251]	eta 0:03:45 lr 0.000894	time 0.7373 (0.7485)	loss 3.1165 (3.5647)	grad_norm 1.1892 (nan)	mem 23874MB
[2022-11-11 16:45:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][1000/1251]	eta 0:03:07 lr 0.000894	time 0.7390 (0.7482)	loss 4.0040 (3.5680)	grad_norm 1.1889 (nan)	mem 23874MB
[2022-11-11 16:45:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][1050/1251]	eta 0:02:30 lr 0.000893	time 0.7376 (0.7482)	loss 3.0464 (3.5627)	grad_norm 1.2569 (nan)	mem 23874MB
[2022-11-11 16:46:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][1100/1251]	eta 0:01:52 lr 0.000893	time 0.7348 (0.7479)	loss 3.5398 (3.5617)	grad_norm 1.3335 (nan)	mem 23874MB
[2022-11-11 16:46:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][1150/1251]	eta 0:01:15 lr 0.000893	time 0.7340 (0.7480)	loss 3.9156 (3.5650)	grad_norm 1.2090 (nan)	mem 23874MB
[2022-11-11 16:47:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][1200/1251]	eta 0:00:38 lr 0.000893	time 0.7441 (0.7478)	loss 3.1435 (3.5682)	grad_norm 1.1467 (nan)	mem 23874MB
[2022-11-11 16:48:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [63/300][1250/1251]	eta 0:00:00 lr 0.000893	time 0.7288 (0.7477)	loss 2.6529 (3.5667)	grad_norm 1.3293 (nan)	mem 23874MB
[2022-11-11 16:48:12 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 63 training takes 0:15:35
[2022-11-11 16:48:12 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_63.pth saving......
[2022-11-11 16:48:13 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_63.pth saved !!!
[2022-11-11 16:48:15 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.630 (1.630)	Loss 1.1279 (1.1279)	Acc@1 73.340 (73.340)	Acc@5 92.090 (92.090)	Mem 23874MB
[2022-11-11 16:48:26 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 74.762 Acc@5 92.784
[2022-11-11 16:48:26 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 74.8%
[2022-11-11 16:48:28 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.850 (1.850)	Loss 0.8631 (0.8631)	Acc@1 78.613 (78.613)	Acc@5 94.922 (94.922)	Mem 23874MB
[2022-11-11 16:48:38 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.722 Acc@5 94.188
[2022-11-11 16:48:38 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.7%
[2022-11-11 16:48:38 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.72% at 63 epoch
[2022-11-11 16:48:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][0/1251]	eta 0:54:56 lr 0.000893	time 2.6351 (2.6351)	loss 3.5667 (3.5667)	grad_norm 1.2838 (1.2838)	mem 23874MB
[2022-11-11 16:49:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][50/1251]	eta 0:15:42 lr 0.000893	time 0.7338 (0.7849)	loss 3.3268 (3.5455)	grad_norm 1.1576 (1.2939)	mem 23874MB
[2022-11-11 16:49:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][100/1251]	eta 0:14:42 lr 0.000893	time 0.8323 (0.7671)	loss 3.6861 (3.5538)	grad_norm 1.3672 (1.2785)	mem 23874MB
[2022-11-11 16:50:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][150/1251]	eta 0:13:57 lr 0.000893	time 0.7356 (0.7606)	loss 3.9497 (3.5211)	grad_norm 1.3408 (1.2894)	mem 23874MB
[2022-11-11 16:51:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][200/1251]	eta 0:13:16 lr 0.000892	time 0.8232 (0.7577)	loss 3.1136 (3.5530)	grad_norm 1.2787 (1.2948)	mem 23874MB
[2022-11-11 16:51:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][250/1251]	eta 0:12:35 lr 0.000892	time 0.7387 (0.7550)	loss 3.3696 (3.5565)	grad_norm 1.2814 (1.2911)	mem 23874MB
[2022-11-11 16:52:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][300/1251]	eta 0:11:57 lr 0.000892	time 0.8234 (0.7541)	loss 3.4781 (3.5558)	grad_norm 1.1651 (1.2909)	mem 23874MB
[2022-11-11 16:53:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][350/1251]	eta 0:11:17 lr 0.000892	time 0.7380 (0.7525)	loss 2.5867 (3.5633)	grad_norm 1.2675 (1.2876)	mem 23874MB
[2022-11-11 16:53:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][400/1251]	eta 0:10:39 lr 0.000892	time 0.7412 (0.7518)	loss 3.5324 (3.5559)	grad_norm 1.3869 (1.2876)	mem 23874MB
[2022-11-11 16:54:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][450/1251]	eta 0:10:02 lr 0.000892	time 0.7436 (0.7516)	loss 3.6056 (3.5531)	grad_norm 1.1443 (1.2859)	mem 23874MB
[2022-11-11 16:54:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][500/1251]	eta 0:09:23 lr 0.000892	time 0.7388 (0.7508)	loss 3.1564 (3.5565)	grad_norm 1.3690 (1.2869)	mem 23874MB
[2022-11-11 16:55:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][550/1251]	eta 0:08:46 lr 0.000892	time 0.8043 (0.7507)	loss 4.4987 (3.5617)	grad_norm 1.2151 (1.2887)	mem 23874MB
[2022-11-11 16:56:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][600/1251]	eta 0:08:08 lr 0.000891	time 0.8411 (0.7502)	loss 4.4719 (3.5629)	grad_norm 1.3943 (1.2885)	mem 23874MB
[2022-11-11 16:56:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][650/1251]	eta 0:07:30 lr 0.000891	time 0.7396 (0.7500)	loss 2.5579 (3.5575)	grad_norm 1.4193 (1.2870)	mem 23874MB
[2022-11-11 16:57:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][700/1251]	eta 0:06:53 lr 0.000891	time 0.8195 (0.7498)	loss 3.3884 (3.5598)	grad_norm 1.3673 (1.2892)	mem 23874MB
[2022-11-11 16:58:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][750/1251]	eta 0:06:15 lr 0.000891	time 0.7381 (0.7494)	loss 3.8224 (3.5582)	grad_norm 1.2930 (1.2886)	mem 23874MB
[2022-11-11 16:58:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][800/1251]	eta 0:05:37 lr 0.000891	time 0.7425 (0.7492)	loss 4.1054 (3.5649)	grad_norm 1.4075 (1.2891)	mem 23874MB
[2022-11-11 16:59:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][850/1251]	eta 0:05:00 lr 0.000891	time 0.7392 (0.7491)	loss 3.5370 (3.5692)	grad_norm 1.2669 (1.2924)	mem 23874MB
[2022-11-11 16:59:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][900/1251]	eta 0:04:22 lr 0.000891	time 0.7403 (0.7489)	loss 3.5701 (3.5760)	grad_norm 1.2466 (1.2926)	mem 23874MB
[2022-11-11 17:00:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][950/1251]	eta 0:03:45 lr 0.000890	time 0.7390 (0.7490)	loss 3.4987 (3.5835)	grad_norm 1.3553 (1.2932)	mem 23874MB
[2022-11-11 17:01:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][1000/1251]	eta 0:03:07 lr 0.000890	time 0.8191 (0.7488)	loss 3.5165 (3.5786)	grad_norm 1.2540 (1.2942)	mem 23874MB
[2022-11-11 17:01:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][1050/1251]	eta 0:02:30 lr 0.000890	time 0.7437 (0.7488)	loss 2.6951 (3.5765)	grad_norm 1.3005 (1.2948)	mem 23874MB
[2022-11-11 17:02:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][1100/1251]	eta 0:01:53 lr 0.000890	time 0.8137 (0.7487)	loss 3.3683 (3.5771)	grad_norm 1.1688 (1.2947)	mem 23874MB
[2022-11-11 17:03:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][1150/1251]	eta 0:01:15 lr 0.000890	time 0.7415 (0.7487)	loss 2.9502 (3.5773)	grad_norm 1.3253 (1.2944)	mem 23874MB
[2022-11-11 17:03:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][1200/1251]	eta 0:00:38 lr 0.000890	time 0.7400 (0.7486)	loss 3.7074 (3.5747)	grad_norm 1.3042 (1.2942)	mem 23874MB
[2022-11-11 17:04:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [64/300][1250/1251]	eta 0:00:00 lr 0.000890	time 0.7243 (0.7485)	loss 3.4810 (3.5708)	grad_norm 1.4611 (1.2944)	mem 23874MB
[2022-11-11 17:04:15 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 64 training takes 0:15:36
[2022-11-11 17:04:15 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_64.pth saving......
[2022-11-11 17:04:16 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_64.pth saved !!!
[2022-11-11 17:04:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.699 (1.699)	Loss 1.0715 (1.0715)	Acc@1 73.535 (73.535)	Acc@5 93.066 (93.066)	Mem 23874MB
[2022-11-11 17:04:29 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.434 Acc@5 93.084
[2022-11-11 17:04:29 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.4%
[2022-11-11 17:04:31 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.948 (1.948)	Loss 0.8519 (0.8519)	Acc@1 79.980 (79.980)	Acc@5 94.336 (94.336)	Mem 23874MB
[2022-11-11 17:04:41 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.868 Acc@5 94.260
[2022-11-11 17:04:41 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 77.9%
[2022-11-11 17:04:41 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.87% at 64 epoch
[2022-11-11 17:04:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][0/1251]	eta 0:52:45 lr 0.000890	time 2.5306 (2.5306)	loss 2.8984 (2.8984)	grad_norm 1.3606 (1.3606)	mem 23874MB
[2022-11-11 17:05:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][50/1251]	eta 0:15:41 lr 0.000890	time 0.7385 (0.7843)	loss 3.0373 (3.5556)	grad_norm 1.1657 (1.3071)	mem 23874MB
[2022-11-11 17:05:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][100/1251]	eta 0:14:42 lr 0.000889	time 0.8256 (0.7670)	loss 3.8338 (3.4973)	grad_norm 1.4447 (1.3063)	mem 23874MB
[2022-11-11 17:06:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][150/1251]	eta 0:13:56 lr 0.000889	time 0.7414 (0.7602)	loss 3.4552 (3.5376)	grad_norm 1.1990 (1.3049)	mem 23874MB
[2022-11-11 17:07:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][200/1251]	eta 0:13:16 lr 0.000889	time 0.7357 (0.7576)	loss 3.9337 (3.5198)	grad_norm 1.2816 (1.3009)	mem 23874MB
[2022-11-11 17:07:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][250/1251]	eta 0:12:36 lr 0.000889	time 0.7429 (0.7555)	loss 4.4992 (3.5113)	grad_norm 1.3175 (1.3090)	mem 23874MB
[2022-11-11 17:08:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][300/1251]	eta 0:11:57 lr 0.000889	time 0.7417 (0.7546)	loss 3.3706 (3.5216)	grad_norm 1.1328 (1.3079)	mem 23874MB
[2022-11-11 17:09:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][350/1251]	eta 0:11:19 lr 0.000889	time 0.7391 (0.7538)	loss 2.7780 (3.5375)	grad_norm 1.1438 (1.3061)	mem 23874MB
[2022-11-11 17:09:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][400/1251]	eta 0:10:41 lr 0.000889	time 0.7349 (0.7533)	loss 2.7095 (3.5402)	grad_norm 1.4330 (1.3077)	mem 23874MB
[2022-11-11 17:10:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][450/1251]	eta 0:10:02 lr 0.000889	time 0.7389 (0.7525)	loss 4.0345 (3.5412)	grad_norm 1.1837 (1.3048)	mem 23874MB
[2022-11-11 17:10:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][500/1251]	eta 0:09:24 lr 0.000888	time 0.7404 (0.7518)	loss 2.5272 (3.5487)	grad_norm 1.2546 (1.3044)	mem 23874MB
[2022-11-11 17:11:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][550/1251]	eta 0:08:47 lr 0.000888	time 0.7413 (0.7518)	loss 3.4275 (3.5553)	grad_norm 1.4285 (1.3067)	mem 23874MB
[2022-11-11 17:12:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][600/1251]	eta 0:08:09 lr 0.000888	time 0.7430 (0.7515)	loss 3.6847 (3.5550)	grad_norm 1.3697 (1.3083)	mem 23874MB
[2022-11-11 17:12:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][650/1251]	eta 0:07:31 lr 0.000888	time 0.7347 (0.7513)	loss 3.9023 (3.5542)	grad_norm 1.2056 (1.3094)	mem 23874MB
[2022-11-11 17:13:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][700/1251]	eta 0:06:53 lr 0.000888	time 0.7428 (0.7509)	loss 3.8301 (3.5645)	grad_norm 1.2677 (1.3093)	mem 23874MB
[2022-11-11 17:14:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][750/1251]	eta 0:06:16 lr 0.000888	time 0.7416 (0.7508)	loss 3.3413 (3.5762)	grad_norm 1.4506 (1.3093)	mem 23874MB
[2022-11-11 17:14:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][800/1251]	eta 0:05:38 lr 0.000888	time 0.7384 (0.7505)	loss 2.9113 (3.5809)	grad_norm 1.1089 (1.3109)	mem 23874MB
[2022-11-11 17:15:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][850/1251]	eta 0:05:00 lr 0.000887	time 0.7500 (0.7506)	loss 3.5957 (3.5791)	grad_norm 1.4385 (1.3108)	mem 23874MB
[2022-11-11 17:15:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][900/1251]	eta 0:04:23 lr 0.000887	time 0.7416 (0.7504)	loss 2.8469 (3.5781)	grad_norm 1.2878 (1.3110)	mem 23874MB
[2022-11-11 17:16:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][950/1251]	eta 0:03:45 lr 0.000887	time 0.8110 (0.7505)	loss 4.0146 (3.5780)	grad_norm 1.2636 (1.3095)	mem 23874MB
[2022-11-11 17:17:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][1000/1251]	eta 0:03:08 lr 0.000887	time 0.8216 (0.7503)	loss 2.3412 (3.5768)	grad_norm 1.4831 (1.3097)	mem 23874MB
[2022-11-11 17:17:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][1050/1251]	eta 0:02:30 lr 0.000887	time 0.7436 (0.7502)	loss 3.7749 (3.5757)	grad_norm 1.2104 (1.3098)	mem 23874MB
[2022-11-11 17:18:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][1100/1251]	eta 0:01:53 lr 0.000887	time 0.7415 (0.7502)	loss 3.7255 (3.5744)	grad_norm 1.2534 (1.3089)	mem 23874MB
[2022-11-11 17:19:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][1150/1251]	eta 0:01:15 lr 0.000887	time 0.7359 (0.7502)	loss 2.8992 (3.5769)	grad_norm 1.4274 (1.3081)	mem 23874MB
[2022-11-11 17:19:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][1200/1251]	eta 0:00:38 lr 0.000887	time 0.7358 (0.7502)	loss 4.2302 (3.5803)	grad_norm 1.1409 (1.3087)	mem 23874MB
[2022-11-11 17:20:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [65/300][1250/1251]	eta 0:00:00 lr 0.000886	time 0.7265 (0.7500)	loss 3.8886 (3.5816)	grad_norm 1.3775 (1.3089)	mem 23874MB
[2022-11-11 17:20:20 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 65 training takes 0:15:38
[2022-11-11 17:20:20 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_65.pth saving......
[2022-11-11 17:20:21 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_65.pth saved !!!
[2022-11-11 17:20:23 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.643 (1.643)	Loss 1.0145 (1.0145)	Acc@1 76.172 (76.172)	Acc@5 94.141 (94.141)	Mem 23874MB
[2022-11-11 17:20:33 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.304 Acc@5 93.048
[2022-11-11 17:20:33 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.3%
[2022-11-11 17:20:35 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.853 (1.853)	Loss 0.8579 (0.8579)	Acc@1 77.930 (77.930)	Acc@5 94.629 (94.629)	Mem 23874MB
[2022-11-11 17:20:46 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.950 Acc@5 94.320
[2022-11-11 17:20:46 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.0%
[2022-11-11 17:20:46 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 77.95% at 65 epoch
[2022-11-11 17:20:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][0/1251]	eta 0:51:21 lr 0.000886	time 2.4636 (2.4636)	loss 3.0125 (3.0125)	grad_norm 1.2752 (1.2752)	mem 23874MB
[2022-11-11 17:21:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][50/1251]	eta 0:15:43 lr 0.000886	time 0.8185 (0.7857)	loss 4.1037 (3.6533)	grad_norm 1.1969 (1.3084)	mem 23874MB
[2022-11-11 17:22:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][100/1251]	eta 0:14:42 lr 0.000886	time 0.7407 (0.7668)	loss 3.2603 (3.5619)	grad_norm 1.1739 (1.3042)	mem 23874MB
[2022-11-11 17:22:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][150/1251]	eta 0:13:57 lr 0.000886	time 0.7385 (0.7605)	loss 3.9295 (3.5924)	grad_norm 1.3277 (1.2914)	mem 23874MB
[2022-11-11 17:23:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][200/1251]	eta 0:13:15 lr 0.000886	time 0.8132 (0.7571)	loss 3.0541 (3.5844)	grad_norm 1.2095 (1.2949)	mem 23874MB
[2022-11-11 17:23:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][250/1251]	eta 0:12:35 lr 0.000886	time 0.7383 (0.7547)	loss 2.7601 (3.5699)	grad_norm 1.3403 (1.2951)	mem 23874MB
[2022-11-11 17:24:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][300/1251]	eta 0:11:57 lr 0.000886	time 0.7391 (0.7541)	loss 3.7504 (3.5898)	grad_norm 1.3163 (1.2977)	mem 23874MB
[2022-11-11 17:25:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][350/1251]	eta 0:11:18 lr 0.000885	time 0.7431 (0.7528)	loss 2.7518 (3.5768)	grad_norm 1.3001 (1.3031)	mem 23874MB
[2022-11-11 17:25:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][400/1251]	eta 0:10:40 lr 0.000885	time 0.7341 (0.7525)	loss 3.8078 (3.5901)	grad_norm 1.4242 (1.3035)	mem 23874MB
[2022-11-11 17:26:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][450/1251]	eta 0:10:02 lr 0.000885	time 0.7360 (0.7518)	loss 3.1457 (3.5784)	grad_norm 1.2683 (1.3060)	mem 23874MB
[2022-11-11 17:27:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][500/1251]	eta 0:09:24 lr 0.000885	time 0.7361 (0.7514)	loss 3.7817 (3.5643)	grad_norm 1.3185 (1.3062)	mem 23874MB
[2022-11-11 17:27:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][550/1251]	eta 0:08:46 lr 0.000885	time 0.7425 (0.7509)	loss 3.9879 (3.5722)	grad_norm 1.1498 (1.3063)	mem 23874MB
[2022-11-11 17:28:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][600/1251]	eta 0:08:08 lr 0.000885	time 0.8238 (0.7507)	loss 3.6989 (3.5748)	grad_norm 1.3803 (1.3093)	mem 23874MB
[2022-11-11 17:28:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][650/1251]	eta 0:07:30 lr 0.000885	time 0.7339 (0.7504)	loss 4.0032 (3.5772)	grad_norm 1.1469 (1.3095)	mem 23874MB
[2022-11-11 17:29:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][700/1251]	eta 0:06:53 lr 0.000885	time 0.7363 (0.7503)	loss 3.0844 (3.5787)	grad_norm 1.3143 (1.3096)	mem 23874MB
[2022-11-11 17:30:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][750/1251]	eta 0:06:15 lr 0.000884	time 0.7472 (0.7499)	loss 3.2119 (3.5794)	grad_norm 1.2976 (1.3097)	mem 23874MB
[2022-11-11 17:30:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][800/1251]	eta 0:05:38 lr 0.000884	time 0.7349 (0.7497)	loss 3.5969 (3.5723)	grad_norm 1.3574 (1.3095)	mem 23874MB
[2022-11-11 17:31:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][850/1251]	eta 0:05:00 lr 0.000884	time 0.7389 (0.7498)	loss 3.8226 (3.5712)	grad_norm 1.1835 (1.3094)	mem 23874MB
[2022-11-11 17:32:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][900/1251]	eta 0:04:23 lr 0.000884	time 0.7315 (0.7497)	loss 2.8055 (3.5720)	grad_norm 1.1558 (1.3094)	mem 23874MB
[2022-11-11 17:32:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][950/1251]	eta 0:03:45 lr 0.000884	time 0.7361 (0.7497)	loss 2.7236 (3.5711)	grad_norm 1.3506 (1.3073)	mem 23874MB
[2022-11-11 17:33:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][1000/1251]	eta 0:03:08 lr 0.000884	time 0.7310 (0.7496)	loss 2.7339 (3.5708)	grad_norm 1.3449 (1.3061)	mem 23874MB
[2022-11-11 17:33:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][1050/1251]	eta 0:02:30 lr 0.000884	time 0.7380 (0.7495)	loss 3.2887 (3.5760)	grad_norm 1.3601 (1.3057)	mem 23874MB
[2022-11-11 17:34:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][1100/1251]	eta 0:01:53 lr 0.000883	time 0.7394 (0.7494)	loss 3.8318 (3.5765)	grad_norm 1.3581 (1.3071)	mem 23874MB
[2022-11-11 17:35:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][1150/1251]	eta 0:01:15 lr 0.000883	time 0.7371 (0.7493)	loss 3.8657 (3.5766)	grad_norm 1.3662 (1.3076)	mem 23874MB
[2022-11-11 17:35:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][1200/1251]	eta 0:00:38 lr 0.000883	time 0.7421 (0.7493)	loss 3.8696 (3.5752)	grad_norm 1.1760 (1.3076)	mem 23874MB
[2022-11-11 17:36:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [66/300][1250/1251]	eta 0:00:00 lr 0.000883	time 0.7270 (0.7492)	loss 3.6539 (3.5792)	grad_norm 1.4705 (1.3069)	mem 23874MB
[2022-11-11 17:36:23 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 66 training takes 0:15:37
[2022-11-11 17:36:23 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_66.pth saving......
[2022-11-11 17:36:24 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_66.pth saved !!!
[2022-11-11 17:36:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.548 (1.548)	Loss 1.0387 (1.0387)	Acc@1 75.098 (75.098)	Acc@5 93.164 (93.164)	Mem 23874MB
[2022-11-11 17:36:36 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.472 Acc@5 93.176
[2022-11-11 17:36:36 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.5%
[2022-11-11 17:36:38 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.740 (1.740)	Loss 0.8733 (0.8733)	Acc@1 78.711 (78.711)	Acc@5 94.727 (94.727)	Mem 23874MB
[2022-11-11 17:36:49 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.044 Acc@5 94.348
[2022-11-11 17:36:49 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.0%
[2022-11-11 17:36:49 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.04% at 66 epoch
[2022-11-11 17:36:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][0/1251]	eta 0:51:10 lr 0.000883	time 2.4542 (2.4542)	loss 3.5324 (3.5324)	grad_norm 1.3864 (1.3864)	mem 23874MB
[2022-11-11 17:37:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][50/1251]	eta 0:15:43 lr 0.000883	time 0.7280 (0.7854)	loss 4.3544 (3.5164)	grad_norm 1.3039 (1.3088)	mem 23874MB
[2022-11-11 17:38:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][100/1251]	eta 0:14:41 lr 0.000883	time 0.7436 (0.7658)	loss 3.2958 (3.5222)	grad_norm 1.2128 (1.3011)	mem 23874MB
[2022-11-11 17:38:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][150/1251]	eta 0:13:57 lr 0.000883	time 0.7592 (0.7605)	loss 2.5343 (3.5045)	grad_norm 1.3904 (1.3088)	mem 23874MB
[2022-11-11 17:39:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][200/1251]	eta 0:13:15 lr 0.000883	time 0.7441 (0.7573)	loss 4.1483 (3.5286)	grad_norm 1.2236 (1.3104)	mem 23874MB
[2022-11-11 17:39:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][250/1251]	eta 0:12:36 lr 0.000882	time 0.7393 (0.7553)	loss 3.8421 (3.5259)	grad_norm 1.3268 (1.3060)	mem 23874MB
[2022-11-11 17:40:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][300/1251]	eta 0:11:57 lr 0.000882	time 0.7396 (0.7540)	loss 3.9897 (3.5441)	grad_norm 1.2029 (1.3096)	mem 23874MB
[2022-11-11 17:41:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][350/1251]	eta 0:11:18 lr 0.000882	time 0.7362 (0.7528)	loss 3.7855 (3.5488)	grad_norm 1.2918 (nan)	mem 23874MB
[2022-11-11 17:41:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][400/1251]	eta 0:10:40 lr 0.000882	time 0.7410 (0.7521)	loss 3.2628 (3.5458)	grad_norm 1.1758 (nan)	mem 23874MB
[2022-11-11 17:42:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][450/1251]	eta 0:10:01 lr 0.000882	time 0.7362 (0.7514)	loss 3.8025 (3.5486)	grad_norm 1.4989 (nan)	mem 23874MB
[2022-11-11 17:43:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][500/1251]	eta 0:09:23 lr 0.000882	time 0.7473 (0.7510)	loss 3.8471 (3.5538)	grad_norm 1.3963 (nan)	mem 23874MB
[2022-11-11 17:43:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][550/1251]	eta 0:08:46 lr 0.000882	time 0.7379 (0.7506)	loss 3.8703 (3.5602)	grad_norm 1.2705 (nan)	mem 23874MB
[2022-11-11 17:44:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][600/1251]	eta 0:08:08 lr 0.000881	time 0.7380 (0.7504)	loss 4.0064 (3.5570)	grad_norm 1.2742 (nan)	mem 23874MB
[2022-11-11 17:44:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][650/1251]	eta 0:07:30 lr 0.000881	time 0.8174 (0.7502)	loss 2.4805 (3.5523)	grad_norm 1.2212 (nan)	mem 23874MB
[2022-11-11 17:45:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][700/1251]	eta 0:06:53 lr 0.000881	time 0.7375 (0.7500)	loss 3.6813 (3.5554)	grad_norm 1.1576 (nan)	mem 23874MB
[2022-11-11 17:46:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][750/1251]	eta 0:06:15 lr 0.000881	time 0.7445 (0.7498)	loss 3.9211 (3.5520)	grad_norm 1.4845 (nan)	mem 23874MB
[2022-11-11 17:46:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][800/1251]	eta 0:05:38 lr 0.000881	time 0.7532 (0.7498)	loss 2.5095 (3.5540)	grad_norm 1.2948 (nan)	mem 23874MB
[2022-11-11 17:47:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][850/1251]	eta 0:05:00 lr 0.000881	time 0.7399 (0.7496)	loss 2.9007 (3.5587)	grad_norm 1.4215 (nan)	mem 23874MB
[2022-11-11 17:48:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][900/1251]	eta 0:04:23 lr 0.000881	time 0.7441 (0.7496)	loss 2.9593 (3.5679)	grad_norm 1.2232 (nan)	mem 23874MB
[2022-11-11 17:48:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][950/1251]	eta 0:03:45 lr 0.000881	time 0.7387 (0.7495)	loss 3.3374 (3.5639)	grad_norm 1.2625 (nan)	mem 23874MB
[2022-11-11 17:49:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][1000/1251]	eta 0:03:08 lr 0.000880	time 0.7374 (0.7494)	loss 3.1042 (3.5658)	grad_norm 1.4052 (nan)	mem 23874MB
[2022-11-11 17:49:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][1050/1251]	eta 0:02:30 lr 0.000880	time 0.7411 (0.7494)	loss 2.9214 (3.5701)	grad_norm 1.5050 (nan)	mem 23874MB
[2022-11-11 17:50:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][1100/1251]	eta 0:01:53 lr 0.000880	time 0.8196 (0.7494)	loss 4.0543 (3.5737)	grad_norm 1.3010 (nan)	mem 23874MB
[2022-11-11 17:51:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][1150/1251]	eta 0:01:15 lr 0.000880	time 0.7483 (0.7492)	loss 3.9731 (3.5754)	grad_norm 1.1658 (nan)	mem 23874MB
[2022-11-11 17:51:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][1200/1251]	eta 0:00:38 lr 0.000880	time 0.7403 (0.7491)	loss 4.0263 (3.5734)	grad_norm 1.3555 (nan)	mem 23874MB
[2022-11-11 17:52:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [67/300][1250/1251]	eta 0:00:00 lr 0.000880	time 0.7254 (0.7489)	loss 3.9275 (3.5715)	grad_norm 1.1713 (nan)	mem 23874MB
[2022-11-11 17:52:26 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 67 training takes 0:15:37
[2022-11-11 17:52:26 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_67.pth saving......
[2022-11-11 17:52:27 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_67.pth saved !!!
[2022-11-11 17:52:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.649 (1.649)	Loss 1.0689 (1.0689)	Acc@1 74.707 (74.707)	Acc@5 93.750 (93.750)	Mem 23874MB
[2022-11-11 17:52:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.394 Acc@5 93.138
[2022-11-11 17:52:40 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.4%
[2022-11-11 17:52:41 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.898 (1.898)	Loss 0.9431 (0.9431)	Acc@1 77.930 (77.930)	Acc@5 94.336 (94.336)	Mem 23874MB
[2022-11-11 17:52:52 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.152 Acc@5 94.394
[2022-11-11 17:52:52 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.2%
[2022-11-11 17:52:52 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.15% at 67 epoch
[2022-11-11 17:52:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][0/1251]	eta 0:51:11 lr 0.000880	time 2.4551 (2.4551)	loss 4.0260 (4.0260)	grad_norm 1.2075 (1.2075)	mem 23874MB
[2022-11-11 17:53:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][50/1251]	eta 0:15:42 lr 0.000880	time 0.7395 (0.7844)	loss 3.8748 (3.5217)	grad_norm 1.4449 (1.2791)	mem 23874MB
[2022-11-11 17:54:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][100/1251]	eta 0:14:42 lr 0.000879	time 0.7371 (0.7670)	loss 2.5373 (3.5538)	grad_norm 1.3451 (1.3076)	mem 23874MB
[2022-11-11 17:54:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][150/1251]	eta 0:13:58 lr 0.000879	time 0.7338 (0.7613)	loss 3.1313 (3.5338)	grad_norm 1.2613 (1.2991)	mem 23874MB
[2022-11-11 17:55:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][200/1251]	eta 0:13:15 lr 0.000879	time 0.7440 (0.7574)	loss 2.6277 (3.5284)	grad_norm 1.4651 (1.3095)	mem 23874MB
[2022-11-11 17:56:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][250/1251]	eta 0:12:37 lr 0.000879	time 0.7462 (0.7563)	loss 3.5792 (3.5218)	grad_norm 1.2671 (1.3141)	mem 23874MB
[2022-11-11 17:56:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][300/1251]	eta 0:11:57 lr 0.000879	time 0.8551 (0.7549)	loss 4.3005 (3.5339)	grad_norm 1.3933 (1.3221)	mem 23874MB
[2022-11-11 17:57:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][350/1251]	eta 0:11:19 lr 0.000879	time 0.7422 (0.7543)	loss 3.8443 (3.5197)	grad_norm 1.3286 (1.3219)	mem 23874MB
[2022-11-11 17:57:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][400/1251]	eta 0:10:41 lr 0.000879	time 0.7382 (0.7533)	loss 3.9820 (3.5404)	grad_norm 1.3526 (1.3247)	mem 23874MB
[2022-11-11 17:58:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][450/1251]	eta 0:10:02 lr 0.000878	time 0.7428 (0.7528)	loss 3.9975 (3.5349)	grad_norm 1.1485 (1.3236)	mem 23874MB
[2022-11-11 17:59:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][500/1251]	eta 0:09:25 lr 0.000878	time 0.7428 (0.7524)	loss 3.5447 (3.5473)	grad_norm 1.3855 (1.3203)	mem 23874MB
[2022-11-11 17:59:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][550/1251]	eta 0:08:47 lr 0.000878	time 0.7404 (0.7523)	loss 2.3056 (3.5446)	grad_norm 1.1762 (1.3190)	mem 23874MB
[2022-11-11 18:00:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][600/1251]	eta 0:08:09 lr 0.000878	time 0.7377 (0.7519)	loss 3.2016 (3.5479)	grad_norm 1.2501 (1.3213)	mem 23874MB
[2022-11-11 18:01:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][650/1251]	eta 0:07:31 lr 0.000878	time 0.7410 (0.7520)	loss 4.1735 (3.5520)	grad_norm 1.3854 (1.3224)	mem 23874MB
[2022-11-11 18:01:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][700/1251]	eta 0:06:54 lr 0.000878	time 0.7346 (0.7514)	loss 4.0902 (3.5593)	grad_norm 1.2726 (1.3211)	mem 23874MB
[2022-11-11 18:02:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][750/1251]	eta 0:06:16 lr 0.000878	time 0.7417 (0.7514)	loss 3.5355 (3.5545)	grad_norm 1.3444 (1.3199)	mem 23874MB
[2022-11-11 18:02:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][800/1251]	eta 0:05:38 lr 0.000878	time 0.7367 (0.7510)	loss 2.8022 (3.5612)	grad_norm 1.2399 (1.3190)	mem 23874MB
[2022-11-11 18:03:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][850/1251]	eta 0:05:01 lr 0.000877	time 0.7355 (0.7511)	loss 3.8856 (3.5560)	grad_norm 1.3063 (1.3169)	mem 23874MB
[2022-11-11 18:04:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][900/1251]	eta 0:04:23 lr 0.000877	time 0.7420 (0.7509)	loss 3.2387 (3.5533)	grad_norm 1.2561 (1.3163)	mem 23874MB
[2022-11-11 18:04:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][950/1251]	eta 0:03:46 lr 0.000877	time 0.7552 (0.7510)	loss 3.7373 (3.5567)	grad_norm 1.3554 (1.3160)	mem 23874MB
[2022-11-11 18:05:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][1000/1251]	eta 0:03:08 lr 0.000877	time 0.8223 (0.7508)	loss 3.9355 (3.5473)	grad_norm 1.3356 (1.3142)	mem 23874MB
[2022-11-11 18:06:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][1050/1251]	eta 0:02:30 lr 0.000877	time 0.7381 (0.7507)	loss 3.9916 (3.5472)	grad_norm 1.4111 (1.3145)	mem 23874MB
[2022-11-11 18:06:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][1100/1251]	eta 0:01:53 lr 0.000877	time 0.7401 (0.7504)	loss 2.9862 (3.5458)	grad_norm 1.4189 (1.3165)	mem 23874MB
[2022-11-11 18:07:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][1150/1251]	eta 0:01:15 lr 0.000877	time 0.7455 (0.7505)	loss 3.8431 (3.5478)	grad_norm 1.3019 (1.3166)	mem 23874MB
[2022-11-11 18:07:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][1200/1251]	eta 0:00:38 lr 0.000876	time 0.7406 (0.7504)	loss 2.4134 (3.5442)	grad_norm 1.3029 (1.3168)	mem 23874MB
[2022-11-11 18:08:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [68/300][1250/1251]	eta 0:00:00 lr 0.000876	time 0.7272 (0.7501)	loss 2.9390 (3.5437)	grad_norm 1.2534 (1.3172)	mem 23874MB
[2022-11-11 18:08:31 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 68 training takes 0:15:38
[2022-11-11 18:08:31 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_68.pth saving......
[2022-11-11 18:08:32 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_68.pth saved !!!
[2022-11-11 18:08:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.628 (1.628)	Loss 1.0250 (1.0250)	Acc@1 74.805 (74.805)	Acc@5 93.555 (93.555)	Mem 23874MB
[2022-11-11 18:08:45 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.440 Acc@5 93.348
[2022-11-11 18:08:45 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.4%
[2022-11-11 18:08:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.035 (2.035)	Loss 0.8940 (0.8940)	Acc@1 77.637 (77.637)	Acc@5 94.141 (94.141)	Mem 23874MB
[2022-11-11 18:08:57 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.246 Acc@5 94.452
[2022-11-11 18:08:57 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.2%
[2022-11-11 18:08:57 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.25% at 68 epoch
[2022-11-11 18:09:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][0/1251]	eta 0:50:14 lr 0.000876	time 2.4097 (2.4097)	loss 3.2809 (3.2809)	grad_norm 1.2138 (1.2138)	mem 23874MB
[2022-11-11 18:09:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][50/1251]	eta 0:15:41 lr 0.000876	time 0.8246 (0.7839)	loss 3.8572 (3.5307)	grad_norm 1.2750 (1.3148)	mem 23874MB
[2022-11-11 18:10:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][100/1251]	eta 0:14:40 lr 0.000876	time 0.7402 (0.7651)	loss 3.9054 (3.5444)	grad_norm 1.1761 (1.3123)	mem 23874MB
[2022-11-11 18:10:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][150/1251]	eta 0:13:56 lr 0.000876	time 0.7378 (0.7599)	loss 3.2665 (3.5611)	grad_norm 1.3874 (1.3208)	mem 23874MB
[2022-11-11 18:11:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][200/1251]	eta 0:13:15 lr 0.000876	time 0.8211 (0.7571)	loss 3.8273 (3.5190)	grad_norm 1.3220 (1.3096)	mem 23874MB
[2022-11-11 18:12:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][250/1251]	eta 0:12:35 lr 0.000876	time 0.7426 (0.7543)	loss 3.2312 (3.4963)	grad_norm 1.3560 (1.3084)	mem 23874MB
[2022-11-11 18:12:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][300/1251]	eta 0:11:56 lr 0.000875	time 0.7352 (0.7536)	loss 3.8237 (3.5050)	grad_norm 1.3033 (1.3079)	mem 23874MB
[2022-11-11 18:13:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][350/1251]	eta 0:11:17 lr 0.000875	time 0.7373 (0.7525)	loss 3.5627 (3.5019)	grad_norm 1.1904 (1.3058)	mem 23874MB
[2022-11-11 18:13:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][400/1251]	eta 0:10:40 lr 0.000875	time 0.7299 (0.7521)	loss 3.6672 (3.4978)	grad_norm 1.2477 (1.3082)	mem 23874MB
[2022-11-11 18:14:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][450/1251]	eta 0:10:02 lr 0.000875	time 0.7500 (0.7517)	loss 2.6397 (3.4946)	grad_norm 1.3064 (1.3097)	mem 23874MB
[2022-11-11 18:15:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][500/1251]	eta 0:09:23 lr 0.000875	time 0.7363 (0.7509)	loss 3.7755 (3.4930)	grad_norm 1.4631 (1.3075)	mem 23874MB
[2022-11-11 18:15:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][550/1251]	eta 0:08:46 lr 0.000875	time 0.7440 (0.7506)	loss 3.9248 (3.5008)	grad_norm 1.2921 (1.3074)	mem 23874MB
[2022-11-11 18:16:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][600/1251]	eta 0:08:08 lr 0.000875	time 0.8244 (0.7505)	loss 4.0645 (3.5138)	grad_norm 1.2911 (1.3059)	mem 23874MB
[2022-11-11 18:17:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][650/1251]	eta 0:07:30 lr 0.000875	time 0.7376 (0.7499)	loss 3.4812 (3.5126)	grad_norm 1.1668 (1.3049)	mem 23874MB
[2022-11-11 18:17:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][700/1251]	eta 0:06:53 lr 0.000874	time 0.7378 (0.7498)	loss 3.4945 (3.5122)	grad_norm 1.2466 (1.3046)	mem 23874MB
[2022-11-11 18:18:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][750/1251]	eta 0:06:15 lr 0.000874	time 0.7379 (0.7494)	loss 3.0035 (3.5129)	grad_norm 1.1792 (1.3032)	mem 23874MB
[2022-11-11 18:18:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][800/1251]	eta 0:05:37 lr 0.000874	time 0.7413 (0.7493)	loss 3.8639 (3.5140)	grad_norm 1.1607 (1.3034)	mem 23874MB
[2022-11-11 18:19:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][850/1251]	eta 0:05:00 lr 0.000874	time 0.7364 (0.7492)	loss 3.0279 (3.5084)	grad_norm 1.3953 (1.3072)	mem 23874MB
[2022-11-11 18:20:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][900/1251]	eta 0:04:22 lr 0.000874	time 0.7398 (0.7489)	loss 3.2482 (3.5150)	grad_norm 1.3511 (nan)	mem 23874MB
[2022-11-11 18:20:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][950/1251]	eta 0:03:45 lr 0.000874	time 0.7445 (0.7490)	loss 3.2079 (3.5231)	grad_norm 1.4896 (nan)	mem 23874MB
[2022-11-11 18:21:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][1000/1251]	eta 0:03:07 lr 0.000874	time 0.8193 (0.7489)	loss 3.7308 (3.5252)	grad_norm 1.4057 (nan)	mem 23874MB
[2022-11-11 18:22:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][1050/1251]	eta 0:02:30 lr 0.000873	time 0.7430 (0.7489)	loss 4.1626 (3.5318)	grad_norm 1.2322 (nan)	mem 23874MB
[2022-11-11 18:22:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][1100/1251]	eta 0:01:53 lr 0.000873	time 0.7403 (0.7488)	loss 3.4079 (3.5309)	grad_norm 1.1611 (nan)	mem 23874MB
[2022-11-11 18:23:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][1150/1251]	eta 0:01:15 lr 0.000873	time 0.7391 (0.7488)	loss 3.0628 (3.5325)	grad_norm 1.1242 (nan)	mem 23874MB
[2022-11-11 18:23:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][1200/1251]	eta 0:00:38 lr 0.000873	time 0.7348 (0.7487)	loss 2.6263 (3.5295)	grad_norm 1.3615 (nan)	mem 23874MB
[2022-11-11 18:24:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [69/300][1250/1251]	eta 0:00:00 lr 0.000873	time 0.7297 (0.7487)	loss 3.6874 (3.5329)	grad_norm 1.2817 (nan)	mem 23874MB
[2022-11-11 18:24:34 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 69 training takes 0:15:36
[2022-11-11 18:24:34 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_69.pth saving......
[2022-11-11 18:24:35 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_69.pth saved !!!
[2022-11-11 18:24:37 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.634 (1.634)	Loss 0.9744 (0.9744)	Acc@1 75.879 (75.879)	Acc@5 94.238 (94.238)	Mem 23874MB
[2022-11-11 18:24:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.604 Acc@5 93.220
[2022-11-11 18:24:47 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.6%
[2022-11-11 18:24:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.778 (1.778)	Loss 0.8156 (0.8156)	Acc@1 80.859 (80.859)	Acc@5 95.410 (95.410)	Mem 23874MB
[2022-11-11 18:25:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.272 Acc@5 94.516
[2022-11-11 18:25:00 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.3%
[2022-11-11 18:25:00 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.27% at 69 epoch
[2022-11-11 18:25:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][0/1251]	eta 0:49:43 lr 0.000873	time 2.3850 (2.3850)	loss 3.6689 (3.6689)	grad_norm 1.3037 (1.3037)	mem 23874MB
[2022-11-11 18:25:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][50/1251]	eta 0:15:45 lr 0.000873	time 0.7362 (0.7876)	loss 3.2774 (3.5263)	grad_norm 1.2932 (1.3098)	mem 23874MB
[2022-11-11 18:26:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][100/1251]	eta 0:14:44 lr 0.000873	time 0.7394 (0.7681)	loss 2.8797 (3.5285)	grad_norm 1.2235 (1.3005)	mem 23874MB
[2022-11-11 18:26:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][150/1251]	eta 0:13:58 lr 0.000872	time 0.7500 (0.7615)	loss 3.4582 (3.5223)	grad_norm 1.2983 (1.3046)	mem 23874MB
[2022-11-11 18:27:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][200/1251]	eta 0:13:17 lr 0.000872	time 0.7367 (0.7586)	loss 2.9889 (3.5386)	grad_norm 1.3528 (1.3110)	mem 23874MB
[2022-11-11 18:28:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][250/1251]	eta 0:12:37 lr 0.000872	time 0.7420 (0.7566)	loss 3.5774 (3.5360)	grad_norm 1.3782 (1.3130)	mem 23874MB
[2022-11-11 18:28:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][300/1251]	eta 0:11:58 lr 0.000872	time 0.7400 (0.7555)	loss 4.2877 (3.5322)	grad_norm 1.2229 (1.3141)	mem 23874MB
[2022-11-11 18:29:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][350/1251]	eta 0:11:20 lr 0.000872	time 0.7348 (0.7550)	loss 3.8880 (3.5281)	grad_norm 1.4035 (1.3165)	mem 23874MB
[2022-11-11 18:30:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][400/1251]	eta 0:10:41 lr 0.000872	time 0.7415 (0.7539)	loss 3.7741 (3.5228)	grad_norm 1.2897 (1.3173)	mem 23874MB
[2022-11-11 18:30:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][450/1251]	eta 0:10:03 lr 0.000872	time 0.7399 (0.7534)	loss 2.5462 (3.5266)	grad_norm 1.4464 (1.3208)	mem 23874MB
[2022-11-11 18:31:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][500/1251]	eta 0:09:25 lr 0.000871	time 0.7421 (0.7527)	loss 3.1143 (3.5233)	grad_norm 1.4327 (1.3221)	mem 23874MB
[2022-11-11 18:31:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][550/1251]	eta 0:08:47 lr 0.000871	time 0.8188 (0.7525)	loss 2.6876 (3.5155)	grad_norm 1.3356 (1.3224)	mem 23874MB
[2022-11-11 18:32:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][600/1251]	eta 0:08:09 lr 0.000871	time 0.7540 (0.7521)	loss 3.6795 (3.5147)	grad_norm 1.2976 (1.3221)	mem 23874MB
[2022-11-11 18:33:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][650/1251]	eta 0:07:31 lr 0.000871	time 0.7410 (0.7519)	loss 3.7941 (3.5191)	grad_norm 1.2019 (1.3239)	mem 23874MB
[2022-11-11 18:33:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][700/1251]	eta 0:06:54 lr 0.000871	time 0.7447 (0.7517)	loss 3.4653 (3.5187)	grad_norm 1.3327 (1.3232)	mem 23874MB
[2022-11-11 18:34:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][750/1251]	eta 0:06:16 lr 0.000871	time 0.8118 (0.7515)	loss 4.4561 (3.5204)	grad_norm 1.2746 (1.3229)	mem 23874MB
[2022-11-11 18:35:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][800/1251]	eta 0:05:38 lr 0.000871	time 0.7369 (0.7513)	loss 3.7483 (3.5220)	grad_norm 1.3794 (1.3217)	mem 23874MB
[2022-11-11 18:35:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][850/1251]	eta 0:05:01 lr 0.000870	time 0.7397 (0.7511)	loss 2.8095 (3.5262)	grad_norm 1.3592 (1.3207)	mem 23874MB
[2022-11-11 18:36:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][900/1251]	eta 0:04:23 lr 0.000870	time 0.7374 (0.7511)	loss 4.1796 (3.5322)	grad_norm 1.3750 (1.3221)	mem 23874MB
[2022-11-11 18:36:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][950/1251]	eta 0:03:46 lr 0.000870	time 0.8083 (0.7510)	loss 3.7457 (3.5360)	grad_norm 1.3959 (1.3228)	mem 23874MB
[2022-11-11 18:37:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][1000/1251]	eta 0:03:08 lr 0.000870	time 0.7436 (0.7509)	loss 3.8498 (3.5353)	grad_norm 1.2655 (1.3221)	mem 23874MB
[2022-11-11 18:38:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][1050/1251]	eta 0:02:30 lr 0.000870	time 0.7423 (0.7508)	loss 3.4211 (3.5400)	grad_norm 1.3333 (1.3231)	mem 23874MB
[2022-11-11 18:38:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][1100/1251]	eta 0:01:53 lr 0.000870	time 0.7388 (0.7507)	loss 4.0101 (3.5442)	grad_norm 1.4311 (1.3226)	mem 23874MB
[2022-11-11 18:39:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][1150/1251]	eta 0:01:15 lr 0.000870	time 0.7387 (0.7507)	loss 3.9706 (3.5443)	grad_norm 1.3054 (1.3231)	mem 23874MB
[2022-11-11 18:40:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][1200/1251]	eta 0:00:38 lr 0.000870	time 0.7418 (0.7506)	loss 3.3695 (3.5418)	grad_norm 1.2568 (1.3244)	mem 23874MB
[2022-11-11 18:40:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [70/300][1250/1251]	eta 0:00:00 lr 0.000869	time 0.7265 (0.7504)	loss 3.7381 (3.5391)	grad_norm 1.2903 (1.3238)	mem 23874MB
[2022-11-11 18:40:39 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 70 training takes 0:15:38
[2022-11-11 18:40:39 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_70.pth saving......
[2022-11-11 18:40:40 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_70.pth saved !!!
[2022-11-11 18:40:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.618 (1.618)	Loss 1.0136 (1.0136)	Acc@1 76.367 (76.367)	Acc@5 93.359 (93.359)	Mem 23874MB
[2022-11-11 18:40:52 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.550 Acc@5 93.246
[2022-11-11 18:40:52 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.5%
[2022-11-11 18:40:54 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.840 (1.840)	Loss 0.9536 (0.9536)	Acc@1 76.660 (76.660)	Acc@5 93.555 (93.555)	Mem 23874MB
[2022-11-11 18:41:05 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.368 Acc@5 94.518
[2022-11-11 18:41:05 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.4%
[2022-11-11 18:41:05 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.37% at 70 epoch
[2022-11-11 18:41:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][0/1251]	eta 0:51:15 lr 0.000869	time 2.4581 (2.4581)	loss 2.9077 (2.9077)	grad_norm 1.4110 (1.4110)	mem 23874MB
[2022-11-11 18:41:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][50/1251]	eta 0:15:39 lr 0.000869	time 0.7380 (0.7824)	loss 3.6111 (3.5096)	grad_norm 1.2797 (1.3386)	mem 23874MB
[2022-11-11 18:42:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][100/1251]	eta 0:14:42 lr 0.000869	time 0.7413 (0.7666)	loss 3.8640 (3.5167)	grad_norm 1.3106 (1.3412)	mem 23874MB
[2022-11-11 18:43:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][150/1251]	eta 0:13:57 lr 0.000869	time 0.7479 (0.7606)	loss 4.0832 (3.5130)	grad_norm 1.3258 (1.3307)	mem 23874MB
[2022-11-11 18:43:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][200/1251]	eta 0:13:17 lr 0.000869	time 0.8350 (0.7586)	loss 4.0331 (3.4990)	grad_norm 1.3150 (1.3366)	mem 23874MB
[2022-11-11 18:44:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][250/1251]	eta 0:12:36 lr 0.000869	time 0.7363 (0.7561)	loss 3.2677 (3.4885)	grad_norm 1.3435 (1.3288)	mem 23874MB
[2022-11-11 18:44:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][300/1251]	eta 0:11:57 lr 0.000869	time 0.7380 (0.7545)	loss 3.9418 (3.4756)	grad_norm 1.2117 (1.3281)	mem 23874MB
[2022-11-11 18:45:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][350/1251]	eta 0:11:18 lr 0.000868	time 0.7401 (0.7535)	loss 3.8375 (3.4773)	grad_norm 1.2283 (1.3329)	mem 23874MB
[2022-11-11 18:46:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][400/1251]	eta 0:10:41 lr 0.000868	time 0.7406 (0.7534)	loss 3.1850 (3.4807)	grad_norm 1.2774 (1.3319)	mem 23874MB
[2022-11-11 18:46:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][450/1251]	eta 0:10:02 lr 0.000868	time 0.7412 (0.7525)	loss 2.8577 (3.4778)	grad_norm 1.4941 (1.3310)	mem 23874MB
[2022-11-11 18:47:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][500/1251]	eta 0:09:25 lr 0.000868	time 0.7417 (0.7523)	loss 3.2170 (3.4937)	grad_norm 1.4609 (1.3279)	mem 23874MB
[2022-11-11 18:47:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][550/1251]	eta 0:08:47 lr 0.000868	time 0.8046 (0.7518)	loss 3.3642 (3.4979)	grad_norm 1.3715 (1.3267)	mem 23874MB
[2022-11-11 18:48:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][600/1251]	eta 0:08:09 lr 0.000868	time 0.8208 (0.7517)	loss 2.7493 (3.4971)	grad_norm 1.4151 (1.3265)	mem 23874MB
[2022-11-11 18:49:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][650/1251]	eta 0:07:31 lr 0.000868	time 0.7425 (0.7514)	loss 3.1697 (3.5047)	grad_norm 1.3367 (1.3264)	mem 23874MB
[2022-11-11 18:49:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][700/1251]	eta 0:06:53 lr 0.000867	time 0.7404 (0.7511)	loss 3.6721 (3.5129)	grad_norm 1.2896 (1.3276)	mem 23874MB
[2022-11-11 18:50:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][750/1251]	eta 0:06:16 lr 0.000867	time 0.7347 (0.7508)	loss 4.1319 (3.5078)	grad_norm 1.3046 (1.3275)	mem 23874MB
[2022-11-11 18:51:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][800/1251]	eta 0:05:38 lr 0.000867	time 0.7445 (0.7506)	loss 4.2219 (3.5204)	grad_norm 1.2762 (1.3281)	mem 23874MB
[2022-11-11 18:51:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][850/1251]	eta 0:05:00 lr 0.000867	time 0.7408 (0.7504)	loss 3.4185 (3.5212)	grad_norm 1.4809 (1.3288)	mem 23874MB
[2022-11-11 18:52:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][900/1251]	eta 0:04:23 lr 0.000867	time 0.7423 (0.7502)	loss 2.7247 (3.5211)	grad_norm 1.4956 (1.3288)	mem 23874MB
[2022-11-11 18:52:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][950/1251]	eta 0:03:45 lr 0.000867	time 0.7355 (0.7500)	loss 2.4048 (3.5198)	grad_norm 1.3286 (1.3305)	mem 23874MB
[2022-11-11 18:53:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][1000/1251]	eta 0:03:08 lr 0.000867	time 0.7522 (0.7499)	loss 3.6495 (3.5247)	grad_norm 1.2386 (1.3317)	mem 23874MB
[2022-11-11 18:54:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][1050/1251]	eta 0:02:30 lr 0.000866	time 0.7413 (0.7496)	loss 3.7681 (3.5219)	grad_norm 1.3790 (1.3318)	mem 23874MB
[2022-11-11 18:54:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][1100/1251]	eta 0:01:53 lr 0.000866	time 0.7344 (0.7495)	loss 3.2070 (3.5230)	grad_norm 1.2305 (1.3312)	mem 23874MB
[2022-11-11 18:55:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][1150/1251]	eta 0:01:15 lr 0.000866	time 0.7390 (0.7494)	loss 3.7271 (3.5244)	grad_norm 1.3238 (1.3315)	mem 23874MB
[2022-11-11 18:56:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][1200/1251]	eta 0:00:38 lr 0.000866	time 0.7428 (0.7494)	loss 3.7314 (3.5186)	grad_norm 1.5115 (1.3319)	mem 23874MB
[2022-11-11 18:56:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [71/300][1250/1251]	eta 0:00:00 lr 0.000866	time 0.7253 (0.7491)	loss 2.5438 (3.5168)	grad_norm 1.3373 (1.3321)	mem 23874MB
[2022-11-11 18:56:42 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 71 training takes 0:15:37
[2022-11-11 18:56:42 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_71.pth saving......
[2022-11-11 18:56:44 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_71.pth saved !!!
[2022-11-11 18:56:45 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.653 (1.653)	Loss 1.0640 (1.0640)	Acc@1 75.684 (75.684)	Acc@5 92.871 (92.871)	Mem 23874MB
[2022-11-11 18:56:56 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.406 Acc@5 93.252
[2022-11-11 18:56:56 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.4%
[2022-11-11 18:56:58 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.806 (1.806)	Loss 0.8499 (0.8499)	Acc@1 79.980 (79.980)	Acc@5 94.043 (94.043)	Mem 23874MB
[2022-11-11 18:57:08 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.436 Acc@5 94.582
[2022-11-11 18:57:08 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.4%
[2022-11-11 18:57:08 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.44% at 71 epoch
[2022-11-11 18:57:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][0/1251]	eta 0:53:38 lr 0.000866	time 2.5725 (2.5725)	loss 3.7202 (3.7202)	grad_norm 1.2431 (1.2431)	mem 23874MB
[2022-11-11 18:57:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][50/1251]	eta 0:15:46 lr 0.000866	time 0.7431 (0.7878)	loss 2.9980 (3.6040)	grad_norm 1.2702 (1.3323)	mem 23874MB
[2022-11-11 18:58:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][100/1251]	eta 0:14:42 lr 0.000866	time 0.7401 (0.7665)	loss 2.4698 (3.5148)	grad_norm 1.2781 (nan)	mem 23874MB
[2022-11-11 18:59:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][150/1251]	eta 0:13:58 lr 0.000865	time 0.7392 (0.7617)	loss 3.1000 (3.5000)	grad_norm 1.2458 (nan)	mem 23874MB
[2022-11-11 18:59:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][200/1251]	eta 0:13:16 lr 0.000865	time 0.7391 (0.7577)	loss 3.8460 (3.5176)	grad_norm 1.4683 (nan)	mem 23874MB
[2022-11-11 19:00:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][250/1251]	eta 0:12:36 lr 0.000865	time 0.7360 (0.7557)	loss 3.7012 (3.4986)	grad_norm 1.5505 (nan)	mem 23874MB
[2022-11-11 19:00:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][300/1251]	eta 0:11:57 lr 0.000865	time 0.7402 (0.7546)	loss 3.9338 (3.4951)	grad_norm 1.2391 (nan)	mem 23874MB
[2022-11-11 19:01:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][350/1251]	eta 0:11:19 lr 0.000865	time 0.7450 (0.7536)	loss 3.9030 (3.5065)	grad_norm 1.5087 (nan)	mem 23874MB
[2022-11-11 19:02:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][400/1251]	eta 0:10:40 lr 0.000865	time 0.7363 (0.7527)	loss 4.2807 (3.5120)	grad_norm 1.2763 (nan)	mem 23874MB
[2022-11-11 19:02:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][450/1251]	eta 0:10:02 lr 0.000865	time 0.7401 (0.7521)	loss 2.4574 (3.5106)	grad_norm 1.2977 (nan)	mem 23874MB
[2022-11-11 19:03:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][500/1251]	eta 0:09:24 lr 0.000864	time 0.7405 (0.7518)	loss 2.6121 (3.5154)	grad_norm 1.2663 (nan)	mem 23874MB
[2022-11-11 19:04:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][550/1251]	eta 0:08:46 lr 0.000864	time 0.7412 (0.7515)	loss 2.8842 (3.5060)	grad_norm 1.2210 (nan)	mem 23874MB
[2022-11-11 19:04:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][600/1251]	eta 0:08:09 lr 0.000864	time 0.7389 (0.7512)	loss 3.4947 (3.4966)	grad_norm 1.2482 (nan)	mem 23874MB
[2022-11-11 19:05:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][650/1251]	eta 0:07:31 lr 0.000864	time 0.7440 (0.7510)	loss 3.8434 (3.5117)	grad_norm 1.2451 (nan)	mem 23874MB
[2022-11-11 19:05:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][700/1251]	eta 0:06:53 lr 0.000864	time 0.7416 (0.7508)	loss 4.2494 (3.5091)	grad_norm 1.3029 (nan)	mem 23874MB
[2022-11-11 19:06:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][750/1251]	eta 0:06:16 lr 0.000864	time 0.7428 (0.7507)	loss 3.9075 (3.5096)	grad_norm 1.4638 (nan)	mem 23874MB
[2022-11-11 19:07:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][800/1251]	eta 0:05:38 lr 0.000864	time 0.7400 (0.7504)	loss 4.0391 (3.5154)	grad_norm 1.4889 (nan)	mem 23874MB
[2022-11-11 19:07:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][850/1251]	eta 0:05:00 lr 0.000863	time 0.7417 (0.7502)	loss 2.6402 (3.5201)	grad_norm 1.3113 (nan)	mem 23874MB
[2022-11-11 19:08:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][900/1251]	eta 0:04:23 lr 0.000863	time 0.7386 (0.7501)	loss 3.6063 (3.5198)	grad_norm 1.2315 (nan)	mem 23874MB
[2022-11-11 19:09:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][950/1251]	eta 0:03:45 lr 0.000863	time 0.7402 (0.7499)	loss 3.0573 (3.5132)	grad_norm 1.3436 (nan)	mem 23874MB
[2022-11-11 19:09:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][1000/1251]	eta 0:03:08 lr 0.000863	time 0.7362 (0.7498)	loss 3.5746 (3.5164)	grad_norm 1.3099 (nan)	mem 23874MB
[2022-11-11 19:10:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][1050/1251]	eta 0:02:30 lr 0.000863	time 0.7385 (0.7497)	loss 2.3487 (3.5154)	grad_norm 1.1757 (nan)	mem 23874MB
[2022-11-11 19:10:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][1100/1251]	eta 0:01:53 lr 0.000863	time 0.7396 (0.7497)	loss 4.1521 (3.5247)	grad_norm 1.4427 (nan)	mem 23874MB
[2022-11-11 19:11:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][1150/1251]	eta 0:01:15 lr 0.000863	time 0.7377 (0.7496)	loss 2.4509 (3.5182)	grad_norm 1.4296 (nan)	mem 23874MB
[2022-11-11 19:12:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][1200/1251]	eta 0:00:38 lr 0.000862	time 0.7426 (0.7495)	loss 3.0801 (3.5186)	grad_norm 1.2553 (nan)	mem 23874MB
[2022-11-11 19:12:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [72/300][1250/1251]	eta 0:00:00 lr 0.000862	time 0.7335 (0.7492)	loss 3.9081 (3.5216)	grad_norm 1.6641 (nan)	mem 23874MB
[2022-11-11 19:12:46 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 72 training takes 0:15:37
[2022-11-11 19:12:46 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_72.pth saving......
[2022-11-11 19:12:47 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_72.pth saved !!!
[2022-11-11 19:12:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.637 (1.637)	Loss 1.1283 (1.1283)	Acc@1 75.195 (75.195)	Acc@5 93.359 (93.359)	Mem 23874MB
[2022-11-11 19:12:59 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.498 Acc@5 93.352
[2022-11-11 19:12:59 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.5%
[2022-11-11 19:13:01 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.753 (1.753)	Loss 0.8479 (0.8479)	Acc@1 79.492 (79.492)	Acc@5 95.312 (95.312)	Mem 23874MB
[2022-11-11 19:13:12 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.496 Acc@5 94.604
[2022-11-11 19:13:12 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.5%
[2022-11-11 19:13:12 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.50% at 72 epoch
[2022-11-11 19:13:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][0/1251]	eta 0:50:16 lr 0.000862	time 2.4116 (2.4116)	loss 3.4628 (3.4628)	grad_norm 1.5631 (1.5631)	mem 23874MB
[2022-11-11 19:13:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][50/1251]	eta 0:15:40 lr 0.000862	time 0.7402 (0.7835)	loss 3.2275 (3.2946)	grad_norm 1.4236 (1.3660)	mem 23874MB
[2022-11-11 19:14:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][100/1251]	eta 0:14:39 lr 0.000862	time 0.7401 (0.7641)	loss 2.9439 (3.3633)	grad_norm 1.2566 (1.3455)	mem 23874MB
[2022-11-11 19:15:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][150/1251]	eta 0:13:55 lr 0.000862	time 0.7437 (0.7592)	loss 3.4400 (3.4167)	grad_norm 1.1924 (1.3390)	mem 23874MB
[2022-11-11 19:15:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][200/1251]	eta 0:13:14 lr 0.000862	time 0.7330 (0.7560)	loss 3.2139 (3.4185)	grad_norm 1.4902 (1.3309)	mem 23874MB
[2022-11-11 19:16:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][250/1251]	eta 0:12:35 lr 0.000862	time 0.7441 (0.7546)	loss 3.7659 (3.4528)	grad_norm 1.2864 (1.3266)	mem 23874MB
[2022-11-11 19:16:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][300/1251]	eta 0:11:56 lr 0.000861	time 0.7387 (0.7533)	loss 3.4638 (3.4576)	grad_norm 1.6093 (1.3294)	mem 23874MB
[2022-11-11 19:17:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][350/1251]	eta 0:11:18 lr 0.000861	time 0.7389 (0.7527)	loss 3.5423 (3.4627)	grad_norm 1.2679 (1.3288)	mem 23874MB
[2022-11-11 19:18:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][400/1251]	eta 0:10:40 lr 0.000861	time 0.7348 (0.7522)	loss 3.3843 (3.4728)	grad_norm 1.1740 (1.3299)	mem 23874MB
[2022-11-11 19:18:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][450/1251]	eta 0:10:02 lr 0.000861	time 0.7424 (0.7516)	loss 3.6569 (3.4602)	grad_norm 1.5748 (1.3259)	mem 23874MB
[2022-11-11 19:19:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][500/1251]	eta 0:09:24 lr 0.000861	time 0.7427 (0.7511)	loss 3.7287 (3.4591)	grad_norm 1.5078 (1.3293)	mem 23874MB
[2022-11-11 19:20:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][550/1251]	eta 0:08:46 lr 0.000861	time 0.7410 (0.7509)	loss 2.8371 (3.4517)	grad_norm 1.4047 (1.3313)	mem 23874MB
[2022-11-11 19:20:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][600/1251]	eta 0:08:08 lr 0.000861	time 0.8056 (0.7505)	loss 3.3604 (3.4532)	grad_norm 1.3118 (1.3317)	mem 23874MB
[2022-11-11 19:21:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][650/1251]	eta 0:07:30 lr 0.000860	time 0.7456 (0.7504)	loss 3.8520 (3.4555)	grad_norm 1.4962 (1.3316)	mem 23874MB
[2022-11-11 19:21:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][700/1251]	eta 0:06:53 lr 0.000860	time 0.7350 (0.7500)	loss 3.7441 (3.4615)	grad_norm 1.2270 (1.3309)	mem 23874MB
[2022-11-11 19:22:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][750/1251]	eta 0:06:15 lr 0.000860	time 0.8219 (0.7499)	loss 2.7574 (3.4623)	grad_norm 1.2210 (1.3296)	mem 23874MB
[2022-11-11 19:23:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][800/1251]	eta 0:05:38 lr 0.000860	time 0.7413 (0.7495)	loss 3.8144 (3.4638)	grad_norm 1.2446 (1.3289)	mem 23874MB
[2022-11-11 19:23:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][850/1251]	eta 0:05:00 lr 0.000860	time 0.7381 (0.7496)	loss 3.5866 (3.4702)	grad_norm 1.3264 (1.3278)	mem 23874MB
[2022-11-11 19:24:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][900/1251]	eta 0:04:22 lr 0.000860	time 0.7385 (0.7492)	loss 3.9696 (3.4760)	grad_norm 1.3366 (1.3273)	mem 23874MB
[2022-11-11 19:25:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][950/1251]	eta 0:03:45 lr 0.000860	time 0.7369 (0.7492)	loss 3.2860 (3.4805)	grad_norm 1.3241 (1.3281)	mem 23874MB
[2022-11-11 19:25:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][1000/1251]	eta 0:03:08 lr 0.000859	time 0.8113 (0.7490)	loss 4.0694 (3.4797)	grad_norm 1.4248 (1.3282)	mem 23874MB
[2022-11-11 19:26:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][1050/1251]	eta 0:02:30 lr 0.000859	time 0.7398 (0.7489)	loss 4.4592 (3.4796)	grad_norm 1.3896 (1.3278)	mem 23874MB
[2022-11-11 19:26:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][1100/1251]	eta 0:01:53 lr 0.000859	time 0.7402 (0.7487)	loss 3.7941 (3.4792)	grad_norm 1.4718 (1.3278)	mem 23874MB
[2022-11-11 19:27:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][1150/1251]	eta 0:01:15 lr 0.000859	time 0.7376 (0.7487)	loss 3.3666 (3.4825)	grad_norm 1.3117 (1.3282)	mem 23874MB
[2022-11-11 19:28:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][1200/1251]	eta 0:00:38 lr 0.000859	time 0.7393 (0.7486)	loss 2.7739 (3.4781)	grad_norm 1.2465 (1.3281)	mem 23874MB
[2022-11-11 19:28:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [73/300][1250/1251]	eta 0:00:00 lr 0.000859	time 0.7299 (0.7485)	loss 3.5561 (3.4799)	grad_norm 1.2772 (1.3275)	mem 23874MB
[2022-11-11 19:28:49 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 73 training takes 0:15:36
[2022-11-11 19:28:49 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_73.pth saving......
[2022-11-11 19:28:50 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_73.pth saved !!!
[2022-11-11 19:28:51 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.636 (1.636)	Loss 1.0303 (1.0303)	Acc@1 76.172 (76.172)	Acc@5 93.848 (93.848)	Mem 23874MB
[2022-11-11 19:29:02 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.626 Acc@5 93.442
[2022-11-11 19:29:02 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.6%
[2022-11-11 19:29:04 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.846 (1.846)	Loss 0.8835 (0.8835)	Acc@1 79.199 (79.199)	Acc@5 95.215 (95.215)	Mem 23874MB
[2022-11-11 19:29:15 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.520 Acc@5 94.596
[2022-11-11 19:29:15 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.5%
[2022-11-11 19:29:15 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.52% at 73 epoch
[2022-11-11 19:29:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][0/1251]	eta 0:49:59 lr 0.000859	time 2.3974 (2.3974)	loss 3.5164 (3.5164)	grad_norm 1.2013 (1.2013)	mem 23874MB
[2022-11-11 19:29:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][50/1251]	eta 0:15:45 lr 0.000859	time 0.8206 (0.7872)	loss 2.7810 (3.3804)	grad_norm 1.2514 (1.3223)	mem 23874MB
[2022-11-11 19:30:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][100/1251]	eta 0:14:40 lr 0.000858	time 0.7344 (0.7653)	loss 3.9066 (3.4974)	grad_norm 1.3354 (1.3417)	mem 23874MB
[2022-11-11 19:31:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][150/1251]	eta 0:13:57 lr 0.000858	time 0.7448 (0.7609)	loss 4.0523 (3.5136)	grad_norm 1.2890 (1.3317)	mem 23874MB
[2022-11-11 19:31:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][200/1251]	eta 0:13:15 lr 0.000858	time 0.7358 (0.7573)	loss 2.5195 (3.5206)	grad_norm 1.2850 (1.3420)	mem 23874MB
[2022-11-11 19:32:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][250/1251]	eta 0:12:35 lr 0.000858	time 0.7500 (0.7551)	loss 2.6480 (3.4787)	grad_norm 1.2640 (1.3409)	mem 23874MB
[2022-11-11 19:33:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][300/1251]	eta 0:11:57 lr 0.000858	time 0.8171 (0.7545)	loss 4.2403 (3.4730)	grad_norm 1.2867 (1.3397)	mem 23874MB
[2022-11-11 19:33:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][350/1251]	eta 0:11:18 lr 0.000858	time 0.7388 (0.7532)	loss 3.3258 (3.4854)	grad_norm 1.3272 (1.3381)	mem 23874MB
[2022-11-11 19:34:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][400/1251]	eta 0:10:40 lr 0.000858	time 0.7415 (0.7527)	loss 3.5248 (3.5068)	grad_norm 1.3160 (1.3374)	mem 23874MB
[2022-11-11 19:34:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][450/1251]	eta 0:10:02 lr 0.000857	time 0.7450 (0.7520)	loss 4.0369 (3.5234)	grad_norm 1.4174 (1.3386)	mem 23874MB
[2022-11-11 19:35:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][500/1251]	eta 0:09:24 lr 0.000857	time 0.7367 (0.7513)	loss 2.9351 (3.5142)	grad_norm 1.3882 (1.3375)	mem 23874MB
[2022-11-11 19:36:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][550/1251]	eta 0:08:46 lr 0.000857	time 0.7344 (0.7508)	loss 3.8885 (3.5265)	grad_norm 1.2757 (1.3406)	mem 23874MB
[2022-11-11 19:36:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][600/1251]	eta 0:08:08 lr 0.000857	time 0.7502 (0.7507)	loss 2.5078 (3.5238)	grad_norm 1.3679 (1.3369)	mem 23874MB
[2022-11-11 19:37:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][650/1251]	eta 0:07:31 lr 0.000857	time 0.7345 (0.7504)	loss 4.3347 (3.5177)	grad_norm 1.3853 (1.3370)	mem 23874MB
[2022-11-11 19:38:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][700/1251]	eta 0:06:53 lr 0.000857	time 0.7366 (0.7505)	loss 4.0346 (3.5126)	grad_norm 1.3539 (1.3362)	mem 23874MB
[2022-11-11 19:38:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][750/1251]	eta 0:06:15 lr 0.000856	time 0.7368 (0.7500)	loss 3.8063 (3.5055)	grad_norm 1.1246 (1.3372)	mem 23874MB
[2022-11-11 19:39:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][800/1251]	eta 0:05:38 lr 0.000856	time 0.7387 (0.7499)	loss 4.1410 (3.5084)	grad_norm 1.5017 (1.3374)	mem 23874MB
[2022-11-11 19:39:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][850/1251]	eta 0:05:00 lr 0.000856	time 0.7426 (0.7499)	loss 3.0645 (3.5004)	grad_norm 1.3458 (1.3393)	mem 23874MB
[2022-11-11 19:40:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][900/1251]	eta 0:04:23 lr 0.000856	time 0.7470 (0.7496)	loss 4.0432 (3.5093)	grad_norm 1.3916 (1.3394)	mem 23874MB
[2022-11-11 19:41:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][950/1251]	eta 0:03:45 lr 0.000856	time 0.7380 (0.7495)	loss 3.3783 (3.5137)	grad_norm 1.2956 (1.3396)	mem 23874MB
[2022-11-11 19:41:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][1000/1251]	eta 0:03:08 lr 0.000856	time 0.7553 (0.7494)	loss 3.7266 (3.5120)	grad_norm 1.4080 (1.3390)	mem 23874MB
[2022-11-11 19:42:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][1050/1251]	eta 0:02:30 lr 0.000856	time 0.7389 (0.7491)	loss 3.2307 (3.5088)	grad_norm 1.3004 (1.3391)	mem 23874MB
[2022-11-11 19:42:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][1100/1251]	eta 0:01:53 lr 0.000855	time 0.7396 (0.7492)	loss 3.8606 (3.5146)	grad_norm 1.3313 (1.3375)	mem 23874MB
[2022-11-11 19:43:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][1150/1251]	eta 0:01:15 lr 0.000855	time 0.7415 (0.7490)	loss 4.1518 (3.5162)	grad_norm 1.3929 (1.3363)	mem 23874MB
[2022-11-11 19:44:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][1200/1251]	eta 0:00:38 lr 0.000855	time 0.7353 (0.7490)	loss 2.8479 (3.5114)	grad_norm 1.2503 (1.3360)	mem 23874MB
[2022-11-11 19:44:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [74/300][1250/1251]	eta 0:00:00 lr 0.000855	time 0.7285 (0.7489)	loss 3.8631 (3.5155)	grad_norm 1.3183 (1.3354)	mem 23874MB
[2022-11-11 19:44:52 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 74 training takes 0:15:37
[2022-11-11 19:44:52 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_74.pth saving......
[2022-11-11 19:44:53 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_74.pth saved !!!
[2022-11-11 19:44:54 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.567 (1.567)	Loss 0.9826 (0.9826)	Acc@1 77.051 (77.051)	Acc@5 93.457 (93.457)	Mem 23874MB
[2022-11-11 19:45:05 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.014 Acc@5 93.510
[2022-11-11 19:45:05 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.0%
[2022-11-11 19:45:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.881 (1.881)	Loss 0.8360 (0.8360)	Acc@1 78.516 (78.516)	Acc@5 95.215 (95.215)	Mem 23874MB
[2022-11-11 19:45:18 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.566 Acc@5 94.642
[2022-11-11 19:45:18 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.6%
[2022-11-11 19:45:18 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.57% at 74 epoch
[2022-11-11 19:45:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][0/1251]	eta 0:50:29 lr 0.000855	time 2.4213 (2.4213)	loss 3.7337 (3.7337)	grad_norm 1.1964 (1.1964)	mem 23874MB
[2022-11-11 19:45:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][50/1251]	eta 0:15:46 lr 0.000855	time 0.7389 (0.7880)	loss 3.9404 (3.5353)	grad_norm 1.2583 (1.3633)	mem 23874MB
[2022-11-11 19:46:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][100/1251]	eta 0:14:46 lr 0.000855	time 0.7378 (0.7704)	loss 3.9438 (3.4850)	grad_norm 1.3155 (1.3574)	mem 23874MB
[2022-11-11 19:47:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][150/1251]	eta 0:13:58 lr 0.000855	time 0.7400 (0.7617)	loss 2.9548 (3.5115)	grad_norm 1.2879 (1.3477)	mem 23874MB
[2022-11-11 19:47:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][200/1251]	eta 0:13:17 lr 0.000854	time 0.7330 (0.7587)	loss 3.7369 (3.5256)	grad_norm 1.5476 (1.3431)	mem 23874MB
[2022-11-11 19:48:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][250/1251]	eta 0:12:38 lr 0.000854	time 0.7450 (0.7574)	loss 3.5682 (3.5121)	grad_norm 1.2621 (1.3412)	mem 23874MB
[2022-11-11 19:49:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][300/1251]	eta 0:11:58 lr 0.000854	time 0.7394 (0.7556)	loss 3.2930 (3.5334)	grad_norm 1.3642 (1.3400)	mem 23874MB
[2022-11-11 19:49:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][350/1251]	eta 0:11:20 lr 0.000854	time 0.7390 (0.7550)	loss 3.5462 (3.5275)	grad_norm 1.1258 (1.3433)	mem 23874MB
[2022-11-11 19:50:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][400/1251]	eta 0:10:41 lr 0.000854	time 0.7442 (0.7544)	loss 4.2386 (3.5247)	grad_norm 1.2280 (inf)	mem 23874MB
[2022-11-11 19:50:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][450/1251]	eta 0:10:03 lr 0.000854	time 0.7407 (0.7535)	loss 2.6555 (3.5201)	grad_norm 1.3061 (nan)	mem 23874MB
[2022-11-11 19:51:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][500/1251]	eta 0:09:25 lr 0.000854	time 0.7342 (0.7531)	loss 3.3175 (3.5241)	grad_norm 1.2080 (nan)	mem 23874MB
[2022-11-11 19:52:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][550/1251]	eta 0:08:47 lr 0.000853	time 0.7418 (0.7526)	loss 2.9738 (3.5240)	grad_norm 1.2398 (nan)	mem 23874MB
[2022-11-11 19:52:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][600/1251]	eta 0:08:09 lr 0.000853	time 0.7419 (0.7526)	loss 3.1538 (3.5332)	grad_norm 1.3053 (nan)	mem 23874MB
[2022-11-11 19:53:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][650/1251]	eta 0:07:32 lr 0.000853	time 0.8273 (0.7525)	loss 3.6657 (3.5338)	grad_norm 1.3255 (nan)	mem 23874MB
[2022-11-11 19:54:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][700/1251]	eta 0:06:54 lr 0.000853	time 0.7409 (0.7519)	loss 3.4546 (3.5357)	grad_norm 1.2439 (nan)	mem 23874MB
[2022-11-11 19:54:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][750/1251]	eta 0:06:16 lr 0.000853	time 0.7446 (0.7517)	loss 3.8136 (3.5239)	grad_norm 1.4677 (nan)	mem 23874MB
[2022-11-11 19:55:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][800/1251]	eta 0:05:38 lr 0.000853	time 0.7421 (0.7515)	loss 2.7441 (3.5290)	grad_norm 1.2737 (nan)	mem 23874MB
[2022-11-11 19:55:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][850/1251]	eta 0:05:01 lr 0.000853	time 0.7412 (0.7516)	loss 3.8149 (3.5269)	grad_norm 1.2226 (nan)	mem 23874MB
[2022-11-11 19:56:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][900/1251]	eta 0:04:23 lr 0.000852	time 0.7421 (0.7515)	loss 3.8194 (3.5265)	grad_norm 1.2434 (nan)	mem 23874MB
[2022-11-11 19:57:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][950/1251]	eta 0:03:46 lr 0.000852	time 0.8190 (0.7513)	loss 4.0707 (3.5249)	grad_norm 1.2897 (nan)	mem 23874MB
[2022-11-11 19:57:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][1000/1251]	eta 0:03:08 lr 0.000852	time 0.8125 (0.7513)	loss 4.1179 (3.5289)	grad_norm 1.3258 (nan)	mem 23874MB
[2022-11-11 19:58:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][1050/1251]	eta 0:02:30 lr 0.000852	time 0.7383 (0.7512)	loss 4.4693 (3.5355)	grad_norm 1.4920 (nan)	mem 23874MB
[2022-11-11 19:59:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][1100/1251]	eta 0:01:53 lr 0.000852	time 0.7348 (0.7510)	loss 3.5562 (3.5322)	grad_norm 1.4227 (nan)	mem 23874MB
[2022-11-11 19:59:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][1150/1251]	eta 0:01:15 lr 0.000852	time 0.7388 (0.7510)	loss 3.3914 (3.5270)	grad_norm 1.1982 (nan)	mem 23874MB
[2022-11-11 20:00:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][1200/1251]	eta 0:00:38 lr 0.000851	time 0.7422 (0.7508)	loss 3.9813 (3.5239)	grad_norm 1.2580 (nan)	mem 23874MB
[2022-11-11 20:00:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [75/300][1250/1251]	eta 0:00:00 lr 0.000851	time 0.7270 (0.7506)	loss 4.1206 (3.5218)	grad_norm 1.4789 (nan)	mem 23874MB
[2022-11-11 20:00:57 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 75 training takes 0:15:39
[2022-11-11 20:00:57 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_75.pth saving......
[2022-11-11 20:00:58 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_75.pth saved !!!
[2022-11-11 20:01:00 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.711 (1.711)	Loss 0.9774 (0.9774)	Acc@1 76.562 (76.562)	Acc@5 94.238 (94.238)	Mem 23874MB
[2022-11-11 20:01:11 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.812 Acc@5 93.486
[2022-11-11 20:01:11 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.8%
[2022-11-11 20:01:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.797 (1.797)	Loss 0.8946 (0.8946)	Acc@1 78.418 (78.418)	Acc@5 94.336 (94.336)	Mem 23874MB
[2022-11-11 20:01:23 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.576 Acc@5 94.654
[2022-11-11 20:01:23 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.6%
[2022-11-11 20:01:23 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.58% at 75 epoch
[2022-11-11 20:01:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][0/1251]	eta 0:50:23 lr 0.000851	time 2.4170 (2.4170)	loss 4.0605 (4.0605)	grad_norm 1.3962 (1.3962)	mem 23874MB
[2022-11-11 20:02:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][50/1251]	eta 0:15:34 lr 0.000851	time 0.8152 (0.7779)	loss 3.2176 (3.5892)	grad_norm 1.2752 (1.3243)	mem 23874MB
[2022-11-11 20:02:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][100/1251]	eta 0:14:38 lr 0.000851	time 0.7381 (0.7637)	loss 3.8725 (3.5655)	grad_norm 1.4176 (1.3302)	mem 23874MB
[2022-11-11 20:03:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][150/1251]	eta 0:13:54 lr 0.000851	time 0.7467 (0.7580)	loss 3.4713 (3.5730)	grad_norm 1.2680 (1.3277)	mem 23874MB
[2022-11-11 20:03:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][200/1251]	eta 0:13:13 lr 0.000851	time 0.7337 (0.7549)	loss 2.4093 (3.5574)	grad_norm 1.1739 (1.3321)	mem 23874MB
[2022-11-11 20:04:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][250/1251]	eta 0:12:34 lr 0.000851	time 0.7409 (0.7534)	loss 2.3891 (3.5303)	grad_norm 1.4055 (1.3342)	mem 23874MB
[2022-11-11 20:05:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][300/1251]	eta 0:11:55 lr 0.000850	time 0.8012 (0.7524)	loss 3.1122 (3.5152)	grad_norm 1.3716 (1.3332)	mem 23874MB
[2022-11-11 20:05:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][350/1251]	eta 0:11:17 lr 0.000850	time 0.7390 (0.7515)	loss 3.2658 (3.5125)	grad_norm 1.3075 (1.3292)	mem 23874MB
[2022-11-11 20:06:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][400/1251]	eta 0:10:39 lr 0.000850	time 0.7358 (0.7513)	loss 2.9820 (3.4910)	grad_norm 1.3160 (1.3319)	mem 23874MB
[2022-11-11 20:07:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][450/1251]	eta 0:10:01 lr 0.000850	time 0.8263 (0.7506)	loss 3.4770 (3.4996)	grad_norm 1.2513 (1.3360)	mem 23874MB
[2022-11-11 20:07:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][500/1251]	eta 0:09:23 lr 0.000850	time 0.7375 (0.7502)	loss 2.9495 (3.5021)	grad_norm 1.5277 (1.3371)	mem 23874MB
[2022-11-11 20:08:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][550/1251]	eta 0:08:45 lr 0.000850	time 0.7348 (0.7500)	loss 3.7702 (3.5061)	grad_norm 1.2924 (1.3399)	mem 23874MB
[2022-11-11 20:08:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][600/1251]	eta 0:08:07 lr 0.000850	time 0.8357 (0.7496)	loss 3.7338 (3.5000)	grad_norm 1.2508 (1.3401)	mem 23874MB
[2022-11-11 20:09:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][650/1251]	eta 0:07:30 lr 0.000849	time 0.7404 (0.7494)	loss 3.9344 (3.5043)	grad_norm 1.4332 (1.3396)	mem 23874MB
[2022-11-11 20:10:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][700/1251]	eta 0:06:52 lr 0.000849	time 0.8099 (0.7490)	loss 4.0852 (3.5093)	grad_norm 1.4385 (1.3406)	mem 23874MB
[2022-11-11 20:10:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][750/1251]	eta 0:06:15 lr 0.000849	time 0.7404 (0.7490)	loss 3.5413 (3.5118)	grad_norm 1.2918 (1.3374)	mem 23874MB
[2022-11-11 20:11:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][800/1251]	eta 0:05:37 lr 0.000849	time 0.7381 (0.7489)	loss 3.6596 (3.5119)	grad_norm 1.3269 (1.3394)	mem 23874MB
[2022-11-11 20:12:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][850/1251]	eta 0:05:00 lr 0.000849	time 0.7388 (0.7487)	loss 3.7223 (3.5159)	grad_norm 1.4196 (1.3392)	mem 23874MB
[2022-11-11 20:12:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][900/1251]	eta 0:04:22 lr 0.000849	time 0.7372 (0.7487)	loss 3.6867 (3.5134)	grad_norm 1.3249 (1.3401)	mem 23874MB
[2022-11-11 20:13:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][950/1251]	eta 0:03:45 lr 0.000849	time 0.7352 (0.7487)	loss 3.5635 (3.5035)	grad_norm 1.2349 (1.3405)	mem 23874MB
[2022-11-11 20:13:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][1000/1251]	eta 0:03:07 lr 0.000848	time 0.7442 (0.7485)	loss 3.7903 (3.5073)	grad_norm 1.4618 (1.3420)	mem 23874MB
[2022-11-11 20:14:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][1050/1251]	eta 0:02:30 lr 0.000848	time 0.7355 (0.7485)	loss 3.9768 (3.5103)	grad_norm 1.1663 (1.3416)	mem 23874MB
[2022-11-11 20:15:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][1100/1251]	eta 0:01:52 lr 0.000848	time 0.8152 (0.7483)	loss 3.3432 (3.5108)	grad_norm 1.2708 (1.3399)	mem 23874MB
[2022-11-11 20:15:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][1150/1251]	eta 0:01:15 lr 0.000848	time 0.7430 (0.7483)	loss 2.3993 (3.5050)	grad_norm 1.2729 (1.3391)	mem 23874MB
[2022-11-11 20:16:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][1200/1251]	eta 0:00:38 lr 0.000848	time 0.7459 (0.7484)	loss 3.9079 (3.5036)	grad_norm 1.2448 (1.3397)	mem 23874MB
[2022-11-11 20:16:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [76/300][1250/1251]	eta 0:00:00 lr 0.000848	time 0.7272 (0.7481)	loss 3.2213 (3.5000)	grad_norm 1.2901 (inf)	mem 23874MB
[2022-11-11 20:16:59 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 76 training takes 0:15:36
[2022-11-11 20:17:00 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_76.pth saving......
[2022-11-11 20:17:01 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_76.pth saved !!!
[2022-11-11 20:17:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.690 (1.690)	Loss 1.0609 (1.0609)	Acc@1 74.609 (74.609)	Acc@5 93.262 (93.262)	Mem 23874MB
[2022-11-11 20:17:13 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.896 Acc@5 93.388
[2022-11-11 20:17:13 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.9%
[2022-11-11 20:17:15 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.031 (2.031)	Loss 0.8639 (0.8639)	Acc@1 78.711 (78.711)	Acc@5 95.020 (95.020)	Mem 23874MB
[2022-11-11 20:17:26 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.598 Acc@5 94.704
[2022-11-11 20:17:26 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.6%
[2022-11-11 20:17:26 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.60% at 76 epoch
[2022-11-11 20:17:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][0/1251]	eta 0:51:10 lr 0.000848	time 2.4542 (2.4542)	loss 2.4577 (2.4577)	grad_norm 1.3506 (1.3506)	mem 23874MB
[2022-11-11 20:18:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][50/1251]	eta 0:15:41 lr 0.000847	time 0.7415 (0.7842)	loss 3.0917 (3.4237)	grad_norm 1.3335 (1.3203)	mem 23874MB
[2022-11-11 20:18:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][100/1251]	eta 0:14:43 lr 0.000847	time 0.8274 (0.7672)	loss 3.4131 (3.5382)	grad_norm 1.3310 (1.3368)	mem 23874MB
[2022-11-11 20:19:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][150/1251]	eta 0:13:56 lr 0.000847	time 0.7416 (0.7601)	loss 3.0251 (3.5458)	grad_norm 1.2514 (1.3337)	mem 23874MB
[2022-11-11 20:19:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][200/1251]	eta 0:13:16 lr 0.000847	time 0.8279 (0.7582)	loss 2.6348 (3.5446)	grad_norm 1.2506 (1.3368)	mem 23874MB
[2022-11-11 20:20:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][250/1251]	eta 0:12:35 lr 0.000847	time 0.7396 (0.7552)	loss 3.9114 (3.5491)	grad_norm 1.2710 (1.3370)	mem 23874MB
[2022-11-11 20:21:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][300/1251]	eta 0:11:57 lr 0.000847	time 0.7377 (0.7545)	loss 3.7805 (3.5532)	grad_norm 1.3343 (1.3432)	mem 23874MB
[2022-11-11 20:21:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][350/1251]	eta 0:11:19 lr 0.000847	time 0.7395 (0.7536)	loss 3.7119 (3.5532)	grad_norm 1.3074 (1.3477)	mem 23874MB
[2022-11-11 20:22:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][400/1251]	eta 0:10:40 lr 0.000846	time 0.7346 (0.7529)	loss 3.4868 (3.5500)	grad_norm 1.2723 (1.3440)	mem 23874MB
[2022-11-11 20:23:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][450/1251]	eta 0:10:02 lr 0.000846	time 0.7426 (0.7523)	loss 3.7531 (3.5534)	grad_norm 1.2765 (1.3436)	mem 23874MB
[2022-11-11 20:23:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][500/1251]	eta 0:09:24 lr 0.000846	time 0.7404 (0.7518)	loss 2.2622 (3.5539)	grad_norm 1.2514 (1.3456)	mem 23874MB
[2022-11-11 20:24:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][550/1251]	eta 0:08:46 lr 0.000846	time 0.7350 (0.7514)	loss 4.1866 (3.5411)	grad_norm 1.6778 (1.3457)	mem 23874MB
[2022-11-11 20:24:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][600/1251]	eta 0:08:09 lr 0.000846	time 0.8043 (0.7512)	loss 3.5432 (3.5421)	grad_norm 1.3183 (1.3437)	mem 23874MB
[2022-11-11 20:25:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][650/1251]	eta 0:07:31 lr 0.000846	time 0.7375 (0.7509)	loss 3.5165 (3.5363)	grad_norm 1.2716 (1.3431)	mem 23874MB
[2022-11-11 20:26:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][700/1251]	eta 0:06:53 lr 0.000846	time 0.7431 (0.7508)	loss 3.8976 (3.5363)	grad_norm 1.3057 (1.3428)	mem 23874MB
[2022-11-11 20:26:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][750/1251]	eta 0:06:16 lr 0.000845	time 0.7360 (0.7506)	loss 3.9605 (3.5319)	grad_norm 1.2472 (1.3412)	mem 23874MB
[2022-11-11 20:27:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][800/1251]	eta 0:05:38 lr 0.000845	time 0.7378 (0.7504)	loss 2.5368 (3.5285)	grad_norm 1.1634 (1.3405)	mem 23874MB
[2022-11-11 20:28:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][850/1251]	eta 0:05:00 lr 0.000845	time 0.7416 (0.7503)	loss 3.7205 (3.5276)	grad_norm 1.4119 (1.3418)	mem 23874MB
[2022-11-11 20:28:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][900/1251]	eta 0:04:23 lr 0.000845	time 0.7369 (0.7500)	loss 2.3650 (3.5236)	grad_norm 1.4851 (1.3427)	mem 23874MB
[2022-11-11 20:29:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][950/1251]	eta 0:03:45 lr 0.000845	time 0.7393 (0.7499)	loss 3.4609 (3.5213)	grad_norm 1.2428 (1.3442)	mem 23874MB
[2022-11-11 20:29:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][1000/1251]	eta 0:03:08 lr 0.000845	time 0.8191 (0.7500)	loss 2.8666 (3.5221)	grad_norm 1.3845 (1.3437)	mem 23874MB
[2022-11-11 20:30:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][1050/1251]	eta 0:02:30 lr 0.000844	time 0.7422 (0.7499)	loss 3.8238 (3.5273)	grad_norm 1.3463 (1.3434)	mem 23874MB
[2022-11-11 20:31:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][1100/1251]	eta 0:01:53 lr 0.000844	time 0.7412 (0.7497)	loss 4.5615 (3.5284)	grad_norm 1.5554 (1.3435)	mem 23874MB
[2022-11-11 20:31:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][1150/1251]	eta 0:01:15 lr 0.000844	time 0.7407 (0.7497)	loss 3.3003 (3.5263)	grad_norm 1.2429 (1.3433)	mem 23874MB
[2022-11-11 20:32:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][1200/1251]	eta 0:00:38 lr 0.000844	time 0.7363 (0.7495)	loss 3.5398 (3.5181)	grad_norm 1.2842 (1.3431)	mem 23874MB
[2022-11-11 20:33:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [77/300][1250/1251]	eta 0:00:00 lr 0.000844	time 0.7270 (0.7494)	loss 3.7002 (3.5218)	grad_norm 1.2302 (1.3432)	mem 23874MB
[2022-11-11 20:33:04 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 77 training takes 0:15:37
[2022-11-11 20:33:04 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_77.pth saving......
[2022-11-11 20:33:05 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_77.pth saved !!!
[2022-11-11 20:33:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.725 (1.725)	Loss 0.9815 (0.9815)	Acc@1 76.660 (76.660)	Acc@5 95.020 (95.020)	Mem 23874MB
[2022-11-11 20:33:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.954 Acc@5 93.458
[2022-11-11 20:33:17 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.0%
[2022-11-11 20:33:19 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.116 (2.116)	Loss 0.7847 (0.7847)	Acc@1 80.859 (80.859)	Acc@5 95.508 (95.508)	Mem 23874MB
[2022-11-11 20:33:30 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.682 Acc@5 94.718
[2022-11-11 20:33:30 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.7%
[2022-11-11 20:33:30 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.68% at 77 epoch
[2022-11-11 20:33:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][0/1251]	eta 0:50:59 lr 0.000844	time 2.4458 (2.4458)	loss 3.9885 (3.9885)	grad_norm 1.3172 (1.3172)	mem 23874MB
[2022-11-11 20:34:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][50/1251]	eta 0:15:42 lr 0.000844	time 0.7394 (0.7844)	loss 2.6542 (3.5669)	grad_norm 1.3095 (1.3578)	mem 23874MB
[2022-11-11 20:34:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][100/1251]	eta 0:14:44 lr 0.000844	time 0.7559 (0.7683)	loss 3.9869 (3.5158)	grad_norm 1.1978 (1.3422)	mem 23874MB
[2022-11-11 20:35:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][150/1251]	eta 0:13:59 lr 0.000843	time 0.7365 (0.7621)	loss 2.6594 (3.4999)	grad_norm 1.1844 (1.3420)	mem 23874MB
[2022-11-11 20:36:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][200/1251]	eta 0:13:17 lr 0.000843	time 0.7387 (0.7588)	loss 3.6772 (3.4627)	grad_norm 1.3232 (1.3386)	mem 23874MB
[2022-11-11 20:36:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][250/1251]	eta 0:12:37 lr 0.000843	time 0.7455 (0.7570)	loss 3.6370 (3.4511)	grad_norm 1.2573 (1.3340)	mem 23874MB
[2022-11-11 20:37:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][300/1251]	eta 0:11:58 lr 0.000843	time 0.7388 (0.7551)	loss 2.7064 (3.4648)	grad_norm 1.2208 (1.3368)	mem 23874MB
[2022-11-11 20:37:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][350/1251]	eta 0:11:19 lr 0.000843	time 0.7419 (0.7545)	loss 2.1792 (3.4531)	grad_norm 1.3020 (1.3382)	mem 23874MB
[2022-11-11 20:38:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][400/1251]	eta 0:10:41 lr 0.000843	time 0.7396 (0.7541)	loss 4.2727 (3.4669)	grad_norm 1.6195 (1.3345)	mem 23874MB
[2022-11-11 20:39:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][450/1251]	eta 0:10:03 lr 0.000842	time 0.7393 (0.7533)	loss 3.5009 (3.4719)	grad_norm 1.4224 (1.3375)	mem 23874MB
[2022-11-11 20:39:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][500/1251]	eta 0:09:25 lr 0.000842	time 0.7389 (0.7527)	loss 4.2577 (3.4796)	grad_norm 1.5000 (1.3354)	mem 23874MB
[2022-11-11 20:40:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][550/1251]	eta 0:08:47 lr 0.000842	time 0.7322 (0.7525)	loss 3.2353 (3.4753)	grad_norm 1.2748 (1.3370)	mem 23874MB
[2022-11-11 20:41:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][600/1251]	eta 0:08:09 lr 0.000842	time 0.7349 (0.7521)	loss 2.8792 (3.4685)	grad_norm 1.5012 (1.3386)	mem 23874MB
[2022-11-11 20:41:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][650/1251]	eta 0:07:31 lr 0.000842	time 0.7430 (0.7521)	loss 3.7793 (3.4641)	grad_norm 1.4453 (1.3417)	mem 23874MB
[2022-11-11 20:42:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][700/1251]	eta 0:06:54 lr 0.000842	time 0.7395 (0.7516)	loss 3.6466 (3.4699)	grad_norm 1.3065 (1.3436)	mem 23874MB
[2022-11-11 20:42:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][750/1251]	eta 0:06:16 lr 0.000842	time 0.7373 (0.7514)	loss 3.9479 (3.4788)	grad_norm 1.5071 (1.3467)	mem 23874MB
[2022-11-11 20:43:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][800/1251]	eta 0:05:38 lr 0.000841	time 0.7453 (0.7513)	loss 2.5855 (3.4846)	grad_norm 1.3862 (1.3450)	mem 23874MB
[2022-11-11 20:44:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][850/1251]	eta 0:05:01 lr 0.000841	time 0.7363 (0.7509)	loss 3.5011 (3.4819)	grad_norm 1.3300 (inf)	mem 23874MB
[2022-11-11 20:44:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][900/1251]	eta 0:04:23 lr 0.000841	time 0.7365 (0.7508)	loss 3.7791 (3.4899)	grad_norm 1.1169 (inf)	mem 23874MB
[2022-11-11 20:45:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][950/1251]	eta 0:03:45 lr 0.000841	time 0.7351 (0.7507)	loss 3.3697 (3.4933)	grad_norm 1.1196 (inf)	mem 23874MB
[2022-11-11 20:46:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][1000/1251]	eta 0:03:08 lr 0.000841	time 0.8200 (0.7505)	loss 2.2747 (3.4955)	grad_norm 1.4333 (inf)	mem 23874MB
[2022-11-11 20:46:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][1050/1251]	eta 0:02:30 lr 0.000841	time 0.7430 (0.7503)	loss 3.1832 (3.4983)	grad_norm 1.2644 (inf)	mem 23874MB
[2022-11-11 20:47:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][1100/1251]	eta 0:01:53 lr 0.000841	time 0.7508 (0.7502)	loss 2.7458 (3.4962)	grad_norm 1.2747 (inf)	mem 23874MB
[2022-11-11 20:47:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][1150/1251]	eta 0:01:15 lr 0.000840	time 0.7404 (0.7501)	loss 3.4127 (3.4999)	grad_norm 1.2700 (inf)	mem 23874MB
[2022-11-11 20:48:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][1200/1251]	eta 0:00:38 lr 0.000840	time 0.7416 (0.7501)	loss 3.8550 (3.5004)	grad_norm 1.3274 (inf)	mem 23874MB
[2022-11-11 20:49:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [78/300][1250/1251]	eta 0:00:00 lr 0.000840	time 0.7282 (0.7498)	loss 3.9835 (3.5002)	grad_norm 1.3519 (inf)	mem 23874MB
[2022-11-11 20:49:08 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 78 training takes 0:15:38
[2022-11-11 20:49:09 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_78.pth saving......
[2022-11-11 20:49:10 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_78.pth saved !!!
[2022-11-11 20:49:11 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.687 (1.687)	Loss 1.0269 (1.0269)	Acc@1 74.609 (74.609)	Acc@5 92.969 (92.969)	Mem 23874MB
[2022-11-11 20:49:22 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.894 Acc@5 93.370
[2022-11-11 20:49:22 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.9%
[2022-11-11 20:49:24 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.756 (1.756)	Loss 0.8686 (0.8686)	Acc@1 78.613 (78.613)	Acc@5 95.215 (95.215)	Mem 23874MB
[2022-11-11 20:49:35 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.736 Acc@5 94.762
[2022-11-11 20:49:35 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.7%
[2022-11-11 20:49:35 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.74% at 78 epoch
[2022-11-11 20:49:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][0/1251]	eta 0:51:09 lr 0.000840	time 2.4540 (2.4540)	loss 2.9979 (2.9979)	grad_norm 1.4740 (1.4740)	mem 23874MB
[2022-11-11 20:50:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][50/1251]	eta 0:15:39 lr 0.000840	time 0.8632 (0.7824)	loss 2.3043 (3.4592)	grad_norm 1.4972 (1.3573)	mem 23874MB
[2022-11-11 20:50:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][100/1251]	eta 0:14:40 lr 0.000840	time 0.7368 (0.7652)	loss 3.0998 (3.4139)	grad_norm 1.3332 (1.3401)	mem 23874MB
[2022-11-11 20:51:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][150/1251]	eta 0:13:57 lr 0.000840	time 0.7379 (0.7603)	loss 3.7827 (3.4419)	grad_norm 1.2744 (1.3461)	mem 23874MB
[2022-11-11 20:52:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][200/1251]	eta 0:13:15 lr 0.000839	time 0.8107 (0.7573)	loss 3.4023 (3.4626)	grad_norm 1.2685 (1.3421)	mem 23874MB
[2022-11-11 20:52:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][250/1251]	eta 0:12:36 lr 0.000839	time 0.7399 (0.7553)	loss 2.3166 (3.4435)	grad_norm 1.2792 (1.3393)	mem 23874MB
[2022-11-11 20:53:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][300/1251]	eta 0:11:57 lr 0.000839	time 0.8057 (0.7544)	loss 3.5141 (3.4600)	grad_norm 1.4155 (1.3413)	mem 23874MB
[2022-11-11 20:53:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][350/1251]	eta 0:11:18 lr 0.000839	time 0.7479 (0.7533)	loss 3.8597 (3.4593)	grad_norm 1.2837 (1.3409)	mem 23874MB
[2022-11-11 20:54:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][400/1251]	eta 0:10:40 lr 0.000839	time 0.7447 (0.7531)	loss 2.8183 (3.4493)	grad_norm 1.2635 (1.3371)	mem 23874MB
[2022-11-11 20:55:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][450/1251]	eta 0:10:02 lr 0.000839	time 0.7415 (0.7523)	loss 3.9614 (3.4709)	grad_norm 1.3115 (1.3376)	mem 23874MB
[2022-11-11 20:55:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][500/1251]	eta 0:09:24 lr 0.000839	time 0.7350 (0.7518)	loss 3.3571 (3.4718)	grad_norm 1.2760 (1.3399)	mem 23874MB
[2022-11-11 20:56:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][550/1251]	eta 0:08:46 lr 0.000838	time 0.8227 (0.7515)	loss 3.7808 (3.4799)	grad_norm 1.2631 (1.3390)	mem 23874MB
[2022-11-11 20:57:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][600/1251]	eta 0:08:08 lr 0.000838	time 0.7246 (0.7511)	loss 3.4715 (3.4909)	grad_norm 1.2520 (1.3398)	mem 23874MB
[2022-11-11 20:57:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][650/1251]	eta 0:07:31 lr 0.000838	time 0.7351 (0.7508)	loss 2.9478 (3.4960)	grad_norm 1.4205 (1.3422)	mem 23874MB
[2022-11-11 20:58:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][700/1251]	eta 0:06:53 lr 0.000838	time 0.8099 (0.7505)	loss 3.8563 (3.4993)	grad_norm 1.3741 (1.3446)	mem 23874MB
[2022-11-11 20:58:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][750/1251]	eta 0:06:15 lr 0.000838	time 0.7391 (0.7504)	loss 3.0768 (3.4973)	grad_norm 1.3586 (1.3446)	mem 23874MB
[2022-11-11 20:59:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][800/1251]	eta 0:05:38 lr 0.000838	time 0.7388 (0.7500)	loss 3.4742 (3.4911)	grad_norm 1.5432 (1.3450)	mem 23874MB
[2022-11-11 21:00:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][850/1251]	eta 0:05:00 lr 0.000837	time 0.7393 (0.7499)	loss 3.8586 (3.4937)	grad_norm 1.4372 (1.3467)	mem 23874MB
[2022-11-11 21:00:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][900/1251]	eta 0:04:23 lr 0.000837	time 0.7337 (0.7497)	loss 3.2994 (3.4880)	grad_norm 1.2754 (1.3482)	mem 23874MB
[2022-11-11 21:01:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][950/1251]	eta 0:03:45 lr 0.000837	time 0.7417 (0.7497)	loss 3.8462 (3.4856)	grad_norm 1.4327 (1.3501)	mem 23874MB
[2022-11-11 21:02:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][1000/1251]	eta 0:03:08 lr 0.000837	time 0.7387 (0.7496)	loss 3.4710 (3.4863)	grad_norm 1.2634 (1.3489)	mem 23874MB
[2022-11-11 21:02:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][1050/1251]	eta 0:02:30 lr 0.000837	time 0.7429 (0.7494)	loss 4.0451 (3.4881)	grad_norm 1.1702 (1.3465)	mem 23874MB
[2022-11-11 21:03:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][1100/1251]	eta 0:01:53 lr 0.000837	time 0.8103 (0.7494)	loss 4.1985 (3.4912)	grad_norm 1.3890 (1.3472)	mem 23874MB
[2022-11-11 21:03:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][1150/1251]	eta 0:01:15 lr 0.000837	time 0.7414 (0.7493)	loss 3.8931 (3.4924)	grad_norm 1.4637 (1.3468)	mem 23874MB
[2022-11-11 21:04:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][1200/1251]	eta 0:00:38 lr 0.000836	time 0.7383 (0.7492)	loss 3.2297 (3.4926)	grad_norm 1.3553 (1.3478)	mem 23874MB
[2022-11-11 21:05:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [79/300][1250/1251]	eta 0:00:00 lr 0.000836	time 0.7269 (0.7490)	loss 4.0967 (3.4885)	grad_norm 1.3241 (1.3469)	mem 23874MB
[2022-11-11 21:05:12 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 79 training takes 0:15:37
[2022-11-11 21:05:12 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_79.pth saving......
[2022-11-11 21:05:13 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_79.pth saved !!!
[2022-11-11 21:05:15 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.640 (1.640)	Loss 1.0718 (1.0718)	Acc@1 75.000 (75.000)	Acc@5 92.578 (92.578)	Mem 23874MB
[2022-11-11 21:05:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.902 Acc@5 93.382
[2022-11-11 21:05:25 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 75.9%
[2022-11-11 21:05:27 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.991 (1.991)	Loss 0.8848 (0.8848)	Acc@1 78.516 (78.516)	Acc@5 94.531 (94.531)	Mem 23874MB
[2022-11-11 21:05:38 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.826 Acc@5 94.794
[2022-11-11 21:05:38 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.8%
[2022-11-11 21:05:38 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.83% at 79 epoch
[2022-11-11 21:05:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][0/1251]	eta 0:51:13 lr 0.000836	time 2.4565 (2.4565)	loss 2.6989 (2.6989)	grad_norm 1.2820 (1.2820)	mem 23874MB
[2022-11-11 21:06:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][50/1251]	eta 0:15:44 lr 0.000836	time 0.7342 (0.7865)	loss 3.5615 (3.3437)	grad_norm 1.3290 (1.3327)	mem 23874MB
[2022-11-11 21:06:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][100/1251]	eta 0:14:42 lr 0.000836	time 0.7390 (0.7664)	loss 3.7952 (3.4283)	grad_norm 1.1821 (1.3450)	mem 23874MB
[2022-11-11 21:07:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][150/1251]	eta 0:13:57 lr 0.000836	time 0.7522 (0.7606)	loss 3.5108 (3.4516)	grad_norm 1.3816 (1.3429)	mem 23874MB
[2022-11-11 21:08:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][200/1251]	eta 0:13:16 lr 0.000836	time 0.7351 (0.7580)	loss 3.9132 (3.4832)	grad_norm 1.3770 (1.3491)	mem 23874MB
[2022-11-11 21:08:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][250/1251]	eta 0:12:36 lr 0.000835	time 0.7352 (0.7554)	loss 3.6163 (3.4976)	grad_norm 1.6482 (1.3536)	mem 23874MB
[2022-11-11 21:09:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][300/1251]	eta 0:11:57 lr 0.000835	time 0.7365 (0.7540)	loss 2.9261 (3.4881)	grad_norm 1.2945 (1.3536)	mem 23874MB
[2022-11-11 21:10:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][350/1251]	eta 0:11:18 lr 0.000835	time 0.7364 (0.7534)	loss 3.7186 (3.4823)	grad_norm 1.4396 (1.3524)	mem 23874MB
[2022-11-11 21:10:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][400/1251]	eta 0:10:40 lr 0.000835	time 0.8281 (0.7526)	loss 3.6207 (3.4672)	grad_norm 1.3894 (1.3538)	mem 23874MB
[2022-11-11 21:11:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][450/1251]	eta 0:10:02 lr 0.000835	time 0.7427 (0.7521)	loss 3.3385 (3.4702)	grad_norm 1.2582 (1.3565)	mem 23874MB
[2022-11-11 21:11:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][500/1251]	eta 0:09:24 lr 0.000835	time 0.7470 (0.7515)	loss 3.7084 (3.4619)	grad_norm 1.0758 (1.3547)	mem 23874MB
[2022-11-11 21:12:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][550/1251]	eta 0:08:46 lr 0.000835	time 0.7429 (0.7509)	loss 3.7662 (3.4573)	grad_norm 1.3691 (1.3551)	mem 23874MB
[2022-11-11 21:13:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][600/1251]	eta 0:08:08 lr 0.000834	time 0.7388 (0.7507)	loss 2.9228 (3.4569)	grad_norm 1.3284 (1.3547)	mem 23874MB
[2022-11-11 21:13:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][650/1251]	eta 0:07:31 lr 0.000834	time 0.7401 (0.7505)	loss 3.2430 (3.4622)	grad_norm 1.3485 (1.3538)	mem 23874MB
[2022-11-11 21:14:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][700/1251]	eta 0:06:53 lr 0.000834	time 0.7376 (0.7502)	loss 2.7632 (3.4631)	grad_norm 1.2447 (inf)	mem 23874MB
[2022-11-11 21:15:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][750/1251]	eta 0:06:15 lr 0.000834	time 0.8156 (0.7503)	loss 3.2047 (3.4676)	grad_norm 1.3404 (inf)	mem 23874MB
[2022-11-11 21:15:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][800/1251]	eta 0:05:38 lr 0.000834	time 0.7362 (0.7498)	loss 3.6819 (3.4688)	grad_norm 1.4944 (inf)	mem 23874MB
[2022-11-11 21:16:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][850/1251]	eta 0:05:00 lr 0.000834	time 0.7464 (0.7499)	loss 3.8255 (3.4628)	grad_norm 1.2198 (inf)	mem 23874MB
[2022-11-11 21:16:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][900/1251]	eta 0:04:23 lr 0.000833	time 0.7447 (0.7496)	loss 3.5312 (3.4592)	grad_norm 1.3662 (inf)	mem 23874MB
[2022-11-11 21:17:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][950/1251]	eta 0:03:45 lr 0.000833	time 0.7400 (0.7496)	loss 3.6226 (3.4608)	grad_norm 1.1699 (inf)	mem 23874MB
[2022-11-11 21:18:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][1000/1251]	eta 0:03:08 lr 0.000833	time 0.7404 (0.7496)	loss 2.3616 (3.4601)	grad_norm 1.4233 (inf)	mem 23874MB
[2022-11-11 21:18:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][1050/1251]	eta 0:02:30 lr 0.000833	time 0.7427 (0.7494)	loss 3.4381 (3.4601)	grad_norm 1.3457 (inf)	mem 23874MB
[2022-11-11 21:19:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][1100/1251]	eta 0:01:53 lr 0.000833	time 0.7366 (0.7494)	loss 2.6414 (3.4644)	grad_norm 1.3489 (inf)	mem 23874MB
[2022-11-11 21:20:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][1150/1251]	eta 0:01:15 lr 0.000833	time 0.7334 (0.7494)	loss 2.5822 (3.4596)	grad_norm 1.1536 (inf)	mem 23874MB
[2022-11-11 21:20:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][1200/1251]	eta 0:00:38 lr 0.000833	time 0.7456 (0.7493)	loss 3.6162 (3.4640)	grad_norm 1.2118 (inf)	mem 23874MB
[2022-11-11 21:21:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [80/300][1250/1251]	eta 0:00:00 lr 0.000832	time 0.7274 (0.7491)	loss 3.9076 (3.4713)	grad_norm 1.3876 (inf)	mem 23874MB
[2022-11-11 21:21:15 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 80 training takes 0:15:37
[2022-11-11 21:21:15 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_80.pth saving......
[2022-11-11 21:21:17 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_80.pth saved !!!
[2022-11-11 21:21:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.598 (1.598)	Loss 0.9485 (0.9485)	Acc@1 77.832 (77.832)	Acc@5 94.922 (94.922)	Mem 23874MB
[2022-11-11 21:21:29 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.174 Acc@5 93.556
[2022-11-11 21:21:29 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.2%
[2022-11-11 21:21:31 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.796 (1.796)	Loss 0.8961 (0.8961)	Acc@1 77.930 (77.930)	Acc@5 94.336 (94.336)	Mem 23874MB
[2022-11-11 21:21:41 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.874 Acc@5 94.824
[2022-11-11 21:21:41 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.9%
[2022-11-11 21:21:41 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.87% at 80 epoch
[2022-11-11 21:21:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][0/1251]	eta 0:53:07 lr 0.000832	time 2.5484 (2.5484)	loss 3.0675 (3.0675)	grad_norm 1.3316 (1.3316)	mem 23876MB
[2022-11-11 21:22:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][50/1251]	eta 0:15:41 lr 0.000832	time 0.7431 (0.7837)	loss 3.9955 (3.4540)	grad_norm 1.3171 (1.3452)	mem 23876MB
[2022-11-11 21:22:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][100/1251]	eta 0:14:41 lr 0.000832	time 0.7376 (0.7663)	loss 4.0590 (3.4902)	grad_norm 1.3666 (1.3360)	mem 23876MB
[2022-11-11 21:23:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][150/1251]	eta 0:13:57 lr 0.000832	time 0.7389 (0.7603)	loss 3.7823 (3.4742)	grad_norm 1.3660 (1.3516)	mem 23876MB
[2022-11-11 21:24:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][200/1251]	eta 0:13:15 lr 0.000832	time 0.8094 (0.7570)	loss 2.9946 (3.4836)	grad_norm 1.4067 (1.3419)	mem 23876MB
[2022-11-11 21:24:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][250/1251]	eta 0:12:35 lr 0.000832	time 0.7372 (0.7551)	loss 3.3946 (3.4795)	grad_norm 1.3411 (1.3395)	mem 23876MB
[2022-11-11 21:25:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][300/1251]	eta 0:11:56 lr 0.000831	time 0.7516 (0.7536)	loss 3.8680 (3.4755)	grad_norm 1.3346 (1.3425)	mem 23876MB
[2022-11-11 21:26:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][350/1251]	eta 0:11:18 lr 0.000831	time 0.7441 (0.7529)	loss 3.3129 (3.4686)	grad_norm 1.2863 (1.3411)	mem 23876MB
[2022-11-11 21:26:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][400/1251]	eta 0:10:40 lr 0.000831	time 0.7423 (0.7523)	loss 3.7834 (3.4798)	grad_norm 1.3898 (1.3386)	mem 23876MB
[2022-11-11 21:27:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][450/1251]	eta 0:10:02 lr 0.000831	time 0.7405 (0.7521)	loss 2.9105 (3.4783)	grad_norm 1.4831 (1.3415)	mem 23876MB
[2022-11-11 21:27:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][500/1251]	eta 0:09:24 lr 0.000831	time 0.7385 (0.7511)	loss 3.6665 (3.4864)	grad_norm 1.2252 (1.3424)	mem 23876MB
[2022-11-11 21:28:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][550/1251]	eta 0:08:46 lr 0.000831	time 0.7404 (0.7508)	loss 4.1175 (3.4764)	grad_norm 1.2922 (1.3429)	mem 23876MB
[2022-11-11 21:29:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][600/1251]	eta 0:08:08 lr 0.000830	time 0.7342 (0.7506)	loss 3.6378 (3.4851)	grad_norm 1.2752 (1.3432)	mem 23876MB
[2022-11-11 21:29:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][650/1251]	eta 0:07:30 lr 0.000830	time 0.7424 (0.7504)	loss 3.0698 (3.4944)	grad_norm 1.4131 (1.3456)	mem 23876MB
[2022-11-11 21:30:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][700/1251]	eta 0:06:53 lr 0.000830	time 0.7383 (0.7504)	loss 4.1123 (3.4965)	grad_norm 1.3812 (1.3486)	mem 23876MB
[2022-11-11 21:31:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][750/1251]	eta 0:06:15 lr 0.000830	time 0.8210 (0.7501)	loss 3.4900 (3.4908)	grad_norm 1.3499 (1.3471)	mem 23876MB
[2022-11-11 21:31:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][800/1251]	eta 0:05:38 lr 0.000830	time 0.7363 (0.7499)	loss 4.3214 (3.4844)	grad_norm 1.4376 (1.3485)	mem 23876MB
[2022-11-11 21:32:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][850/1251]	eta 0:05:00 lr 0.000830	time 0.7449 (0.7498)	loss 3.8335 (3.4942)	grad_norm 1.2528 (1.3491)	mem 23876MB
[2022-11-11 21:32:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][900/1251]	eta 0:04:23 lr 0.000830	time 0.7343 (0.7496)	loss 3.8322 (3.4932)	grad_norm 1.4425 (1.3499)	mem 23876MB
[2022-11-11 21:33:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][950/1251]	eta 0:03:45 lr 0.000829	time 0.7365 (0.7496)	loss 3.9797 (3.4915)	grad_norm 1.2130 (1.3513)	mem 23876MB
[2022-11-11 21:34:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][1000/1251]	eta 0:03:08 lr 0.000829	time 0.7380 (0.7494)	loss 3.4473 (3.4909)	grad_norm 1.4860 (1.3505)	mem 23876MB
[2022-11-11 21:34:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][1050/1251]	eta 0:02:30 lr 0.000829	time 0.7373 (0.7492)	loss 3.9821 (3.4909)	grad_norm 1.3805 (1.3507)	mem 23876MB
[2022-11-11 21:35:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][1100/1251]	eta 0:01:53 lr 0.000829	time 0.7408 (0.7492)	loss 3.3249 (3.4845)	grad_norm 1.3545 (1.3494)	mem 23876MB
[2022-11-11 21:36:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][1150/1251]	eta 0:01:15 lr 0.000829	time 0.7412 (0.7492)	loss 4.1404 (3.4805)	grad_norm 1.7039 (1.3494)	mem 23876MB
[2022-11-11 21:36:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][1200/1251]	eta 0:00:38 lr 0.000829	time 0.7404 (0.7492)	loss 2.8364 (3.4838)	grad_norm 1.3353 (1.3501)	mem 23876MB
[2022-11-11 21:37:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [81/300][1250/1251]	eta 0:00:00 lr 0.000828	time 0.7313 (0.7489)	loss 3.6765 (3.4848)	grad_norm 1.3040 (1.3492)	mem 23876MB
[2022-11-11 21:37:18 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 81 training takes 0:15:37
[2022-11-11 21:37:19 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_81.pth saving......
[2022-11-11 21:37:20 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_81.pth saved !!!
[2022-11-11 21:37:21 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.636 (1.636)	Loss 0.9822 (0.9822)	Acc@1 77.441 (77.441)	Acc@5 93.945 (93.945)	Mem 23876MB
[2022-11-11 21:37:32 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.462 Acc@5 93.740
[2022-11-11 21:37:32 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.5%
[2022-11-11 21:37:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.835 (1.835)	Loss 0.7886 (0.7886)	Acc@1 81.738 (81.738)	Acc@5 94.922 (94.922)	Mem 23876MB
[2022-11-11 21:37:45 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.920 Acc@5 94.854
[2022-11-11 21:37:45 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 78.9%
[2022-11-11 21:37:45 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 78.92% at 81 epoch
[2022-11-11 21:37:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][0/1251]	eta 0:51:02 lr 0.000828	time 2.4482 (2.4482)	loss 2.0135 (2.0135)	grad_norm 1.2637 (1.2637)	mem 23876MB
[2022-11-11 21:38:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][50/1251]	eta 0:15:42 lr 0.000828	time 0.7380 (0.7844)	loss 3.8135 (3.4979)	grad_norm 1.3693 (1.3563)	mem 23876MB
[2022-11-11 21:39:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][100/1251]	eta 0:14:40 lr 0.000828	time 0.7514 (0.7653)	loss 3.6582 (3.4714)	grad_norm 1.4300 (1.3463)	mem 23876MB
[2022-11-11 21:39:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][150/1251]	eta 0:13:57 lr 0.000828	time 0.7337 (0.7606)	loss 3.3901 (3.4907)	grad_norm 1.3985 (1.3570)	mem 23876MB
[2022-11-11 21:40:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][200/1251]	eta 0:13:15 lr 0.000828	time 0.7384 (0.7565)	loss 2.6047 (3.5004)	grad_norm 1.2965 (1.3536)	mem 23876MB
[2022-11-11 21:40:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][250/1251]	eta 0:12:35 lr 0.000828	time 0.7387 (0.7547)	loss 2.6926 (3.4812)	grad_norm 1.2476 (1.3456)	mem 23876MB
[2022-11-11 21:41:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][300/1251]	eta 0:11:56 lr 0.000828	time 0.7433 (0.7536)	loss 3.6689 (3.4866)	grad_norm 1.5650 (1.3418)	mem 23876MB
[2022-11-11 21:42:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][350/1251]	eta 0:11:18 lr 0.000827	time 0.7373 (0.7527)	loss 3.9322 (3.4858)	grad_norm 1.5800 (1.3448)	mem 23876MB
[2022-11-11 21:42:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][400/1251]	eta 0:10:40 lr 0.000827	time 0.7432 (0.7522)	loss 2.5336 (3.4851)	grad_norm 1.3770 (1.3439)	mem 23876MB
[2022-11-11 21:43:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][450/1251]	eta 0:10:02 lr 0.000827	time 0.7374 (0.7520)	loss 3.9539 (3.4795)	grad_norm 1.2538 (1.3437)	mem 23876MB
[2022-11-11 21:44:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][500/1251]	eta 0:09:24 lr 0.000827	time 0.7365 (0.7515)	loss 3.6958 (3.4732)	grad_norm 1.6265 (1.3461)	mem 23876MB
[2022-11-11 21:44:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][550/1251]	eta 0:08:46 lr 0.000827	time 0.7403 (0.7512)	loss 3.4841 (3.4769)	grad_norm 1.1995 (1.3479)	mem 23876MB
[2022-11-11 21:45:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][600/1251]	eta 0:08:08 lr 0.000827	time 0.7241 (0.7511)	loss 4.2460 (3.4736)	grad_norm 1.4980 (1.3496)	mem 23876MB
[2022-11-11 21:45:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][650/1251]	eta 0:07:31 lr 0.000826	time 0.7356 (0.7508)	loss 4.0391 (3.4822)	grad_norm 1.4172 (1.3503)	mem 23876MB
[2022-11-11 21:46:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][700/1251]	eta 0:06:53 lr 0.000826	time 0.7426 (0.7509)	loss 3.8309 (3.4829)	grad_norm 1.3543 (nan)	mem 23876MB
[2022-11-11 21:47:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][750/1251]	eta 0:06:16 lr 0.000826	time 0.7383 (0.7505)	loss 3.7577 (3.4791)	grad_norm 1.2506 (nan)	mem 23876MB
[2022-11-11 21:47:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][800/1251]	eta 0:05:38 lr 0.000826	time 0.7380 (0.7504)	loss 2.6906 (3.4779)	grad_norm 1.3102 (nan)	mem 23876MB
[2022-11-11 21:48:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][850/1251]	eta 0:05:00 lr 0.000826	time 0.7417 (0.7505)	loss 4.0706 (3.4904)	grad_norm 1.5105 (nan)	mem 23876MB
[2022-11-11 21:49:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][900/1251]	eta 0:04:23 lr 0.000826	time 0.7399 (0.7503)	loss 3.6595 (3.4859)	grad_norm 1.2854 (nan)	mem 23876MB
[2022-11-11 21:49:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][950/1251]	eta 0:03:45 lr 0.000825	time 0.7399 (0.7502)	loss 3.9024 (3.4860)	grad_norm 1.3258 (nan)	mem 23876MB
[2022-11-11 21:50:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][1000/1251]	eta 0:03:08 lr 0.000825	time 0.7367 (0.7500)	loss 2.8213 (3.4863)	grad_norm 1.4218 (nan)	mem 23876MB
[2022-11-11 21:50:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][1050/1251]	eta 0:02:30 lr 0.000825	time 0.7434 (0.7498)	loss 4.2860 (3.4799)	grad_norm 1.5658 (nan)	mem 23876MB
[2022-11-11 21:51:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][1100/1251]	eta 0:01:53 lr 0.000825	time 0.7361 (0.7497)	loss 3.7865 (3.4827)	grad_norm 1.3327 (nan)	mem 23876MB
[2022-11-11 21:52:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][1150/1251]	eta 0:01:15 lr 0.000825	time 0.7334 (0.7496)	loss 2.4862 (3.4826)	grad_norm 1.2291 (nan)	mem 23876MB
[2022-11-11 21:52:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][1200/1251]	eta 0:00:38 lr 0.000825	time 0.7417 (0.7497)	loss 4.0741 (3.4773)	grad_norm 1.2827 (nan)	mem 23876MB
[2022-11-11 21:53:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [82/300][1250/1251]	eta 0:00:00 lr 0.000825	time 0.7263 (0.7496)	loss 3.6740 (3.4785)	grad_norm 1.3472 (nan)	mem 23876MB
[2022-11-11 21:53:23 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 82 training takes 0:15:37
[2022-11-11 21:53:23 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_82.pth saving......
[2022-11-11 21:53:24 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_82.pth saved !!!
[2022-11-11 21:53:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.663 (1.663)	Loss 1.0350 (1.0350)	Acc@1 75.488 (75.488)	Acc@5 93.457 (93.457)	Mem 23876MB
[2022-11-11 21:53:36 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.384 Acc@5 93.542
[2022-11-11 21:53:36 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.4%
[2022-11-11 21:53:38 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.802 (1.802)	Loss 0.7676 (0.7676)	Acc@1 81.055 (81.055)	Acc@5 95.703 (95.703)	Mem 23876MB
[2022-11-11 21:53:49 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.016 Acc@5 94.878
[2022-11-11 21:53:49 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.0%
[2022-11-11 21:53:49 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.02% at 82 epoch
[2022-11-11 21:53:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][0/1251]	eta 0:52:20 lr 0.000825	time 2.5107 (2.5107)	loss 3.1055 (3.1055)	grad_norm 1.3554 (1.3554)	mem 23876MB
[2022-11-11 21:54:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][50/1251]	eta 0:15:48 lr 0.000824	time 0.7435 (0.7896)	loss 4.0256 (3.4881)	grad_norm 1.2577 (1.3361)	mem 23876MB
[2022-11-11 21:55:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][100/1251]	eta 0:14:43 lr 0.000824	time 0.7430 (0.7678)	loss 3.2735 (3.5506)	grad_norm 1.4739 (1.3430)	mem 23876MB
[2022-11-11 21:55:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][150/1251]	eta 0:13:58 lr 0.000824	time 0.8238 (0.7614)	loss 3.7124 (3.5256)	grad_norm 1.2073 (1.3401)	mem 23876MB
[2022-11-11 21:56:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][200/1251]	eta 0:13:15 lr 0.000824	time 0.7432 (0.7571)	loss 3.2038 (3.5057)	grad_norm 1.3637 (1.3497)	mem 23876MB
[2022-11-11 21:56:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][250/1251]	eta 0:12:36 lr 0.000824	time 0.7389 (0.7555)	loss 2.9336 (3.5084)	grad_norm 1.1923 (1.3494)	mem 23876MB
[2022-11-11 21:57:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][300/1251]	eta 0:11:57 lr 0.000824	time 0.8448 (0.7545)	loss 4.0079 (3.4972)	grad_norm 1.2419 (1.3542)	mem 23876MB
[2022-11-11 21:58:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][350/1251]	eta 0:11:19 lr 0.000823	time 0.7425 (0.7537)	loss 3.2456 (3.4822)	grad_norm 1.4288 (1.3503)	mem 23876MB
[2022-11-11 21:58:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][400/1251]	eta 0:10:40 lr 0.000823	time 0.7411 (0.7529)	loss 3.0725 (3.4768)	grad_norm 1.4346 (1.3490)	mem 23876MB
[2022-11-11 21:59:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][450/1251]	eta 0:10:02 lr 0.000823	time 0.7340 (0.7521)	loss 4.0180 (3.4726)	grad_norm 1.4105 (1.3494)	mem 23876MB
[2022-11-11 22:00:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][500/1251]	eta 0:09:24 lr 0.000823	time 0.7368 (0.7516)	loss 3.2263 (3.4660)	grad_norm 1.3938 (1.3503)	mem 23876MB
[2022-11-11 22:00:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][550/1251]	eta 0:08:46 lr 0.000823	time 0.7414 (0.7514)	loss 3.4488 (3.4651)	grad_norm 1.5148 (1.3497)	mem 23876MB
[2022-11-11 22:01:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][600/1251]	eta 0:08:09 lr 0.000823	time 0.7415 (0.7513)	loss 2.5515 (3.4634)	grad_norm 1.3227 (1.3523)	mem 23876MB
[2022-11-11 22:01:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][650/1251]	eta 0:07:31 lr 0.000822	time 0.7405 (0.7511)	loss 4.1104 (3.4695)	grad_norm 1.3546 (1.3541)	mem 23876MB
[2022-11-11 22:02:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][700/1251]	eta 0:06:53 lr 0.000822	time 0.7399 (0.7507)	loss 2.7946 (3.4646)	grad_norm 1.3938 (1.3549)	mem 23876MB
[2022-11-11 22:03:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][750/1251]	eta 0:06:15 lr 0.000822	time 0.7381 (0.7505)	loss 3.6722 (3.4663)	grad_norm 1.3660 (1.3557)	mem 23876MB
[2022-11-11 22:03:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][800/1251]	eta 0:05:38 lr 0.000822	time 0.7433 (0.7503)	loss 3.0566 (3.4617)	grad_norm 1.2709 (1.3553)	mem 23876MB
[2022-11-11 22:04:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][850/1251]	eta 0:05:00 lr 0.000822	time 0.7280 (0.7503)	loss 3.5757 (3.4567)	grad_norm 1.2509 (1.3541)	mem 23876MB
[2022-11-11 22:05:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][900/1251]	eta 0:04:23 lr 0.000822	time 0.7531 (0.7501)	loss 3.2613 (3.4603)	grad_norm 1.3885 (1.3554)	mem 23876MB
[2022-11-11 22:05:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][950/1251]	eta 0:03:45 lr 0.000821	time 0.7409 (0.7499)	loss 3.7272 (3.4622)	grad_norm 1.3890 (1.3580)	mem 23876MB
[2022-11-11 22:06:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][1000/1251]	eta 0:03:08 lr 0.000821	time 0.7376 (0.7500)	loss 3.8113 (3.4655)	grad_norm 1.3076 (1.3582)	mem 23876MB
[2022-11-11 22:06:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][1050/1251]	eta 0:02:30 lr 0.000821	time 0.7362 (0.7498)	loss 3.8279 (3.4597)	grad_norm 1.4222 (1.3581)	mem 23876MB
[2022-11-11 22:07:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][1100/1251]	eta 0:01:53 lr 0.000821	time 0.7371 (0.7498)	loss 3.2956 (3.4608)	grad_norm 1.3929 (1.3580)	mem 23876MB
[2022-11-11 22:08:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][1150/1251]	eta 0:01:15 lr 0.000821	time 0.7466 (0.7496)	loss 2.7229 (3.4611)	grad_norm 1.3132 (1.3580)	mem 23876MB
[2022-11-11 22:08:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][1200/1251]	eta 0:00:38 lr 0.000821	time 0.7424 (0.7497)	loss 3.0544 (3.4631)	grad_norm 1.2677 (1.3589)	mem 23876MB
[2022-11-11 22:09:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [83/300][1250/1251]	eta 0:00:00 lr 0.000821	time 0.7277 (0.7493)	loss 3.6371 (3.4626)	grad_norm 1.2068 (1.3601)	mem 23876MB
[2022-11-11 22:09:26 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 83 training takes 0:15:37
[2022-11-11 22:09:26 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_83.pth saving......
[2022-11-11 22:09:28 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_83.pth saved !!!
[2022-11-11 22:09:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.781 (1.781)	Loss 1.0447 (1.0447)	Acc@1 76.660 (76.660)	Acc@5 93.750 (93.750)	Mem 23876MB
[2022-11-11 22:09:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.326 Acc@5 93.536
[2022-11-11 22:09:40 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.3%
[2022-11-11 22:09:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.940 (1.940)	Loss 0.8036 (0.8036)	Acc@1 81.348 (81.348)	Acc@5 94.629 (94.629)	Mem 23876MB
[2022-11-11 22:09:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.072 Acc@5 94.900
[2022-11-11 22:09:53 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.1%
[2022-11-11 22:09:53 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.07% at 83 epoch
[2022-11-11 22:09:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][0/1251]	eta 0:48:43 lr 0.000821	time 2.3370 (2.3370)	loss 3.3398 (3.3398)	grad_norm 1.2569 (1.2569)	mem 23876MB
[2022-11-11 22:10:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][50/1251]	eta 0:15:36 lr 0.000820	time 0.7373 (0.7794)	loss 3.2722 (3.4264)	grad_norm 1.2628 (1.3358)	mem 23876MB
[2022-11-11 22:11:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][100/1251]	eta 0:14:38 lr 0.000820	time 0.7371 (0.7629)	loss 2.4265 (3.4330)	grad_norm 1.2570 (1.3429)	mem 23876MB
[2022-11-11 22:11:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][150/1251]	eta 0:13:55 lr 0.000820	time 0.8030 (0.7589)	loss 2.8733 (3.4581)	grad_norm 1.4603 (1.3587)	mem 23876MB
[2022-11-11 22:12:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][200/1251]	eta 0:13:14 lr 0.000820	time 0.7354 (0.7563)	loss 3.7746 (3.4578)	grad_norm 1.3659 (1.3582)	mem 23876MB
[2022-11-11 22:13:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][250/1251]	eta 0:12:35 lr 0.000820	time 0.7438 (0.7546)	loss 3.0619 (3.4297)	grad_norm 1.3787 (nan)	mem 23876MB
[2022-11-11 22:13:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][300/1251]	eta 0:11:56 lr 0.000820	time 0.7452 (0.7531)	loss 2.6796 (3.4377)	grad_norm 1.3040 (nan)	mem 23876MB
[2022-11-11 22:14:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][350/1251]	eta 0:11:17 lr 0.000819	time 0.7449 (0.7519)	loss 2.4524 (3.4474)	grad_norm 1.3272 (nan)	mem 23876MB
[2022-11-11 22:14:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][400/1251]	eta 0:10:39 lr 0.000819	time 0.7366 (0.7514)	loss 2.6858 (3.4544)	grad_norm 1.4796 (nan)	mem 23876MB
[2022-11-11 22:15:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][450/1251]	eta 0:10:01 lr 0.000819	time 0.7394 (0.7508)	loss 3.5669 (3.4579)	grad_norm 1.3568 (nan)	mem 23876MB
[2022-11-11 22:16:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][500/1251]	eta 0:09:23 lr 0.000819	time 0.7470 (0.7508)	loss 3.6865 (3.4628)	grad_norm 1.1628 (nan)	mem 23876MB
[2022-11-11 22:16:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][550/1251]	eta 0:08:46 lr 0.000819	time 0.8207 (0.7504)	loss 3.5708 (3.4612)	grad_norm 1.5331 (nan)	mem 23876MB
[2022-11-11 22:17:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][600/1251]	eta 0:08:08 lr 0.000819	time 0.8234 (0.7503)	loss 3.6912 (3.4546)	grad_norm 1.2567 (nan)	mem 23876MB
[2022-11-11 22:18:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][650/1251]	eta 0:07:30 lr 0.000818	time 0.7361 (0.7500)	loss 3.9718 (3.4564)	grad_norm 1.2054 (nan)	mem 23876MB
[2022-11-11 22:18:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][700/1251]	eta 0:06:53 lr 0.000818	time 0.7411 (0.7498)	loss 2.9684 (3.4647)	grad_norm 1.3030 (nan)	mem 23876MB
[2022-11-11 22:19:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][750/1251]	eta 0:06:15 lr 0.000818	time 0.7343 (0.7496)	loss 3.5866 (3.4669)	grad_norm 1.2881 (nan)	mem 23876MB
[2022-11-11 22:19:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][800/1251]	eta 0:05:37 lr 0.000818	time 0.7444 (0.7494)	loss 3.7168 (3.4667)	grad_norm 1.2989 (nan)	mem 23876MB
[2022-11-11 22:20:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][850/1251]	eta 0:05:00 lr 0.000818	time 0.7376 (0.7494)	loss 3.8457 (3.4752)	grad_norm 1.2964 (nan)	mem 23876MB
[2022-11-11 22:21:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][900/1251]	eta 0:04:22 lr 0.000818	time 0.7385 (0.7492)	loss 3.8751 (3.4758)	grad_norm 1.5477 (nan)	mem 23876MB
[2022-11-11 22:21:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][950/1251]	eta 0:03:45 lr 0.000817	time 0.8123 (0.7493)	loss 3.7097 (3.4762)	grad_norm 1.3284 (nan)	mem 23876MB
[2022-11-11 22:22:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][1000/1251]	eta 0:03:08 lr 0.000817	time 0.7417 (0.7493)	loss 2.7796 (3.4769)	grad_norm 1.4895 (nan)	mem 23876MB
[2022-11-11 22:23:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][1050/1251]	eta 0:02:30 lr 0.000817	time 0.7399 (0.7493)	loss 3.5963 (3.4765)	grad_norm 1.5335 (nan)	mem 23876MB
[2022-11-11 22:23:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][1100/1251]	eta 0:01:53 lr 0.000817	time 0.7414 (0.7492)	loss 2.6043 (3.4736)	grad_norm 1.1823 (nan)	mem 23876MB
[2022-11-11 22:24:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][1150/1251]	eta 0:01:15 lr 0.000817	time 0.7401 (0.7491)	loss 3.4088 (3.4738)	grad_norm 1.4013 (nan)	mem 23876MB
[2022-11-11 22:24:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][1200/1251]	eta 0:00:38 lr 0.000817	time 0.7404 (0.7491)	loss 4.0663 (3.4758)	grad_norm 1.2354 (nan)	mem 23876MB
[2022-11-11 22:25:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [84/300][1250/1251]	eta 0:00:00 lr 0.000817	time 0.7273 (0.7491)	loss 3.6578 (3.4740)	grad_norm 1.2470 (nan)	mem 23876MB
[2022-11-11 22:25:30 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 84 training takes 0:15:37
[2022-11-11 22:25:30 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_84.pth saving......
[2022-11-11 22:25:31 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_84.pth saved !!!
[2022-11-11 22:25:33 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.731 (1.731)	Loss 1.0934 (1.0934)	Acc@1 73.828 (73.828)	Acc@5 93.262 (93.262)	Mem 23876MB
[2022-11-11 22:25:44 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 75.992 Acc@5 93.524
[2022-11-11 22:25:44 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.0%
[2022-11-11 22:25:46 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.838 (1.838)	Loss 0.9123 (0.9123)	Acc@1 79.102 (79.102)	Acc@5 93.750 (93.750)	Mem 23876MB
[2022-11-11 22:25:56 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.102 Acc@5 94.926
[2022-11-11 22:25:56 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.1%
[2022-11-11 22:25:56 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.10% at 84 epoch
[2022-11-11 22:25:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][0/1251]	eta 0:49:54 lr 0.000817	time 2.3936 (2.3936)	loss 2.2242 (2.2242)	grad_norm 1.4169 (1.4169)	mem 23876MB
[2022-11-11 22:26:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][50/1251]	eta 0:15:39 lr 0.000816	time 0.7456 (0.7820)	loss 4.2691 (3.4917)	grad_norm 1.4646 (1.3472)	mem 23876MB
[2022-11-11 22:27:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][100/1251]	eta 0:14:40 lr 0.000816	time 0.7492 (0.7653)	loss 2.0202 (3.4526)	grad_norm 1.3903 (1.3624)	mem 23876MB
[2022-11-11 22:27:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][150/1251]	eta 0:13:56 lr 0.000816	time 0.8144 (0.7595)	loss 3.4510 (3.4363)	grad_norm 1.3413 (1.3648)	mem 23876MB
[2022-11-11 22:28:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][200/1251]	eta 0:13:15 lr 0.000816	time 0.7390 (0.7565)	loss 4.0277 (3.4452)	grad_norm 1.3831 (1.3685)	mem 23876MB
[2022-11-11 22:29:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][250/1251]	eta 0:12:35 lr 0.000816	time 0.7397 (0.7551)	loss 3.8283 (3.4346)	grad_norm 1.3166 (1.3710)	mem 23876MB
[2022-11-11 22:29:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][300/1251]	eta 0:11:56 lr 0.000816	time 0.7366 (0.7537)	loss 3.7347 (3.4529)	grad_norm 1.3781 (1.3696)	mem 23876MB
[2022-11-11 22:30:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][350/1251]	eta 0:11:18 lr 0.000815	time 0.7451 (0.7530)	loss 3.7227 (3.4534)	grad_norm 1.2504 (1.3652)	mem 23876MB
[2022-11-11 22:30:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][400/1251]	eta 0:10:40 lr 0.000815	time 0.7406 (0.7525)	loss 3.5961 (3.4511)	grad_norm 1.4769 (1.3602)	mem 23876MB
[2022-11-11 22:31:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][450/1251]	eta 0:10:02 lr 0.000815	time 0.7377 (0.7517)	loss 3.6390 (3.4506)	grad_norm 1.3836 (1.3629)	mem 23876MB
[2022-11-11 22:32:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][500/1251]	eta 0:09:24 lr 0.000815	time 0.7402 (0.7514)	loss 3.4358 (3.4461)	grad_norm 1.2631 (1.3650)	mem 23876MB
[2022-11-11 22:32:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][550/1251]	eta 0:08:46 lr 0.000815	time 0.8400 (0.7512)	loss 3.5722 (3.4448)	grad_norm 1.5198 (1.3655)	mem 23876MB
[2022-11-11 22:33:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][600/1251]	eta 0:08:08 lr 0.000815	time 0.7417 (0.7508)	loss 2.9074 (3.4511)	grad_norm 1.2703 (1.3688)	mem 23876MB
[2022-11-11 22:34:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][650/1251]	eta 0:07:31 lr 0.000814	time 0.7277 (0.7507)	loss 3.1913 (3.4546)	grad_norm 1.3169 (1.3698)	mem 23876MB
[2022-11-11 22:34:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][700/1251]	eta 0:06:53 lr 0.000814	time 0.7444 (0.7502)	loss 3.9838 (3.4555)	grad_norm 1.4616 (1.3693)	mem 23876MB
[2022-11-11 22:35:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][750/1251]	eta 0:06:15 lr 0.000814	time 0.7437 (0.7502)	loss 3.6732 (3.4540)	grad_norm 1.2058 (1.3686)	mem 23876MB
[2022-11-11 22:35:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][800/1251]	eta 0:05:38 lr 0.000814	time 0.7347 (0.7502)	loss 3.6594 (3.4487)	grad_norm 1.3422 (1.3690)	mem 23876MB
[2022-11-11 22:36:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][850/1251]	eta 0:05:00 lr 0.000814	time 0.7333 (0.7499)	loss 3.9234 (3.4445)	grad_norm 1.3509 (1.3711)	mem 23876MB
[2022-11-11 22:37:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][900/1251]	eta 0:04:23 lr 0.000814	time 0.7378 (0.7498)	loss 4.0213 (3.4476)	grad_norm 1.4306 (1.3734)	mem 23876MB
[2022-11-11 22:37:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][950/1251]	eta 0:03:45 lr 0.000813	time 0.7482 (0.7496)	loss 3.6536 (3.4515)	grad_norm 1.2982 (1.3724)	mem 23876MB
[2022-11-11 22:38:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][1000/1251]	eta 0:03:08 lr 0.000813	time 0.7348 (0.7496)	loss 3.3355 (3.4522)	grad_norm 1.3772 (1.3713)	mem 23876MB
[2022-11-11 22:39:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][1050/1251]	eta 0:02:30 lr 0.000813	time 0.7373 (0.7495)	loss 4.3368 (3.4528)	grad_norm 1.2628 (1.3698)	mem 23876MB
[2022-11-11 22:39:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][1100/1251]	eta 0:01:53 lr 0.000813	time 0.7434 (0.7495)	loss 3.7484 (3.4526)	grad_norm 1.2465 (1.3686)	mem 23876MB
[2022-11-11 22:40:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][1150/1251]	eta 0:01:15 lr 0.000813	time 0.7415 (0.7493)	loss 4.3765 (3.4581)	grad_norm 1.4934 (1.3684)	mem 23876MB
[2022-11-11 22:40:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][1200/1251]	eta 0:00:38 lr 0.000813	time 0.8201 (0.7494)	loss 4.2674 (3.4563)	grad_norm 1.3173 (nan)	mem 23876MB
[2022-11-11 22:41:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [85/300][1250/1251]	eta 0:00:00 lr 0.000812	time 0.7258 (0.7489)	loss 3.7837 (3.4593)	grad_norm 1.2121 (nan)	mem 23876MB
[2022-11-11 22:41:34 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 85 training takes 0:15:37
[2022-11-11 22:41:34 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_85.pth saving......
[2022-11-11 22:41:35 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_85.pth saved !!!
[2022-11-11 22:41:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.578 (1.578)	Loss 1.0345 (1.0345)	Acc@1 76.562 (76.562)	Acc@5 93.750 (93.750)	Mem 23876MB
[2022-11-11 22:41:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.398 Acc@5 93.544
[2022-11-11 22:41:47 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.4%
[2022-11-11 22:41:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.876 (1.876)	Loss 0.8581 (0.8581)	Acc@1 78.027 (78.027)	Acc@5 95.605 (95.605)	Mem 23876MB
[2022-11-11 22:42:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.190 Acc@5 94.944
[2022-11-11 22:42:00 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.2%
[2022-11-11 22:42:00 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.19% at 85 epoch
[2022-11-11 22:42:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][0/1251]	eta 0:50:54 lr 0.000812	time 2.4419 (2.4419)	loss 3.7776 (3.7776)	grad_norm 1.3854 (1.3854)	mem 23876MB
[2022-11-11 22:42:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][50/1251]	eta 0:15:39 lr 0.000812	time 0.7433 (0.7821)	loss 2.5380 (3.5740)	grad_norm 1.3702 (1.3637)	mem 23876MB
[2022-11-11 22:43:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][100/1251]	eta 0:14:40 lr 0.000812	time 0.7342 (0.7646)	loss 4.0059 (3.5215)	grad_norm 1.5199 (1.3763)	mem 23876MB
[2022-11-11 22:43:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][150/1251]	eta 0:13:55 lr 0.000812	time 0.7392 (0.7589)	loss 2.8411 (3.4947)	grad_norm 1.2329 (1.3684)	mem 23876MB
[2022-11-11 22:44:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][200/1251]	eta 0:13:13 lr 0.000812	time 0.7359 (0.7554)	loss 4.0224 (3.4953)	grad_norm 1.4047 (1.3603)	mem 23876MB
[2022-11-11 22:45:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][250/1251]	eta 0:12:35 lr 0.000812	time 0.7347 (0.7543)	loss 3.7078 (3.5164)	grad_norm 1.5757 (1.3626)	mem 23876MB
[2022-11-11 22:45:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][300/1251]	eta 0:11:55 lr 0.000811	time 0.7414 (0.7528)	loss 3.5055 (3.5331)	grad_norm 1.1559 (1.3636)	mem 23876MB
[2022-11-11 22:46:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][350/1251]	eta 0:11:17 lr 0.000811	time 0.7430 (0.7525)	loss 3.3456 (3.5184)	grad_norm 1.2801 (1.3637)	mem 23876MB
[2022-11-11 22:47:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][400/1251]	eta 0:10:39 lr 0.000811	time 0.7427 (0.7515)	loss 3.8210 (3.5078)	grad_norm 1.2834 (1.3580)	mem 23876MB
[2022-11-11 22:47:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][450/1251]	eta 0:10:01 lr 0.000811	time 0.7362 (0.7513)	loss 4.0590 (3.5027)	grad_norm 1.3127 (1.3596)	mem 23876MB
[2022-11-11 22:48:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][500/1251]	eta 0:09:23 lr 0.000811	time 0.7462 (0.7509)	loss 3.6772 (3.4911)	grad_norm 1.2662 (1.3596)	mem 23876MB
[2022-11-11 22:48:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][550/1251]	eta 0:08:46 lr 0.000811	time 0.7353 (0.7505)	loss 2.7027 (3.4876)	grad_norm 1.4600 (1.3579)	mem 23876MB
[2022-11-11 22:49:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][600/1251]	eta 0:08:08 lr 0.000811	time 0.8231 (0.7502)	loss 3.7669 (3.4824)	grad_norm 1.4157 (1.3581)	mem 23876MB
[2022-11-11 22:50:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][650/1251]	eta 0:07:30 lr 0.000810	time 0.7356 (0.7502)	loss 4.1101 (3.4907)	grad_norm 1.3139 (1.3596)	mem 23876MB
[2022-11-11 22:50:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][700/1251]	eta 0:06:53 lr 0.000810	time 0.7394 (0.7497)	loss 2.5817 (3.4862)	grad_norm 1.6050 (1.3608)	mem 23876MB
[2022-11-11 22:51:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][750/1251]	eta 0:06:15 lr 0.000810	time 0.7366 (0.7496)	loss 2.5400 (3.4835)	grad_norm 1.4201 (1.3621)	mem 23876MB
[2022-11-11 22:52:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][800/1251]	eta 0:05:37 lr 0.000810	time 0.7435 (0.7493)	loss 4.0158 (3.4831)	grad_norm 1.4478 (1.3645)	mem 23876MB
[2022-11-11 22:52:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][850/1251]	eta 0:05:00 lr 0.000810	time 0.7365 (0.7494)	loss 3.6784 (3.4859)	grad_norm 1.3159 (1.3655)	mem 23876MB
[2022-11-11 22:53:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][900/1251]	eta 0:04:22 lr 0.000810	time 0.7429 (0.7490)	loss 4.5126 (3.4817)	grad_norm 1.3996 (1.3650)	mem 23876MB
[2022-11-11 22:53:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][950/1251]	eta 0:03:45 lr 0.000809	time 0.7324 (0.7490)	loss 3.9962 (3.4814)	grad_norm 1.2612 (1.3644)	mem 23876MB
[2022-11-11 22:54:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][1000/1251]	eta 0:03:07 lr 0.000809	time 0.8453 (0.7488)	loss 3.7835 (3.4735)	grad_norm 1.4587 (1.3662)	mem 23876MB
[2022-11-11 22:55:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][1050/1251]	eta 0:02:30 lr 0.000809	time 0.7436 (0.7488)	loss 2.7786 (3.4699)	grad_norm 1.4580 (1.3664)	mem 23876MB
[2022-11-11 22:55:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][1100/1251]	eta 0:01:53 lr 0.000809	time 0.7429 (0.7486)	loss 3.1932 (3.4724)	grad_norm 1.2279 (1.3669)	mem 23876MB
[2022-11-11 22:56:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][1150/1251]	eta 0:01:15 lr 0.000809	time 0.7394 (0.7486)	loss 3.7330 (3.4762)	grad_norm 1.4111 (1.3658)	mem 23876MB
[2022-11-11 22:56:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][1200/1251]	eta 0:00:38 lr 0.000809	time 0.7437 (0.7483)	loss 3.8402 (3.4697)	grad_norm 1.2994 (1.3655)	mem 23876MB
[2022-11-11 22:57:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [86/300][1250/1251]	eta 0:00:00 lr 0.000808	time 0.7264 (0.7482)	loss 3.5946 (3.4707)	grad_norm 1.4414 (1.3638)	mem 23876MB
[2022-11-11 22:57:36 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 86 training takes 0:15:36
[2022-11-11 22:57:36 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_86.pth saving......
[2022-11-11 22:57:37 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_86.pth saved !!!
[2022-11-11 22:57:39 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.767 (1.767)	Loss 0.9591 (0.9591)	Acc@1 76.074 (76.074)	Acc@5 94.043 (94.043)	Mem 23876MB
[2022-11-11 22:57:50 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.400 Acc@5 93.668
[2022-11-11 22:57:50 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.4%
[2022-11-11 22:57:51 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.774 (1.774)	Loss 0.8316 (0.8316)	Acc@1 78.418 (78.418)	Acc@5 94.824 (94.824)	Mem 23876MB
[2022-11-11 22:58:02 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.198 Acc@5 94.992
[2022-11-11 22:58:02 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.2%
[2022-11-11 22:58:02 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.20% at 86 epoch
[2022-11-11 22:58:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][0/1251]	eta 0:49:28 lr 0.000808	time 2.3726 (2.3726)	loss 3.5424 (3.5424)	grad_norm 1.3417 (1.3417)	mem 23876MB
[2022-11-11 22:58:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][50/1251]	eta 0:15:42 lr 0.000808	time 0.7510 (0.7851)	loss 3.3099 (3.4248)	grad_norm 1.2452 (1.3510)	mem 23876MB
[2022-11-11 22:59:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][100/1251]	eta 0:14:41 lr 0.000808	time 0.7373 (0.7657)	loss 3.9463 (3.4613)	grad_norm 1.4566 (1.3726)	mem 23876MB
[2022-11-11 22:59:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][150/1251]	eta 0:13:56 lr 0.000808	time 0.7284 (0.7602)	loss 3.6304 (3.3907)	grad_norm 1.3656 (1.3752)	mem 23876MB
[2022-11-11 23:00:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][200/1251]	eta 0:13:16 lr 0.000808	time 0.8353 (0.7574)	loss 2.9180 (3.3764)	grad_norm 1.1775 (1.3796)	mem 23876MB
[2022-11-11 23:01:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][250/1251]	eta 0:12:36 lr 0.000808	time 0.8129 (0.7557)	loss 3.1451 (3.3918)	grad_norm 1.3836 (1.3817)	mem 23876MB
[2022-11-11 23:01:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][300/1251]	eta 0:11:57 lr 0.000807	time 0.7442 (0.7546)	loss 3.2978 (3.3989)	grad_norm 1.4107 (1.3814)	mem 23876MB
[2022-11-11 23:02:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][350/1251]	eta 0:11:18 lr 0.000807	time 0.7359 (0.7536)	loss 2.8280 (3.3956)	grad_norm 1.5818 (1.3782)	mem 23876MB
[2022-11-11 23:03:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][400/1251]	eta 0:10:40 lr 0.000807	time 0.7446 (0.7527)	loss 2.2760 (3.4051)	grad_norm 1.3174 (1.3791)	mem 23876MB
[2022-11-11 23:03:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][450/1251]	eta 0:10:02 lr 0.000807	time 0.7339 (0.7526)	loss 3.5293 (3.4022)	grad_norm 1.2878 (1.3789)	mem 23876MB
[2022-11-11 23:04:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][500/1251]	eta 0:09:24 lr 0.000807	time 0.7380 (0.7516)	loss 3.7675 (3.4118)	grad_norm 1.2796 (1.3778)	mem 23876MB
[2022-11-11 23:04:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][550/1251]	eta 0:08:46 lr 0.000807	time 0.7357 (0.7512)	loss 3.0589 (3.4119)	grad_norm 1.3098 (1.3809)	mem 23876MB
[2022-11-11 23:05:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][600/1251]	eta 0:08:08 lr 0.000806	time 0.8202 (0.7511)	loss 3.1158 (3.4222)	grad_norm 1.2914 (1.3814)	mem 23876MB
[2022-11-11 23:06:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][650/1251]	eta 0:07:31 lr 0.000806	time 0.7387 (0.7508)	loss 3.8826 (3.4268)	grad_norm 1.4972 (1.3817)	mem 23876MB
[2022-11-11 23:06:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][700/1251]	eta 0:06:53 lr 0.000806	time 0.7422 (0.7508)	loss 2.6314 (3.4234)	grad_norm 1.2626 (1.3804)	mem 23876MB
[2022-11-11 23:07:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][750/1251]	eta 0:06:16 lr 0.000806	time 0.7422 (0.7506)	loss 2.7490 (3.4261)	grad_norm 1.3502 (1.3787)	mem 23876MB
[2022-11-11 23:08:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][800/1251]	eta 0:05:38 lr 0.000806	time 0.7411 (0.7504)	loss 3.6365 (3.4275)	grad_norm 1.2608 (inf)	mem 23876MB
[2022-11-11 23:08:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][850/1251]	eta 0:05:00 lr 0.000806	time 0.7415 (0.7503)	loss 2.6004 (3.4210)	grad_norm 1.2816 (inf)	mem 23876MB
[2022-11-11 23:09:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][900/1251]	eta 0:04:23 lr 0.000805	time 0.7397 (0.7499)	loss 3.4544 (3.4217)	grad_norm 1.3952 (inf)	mem 23876MB
[2022-11-11 23:09:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][950/1251]	eta 0:03:45 lr 0.000805	time 0.7356 (0.7498)	loss 3.5276 (3.4243)	grad_norm 1.3556 (inf)	mem 23876MB
[2022-11-11 23:10:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][1000/1251]	eta 0:03:08 lr 0.000805	time 0.8228 (0.7497)	loss 2.2193 (3.4240)	grad_norm 1.2908 (inf)	mem 23876MB
[2022-11-11 23:11:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][1050/1251]	eta 0:02:30 lr 0.000805	time 0.7238 (0.7496)	loss 3.3177 (3.4308)	grad_norm 1.2697 (inf)	mem 23876MB
[2022-11-11 23:11:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][1100/1251]	eta 0:01:53 lr 0.000805	time 0.7424 (0.7495)	loss 3.8985 (3.4334)	grad_norm 1.5216 (inf)	mem 23876MB
[2022-11-11 23:12:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][1150/1251]	eta 0:01:15 lr 0.000805	time 0.7378 (0.7493)	loss 3.4664 (3.4362)	grad_norm 1.2098 (inf)	mem 23876MB
[2022-11-11 23:13:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][1200/1251]	eta 0:00:38 lr 0.000804	time 0.7406 (0.7492)	loss 3.6037 (3.4367)	grad_norm 1.3513 (inf)	mem 23876MB
[2022-11-11 23:13:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [87/300][1250/1251]	eta 0:00:00 lr 0.000804	time 0.7312 (0.7491)	loss 2.7712 (3.4356)	grad_norm 1.4335 (inf)	mem 23876MB
[2022-11-11 23:13:39 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 87 training takes 0:15:37
[2022-11-11 23:13:40 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_87.pth saving......
[2022-11-11 23:13:41 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_87.pth saved !!!
[2022-11-11 23:13:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.622 (1.622)	Loss 0.9902 (0.9902)	Acc@1 75.488 (75.488)	Acc@5 94.141 (94.141)	Mem 23876MB
[2022-11-11 23:13:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.438 Acc@5 93.792
[2022-11-11 23:13:53 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.4%
[2022-11-11 23:13:55 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.829 (1.829)	Loss 0.8662 (0.8662)	Acc@1 78.223 (78.223)	Acc@5 94.434 (94.434)	Mem 23876MB
[2022-11-11 23:14:06 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.184 Acc@5 94.996
[2022-11-11 23:14:06 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.2%
[2022-11-11 23:14:06 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.20% at 86 epoch
[2022-11-11 23:14:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][0/1251]	eta 0:49:34 lr 0.000804	time 2.3776 (2.3776)	loss 3.7125 (3.7125)	grad_norm 1.7476 (1.7476)	mem 23876MB
[2022-11-11 23:14:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][50/1251]	eta 0:15:43 lr 0.000804	time 0.7490 (0.7854)	loss 2.4027 (3.4015)	grad_norm 1.3406 (1.3591)	mem 23876MB
[2022-11-11 23:15:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][100/1251]	eta 0:14:44 lr 0.000804	time 0.7474 (0.7685)	loss 3.6477 (3.3908)	grad_norm 1.5090 (1.3720)	mem 23876MB
[2022-11-11 23:16:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][150/1251]	eta 0:13:58 lr 0.000804	time 0.7444 (0.7612)	loss 4.0794 (3.4196)	grad_norm 1.4023 (1.3771)	mem 23876MB
[2022-11-11 23:16:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][200/1251]	eta 0:13:17 lr 0.000804	time 0.7397 (0.7584)	loss 4.1331 (3.4172)	grad_norm 1.4013 (1.3798)	mem 23876MB
[2022-11-11 23:17:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][250/1251]	eta 0:12:37 lr 0.000803	time 0.7380 (0.7565)	loss 3.7143 (3.4230)	grad_norm 1.3867 (1.3823)	mem 23876MB
[2022-11-11 23:17:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][300/1251]	eta 0:11:58 lr 0.000803	time 0.7348 (0.7552)	loss 3.5947 (3.4244)	grad_norm 1.3084 (1.3819)	mem 23876MB
[2022-11-11 23:18:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][350/1251]	eta 0:11:19 lr 0.000803	time 0.7423 (0.7543)	loss 2.5879 (3.4077)	grad_norm 1.4201 (1.3809)	mem 23876MB
[2022-11-11 23:19:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][400/1251]	eta 0:10:41 lr 0.000803	time 0.8078 (0.7533)	loss 3.3246 (3.4118)	grad_norm 1.4238 (1.3816)	mem 23876MB
[2022-11-11 23:19:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][450/1251]	eta 0:10:02 lr 0.000803	time 0.7437 (0.7527)	loss 3.7667 (3.4299)	grad_norm 1.4732 (1.3802)	mem 23876MB
[2022-11-11 23:20:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][500/1251]	eta 0:09:24 lr 0.000803	time 0.7397 (0.7522)	loss 2.6064 (3.4274)	grad_norm 1.3011 (1.3795)	mem 23876MB
[2022-11-11 23:21:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][550/1251]	eta 0:08:47 lr 0.000802	time 0.7403 (0.7519)	loss 3.1465 (3.4253)	grad_norm 1.2464 (1.3789)	mem 23876MB
[2022-11-11 23:21:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][600/1251]	eta 0:08:09 lr 0.000802	time 0.7402 (0.7516)	loss 3.9417 (3.4311)	grad_norm 1.3493 (1.3794)	mem 23876MB
[2022-11-11 23:22:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][650/1251]	eta 0:07:31 lr 0.000802	time 0.7394 (0.7513)	loss 3.5728 (3.4242)	grad_norm 1.4397 (1.3792)	mem 23876MB
[2022-11-11 23:22:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][700/1251]	eta 0:06:53 lr 0.000802	time 0.7381 (0.7507)	loss 3.9479 (3.4293)	grad_norm 1.4926 (1.3794)	mem 23876MB
[2022-11-11 23:23:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][750/1251]	eta 0:06:15 lr 0.000802	time 0.7373 (0.7504)	loss 2.3180 (3.4202)	grad_norm 1.5599 (1.3818)	mem 23876MB
[2022-11-11 23:24:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][800/1251]	eta 0:05:38 lr 0.000802	time 0.7417 (0.7503)	loss 2.3353 (3.4233)	grad_norm 1.5068 (1.3826)	mem 23876MB
[2022-11-11 23:24:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][850/1251]	eta 0:05:00 lr 0.000801	time 0.7413 (0.7500)	loss 3.6826 (3.4282)	grad_norm 1.3016 (1.3822)	mem 23876MB
[2022-11-11 23:25:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][900/1251]	eta 0:04:23 lr 0.000801	time 0.7382 (0.7501)	loss 2.8092 (3.4340)	grad_norm 1.3971 (1.3820)	mem 23876MB
[2022-11-11 23:25:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][950/1251]	eta 0:03:45 lr 0.000801	time 0.8154 (0.7499)	loss 3.7023 (3.4387)	grad_norm 1.6049 (1.3819)	mem 23876MB
[2022-11-11 23:26:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][1000/1251]	eta 0:03:08 lr 0.000801	time 0.8449 (0.7498)	loss 3.7887 (3.4475)	grad_norm 1.5594 (1.3799)	mem 23876MB
[2022-11-11 23:27:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][1050/1251]	eta 0:02:30 lr 0.000801	time 0.7378 (0.7497)	loss 3.3617 (3.4506)	grad_norm 1.2278 (1.3793)	mem 23876MB
[2022-11-11 23:27:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][1100/1251]	eta 0:01:53 lr 0.000801	time 0.7365 (0.7495)	loss 3.0649 (3.4565)	grad_norm 1.3011 (1.3786)	mem 23876MB
[2022-11-11 23:28:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][1150/1251]	eta 0:01:15 lr 0.000800	time 0.7382 (0.7495)	loss 3.8880 (3.4553)	grad_norm 1.4627 (1.3782)	mem 23876MB
[2022-11-11 23:29:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][1200/1251]	eta 0:00:38 lr 0.000800	time 0.7373 (0.7495)	loss 3.9007 (3.4530)	grad_norm 1.4266 (1.3782)	mem 23876MB
[2022-11-11 23:29:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [88/300][1250/1251]	eta 0:00:00 lr 0.000800	time 0.7267 (0.7492)	loss 3.9678 (3.4499)	grad_norm 1.3477 (1.3781)	mem 23876MB
[2022-11-11 23:29:43 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 88 training takes 0:15:37
[2022-11-11 23:29:43 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_88.pth saving......
[2022-11-11 23:29:44 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_88.pth saved !!!
[2022-11-11 23:29:46 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.585 (1.585)	Loss 0.9405 (0.9405)	Acc@1 76.270 (76.270)	Acc@5 94.531 (94.531)	Mem 23876MB
[2022-11-11 23:29:57 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.718 Acc@5 93.774
[2022-11-11 23:29:57 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.7%
[2022-11-11 23:29:58 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.899 (1.899)	Loss 0.9028 (0.9028)	Acc@1 78.613 (78.613)	Acc@5 94.727 (94.727)	Mem 23876MB
[2022-11-11 23:30:09 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.220 Acc@5 95.028
[2022-11-11 23:30:09 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.2%
[2022-11-11 23:30:09 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.22% at 88 epoch
[2022-11-11 23:30:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][0/1251]	eta 0:49:22 lr 0.000800	time 2.3684 (2.3684)	loss 3.3887 (3.3887)	grad_norm 1.4958 (1.4958)	mem 23876MB
[2022-11-11 23:30:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][50/1251]	eta 0:15:34 lr 0.000800	time 0.8173 (0.7781)	loss 3.7993 (3.3733)	grad_norm 1.4379 (1.3997)	mem 23876MB
[2022-11-11 23:31:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][100/1251]	eta 0:14:37 lr 0.000800	time 0.7363 (0.7626)	loss 2.9494 (3.3329)	grad_norm 1.2282 (1.3816)	mem 23876MB
[2022-11-11 23:32:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][150/1251]	eta 0:13:53 lr 0.000800	time 0.7379 (0.7568)	loss 3.5869 (3.3672)	grad_norm 1.4041 (1.3776)	mem 23876MB
[2022-11-11 23:32:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][200/1251]	eta 0:13:12 lr 0.000799	time 0.7395 (0.7544)	loss 3.9997 (3.3910)	grad_norm 1.1785 (1.3721)	mem 23876MB
[2022-11-11 23:33:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][250/1251]	eta 0:12:33 lr 0.000799	time 0.7383 (0.7529)	loss 3.5319 (3.4206)	grad_norm 1.3279 (1.3689)	mem 23876MB
[2022-11-11 23:33:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][300/1251]	eta 0:11:54 lr 0.000799	time 0.7377 (0.7514)	loss 4.1207 (3.4257)	grad_norm 1.3650 (1.3723)	mem 23876MB
[2022-11-11 23:34:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][350/1251]	eta 0:11:16 lr 0.000799	time 0.7350 (0.7506)	loss 3.3981 (3.4335)	grad_norm 1.2780 (1.3729)	mem 23876MB
[2022-11-11 23:35:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][400/1251]	eta 0:10:38 lr 0.000799	time 0.7363 (0.7501)	loss 3.7950 (3.4255)	grad_norm 1.1414 (1.3771)	mem 23876MB
[2022-11-11 23:35:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][450/1251]	eta 0:10:00 lr 0.000799	time 0.7376 (0.7493)	loss 4.1031 (3.4235)	grad_norm 1.4145 (1.3782)	mem 23876MB
[2022-11-11 23:36:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][500/1251]	eta 0:09:22 lr 0.000798	time 0.7364 (0.7489)	loss 3.9916 (3.4239)	grad_norm 1.4067 (1.3782)	mem 23876MB
[2022-11-11 23:37:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][550/1251]	eta 0:08:44 lr 0.000798	time 0.7357 (0.7485)	loss 3.6911 (3.4213)	grad_norm 1.3597 (1.3761)	mem 23876MB
[2022-11-11 23:37:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][600/1251]	eta 0:08:06 lr 0.000798	time 0.8113 (0.7480)	loss 3.9959 (3.4236)	grad_norm 1.3330 (nan)	mem 23876MB
[2022-11-11 23:38:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][650/1251]	eta 0:07:29 lr 0.000798	time 0.7424 (0.7479)	loss 2.8635 (3.4197)	grad_norm 1.3188 (nan)	mem 23876MB
[2022-11-11 23:38:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][700/1251]	eta 0:06:51 lr 0.000798	time 0.7383 (0.7475)	loss 3.6746 (3.4308)	grad_norm 1.3784 (nan)	mem 23876MB
[2022-11-11 23:39:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][750/1251]	eta 0:06:14 lr 0.000798	time 0.7374 (0.7474)	loss 2.3999 (3.4291)	grad_norm 1.4498 (nan)	mem 23876MB
[2022-11-11 23:40:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][800/1251]	eta 0:05:36 lr 0.000797	time 0.7401 (0.7472)	loss 3.3681 (3.4350)	grad_norm 1.2938 (nan)	mem 23876MB
[2022-11-11 23:40:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][850/1251]	eta 0:04:59 lr 0.000797	time 0.7612 (0.7471)	loss 3.6197 (3.4392)	grad_norm 1.2278 (nan)	mem 23876MB
[2022-11-11 23:41:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][900/1251]	eta 0:04:22 lr 0.000797	time 0.7386 (0.7470)	loss 3.4023 (3.4402)	grad_norm 1.3124 (nan)	mem 23876MB
[2022-11-11 23:42:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][950/1251]	eta 0:03:44 lr 0.000797	time 0.7429 (0.7470)	loss 2.5023 (3.4463)	grad_norm 1.2596 (nan)	mem 23876MB
[2022-11-11 23:42:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][1000/1251]	eta 0:03:07 lr 0.000797	time 0.7429 (0.7469)	loss 2.8444 (3.4412)	grad_norm 1.2235 (nan)	mem 23876MB
[2022-11-11 23:43:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][1050/1251]	eta 0:02:30 lr 0.000797	time 0.7419 (0.7471)	loss 3.5921 (3.4420)	grad_norm 1.2287 (nan)	mem 23876MB
[2022-11-11 23:43:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][1100/1251]	eta 0:01:52 lr 0.000796	time 0.7444 (0.7469)	loss 4.1254 (3.4453)	grad_norm 1.2888 (nan)	mem 23876MB
[2022-11-11 23:44:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][1150/1251]	eta 0:01:15 lr 0.000796	time 0.7418 (0.7470)	loss 3.2670 (3.4405)	grad_norm 1.3264 (nan)	mem 23876MB
[2022-11-11 23:45:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][1200/1251]	eta 0:00:38 lr 0.000796	time 0.7542 (0.7469)	loss 3.7540 (3.4398)	grad_norm 1.2494 (nan)	mem 23876MB
[2022-11-11 23:45:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [89/300][1250/1251]	eta 0:00:00 lr 0.000796	time 0.7241 (0.7467)	loss 3.2497 (3.4365)	grad_norm 1.5185 (nan)	mem 23876MB
[2022-11-11 23:45:44 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 89 training takes 0:15:34
[2022-11-11 23:45:44 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_89.pth saving......
[2022-11-11 23:45:45 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_89.pth saved !!!
[2022-11-11 23:45:46 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.645 (1.645)	Loss 1.0011 (1.0011)	Acc@1 76.562 (76.562)	Acc@5 93.457 (93.457)	Mem 23876MB
[2022-11-11 23:45:57 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.330 Acc@5 93.654
[2022-11-11 23:45:57 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.3%
[2022-11-11 23:45:59 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.872 (1.872)	Loss 0.8325 (0.8325)	Acc@1 79.688 (79.688)	Acc@5 94.922 (94.922)	Mem 23876MB
[2022-11-11 23:46:10 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.290 Acc@5 95.050
[2022-11-11 23:46:10 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.3%
[2022-11-11 23:46:10 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.29% at 89 epoch
[2022-11-11 23:46:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][0/1251]	eta 0:50:49 lr 0.000796	time 2.4376 (2.4376)	loss 3.9340 (3.9340)	grad_norm 1.3059 (1.3059)	mem 23876MB
[2022-11-11 23:46:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][50/1251]	eta 0:15:41 lr 0.000796	time 0.7384 (0.7837)	loss 3.0875 (3.5092)	grad_norm 1.5453 (1.3639)	mem 23876MB
[2022-11-11 23:47:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][100/1251]	eta 0:14:41 lr 0.000796	time 0.7398 (0.7656)	loss 4.2477 (3.4869)	grad_norm 1.3715 (1.3696)	mem 23876MB
[2022-11-11 23:48:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][150/1251]	eta 0:13:56 lr 0.000795	time 0.7370 (0.7596)	loss 3.1909 (3.4239)	grad_norm 1.3320 (1.3700)	mem 23876MB
[2022-11-11 23:48:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][200/1251]	eta 0:13:15 lr 0.000795	time 0.8059 (0.7570)	loss 3.6573 (3.4161)	grad_norm 1.2721 (1.3742)	mem 23876MB
[2022-11-11 23:49:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][250/1251]	eta 0:12:35 lr 0.000795	time 0.7396 (0.7544)	loss 3.8632 (3.4146)	grad_norm 1.4030 (1.3720)	mem 23876MB
[2022-11-11 23:49:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][300/1251]	eta 0:11:56 lr 0.000795	time 0.7413 (0.7536)	loss 3.8477 (3.4231)	grad_norm 1.2444 (1.3767)	mem 23876MB
[2022-11-11 23:50:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][350/1251]	eta 0:11:17 lr 0.000795	time 0.7439 (0.7525)	loss 3.4075 (3.4257)	grad_norm 1.3401 (1.3808)	mem 23876MB
[2022-11-11 23:51:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][400/1251]	eta 0:10:39 lr 0.000795	time 0.7303 (0.7519)	loss 2.7706 (3.4280)	grad_norm 1.3116 (1.3811)	mem 23876MB
[2022-11-11 23:51:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][450/1251]	eta 0:10:01 lr 0.000794	time 0.7410 (0.7514)	loss 3.3600 (3.4371)	grad_norm 1.3762 (1.3791)	mem 23876MB
[2022-11-11 23:52:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][500/1251]	eta 0:09:23 lr 0.000794	time 0.7401 (0.7509)	loss 3.6325 (3.4327)	grad_norm 1.2466 (1.3769)	mem 23876MB
[2022-11-11 23:53:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][550/1251]	eta 0:08:46 lr 0.000794	time 0.7365 (0.7506)	loss 4.0394 (3.4277)	grad_norm 1.4026 (1.3773)	mem 23876MB
[2022-11-11 23:53:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][600/1251]	eta 0:08:08 lr 0.000794	time 0.8008 (0.7503)	loss 3.4916 (3.4245)	grad_norm 1.3289 (1.3790)	mem 23876MB
[2022-11-11 23:54:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][650/1251]	eta 0:07:30 lr 0.000794	time 0.7344 (0.7500)	loss 3.4459 (3.4269)	grad_norm 1.3921 (1.3789)	mem 23876MB
[2022-11-11 23:54:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][700/1251]	eta 0:06:53 lr 0.000794	time 0.7348 (0.7499)	loss 4.0236 (3.4282)	grad_norm 1.3001 (1.3800)	mem 23876MB
[2022-11-11 23:55:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][750/1251]	eta 0:06:15 lr 0.000793	time 0.7370 (0.7497)	loss 3.6432 (3.4324)	grad_norm 1.6438 (1.3819)	mem 23876MB
[2022-11-11 23:56:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][800/1251]	eta 0:05:38 lr 0.000793	time 0.7372 (0.7496)	loss 4.2506 (3.4284)	grad_norm 1.4499 (1.3818)	mem 23876MB
[2022-11-11 23:56:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][850/1251]	eta 0:05:00 lr 0.000793	time 0.7378 (0.7495)	loss 3.2709 (3.4299)	grad_norm 1.6150 (1.3831)	mem 23876MB
[2022-11-11 23:57:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][900/1251]	eta 0:04:23 lr 0.000793	time 0.7420 (0.7493)	loss 3.4427 (3.4298)	grad_norm 1.4937 (1.3827)	mem 23876MB
[2022-11-11 23:58:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][950/1251]	eta 0:03:45 lr 0.000793	time 0.7404 (0.7491)	loss 2.9953 (3.4296)	grad_norm 1.3076 (1.3825)	mem 23876MB
[2022-11-11 23:58:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][1000/1251]	eta 0:03:08 lr 0.000793	time 0.8161 (0.7492)	loss 3.4784 (3.4293)	grad_norm 1.3687 (1.3819)	mem 23876MB
[2022-11-11 23:59:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][1050/1251]	eta 0:02:30 lr 0.000792	time 0.7409 (0.7491)	loss 3.8142 (3.4274)	grad_norm 1.3229 (1.3823)	mem 23876MB
[2022-11-11 23:59:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][1100/1251]	eta 0:01:53 lr 0.000792	time 0.7387 (0.7492)	loss 3.5013 (3.4301)	grad_norm 1.3967 (1.3827)	mem 23876MB
[2022-11-12 00:00:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][1150/1251]	eta 0:01:15 lr 0.000792	time 0.7419 (0.7491)	loss 2.4684 (3.4258)	grad_norm 1.2313 (1.3807)	mem 23876MB
[2022-11-12 00:01:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][1200/1251]	eta 0:00:38 lr 0.000792	time 0.7415 (0.7490)	loss 2.7463 (3.4254)	grad_norm 1.2414 (1.3802)	mem 23876MB
[2022-11-12 00:01:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [90/300][1250/1251]	eta 0:00:00 lr 0.000792	time 0.7328 (0.7489)	loss 3.1793 (3.4291)	grad_norm 1.3449 (1.3806)	mem 23876MB
[2022-11-12 00:01:47 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 90 training takes 0:15:36
[2022-11-12 00:01:47 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_90.pth saving......
[2022-11-12 00:01:48 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_90.pth saved !!!
[2022-11-12 00:01:50 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.710 (1.710)	Loss 1.0242 (1.0242)	Acc@1 76.953 (76.953)	Acc@5 93.262 (93.262)	Mem 23876MB
[2022-11-12 00:02:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.746 Acc@5 93.762
[2022-11-12 00:02:00 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.7%
[2022-11-12 00:02:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.940 (1.940)	Loss 0.8965 (0.8965)	Acc@1 78.516 (78.516)	Acc@5 93.945 (93.945)	Mem 23876MB
[2022-11-12 00:02:13 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.360 Acc@5 95.066
[2022-11-12 00:02:13 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.4%
[2022-11-12 00:02:13 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.36% at 90 epoch
[2022-11-12 00:02:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][0/1251]	eta 0:49:46 lr 0.000792	time 2.3870 (2.3870)	loss 3.1456 (3.1456)	grad_norm 1.3063 (1.3063)	mem 23876MB
[2022-11-12 00:02:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][50/1251]	eta 0:15:43 lr 0.000792	time 0.7429 (0.7855)	loss 3.7036 (3.4310)	grad_norm 1.7224 (1.3954)	mem 23876MB
[2022-11-12 00:03:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][100/1251]	eta 0:14:42 lr 0.000791	time 0.7405 (0.7668)	loss 3.6674 (3.4657)	grad_norm 1.3476 (1.4025)	mem 23876MB
[2022-11-12 00:04:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][150/1251]	eta 0:13:57 lr 0.000791	time 0.7433 (0.7603)	loss 3.3327 (3.4415)	grad_norm 1.2725 (1.3988)	mem 23876MB
[2022-11-12 00:04:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][200/1251]	eta 0:13:16 lr 0.000791	time 0.7409 (0.7574)	loss 3.8634 (3.4603)	grad_norm 1.3139 (1.3935)	mem 23876MB
[2022-11-12 00:05:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][250/1251]	eta 0:12:36 lr 0.000791	time 0.7331 (0.7555)	loss 2.6965 (3.4482)	grad_norm 1.2429 (1.3868)	mem 23876MB
[2022-11-12 00:06:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][300/1251]	eta 0:11:57 lr 0.000791	time 0.7390 (0.7542)	loss 3.8492 (3.4433)	grad_norm 1.3334 (1.3826)	mem 23876MB
[2022-11-12 00:06:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][350/1251]	eta 0:11:19 lr 0.000791	time 0.7401 (0.7539)	loss 3.7228 (3.4499)	grad_norm 1.5281 (1.3810)	mem 23876MB
[2022-11-12 00:07:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][400/1251]	eta 0:10:40 lr 0.000790	time 0.7450 (0.7527)	loss 2.4978 (3.4602)	grad_norm 1.4259 (1.3805)	mem 23876MB
[2022-11-12 00:07:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][450/1251]	eta 0:10:02 lr 0.000790	time 0.7401 (0.7526)	loss 2.6596 (3.4454)	grad_norm 1.6051 (1.3817)	mem 23876MB
[2022-11-12 00:08:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][500/1251]	eta 0:09:24 lr 0.000790	time 0.7361 (0.7517)	loss 3.0107 (3.4547)	grad_norm 1.2929 (1.3810)	mem 23876MB
[2022-11-12 00:09:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][550/1251]	eta 0:08:47 lr 0.000790	time 0.7448 (0.7518)	loss 3.9457 (3.4595)	grad_norm 1.2983 (1.3809)	mem 23876MB
[2022-11-12 00:09:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][600/1251]	eta 0:08:09 lr 0.000790	time 0.7427 (0.7513)	loss 3.4216 (3.4583)	grad_norm 1.7031 (1.3835)	mem 23876MB
[2022-11-12 00:10:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][650/1251]	eta 0:07:31 lr 0.000790	time 0.7459 (0.7513)	loss 3.8187 (3.4606)	grad_norm 1.3180 (1.3811)	mem 23876MB
[2022-11-12 00:11:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][700/1251]	eta 0:06:53 lr 0.000789	time 0.7412 (0.7510)	loss 4.2746 (3.4668)	grad_norm 1.4473 (1.3817)	mem 23876MB
[2022-11-12 00:11:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][750/1251]	eta 0:06:16 lr 0.000789	time 0.8299 (0.7508)	loss 2.6977 (3.4643)	grad_norm 1.3901 (inf)	mem 23876MB
[2022-11-12 00:12:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][800/1251]	eta 0:05:38 lr 0.000789	time 0.7458 (0.7506)	loss 3.7114 (3.4679)	grad_norm 1.4368 (inf)	mem 23876MB
[2022-11-12 00:12:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][850/1251]	eta 0:05:00 lr 0.000789	time 0.7418 (0.7505)	loss 3.2992 (3.4710)	grad_norm 1.3258 (inf)	mem 23876MB
[2022-11-12 00:13:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][900/1251]	eta 0:04:23 lr 0.000789	time 0.7385 (0.7503)	loss 3.8422 (3.4699)	grad_norm 1.2529 (inf)	mem 23876MB
[2022-11-12 00:14:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][950/1251]	eta 0:03:45 lr 0.000789	time 0.7357 (0.7502)	loss 2.6107 (3.4684)	grad_norm 1.5285 (inf)	mem 23876MB
[2022-11-12 00:14:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][1000/1251]	eta 0:03:08 lr 0.000788	time 0.8076 (0.7500)	loss 3.7534 (3.4710)	grad_norm 1.4397 (inf)	mem 23876MB
[2022-11-12 00:15:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][1050/1251]	eta 0:02:30 lr 0.000788	time 0.7385 (0.7499)	loss 3.9084 (3.4708)	grad_norm 1.3327 (inf)	mem 23876MB
[2022-11-12 00:15:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][1100/1251]	eta 0:01:53 lr 0.000788	time 0.7397 (0.7496)	loss 3.2146 (3.4696)	grad_norm 1.6284 (inf)	mem 23876MB
[2022-11-12 00:16:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][1150/1251]	eta 0:01:15 lr 0.000788	time 0.7351 (0.7497)	loss 3.5016 (3.4686)	grad_norm 1.1856 (inf)	mem 23876MB
[2022-11-12 00:17:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][1200/1251]	eta 0:00:38 lr 0.000788	time 0.7563 (0.7495)	loss 2.2750 (3.4632)	grad_norm 1.3386 (inf)	mem 23876MB
[2022-11-12 00:17:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [91/300][1250/1251]	eta 0:00:00 lr 0.000788	time 0.7274 (0.7495)	loss 4.2592 (3.4607)	grad_norm 1.4556 (inf)	mem 23876MB
[2022-11-12 00:17:51 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 91 training takes 0:15:37
[2022-11-12 00:17:51 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_91.pth saving......
[2022-11-12 00:17:52 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_91.pth saved !!!
[2022-11-12 00:17:54 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.663 (1.663)	Loss 1.0478 (1.0478)	Acc@1 75.781 (75.781)	Acc@5 93.457 (93.457)	Mem 23876MB
[2022-11-12 00:18:04 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.758 Acc@5 93.828
[2022-11-12 00:18:04 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.8%
[2022-11-12 00:18:06 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.939 (1.939)	Loss 0.8703 (0.8703)	Acc@1 78.516 (78.516)	Acc@5 94.629 (94.629)	Mem 23876MB
[2022-11-12 00:18:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.382 Acc@5 95.102
[2022-11-12 00:18:17 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.4%
[2022-11-12 00:18:17 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.38% at 91 epoch
[2022-11-12 00:18:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][0/1251]	eta 0:51:56 lr 0.000788	time 2.4914 (2.4914)	loss 3.4262 (3.4262)	grad_norm 1.4461 (1.4461)	mem 23876MB
[2022-11-12 00:18:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][50/1251]	eta 0:15:45 lr 0.000787	time 0.8319 (0.7873)	loss 3.7698 (3.3428)	grad_norm 1.5678 (1.4007)	mem 23876MB
[2022-11-12 00:19:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][100/1251]	eta 0:14:43 lr 0.000787	time 0.7461 (0.7673)	loss 3.5359 (3.3303)	grad_norm 1.1613 (1.3864)	mem 23876MB
[2022-11-12 00:20:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][150/1251]	eta 0:13:58 lr 0.000787	time 0.7380 (0.7618)	loss 3.2430 (3.4015)	grad_norm 1.2335 (1.3805)	mem 23876MB
[2022-11-12 00:20:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][200/1251]	eta 0:13:17 lr 0.000787	time 0.8179 (0.7588)	loss 2.8725 (3.4154)	grad_norm 1.3496 (1.3754)	mem 23876MB
[2022-11-12 00:21:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][250/1251]	eta 0:12:36 lr 0.000787	time 0.7372 (0.7558)	loss 2.6449 (3.4316)	grad_norm 1.2003 (1.3778)	mem 23876MB
[2022-11-12 00:22:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][300/1251]	eta 0:11:57 lr 0.000786	time 0.7417 (0.7548)	loss 3.1668 (3.4265)	grad_norm 1.2782 (1.3735)	mem 23876MB
[2022-11-12 00:22:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][350/1251]	eta 0:11:18 lr 0.000786	time 0.7360 (0.7532)	loss 3.6996 (3.4336)	grad_norm 1.4434 (1.3763)	mem 23876MB
[2022-11-12 00:23:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][400/1251]	eta 0:10:40 lr 0.000786	time 0.7387 (0.7529)	loss 3.1992 (3.4353)	grad_norm 1.3163 (1.3786)	mem 23876MB
[2022-11-12 00:23:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][450/1251]	eta 0:10:02 lr 0.000786	time 0.7387 (0.7525)	loss 2.9203 (3.4380)	grad_norm 1.1894 (1.3814)	mem 23876MB
[2022-11-12 00:24:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][500/1251]	eta 0:09:24 lr 0.000786	time 0.7401 (0.7519)	loss 2.3362 (3.4460)	grad_norm 1.5484 (1.3837)	mem 23876MB
[2022-11-12 00:25:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][550/1251]	eta 0:08:46 lr 0.000786	time 0.7486 (0.7517)	loss 3.8020 (3.4475)	grad_norm 1.3277 (1.3825)	mem 23876MB
[2022-11-12 00:25:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][600/1251]	eta 0:08:09 lr 0.000785	time 0.8174 (0.7515)	loss 3.9736 (3.4469)	grad_norm 1.6716 (1.3820)	mem 23876MB
[2022-11-12 00:26:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][650/1251]	eta 0:07:31 lr 0.000785	time 0.7424 (0.7511)	loss 3.8973 (3.4490)	grad_norm 1.2815 (1.3813)	mem 23876MB
[2022-11-12 00:27:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][700/1251]	eta 0:06:53 lr 0.000785	time 0.7268 (0.7511)	loss 3.2197 (3.4462)	grad_norm 1.2662 (1.3799)	mem 23876MB
[2022-11-12 00:27:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][750/1251]	eta 0:06:16 lr 0.000785	time 0.7366 (0.7507)	loss 4.1761 (3.4527)	grad_norm 1.4215 (1.3800)	mem 23876MB
[2022-11-12 00:28:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][800/1251]	eta 0:05:38 lr 0.000785	time 0.7316 (0.7506)	loss 3.7220 (3.4493)	grad_norm 1.1867 (1.3821)	mem 23876MB
[2022-11-12 00:28:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][850/1251]	eta 0:05:00 lr 0.000785	time 0.7346 (0.7504)	loss 3.5402 (3.4477)	grad_norm 1.3551 (1.3830)	mem 23876MB
[2022-11-12 00:29:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][900/1251]	eta 0:04:23 lr 0.000784	time 0.7473 (0.7502)	loss 4.0839 (3.4488)	grad_norm 1.5659 (1.3823)	mem 23876MB
[2022-11-12 00:30:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][950/1251]	eta 0:03:45 lr 0.000784	time 0.7385 (0.7501)	loss 2.5001 (3.4463)	grad_norm 1.3953 (1.3825)	mem 23876MB
[2022-11-12 00:30:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][1000/1251]	eta 0:03:08 lr 0.000784	time 0.8302 (0.7501)	loss 4.0963 (3.4432)	grad_norm 1.2629 (1.3828)	mem 23876MB
[2022-11-12 00:31:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][1050/1251]	eta 0:02:30 lr 0.000784	time 0.7429 (0.7498)	loss 3.6090 (3.4438)	grad_norm 1.4320 (1.3848)	mem 23876MB
[2022-11-12 00:32:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][1100/1251]	eta 0:01:53 lr 0.000784	time 0.7402 (0.7498)	loss 3.5847 (3.4412)	grad_norm 1.3194 (1.3854)	mem 23876MB
[2022-11-12 00:32:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][1150/1251]	eta 0:01:15 lr 0.000784	time 0.7389 (0.7496)	loss 3.7548 (3.4406)	grad_norm 1.2873 (1.3847)	mem 23876MB
[2022-11-12 00:33:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][1200/1251]	eta 0:00:38 lr 0.000783	time 0.7431 (0.7496)	loss 3.0558 (3.4394)	grad_norm 1.3692 (1.3851)	mem 23876MB
[2022-11-12 00:33:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [92/300][1250/1251]	eta 0:00:00 lr 0.000783	time 0.7285 (0.7495)	loss 2.9041 (3.4409)	grad_norm 1.1905 (1.3845)	mem 23876MB
[2022-11-12 00:33:55 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 92 training takes 0:15:37
[2022-11-12 00:33:55 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_92.pth saving......
[2022-11-12 00:33:56 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_92.pth saved !!!
[2022-11-12 00:33:58 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.572 (1.572)	Loss 1.0347 (1.0347)	Acc@1 74.414 (74.414)	Acc@5 93.652 (93.652)	Mem 23876MB
[2022-11-12 00:34:08 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.650 Acc@5 93.744
[2022-11-12 00:34:08 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.7%
[2022-11-12 00:34:10 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.793 (1.793)	Loss 0.8735 (0.8735)	Acc@1 78.223 (78.223)	Acc@5 95.312 (95.312)	Mem 23876MB
[2022-11-12 00:34:21 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.414 Acc@5 95.104
[2022-11-12 00:34:21 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.4%
[2022-11-12 00:34:21 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.41% at 92 epoch
[2022-11-12 00:34:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][0/1251]	eta 0:49:37 lr 0.000783	time 2.3798 (2.3798)	loss 4.0047 (4.0047)	grad_norm 1.3207 (1.3207)	mem 23876MB
[2022-11-12 00:35:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][50/1251]	eta 0:15:46 lr 0.000783	time 0.7415 (0.7878)	loss 3.2191 (3.4326)	grad_norm 1.3833 (1.3846)	mem 23876MB
[2022-11-12 00:35:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][100/1251]	eta 0:14:44 lr 0.000783	time 0.7347 (0.7686)	loss 3.6111 (3.4991)	grad_norm 1.5135 (1.4002)	mem 23876MB
[2022-11-12 00:36:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][150/1251]	eta 0:13:57 lr 0.000783	time 0.7621 (0.7611)	loss 3.6783 (3.4850)	grad_norm 1.2729 (1.3973)	mem 23876MB
[2022-11-12 00:36:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][200/1251]	eta 0:13:16 lr 0.000783	time 0.7375 (0.7582)	loss 3.7233 (3.4709)	grad_norm 1.4046 (1.4021)	mem 23876MB
[2022-11-12 00:37:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][250/1251]	eta 0:12:37 lr 0.000782	time 0.7367 (0.7563)	loss 4.0000 (3.4565)	grad_norm 1.4219 (1.4015)	mem 23876MB
[2022-11-12 00:38:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][300/1251]	eta 0:11:57 lr 0.000782	time 0.7380 (0.7546)	loss 2.0469 (3.4718)	grad_norm 1.4973 (1.4015)	mem 23876MB
[2022-11-12 00:38:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][350/1251]	eta 0:11:19 lr 0.000782	time 0.7403 (0.7541)	loss 3.7226 (3.4658)	grad_norm 1.3093 (1.3995)	mem 23876MB
[2022-11-12 00:39:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][400/1251]	eta 0:10:40 lr 0.000782	time 0.7396 (0.7531)	loss 2.6774 (3.4626)	grad_norm 1.4999 (1.3971)	mem 23876MB
[2022-11-12 00:40:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][450/1251]	eta 0:10:02 lr 0.000782	time 0.7363 (0.7526)	loss 2.8051 (3.4618)	grad_norm 1.3415 (1.3964)	mem 23876MB
[2022-11-12 00:40:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][500/1251]	eta 0:09:25 lr 0.000782	time 0.7345 (0.7523)	loss 3.6993 (3.4665)	grad_norm 1.2947 (1.3972)	mem 23876MB
[2022-11-12 00:41:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][550/1251]	eta 0:08:47 lr 0.000781	time 0.7339 (0.7520)	loss 3.6156 (3.4498)	grad_norm 1.3821 (1.3990)	mem 23876MB
[2022-11-12 00:41:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][600/1251]	eta 0:08:09 lr 0.000781	time 0.7294 (0.7519)	loss 3.2992 (3.4545)	grad_norm 1.1925 (1.4000)	mem 23876MB
[2022-11-12 00:42:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][650/1251]	eta 0:07:31 lr 0.000781	time 0.7387 (0.7515)	loss 3.4740 (3.4533)	grad_norm 1.3454 (1.3989)	mem 23876MB
[2022-11-12 00:43:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][700/1251]	eta 0:06:53 lr 0.000781	time 0.7448 (0.7512)	loss 4.2168 (3.4518)	grad_norm 1.3697 (1.3986)	mem 23876MB
[2022-11-12 00:43:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][750/1251]	eta 0:06:16 lr 0.000781	time 0.7409 (0.7511)	loss 3.8560 (3.4546)	grad_norm 1.4780 (1.3969)	mem 23876MB
[2022-11-12 00:44:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][800/1251]	eta 0:05:38 lr 0.000780	time 0.7396 (0.7509)	loss 2.7564 (3.4553)	grad_norm 1.2607 (1.3948)	mem 23876MB
[2022-11-12 00:45:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][850/1251]	eta 0:05:01 lr 0.000780	time 0.7429 (0.7509)	loss 3.7987 (3.4526)	grad_norm 1.3537 (1.3941)	mem 23876MB
[2022-11-12 00:45:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][900/1251]	eta 0:04:23 lr 0.000780	time 0.7399 (0.7507)	loss 2.1956 (3.4541)	grad_norm 1.3244 (1.3936)	mem 23876MB
[2022-11-12 00:46:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][950/1251]	eta 0:03:45 lr 0.000780	time 0.8303 (0.7507)	loss 3.4883 (3.4533)	grad_norm 1.3058 (1.3929)	mem 23876MB
[2022-11-12 00:46:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][1000/1251]	eta 0:03:08 lr 0.000780	time 0.7365 (0.7504)	loss 4.1539 (3.4511)	grad_norm 1.4700 (1.3931)	mem 23876MB
[2022-11-12 00:47:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][1050/1251]	eta 0:02:30 lr 0.000780	time 0.7367 (0.7504)	loss 3.3445 (3.4501)	grad_norm 1.4034 (1.3932)	mem 23876MB
[2022-11-12 00:48:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][1100/1251]	eta 0:01:53 lr 0.000779	time 0.7418 (0.7502)	loss 3.5208 (3.4514)	grad_norm 1.3894 (1.3925)	mem 23876MB
[2022-11-12 00:48:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][1150/1251]	eta 0:01:15 lr 0.000779	time 0.7340 (0.7504)	loss 2.8499 (3.4488)	grad_norm 1.2986 (1.3934)	mem 23876MB
[2022-11-12 00:49:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][1200/1251]	eta 0:00:38 lr 0.000779	time 0.7413 (0.7501)	loss 3.1415 (3.4513)	grad_norm 1.4747 (1.3926)	mem 23876MB
[2022-11-12 00:49:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [93/300][1250/1251]	eta 0:00:00 lr 0.000779	time 0.7296 (0.7499)	loss 3.7689 (3.4536)	grad_norm 1.2930 (1.3937)	mem 23876MB
[2022-11-12 00:49:59 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 93 training takes 0:15:38
[2022-11-12 00:49:59 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_93.pth saving......
[2022-11-12 00:50:00 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_93.pth saved !!!
[2022-11-12 00:50:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.627 (1.627)	Loss 0.9392 (0.9392)	Acc@1 77.734 (77.734)	Acc@5 94.434 (94.434)	Mem 23876MB
[2022-11-12 00:50:13 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.772 Acc@5 93.950
[2022-11-12 00:50:13 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 76.8%
[2022-11-12 00:50:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.781 (1.781)	Loss 0.8157 (0.8157)	Acc@1 80.273 (80.273)	Acc@5 95.703 (95.703)	Mem 23876MB
[2022-11-12 00:50:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.460 Acc@5 95.110
[2022-11-12 00:50:25 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.5%
[2022-11-12 00:50:25 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.46% at 93 epoch
[2022-11-12 00:50:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][0/1251]	eta 0:51:07 lr 0.000779	time 2.4520 (2.4520)	loss 2.9320 (2.9320)	grad_norm 1.4068 (1.4068)	mem 23876MB
[2022-11-12 00:51:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][50/1251]	eta 0:15:34 lr 0.000779	time 0.7419 (0.7783)	loss 3.8158 (3.3757)	grad_norm 1.3609 (1.3911)	mem 23876MB
[2022-11-12 00:51:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][100/1251]	eta 0:14:39 lr 0.000779	time 0.7359 (0.7641)	loss 3.9636 (3.3844)	grad_norm 1.6946 (1.3890)	mem 23876MB
[2022-11-12 00:52:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][150/1251]	eta 0:13:55 lr 0.000778	time 0.7397 (0.7591)	loss 3.6979 (3.4258)	grad_norm 1.5183 (1.3840)	mem 23876MB
[2022-11-12 00:52:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][200/1251]	eta 0:13:15 lr 0.000778	time 0.7399 (0.7570)	loss 2.4869 (3.4218)	grad_norm 1.3011 (1.3836)	mem 23876MB
[2022-11-12 00:53:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][250/1251]	eta 0:12:35 lr 0.000778	time 0.7407 (0.7549)	loss 3.5237 (3.4060)	grad_norm 1.4815 (1.3844)	mem 23876MB
[2022-11-12 00:54:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][300/1251]	eta 0:11:56 lr 0.000778	time 0.7379 (0.7533)	loss 2.9998 (3.4118)	grad_norm 1.3123 (1.3820)	mem 23876MB
[2022-11-12 00:54:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][350/1251]	eta 0:11:18 lr 0.000778	time 0.7223 (0.7527)	loss 3.6877 (3.4306)	grad_norm 1.4365 (1.3872)	mem 23876MB
[2022-11-12 00:55:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][400/1251]	eta 0:10:40 lr 0.000778	time 0.7347 (0.7522)	loss 3.7102 (3.4453)	grad_norm 1.3943 (1.3859)	mem 23876MB
[2022-11-12 00:56:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][450/1251]	eta 0:10:01 lr 0.000777	time 0.7414 (0.7515)	loss 3.4979 (3.4379)	grad_norm 1.5150 (1.3862)	mem 23876MB
[2022-11-12 00:56:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][500/1251]	eta 0:09:24 lr 0.000777	time 0.7446 (0.7510)	loss 3.6607 (3.4211)	grad_norm 1.3903 (1.3858)	mem 23876MB
[2022-11-12 00:57:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][550/1251]	eta 0:08:46 lr 0.000777	time 0.7576 (0.7506)	loss 3.3402 (3.4255)	grad_norm 1.5783 (1.3874)	mem 23876MB
[2022-11-12 00:57:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][600/1251]	eta 0:08:08 lr 0.000777	time 0.8118 (0.7502)	loss 3.0536 (3.4191)	grad_norm 1.2177 (1.3882)	mem 23876MB
[2022-11-12 00:58:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][650/1251]	eta 0:07:30 lr 0.000777	time 0.7418 (0.7499)	loss 3.4903 (3.4169)	grad_norm 1.2304 (1.3891)	mem 23876MB
[2022-11-12 00:59:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][700/1251]	eta 0:06:52 lr 0.000777	time 0.7390 (0.7493)	loss 3.0702 (3.4300)	grad_norm 1.1655 (1.3898)	mem 23876MB
[2022-11-12 00:59:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][750/1251]	eta 0:06:15 lr 0.000776	time 0.8001 (0.7493)	loss 4.0692 (3.4372)	grad_norm 1.4205 (1.3907)	mem 23876MB
[2022-11-12 01:00:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][800/1251]	eta 0:05:37 lr 0.000776	time 0.7371 (0.7491)	loss 3.3694 (3.4368)	grad_norm 1.4680 (1.3920)	mem 23876MB
[2022-11-12 01:01:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][850/1251]	eta 0:05:00 lr 0.000776	time 0.7375 (0.7489)	loss 3.8006 (3.4293)	grad_norm 1.3159 (1.3928)	mem 23876MB
[2022-11-12 01:01:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][900/1251]	eta 0:04:22 lr 0.000776	time 0.7431 (0.7488)	loss 3.7795 (3.4326)	grad_norm 1.6090 (1.3949)	mem 23876MB
[2022-11-12 01:02:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][950/1251]	eta 0:03:45 lr 0.000776	time 0.7522 (0.7488)	loss 2.5334 (3.4288)	grad_norm 1.3255 (1.3930)	mem 23876MB
[2022-11-12 01:02:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][1000/1251]	eta 0:03:07 lr 0.000775	time 0.7386 (0.7487)	loss 3.7588 (3.4319)	grad_norm 1.3254 (1.3935)	mem 23876MB
[2022-11-12 01:03:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][1050/1251]	eta 0:02:30 lr 0.000775	time 0.7374 (0.7486)	loss 3.7836 (3.4290)	grad_norm 1.5587 (1.3948)	mem 23876MB
[2022-11-12 01:04:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][1100/1251]	eta 0:01:53 lr 0.000775	time 0.8122 (0.7485)	loss 3.4518 (3.4281)	grad_norm 1.2694 (1.3944)	mem 23876MB
[2022-11-12 01:04:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][1150/1251]	eta 0:01:15 lr 0.000775	time 0.7374 (0.7484)	loss 3.1889 (3.4252)	grad_norm 1.2764 (1.3927)	mem 23876MB
[2022-11-12 01:05:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][1200/1251]	eta 0:00:38 lr 0.000775	time 0.7365 (0.7483)	loss 2.8652 (3.4220)	grad_norm 1.3246 (1.3948)	mem 23876MB
[2022-11-12 01:06:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [94/300][1250/1251]	eta 0:00:00 lr 0.000775	time 0.7271 (0.7482)	loss 3.5195 (3.4209)	grad_norm 1.3704 (1.3951)	mem 23876MB
[2022-11-12 01:06:01 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 94 training takes 0:15:36
[2022-11-12 01:06:02 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_94.pth saving......
[2022-11-12 01:06:03 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_94.pth saved !!!
[2022-11-12 01:06:04 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.654 (1.654)	Loss 0.9364 (0.9364)	Acc@1 77.148 (77.148)	Acc@5 94.531 (94.531)	Mem 23876MB
[2022-11-12 01:06:15 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.062 Acc@5 93.958
[2022-11-12 01:06:15 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.1%
[2022-11-12 01:06:17 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.807 (1.807)	Loss 0.8257 (0.8257)	Acc@1 80.664 (80.664)	Acc@5 95.215 (95.215)	Mem 23876MB
[2022-11-12 01:06:28 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.522 Acc@5 95.136
[2022-11-12 01:06:28 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.5%
[2022-11-12 01:06:28 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.52% at 94 epoch
[2022-11-12 01:06:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][0/1251]	eta 0:51:02 lr 0.000775	time 2.4483 (2.4483)	loss 3.4534 (3.4534)	grad_norm 1.2820 (1.2820)	mem 23876MB
[2022-11-12 01:07:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][50/1251]	eta 0:15:45 lr 0.000774	time 0.7481 (0.7869)	loss 3.5482 (3.3588)	grad_norm 1.5397 (1.3856)	mem 23876MB
[2022-11-12 01:07:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][100/1251]	eta 0:14:40 lr 0.000774	time 0.7398 (0.7651)	loss 2.7639 (3.4309)	grad_norm 1.3196 (nan)	mem 23876MB
[2022-11-12 01:08:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][150/1251]	eta 0:13:57 lr 0.000774	time 0.7307 (0.7605)	loss 3.7471 (3.4336)	grad_norm 1.2970 (nan)	mem 23876MB
[2022-11-12 01:09:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][200/1251]	eta 0:13:15 lr 0.000774	time 0.7418 (0.7570)	loss 2.8059 (3.4001)	grad_norm 1.4048 (nan)	mem 23876MB
[2022-11-12 01:09:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][250/1251]	eta 0:12:35 lr 0.000774	time 0.7371 (0.7552)	loss 3.7479 (3.4149)	grad_norm 1.3321 (nan)	mem 23876MB
[2022-11-12 01:10:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][300/1251]	eta 0:11:57 lr 0.000774	time 0.8137 (0.7540)	loss 4.3129 (3.4339)	grad_norm 1.5119 (nan)	mem 23876MB
[2022-11-12 01:10:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][350/1251]	eta 0:11:18 lr 0.000773	time 0.7387 (0.7529)	loss 2.9901 (3.4371)	grad_norm 1.3666 (nan)	mem 23876MB
[2022-11-12 01:11:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][400/1251]	eta 0:10:40 lr 0.000773	time 0.7446 (0.7523)	loss 3.8848 (3.4300)	grad_norm 1.2959 (nan)	mem 23876MB
[2022-11-12 01:12:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][450/1251]	eta 0:10:01 lr 0.000773	time 0.7392 (0.7515)	loss 3.5549 (3.4209)	grad_norm 1.2941 (nan)	mem 23876MB
[2022-11-12 01:12:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][500/1251]	eta 0:09:23 lr 0.000773	time 0.7406 (0.7509)	loss 2.4179 (3.4228)	grad_norm 1.2085 (nan)	mem 23876MB
[2022-11-12 01:13:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][550/1251]	eta 0:08:46 lr 0.000773	time 0.7365 (0.7505)	loss 3.2654 (3.4307)	grad_norm 1.6794 (nan)	mem 23876MB
[2022-11-12 01:13:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][600/1251]	eta 0:08:08 lr 0.000773	time 0.7540 (0.7504)	loss 2.2015 (3.4292)	grad_norm 1.2338 (nan)	mem 23876MB
[2022-11-12 01:14:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][650/1251]	eta 0:07:30 lr 0.000772	time 0.7544 (0.7502)	loss 4.0456 (3.4266)	grad_norm 1.2822 (nan)	mem 23876MB
[2022-11-12 01:15:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][700/1251]	eta 0:06:53 lr 0.000772	time 0.8169 (0.7499)	loss 2.9750 (3.4214)	grad_norm 1.4399 (nan)	mem 23876MB
[2022-11-12 01:15:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][750/1251]	eta 0:06:15 lr 0.000772	time 0.7375 (0.7497)	loss 3.3825 (3.4169)	grad_norm 1.2608 (nan)	mem 23876MB
[2022-11-12 01:16:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][800/1251]	eta 0:05:38 lr 0.000772	time 0.7318 (0.7495)	loss 3.0305 (3.4192)	grad_norm 1.2408 (nan)	mem 23876MB
[2022-11-12 01:17:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][850/1251]	eta 0:05:00 lr 0.000772	time 0.7361 (0.7494)	loss 3.7167 (3.4191)	grad_norm 1.3649 (nan)	mem 23876MB
[2022-11-12 01:17:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][900/1251]	eta 0:04:22 lr 0.000771	time 0.7445 (0.7492)	loss 3.9151 (3.4162)	grad_norm 1.2485 (nan)	mem 23876MB
[2022-11-12 01:18:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][950/1251]	eta 0:03:45 lr 0.000771	time 0.7355 (0.7492)	loss 3.3748 (3.4202)	grad_norm 1.4375 (nan)	mem 23876MB
[2022-11-12 01:18:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][1000/1251]	eta 0:03:08 lr 0.000771	time 0.7464 (0.7491)	loss 2.7559 (3.4153)	grad_norm 1.3979 (nan)	mem 23876MB
[2022-11-12 01:19:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][1050/1251]	eta 0:02:30 lr 0.000771	time 0.7381 (0.7489)	loss 2.9615 (3.4171)	grad_norm 1.4808 (nan)	mem 23876MB
[2022-11-12 01:20:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][1100/1251]	eta 0:01:53 lr 0.000771	time 0.8154 (0.7490)	loss 4.1798 (3.4195)	grad_norm 1.5019 (nan)	mem 23876MB
[2022-11-12 01:20:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][1150/1251]	eta 0:01:15 lr 0.000771	time 0.7343 (0.7489)	loss 3.8002 (3.4175)	grad_norm 1.4433 (nan)	mem 23876MB
[2022-11-12 01:21:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][1200/1251]	eta 0:00:38 lr 0.000770	time 0.7364 (0.7488)	loss 3.5796 (3.4180)	grad_norm 1.4615 (nan)	mem 23876MB
[2022-11-12 01:22:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [95/300][1250/1251]	eta 0:00:00 lr 0.000770	time 0.7283 (0.7487)	loss 3.7072 (3.4184)	grad_norm 1.3707 (nan)	mem 23876MB
[2022-11-12 01:22:04 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 95 training takes 0:15:36
[2022-11-12 01:22:04 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_95.pth saving......
[2022-11-12 01:22:06 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_95.pth saved !!!
[2022-11-12 01:22:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.730 (1.730)	Loss 0.9673 (0.9673)	Acc@1 75.977 (75.977)	Acc@5 93.457 (93.457)	Mem 23876MB
[2022-11-12 01:22:18 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 76.950 Acc@5 93.934
[2022-11-12 01:22:18 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.0%
[2022-11-12 01:22:20 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.824 (1.824)	Loss 0.7693 (0.7693)	Acc@1 82.520 (82.520)	Acc@5 95.898 (95.898)	Mem 23876MB
[2022-11-12 01:22:31 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.566 Acc@5 95.158
[2022-11-12 01:22:31 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.6%
[2022-11-12 01:22:31 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.57% at 95 epoch
[2022-11-12 01:22:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][0/1251]	eta 0:52:35 lr 0.000770	time 2.5220 (2.5220)	loss 4.2147 (4.2147)	grad_norm 1.5192 (1.5192)	mem 23876MB
[2022-11-12 01:23:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][50/1251]	eta 0:15:44 lr 0.000770	time 0.7304 (0.7863)	loss 3.2954 (3.2922)	grad_norm 1.3037 (1.3950)	mem 23876MB
[2022-11-12 01:23:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][100/1251]	eta 0:14:43 lr 0.000770	time 0.7356 (0.7676)	loss 3.9273 (3.3089)	grad_norm 1.7152 (1.3951)	mem 23876MB
[2022-11-12 01:24:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][150/1251]	eta 0:13:57 lr 0.000770	time 0.7403 (0.7610)	loss 4.1287 (3.3677)	grad_norm 1.3731 (1.3929)	mem 23876MB
[2022-11-12 01:25:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][200/1251]	eta 0:13:17 lr 0.000770	time 0.7384 (0.7585)	loss 2.2780 (3.3884)	grad_norm 1.2798 (1.4020)	mem 23876MB
[2022-11-12 01:25:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][250/1251]	eta 0:12:36 lr 0.000769	time 0.8255 (0.7561)	loss 3.8737 (3.3848)	grad_norm 1.4092 (1.3954)	mem 23876MB
[2022-11-12 01:26:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][300/1251]	eta 0:11:57 lr 0.000769	time 0.7352 (0.7549)	loss 2.6745 (3.3944)	grad_norm 1.2010 (1.3938)	mem 23876MB
[2022-11-12 01:26:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][350/1251]	eta 0:11:19 lr 0.000769	time 0.7376 (0.7541)	loss 4.0622 (3.4030)	grad_norm 1.6559 (1.3935)	mem 23876MB
[2022-11-12 01:27:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][400/1251]	eta 0:10:41 lr 0.000769	time 0.7368 (0.7536)	loss 2.5320 (3.4081)	grad_norm 1.4958 (1.3926)	mem 23876MB
[2022-11-12 01:28:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][450/1251]	eta 0:10:03 lr 0.000769	time 0.7457 (0.7530)	loss 3.9279 (3.4122)	grad_norm 1.3893 (1.3943)	mem 23876MB
[2022-11-12 01:28:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][500/1251]	eta 0:09:25 lr 0.000768	time 0.7422 (0.7525)	loss 2.8090 (3.3981)	grad_norm 1.4115 (1.3979)	mem 23876MB
[2022-11-12 01:29:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][550/1251]	eta 0:08:47 lr 0.000768	time 0.7397 (0.7522)	loss 3.5057 (3.4102)	grad_norm 1.5963 (inf)	mem 23876MB
[2022-11-12 01:30:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][600/1251]	eta 0:08:09 lr 0.000768	time 0.7414 (0.7518)	loss 3.5032 (3.4219)	grad_norm 1.5450 (inf)	mem 23876MB
[2022-11-12 01:30:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][650/1251]	eta 0:07:31 lr 0.000768	time 0.7490 (0.7519)	loss 3.1203 (3.4311)	grad_norm 1.5744 (inf)	mem 23876MB
[2022-11-12 01:31:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][700/1251]	eta 0:06:54 lr 0.000768	time 0.7389 (0.7514)	loss 3.9955 (3.4253)	grad_norm 1.4994 (inf)	mem 23876MB
[2022-11-12 01:31:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][750/1251]	eta 0:06:16 lr 0.000768	time 0.8305 (0.7513)	loss 3.0057 (3.4268)	grad_norm 1.2876 (inf)	mem 23876MB
[2022-11-12 01:32:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][800/1251]	eta 0:05:38 lr 0.000767	time 0.7352 (0.7510)	loss 3.8915 (3.4331)	grad_norm 1.3734 (inf)	mem 23876MB
[2022-11-12 01:33:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][850/1251]	eta 0:05:01 lr 0.000767	time 0.7389 (0.7510)	loss 3.8433 (3.4338)	grad_norm 1.3332 (inf)	mem 23876MB
[2022-11-12 01:33:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][900/1251]	eta 0:04:23 lr 0.000767	time 0.7395 (0.7509)	loss 4.3542 (3.4368)	grad_norm 1.3899 (inf)	mem 23876MB
[2022-11-12 01:34:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][950/1251]	eta 0:03:45 lr 0.000767	time 0.7390 (0.7507)	loss 3.1559 (3.4293)	grad_norm 1.3379 (inf)	mem 23876MB
[2022-11-12 01:35:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][1000/1251]	eta 0:03:08 lr 0.000767	time 0.8068 (0.7506)	loss 3.5216 (3.4288)	grad_norm 1.2568 (inf)	mem 23876MB
[2022-11-12 01:35:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][1050/1251]	eta 0:02:30 lr 0.000767	time 0.7384 (0.7504)	loss 3.5260 (3.4252)	grad_norm 1.3199 (inf)	mem 23876MB
[2022-11-12 01:36:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][1100/1251]	eta 0:01:53 lr 0.000766	time 0.7369 (0.7503)	loss 3.0256 (3.4312)	grad_norm 1.3566 (inf)	mem 23876MB
[2022-11-12 01:36:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][1150/1251]	eta 0:01:15 lr 0.000766	time 0.7408 (0.7503)	loss 3.3615 (3.4333)	grad_norm 1.3208 (inf)	mem 23876MB
[2022-11-12 01:37:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][1200/1251]	eta 0:00:38 lr 0.000766	time 0.7421 (0.7501)	loss 3.6288 (3.4348)	grad_norm 1.3106 (inf)	mem 23876MB
[2022-11-12 01:38:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [96/300][1250/1251]	eta 0:00:00 lr 0.000766	time 0.7286 (0.7500)	loss 3.9123 (3.4338)	grad_norm 1.3702 (inf)	mem 23876MB
[2022-11-12 01:38:09 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 96 training takes 0:15:38
[2022-11-12 01:38:09 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_96.pth saving......
[2022-11-12 01:38:10 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_96.pth saved !!!
[2022-11-12 01:38:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.677 (1.677)	Loss 0.9741 (0.9741)	Acc@1 76.465 (76.465)	Acc@5 94.629 (94.629)	Mem 23876MB
[2022-11-12 01:38:23 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.046 Acc@5 93.926
[2022-11-12 01:38:23 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.0%
[2022-11-12 01:38:24 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.888 (1.888)	Loss 0.8268 (0.8268)	Acc@1 79.199 (79.199)	Acc@5 95.508 (95.508)	Mem 23876MB
[2022-11-12 01:38:35 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.612 Acc@5 95.186
[2022-11-12 01:38:35 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.6%
[2022-11-12 01:38:35 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.61% at 96 epoch
[2022-11-12 01:38:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][0/1251]	eta 0:50:30 lr 0.000766	time 2.4221 (2.4221)	loss 3.1265 (3.1265)	grad_norm 1.2500 (1.2500)	mem 23876MB
[2022-11-12 01:39:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][50/1251]	eta 0:15:40 lr 0.000766	time 0.8210 (0.7830)	loss 3.7717 (3.4868)	grad_norm 1.2064 (1.4493)	mem 23876MB
[2022-11-12 01:39:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][100/1251]	eta 0:14:39 lr 0.000765	time 0.7336 (0.7641)	loss 3.7049 (3.4244)	grad_norm 1.4056 (1.4496)	mem 23876MB
[2022-11-12 01:40:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][150/1251]	eta 0:13:56 lr 0.000765	time 0.7361 (0.7597)	loss 4.1763 (3.4637)	grad_norm 1.5436 (1.4374)	mem 23876MB
[2022-11-12 01:41:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][200/1251]	eta 0:13:14 lr 0.000765	time 0.7368 (0.7561)	loss 3.4296 (3.4626)	grad_norm 1.3437 (1.4252)	mem 23876MB
[2022-11-12 01:41:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][250/1251]	eta 0:12:35 lr 0.000765	time 0.7379 (0.7543)	loss 3.2468 (3.4585)	grad_norm 1.4038 (1.4214)	mem 23876MB
[2022-11-12 01:42:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][300/1251]	eta 0:11:56 lr 0.000765	time 0.7356 (0.7531)	loss 3.9065 (3.4583)	grad_norm 1.4103 (1.4177)	mem 23876MB
[2022-11-12 01:42:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][350/1251]	eta 0:11:17 lr 0.000765	time 0.7399 (0.7524)	loss 2.5586 (3.4553)	grad_norm 1.3426 (1.4110)	mem 23876MB
[2022-11-12 01:43:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][400/1251]	eta 0:10:40 lr 0.000764	time 0.7408 (0.7522)	loss 3.7955 (3.4621)	grad_norm 1.3981 (1.4120)	mem 23876MB
[2022-11-12 01:44:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][450/1251]	eta 0:10:01 lr 0.000764	time 0.7394 (0.7515)	loss 3.6125 (3.4514)	grad_norm 1.4609 (1.4073)	mem 23876MB
[2022-11-12 01:44:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][500/1251]	eta 0:09:24 lr 0.000764	time 0.7427 (0.7514)	loss 3.6647 (3.4462)	grad_norm 1.3327 (1.4080)	mem 23876MB
[2022-11-12 01:45:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][550/1251]	eta 0:08:46 lr 0.000764	time 0.7345 (0.7509)	loss 3.9471 (3.4371)	grad_norm 1.4339 (1.4131)	mem 23876MB
[2022-11-12 01:46:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][600/1251]	eta 0:08:08 lr 0.000764	time 0.7476 (0.7506)	loss 3.9954 (3.4365)	grad_norm 1.3146 (1.4111)	mem 23876MB
[2022-11-12 01:46:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][650/1251]	eta 0:07:30 lr 0.000764	time 0.7381 (0.7504)	loss 3.3790 (3.4371)	grad_norm 1.1921 (1.4131)	mem 23876MB
[2022-11-12 01:47:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][700/1251]	eta 0:06:53 lr 0.000763	time 0.7431 (0.7502)	loss 3.5115 (3.4333)	grad_norm 1.5454 (1.4100)	mem 23876MB
[2022-11-12 01:47:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][750/1251]	eta 0:06:15 lr 0.000763	time 0.7390 (0.7501)	loss 2.6790 (3.4263)	grad_norm 1.5894 (1.4063)	mem 23876MB
[2022-11-12 01:48:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][800/1251]	eta 0:05:38 lr 0.000763	time 0.7394 (0.7499)	loss 4.1098 (3.4282)	grad_norm 1.4279 (1.4083)	mem 23876MB
[2022-11-12 01:49:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][850/1251]	eta 0:05:00 lr 0.000763	time 0.7378 (0.7497)	loss 3.6477 (3.4300)	grad_norm 1.2339 (1.4098)	mem 23876MB
[2022-11-12 01:49:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][900/1251]	eta 0:04:23 lr 0.000763	time 0.7406 (0.7497)	loss 4.1327 (3.4316)	grad_norm 1.4224 (1.4100)	mem 23876MB
[2022-11-12 01:50:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][950/1251]	eta 0:03:45 lr 0.000762	time 0.7403 (0.7495)	loss 3.4751 (3.4256)	grad_norm 1.5569 (1.4091)	mem 23876MB
[2022-11-12 01:51:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][1000/1251]	eta 0:03:08 lr 0.000762	time 0.7427 (0.7494)	loss 2.6668 (3.4236)	grad_norm 1.4433 (1.4091)	mem 23876MB
[2022-11-12 01:51:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][1050/1251]	eta 0:02:30 lr 0.000762	time 0.7400 (0.7493)	loss 3.8113 (3.4231)	grad_norm 1.4479 (1.4102)	mem 23876MB
[2022-11-12 01:52:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][1100/1251]	eta 0:01:53 lr 0.000762	time 0.7445 (0.7492)	loss 3.3486 (3.4158)	grad_norm 1.4029 (1.4096)	mem 23876MB
[2022-11-12 01:52:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][1150/1251]	eta 0:01:15 lr 0.000762	time 0.7380 (0.7491)	loss 3.1234 (3.4158)	grad_norm 1.4134 (1.4103)	mem 23876MB
[2022-11-12 01:53:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][1200/1251]	eta 0:00:38 lr 0.000762	time 0.7427 (0.7491)	loss 3.1373 (3.4201)	grad_norm 1.2876 (1.4104)	mem 23876MB
[2022-11-12 01:54:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [97/300][1250/1251]	eta 0:00:00 lr 0.000761	time 0.7289 (0.7489)	loss 3.8949 (3.4151)	grad_norm 1.5042 (1.4090)	mem 23876MB
[2022-11-12 01:54:12 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 97 training takes 0:15:37
[2022-11-12 01:54:12 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_97.pth saving......
[2022-11-12 01:54:14 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_97.pth saved !!!
[2022-11-12 01:54:15 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.658 (1.658)	Loss 0.9234 (0.9234)	Acc@1 78.906 (78.906)	Acc@5 94.336 (94.336)	Mem 23876MB
[2022-11-12 01:54:26 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.260 Acc@5 94.152
[2022-11-12 01:54:26 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.3%
[2022-11-12 01:54:28 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.860 (1.860)	Loss 0.8590 (0.8590)	Acc@1 80.078 (80.078)	Acc@5 94.434 (94.434)	Mem 23876MB
[2022-11-12 01:54:39 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.640 Acc@5 95.202
[2022-11-12 01:54:39 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.6%
[2022-11-12 01:54:39 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.64% at 97 epoch
[2022-11-12 01:54:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][0/1251]	eta 0:50:15 lr 0.000761	time 2.4108 (2.4108)	loss 3.8203 (3.8203)	grad_norm 1.4121 (1.4121)	mem 23876MB
[2022-11-12 01:55:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][50/1251]	eta 0:15:41 lr 0.000761	time 0.7370 (0.7839)	loss 3.4107 (3.4498)	grad_norm 1.2237 (1.3836)	mem 23876MB
[2022-11-12 01:55:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][100/1251]	eta 0:14:40 lr 0.000761	time 0.7381 (0.7653)	loss 3.9126 (3.4159)	grad_norm 1.5024 (1.4038)	mem 23876MB
[2022-11-12 01:56:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][150/1251]	eta 0:13:56 lr 0.000761	time 0.7444 (0.7596)	loss 3.6153 (3.3858)	grad_norm 1.4791 (1.4036)	mem 23876MB
[2022-11-12 01:57:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][200/1251]	eta 0:13:15 lr 0.000761	time 0.7394 (0.7565)	loss 4.0434 (3.3974)	grad_norm 1.3523 (1.3995)	mem 23876MB
[2022-11-12 01:57:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][250/1251]	eta 0:12:35 lr 0.000761	time 0.7440 (0.7547)	loss 2.9464 (3.4159)	grad_norm 1.3805 (1.3931)	mem 23876MB
[2022-11-12 01:58:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][300/1251]	eta 0:11:56 lr 0.000760	time 0.7413 (0.7533)	loss 3.6402 (3.4151)	grad_norm 1.2721 (1.3915)	mem 23876MB
[2022-11-12 01:59:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][350/1251]	eta 0:11:17 lr 0.000760	time 0.7443 (0.7522)	loss 3.7692 (3.4094)	grad_norm 1.3330 (nan)	mem 23876MB
[2022-11-12 01:59:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][400/1251]	eta 0:10:39 lr 0.000760	time 0.7368 (0.7512)	loss 3.1875 (3.4120)	grad_norm 1.3588 (nan)	mem 23876MB
[2022-11-12 02:00:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][450/1251]	eta 0:10:01 lr 0.000760	time 0.7365 (0.7508)	loss 3.8325 (3.4258)	grad_norm 1.6050 (nan)	mem 23876MB
[2022-11-12 02:00:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][500/1251]	eta 0:09:23 lr 0.000760	time 0.7383 (0.7503)	loss 3.1894 (3.4124)	grad_norm 1.4822 (nan)	mem 23876MB
[2022-11-12 02:01:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][550/1251]	eta 0:08:45 lr 0.000759	time 0.7345 (0.7499)	loss 2.3659 (3.4043)	grad_norm 1.5686 (nan)	mem 23876MB
[2022-11-12 02:02:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][600/1251]	eta 0:08:08 lr 0.000759	time 0.7257 (0.7497)	loss 3.6916 (3.4044)	grad_norm 1.6195 (nan)	mem 23876MB
[2022-11-12 02:02:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][650/1251]	eta 0:07:30 lr 0.000759	time 0.7377 (0.7496)	loss 3.3077 (3.3949)	grad_norm 1.4654 (nan)	mem 23876MB
[2022-11-12 02:03:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][700/1251]	eta 0:06:52 lr 0.000759	time 0.7490 (0.7492)	loss 2.8861 (3.4031)	grad_norm 1.6313 (nan)	mem 23876MB
[2022-11-12 02:04:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][750/1251]	eta 0:06:15 lr 0.000759	time 0.8023 (0.7492)	loss 2.8246 (3.4017)	grad_norm 1.4733 (nan)	mem 23876MB
[2022-11-12 02:04:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][800/1251]	eta 0:05:37 lr 0.000759	time 0.7325 (0.7488)	loss 2.9056 (3.4023)	grad_norm 1.2248 (nan)	mem 23876MB
[2022-11-12 02:05:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][850/1251]	eta 0:05:00 lr 0.000758	time 0.7344 (0.7489)	loss 3.8574 (3.3924)	grad_norm 1.6612 (nan)	mem 23876MB
[2022-11-12 02:05:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][900/1251]	eta 0:04:22 lr 0.000758	time 0.7505 (0.7488)	loss 3.0987 (3.3988)	grad_norm 1.2770 (nan)	mem 23876MB
[2022-11-12 02:06:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][950/1251]	eta 0:03:45 lr 0.000758	time 0.7395 (0.7487)	loss 3.8070 (3.4076)	grad_norm 1.4328 (nan)	mem 23876MB
[2022-11-12 02:07:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][1000/1251]	eta 0:03:07 lr 0.000758	time 0.7407 (0.7486)	loss 3.9812 (3.4082)	grad_norm 1.3100 (nan)	mem 23876MB
[2022-11-12 02:07:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][1050/1251]	eta 0:02:30 lr 0.000758	time 0.7401 (0.7486)	loss 3.7298 (3.4105)	grad_norm 1.3080 (nan)	mem 23876MB
[2022-11-12 02:08:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][1100/1251]	eta 0:01:53 lr 0.000758	time 0.7383 (0.7485)	loss 3.7163 (3.4092)	grad_norm 1.4358 (nan)	mem 23876MB
[2022-11-12 02:09:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][1150/1251]	eta 0:01:15 lr 0.000757	time 0.7440 (0.7485)	loss 4.0065 (3.4101)	grad_norm 1.6234 (nan)	mem 23876MB
[2022-11-12 02:09:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][1200/1251]	eta 0:00:38 lr 0.000757	time 0.7379 (0.7484)	loss 2.5840 (3.4125)	grad_norm 1.3881 (nan)	mem 23876MB
[2022-11-12 02:10:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [98/300][1250/1251]	eta 0:00:00 lr 0.000757	time 0.7264 (0.7483)	loss 3.6486 (3.4111)	grad_norm 1.3563 (nan)	mem 23876MB
[2022-11-12 02:10:15 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 98 training takes 0:15:36
[2022-11-12 02:10:15 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_98.pth saving......
[2022-11-12 02:10:16 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_98.pth saved !!!
[2022-11-12 02:10:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.685 (1.685)	Loss 0.9594 (0.9594)	Acc@1 76.270 (76.270)	Acc@5 94.238 (94.238)	Mem 23876MB
[2022-11-12 02:10:28 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.284 Acc@5 94.076
[2022-11-12 02:10:28 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.3%
[2022-11-12 02:10:30 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.870 (1.870)	Loss 0.8298 (0.8298)	Acc@1 79.883 (79.883)	Acc@5 95.020 (95.020)	Mem 23876MB
[2022-11-12 02:10:41 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.618 Acc@5 95.256
[2022-11-12 02:10:41 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.6%
[2022-11-12 02:10:41 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.64% at 97 epoch
[2022-11-12 02:10:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][0/1251]	eta 0:51:37 lr 0.000757	time 2.4757 (2.4757)	loss 2.7877 (2.7877)	grad_norm 1.3309 (1.3309)	mem 23876MB
[2022-11-12 02:11:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][50/1251]	eta 0:15:40 lr 0.000757	time 0.7353 (0.7827)	loss 3.7507 (3.4495)	grad_norm 1.3776 (1.4239)	mem 23876MB
[2022-11-12 02:11:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][100/1251]	eta 0:14:39 lr 0.000757	time 0.7319 (0.7639)	loss 2.2357 (3.3490)	grad_norm 1.3058 (1.4098)	mem 23876MB
[2022-11-12 02:12:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][150/1251]	eta 0:13:54 lr 0.000756	time 0.7418 (0.7582)	loss 3.5156 (3.3209)	grad_norm 1.5473 (1.4249)	mem 23876MB
[2022-11-12 02:13:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][200/1251]	eta 0:13:13 lr 0.000756	time 0.8081 (0.7552)	loss 2.7354 (3.3460)	grad_norm 1.3519 (1.4220)	mem 23876MB
[2022-11-12 02:13:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][250/1251]	eta 0:12:33 lr 0.000756	time 0.7453 (0.7532)	loss 3.6239 (3.3588)	grad_norm 1.3393 (1.4219)	mem 23876MB
[2022-11-12 02:14:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][300/1251]	eta 0:11:55 lr 0.000756	time 0.7381 (0.7526)	loss 2.6580 (3.3529)	grad_norm 1.5965 (1.4207)	mem 23876MB
[2022-11-12 02:15:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][350/1251]	eta 0:11:17 lr 0.000756	time 0.7415 (0.7516)	loss 2.9237 (3.3735)	grad_norm 1.3052 (1.4197)	mem 23876MB
[2022-11-12 02:15:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][400/1251]	eta 0:10:39 lr 0.000756	time 0.7389 (0.7511)	loss 2.9624 (3.3888)	grad_norm 1.4150 (1.4173)	mem 23876MB
[2022-11-12 02:16:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][450/1251]	eta 0:10:01 lr 0.000755	time 0.7410 (0.7505)	loss 2.7272 (3.3935)	grad_norm 1.2060 (1.4155)	mem 23876MB
[2022-11-12 02:16:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][500/1251]	eta 0:09:23 lr 0.000755	time 0.7418 (0.7500)	loss 2.9941 (3.3923)	grad_norm 1.8174 (1.4160)	mem 23876MB
[2022-11-12 02:17:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][550/1251]	eta 0:08:45 lr 0.000755	time 0.7522 (0.7499)	loss 2.2310 (3.3805)	grad_norm 1.3328 (1.4157)	mem 23876MB
[2022-11-12 02:18:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][600/1251]	eta 0:08:08 lr 0.000755	time 0.8170 (0.7497)	loss 3.9038 (3.3834)	grad_norm 1.2917 (1.4152)	mem 23876MB
[2022-11-12 02:18:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][650/1251]	eta 0:07:30 lr 0.000755	time 0.7423 (0.7494)	loss 3.9758 (3.3862)	grad_norm 1.4070 (1.4143)	mem 23876MB
[2022-11-12 02:19:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][700/1251]	eta 0:06:52 lr 0.000754	time 0.7379 (0.7490)	loss 3.1307 (3.3843)	grad_norm 1.5037 (1.4125)	mem 23876MB
[2022-11-12 02:20:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][750/1251]	eta 0:06:15 lr 0.000754	time 0.7427 (0.7491)	loss 3.8883 (3.3849)	grad_norm 1.3333 (1.4119)	mem 23876MB
[2022-11-12 02:20:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][800/1251]	eta 0:05:37 lr 0.000754	time 0.7402 (0.7490)	loss 3.8011 (3.3863)	grad_norm 1.5263 (1.4137)	mem 23876MB
[2022-11-12 02:21:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][850/1251]	eta 0:05:00 lr 0.000754	time 0.7406 (0.7490)	loss 3.2482 (3.3929)	grad_norm 1.3313 (1.4127)	mem 23876MB
[2022-11-12 02:21:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][900/1251]	eta 0:04:22 lr 0.000754	time 0.7460 (0.7487)	loss 2.5967 (3.3967)	grad_norm 1.3602 (1.4116)	mem 23876MB
[2022-11-12 02:22:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][950/1251]	eta 0:03:45 lr 0.000754	time 0.7383 (0.7488)	loss 4.0594 (3.3991)	grad_norm 1.4909 (1.4102)	mem 23876MB
[2022-11-12 02:23:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][1000/1251]	eta 0:03:07 lr 0.000753	time 0.8222 (0.7487)	loss 2.6991 (3.4024)	grad_norm 1.3338 (1.4101)	mem 23876MB
[2022-11-12 02:23:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][1050/1251]	eta 0:02:30 lr 0.000753	time 0.7426 (0.7485)	loss 2.6852 (3.4044)	grad_norm 1.4031 (1.4099)	mem 23876MB
[2022-11-12 02:24:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][1100/1251]	eta 0:01:53 lr 0.000753	time 0.7334 (0.7484)	loss 2.4918 (3.4052)	grad_norm 1.3939 (1.4108)	mem 23876MB
[2022-11-12 02:25:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][1150/1251]	eta 0:01:15 lr 0.000753	time 0.7351 (0.7484)	loss 3.9569 (3.4049)	grad_norm 1.6449 (1.4109)	mem 23876MB
[2022-11-12 02:25:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][1200/1251]	eta 0:00:38 lr 0.000753	time 0.7357 (0.7482)	loss 3.8089 (3.4081)	grad_norm 1.4110 (1.4118)	mem 23876MB
[2022-11-12 02:26:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [99/300][1250/1251]	eta 0:00:00 lr 0.000753	time 0.7355 (0.7481)	loss 4.0772 (3.4122)	grad_norm 1.4572 (1.4141)	mem 23876MB
[2022-11-12 02:26:17 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 99 training takes 0:15:36
[2022-11-12 02:26:18 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_99.pth saving......
[2022-11-12 02:26:19 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_99.pth saved !!!
[2022-11-12 02:26:20 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.623 (1.623)	Loss 0.9385 (0.9385)	Acc@1 76.367 (76.367)	Acc@5 94.434 (94.434)	Mem 23876MB
[2022-11-12 02:26:31 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.210 Acc@5 94.036
[2022-11-12 02:26:31 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.2%
[2022-11-12 02:26:33 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.049 (2.049)	Loss 0.8674 (0.8674)	Acc@1 80.078 (80.078)	Acc@5 95.410 (95.410)	Mem 23876MB
[2022-11-12 02:26:44 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.664 Acc@5 95.310
[2022-11-12 02:26:44 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.7%
[2022-11-12 02:26:44 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.66% at 99 epoch
[2022-11-12 02:26:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][0/1251]	eta 0:50:20 lr 0.000753	time 2.4141 (2.4141)	loss 2.6816 (2.6816)	grad_norm 1.4932 (1.4932)	mem 23876MB
[2022-11-12 02:27:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][50/1251]	eta 0:15:41 lr 0.000752	time 0.7320 (0.7839)	loss 4.0093 (3.4620)	grad_norm 1.3463 (1.4356)	mem 23876MB
[2022-11-12 02:28:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][100/1251]	eta 0:14:41 lr 0.000752	time 0.7384 (0.7657)	loss 2.6872 (3.4103)	grad_norm 1.4119 (1.4293)	mem 23876MB
[2022-11-12 02:28:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][150/1251]	eta 0:13:57 lr 0.000752	time 0.7384 (0.7605)	loss 3.8753 (3.3411)	grad_norm 1.4898 (1.4196)	mem 23876MB
[2022-11-12 02:29:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][200/1251]	eta 0:13:15 lr 0.000752	time 0.8216 (0.7571)	loss 2.8976 (3.3323)	grad_norm 1.2792 (1.4200)	mem 23876MB
[2022-11-12 02:29:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][250/1251]	eta 0:12:35 lr 0.000752	time 0.7416 (0.7547)	loss 3.1600 (3.3526)	grad_norm 1.4658 (1.4156)	mem 23876MB
[2022-11-12 02:30:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][300/1251]	eta 0:11:57 lr 0.000751	time 0.7428 (0.7539)	loss 3.0432 (3.3476)	grad_norm 1.2216 (1.4138)	mem 23876MB
[2022-11-12 02:31:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][350/1251]	eta 0:11:18 lr 0.000751	time 0.7368 (0.7529)	loss 3.3033 (3.3659)	grad_norm 1.3169 (1.4212)	mem 23876MB
[2022-11-12 02:31:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][400/1251]	eta 0:10:40 lr 0.000751	time 0.7446 (0.7524)	loss 3.4298 (3.3612)	grad_norm 1.3327 (1.4267)	mem 23876MB
[2022-11-12 02:32:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][450/1251]	eta 0:10:02 lr 0.000751	time 0.7376 (0.7521)	loss 2.9094 (3.3661)	grad_norm 1.3246 (1.4270)	mem 23876MB
[2022-11-12 02:33:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][500/1251]	eta 0:09:24 lr 0.000751	time 0.7387 (0.7512)	loss 3.4448 (3.3824)	grad_norm 1.5346 (1.4266)	mem 23876MB
[2022-11-12 02:33:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][550/1251]	eta 0:08:46 lr 0.000751	time 0.7422 (0.7511)	loss 3.9627 (3.3883)	grad_norm 1.3483 (1.4272)	mem 23876MB
[2022-11-12 02:34:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][600/1251]	eta 0:08:08 lr 0.000750	time 0.8131 (0.7508)	loss 3.0154 (3.3962)	grad_norm 1.5507 (1.4268)	mem 23876MB
[2022-11-12 02:34:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][650/1251]	eta 0:07:31 lr 0.000750	time 0.7381 (0.7506)	loss 2.6720 (3.3900)	grad_norm 1.5121 (1.4260)	mem 23876MB
[2022-11-12 02:35:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][700/1251]	eta 0:06:53 lr 0.000750	time 0.7389 (0.7503)	loss 3.7737 (3.4037)	grad_norm 1.4208 (1.4240)	mem 23876MB
[2022-11-12 02:36:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][750/1251]	eta 0:06:15 lr 0.000750	time 0.7412 (0.7500)	loss 4.0199 (3.3998)	grad_norm 1.5208 (1.4269)	mem 23876MB
[2022-11-12 02:36:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][800/1251]	eta 0:05:38 lr 0.000750	time 0.8342 (0.7499)	loss 3.6670 (3.3964)	grad_norm 1.3821 (1.4276)	mem 23876MB
[2022-11-12 02:37:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][850/1251]	eta 0:05:00 lr 0.000749	time 0.7329 (0.7498)	loss 3.2757 (3.3937)	grad_norm 1.7871 (1.4267)	mem 23876MB
[2022-11-12 02:37:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][900/1251]	eta 0:04:23 lr 0.000749	time 0.8388 (0.7495)	loss 3.5517 (3.3948)	grad_norm 1.4021 (1.4254)	mem 23876MB
[2022-11-12 02:38:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][950/1251]	eta 0:03:45 lr 0.000749	time 0.7399 (0.7495)	loss 3.2636 (3.3969)	grad_norm 1.5318 (1.4266)	mem 23876MB
[2022-11-12 02:39:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][1000/1251]	eta 0:03:08 lr 0.000749	time 0.8230 (0.7495)	loss 3.2252 (3.3943)	grad_norm 1.2864 (1.4254)	mem 23876MB
[2022-11-12 02:39:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][1050/1251]	eta 0:02:30 lr 0.000749	time 0.7440 (0.7492)	loss 3.4208 (3.3914)	grad_norm 1.4317 (1.4249)	mem 23876MB
[2022-11-12 02:40:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][1100/1251]	eta 0:01:53 lr 0.000749	time 0.7361 (0.7492)	loss 2.8536 (3.4000)	grad_norm 1.4455 (1.4243)	mem 23876MB
[2022-11-12 02:41:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][1150/1251]	eta 0:01:15 lr 0.000748	time 0.7391 (0.7491)	loss 2.5887 (3.4003)	grad_norm 1.3752 (1.4229)	mem 23876MB
[2022-11-12 02:41:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][1200/1251]	eta 0:00:38 lr 0.000748	time 0.7422 (0.7491)	loss 2.7370 (3.3991)	grad_norm 1.3423 (1.4234)	mem 23876MB
[2022-11-12 02:42:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [100/300][1250/1251]	eta 0:00:00 lr 0.000748	time 0.7268 (0.7490)	loss 3.3864 (3.4000)	grad_norm 1.4762 (1.4232)	mem 23876MB
[2022-11-12 02:42:21 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 100 training takes 0:15:37
[2022-11-12 02:42:21 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_100.pth saving......
[2022-11-12 02:42:22 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_100.pth saved !!!
[2022-11-12 02:42:24 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.638 (1.638)	Loss 0.9149 (0.9149)	Acc@1 78.516 (78.516)	Acc@5 94.824 (94.824)	Mem 23876MB
[2022-11-12 02:42:34 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.220 Acc@5 93.998
[2022-11-12 02:42:34 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.2%
[2022-11-12 02:42:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.821 (1.821)	Loss 0.7754 (0.7754)	Acc@1 80.762 (80.762)	Acc@5 96.484 (96.484)	Mem 23876MB
[2022-11-12 02:42:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.734 Acc@5 95.302
[2022-11-12 02:42:47 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.7%
[2022-11-12 02:42:47 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.73% at 100 epoch
[2022-11-12 02:42:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][0/1251]	eta 0:49:37 lr 0.000748	time 2.3798 (2.3798)	loss 2.8884 (2.8884)	grad_norm 1.2316 (1.2316)	mem 23876MB
[2022-11-12 02:43:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][50/1251]	eta 0:15:41 lr 0.000748	time 0.7376 (0.7837)	loss 3.6779 (3.3270)	grad_norm 1.4264 (1.4141)	mem 23876MB
[2022-11-12 02:44:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][100/1251]	eta 0:14:41 lr 0.000748	time 0.7350 (0.7655)	loss 3.5865 (3.4040)	grad_norm 1.3682 (1.4116)	mem 23876MB
[2022-11-12 02:44:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][150/1251]	eta 0:13:56 lr 0.000747	time 0.7373 (0.7595)	loss 3.5996 (3.4146)	grad_norm 1.2763 (1.4274)	mem 23876MB
[2022-11-12 02:45:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][200/1251]	eta 0:13:14 lr 0.000747	time 0.7399 (0.7561)	loss 3.4723 (3.4153)	grad_norm 1.2439 (1.4276)	mem 23876MB
[2022-11-12 02:45:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][250/1251]	eta 0:12:35 lr 0.000747	time 0.7393 (0.7544)	loss 2.8510 (3.3849)	grad_norm 1.5276 (1.4235)	mem 23876MB
[2022-11-12 02:46:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][300/1251]	eta 0:11:56 lr 0.000747	time 0.7388 (0.7534)	loss 3.2987 (3.3821)	grad_norm 1.4217 (1.4242)	mem 23876MB
[2022-11-12 02:47:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][350/1251]	eta 0:11:17 lr 0.000747	time 0.7369 (0.7523)	loss 3.0637 (3.3878)	grad_norm 1.3561 (1.4235)	mem 23876MB
[2022-11-12 02:47:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][400/1251]	eta 0:10:39 lr 0.000747	time 0.7443 (0.7520)	loss 2.8211 (3.3773)	grad_norm 1.3762 (inf)	mem 23876MB
[2022-11-12 02:48:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][450/1251]	eta 0:10:01 lr 0.000746	time 0.7379 (0.7511)	loss 2.6284 (3.3795)	grad_norm 1.6233 (inf)	mem 23876MB
[2022-11-12 02:49:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][500/1251]	eta 0:09:23 lr 0.000746	time 0.7361 (0.7509)	loss 2.7885 (3.3810)	grad_norm 1.2527 (inf)	mem 23876MB
[2022-11-12 02:49:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][550/1251]	eta 0:08:46 lr 0.000746	time 0.7378 (0.7506)	loss 3.3852 (3.3849)	grad_norm 1.3713 (inf)	mem 23876MB
[2022-11-12 02:50:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][600/1251]	eta 0:08:08 lr 0.000746	time 0.7405 (0.7503)	loss 3.0196 (3.3891)	grad_norm 1.2519 (inf)	mem 23876MB
[2022-11-12 02:50:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][650/1251]	eta 0:07:30 lr 0.000746	time 0.7383 (0.7502)	loss 3.9604 (3.3889)	grad_norm 1.4526 (inf)	mem 23876MB
[2022-11-12 02:51:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][700/1251]	eta 0:06:53 lr 0.000745	time 0.7397 (0.7498)	loss 3.7483 (3.3867)	grad_norm 1.4831 (inf)	mem 23876MB
[2022-11-12 02:52:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][750/1251]	eta 0:06:15 lr 0.000745	time 0.7372 (0.7495)	loss 3.3178 (3.3922)	grad_norm 1.7466 (inf)	mem 23876MB
[2022-11-12 02:52:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][800/1251]	eta 0:05:37 lr 0.000745	time 0.7517 (0.7494)	loss 3.7780 (3.3978)	grad_norm 1.4393 (inf)	mem 23876MB
[2022-11-12 02:53:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][850/1251]	eta 0:05:00 lr 0.000745	time 0.7433 (0.7495)	loss 3.7329 (3.3965)	grad_norm 1.4437 (inf)	mem 23876MB
[2022-11-12 02:54:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][900/1251]	eta 0:04:22 lr 0.000745	time 0.7433 (0.7493)	loss 3.2625 (3.3957)	grad_norm 1.4512 (inf)	mem 23876MB
[2022-11-12 02:54:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][950/1251]	eta 0:03:45 lr 0.000745	time 0.7368 (0.7491)	loss 2.4843 (3.3972)	grad_norm 1.3864 (inf)	mem 23876MB
[2022-11-12 02:55:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][1000/1251]	eta 0:03:07 lr 0.000744	time 0.8243 (0.7489)	loss 3.1386 (3.3967)	grad_norm 1.3711 (inf)	mem 23876MB
[2022-11-12 02:55:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][1050/1251]	eta 0:02:30 lr 0.000744	time 0.7419 (0.7489)	loss 3.9149 (3.3964)	grad_norm 1.3807 (inf)	mem 23876MB
[2022-11-12 02:56:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][1100/1251]	eta 0:01:53 lr 0.000744	time 0.7439 (0.7488)	loss 3.3386 (3.3968)	grad_norm 1.5942 (inf)	mem 23876MB
[2022-11-12 02:57:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][1150/1251]	eta 0:01:15 lr 0.000744	time 0.7393 (0.7488)	loss 4.0829 (3.3978)	grad_norm 1.6599 (inf)	mem 23876MB
[2022-11-12 02:57:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][1200/1251]	eta 0:00:38 lr 0.000744	time 0.7534 (0.7486)	loss 3.8248 (3.3988)	grad_norm 1.3606 (inf)	mem 23876MB
[2022-11-12 02:58:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [101/300][1250/1251]	eta 0:00:00 lr 0.000743	time 0.7267 (0.7484)	loss 4.0599 (3.4006)	grad_norm 1.3773 (inf)	mem 23876MB
[2022-11-12 02:58:24 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 101 training takes 0:15:36
[2022-11-12 02:58:24 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_101.pth saving......
[2022-11-12 02:58:25 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_101.pth saved !!!
[2022-11-12 02:58:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.640 (1.640)	Loss 0.9305 (0.9305)	Acc@1 76.855 (76.855)	Acc@5 94.238 (94.238)	Mem 23876MB
[2022-11-12 02:58:37 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.188 Acc@5 94.044
[2022-11-12 02:58:37 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.2%
[2022-11-12 02:58:39 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.763 (1.763)	Loss 0.9114 (0.9114)	Acc@1 76.953 (76.953)	Acc@5 94.238 (94.238)	Mem 23876MB
[2022-11-12 02:58:50 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.766 Acc@5 95.354
[2022-11-12 02:58:50 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.8%
[2022-11-12 02:58:50 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.77% at 101 epoch
[2022-11-12 02:58:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][0/1251]	eta 0:51:52 lr 0.000743	time 2.4878 (2.4878)	loss 3.7227 (3.7227)	grad_norm 1.3303 (1.3303)	mem 23876MB
[2022-11-12 02:59:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][50/1251]	eta 0:15:41 lr 0.000743	time 0.8205 (0.7839)	loss 4.1359 (3.3559)	grad_norm 1.5191 (1.4369)	mem 23876MB
[2022-11-12 03:00:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][100/1251]	eta 0:14:40 lr 0.000743	time 0.7402 (0.7654)	loss 3.9545 (3.3318)	grad_norm 1.4605 (1.4197)	mem 23876MB
[2022-11-12 03:00:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][150/1251]	eta 0:13:57 lr 0.000743	time 0.7368 (0.7603)	loss 2.2928 (3.3123)	grad_norm 1.7026 (1.4173)	mem 23876MB
[2022-11-12 03:01:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][200/1251]	eta 0:13:15 lr 0.000743	time 0.7359 (0.7570)	loss 3.5658 (3.3155)	grad_norm 1.8503 (1.4292)	mem 23876MB
[2022-11-12 03:01:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][250/1251]	eta 0:12:35 lr 0.000743	time 0.7414 (0.7552)	loss 3.7042 (3.3268)	grad_norm 1.3881 (1.4261)	mem 23876MB
[2022-11-12 03:02:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][300/1251]	eta 0:11:57 lr 0.000742	time 0.7419 (0.7540)	loss 3.4834 (3.3582)	grad_norm 1.5906 (1.4230)	mem 23876MB
[2022-11-12 03:03:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][350/1251]	eta 0:11:18 lr 0.000742	time 0.7464 (0.7531)	loss 3.1088 (3.3766)	grad_norm 1.3329 (1.4251)	mem 23876MB
[2022-11-12 03:03:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][400/1251]	eta 0:10:40 lr 0.000742	time 0.7427 (0.7524)	loss 3.6481 (3.3809)	grad_norm 1.3078 (1.4277)	mem 23876MB
[2022-11-12 03:04:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][450/1251]	eta 0:10:02 lr 0.000742	time 0.7380 (0.7516)	loss 3.0517 (3.3791)	grad_norm 1.6028 (1.4248)	mem 23876MB
[2022-11-12 03:05:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][500/1251]	eta 0:09:23 lr 0.000742	time 0.7355 (0.7510)	loss 3.1050 (3.3774)	grad_norm 1.2133 (1.4240)	mem 23876MB
[2022-11-12 03:05:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][550/1251]	eta 0:08:46 lr 0.000741	time 0.7385 (0.7508)	loss 3.7092 (3.3686)	grad_norm 1.5857 (1.4250)	mem 23876MB
[2022-11-12 03:06:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][600/1251]	eta 0:08:08 lr 0.000741	time 0.7392 (0.7504)	loss 2.9684 (3.3697)	grad_norm 1.4330 (1.4270)	mem 23876MB
[2022-11-12 03:06:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][650/1251]	eta 0:07:30 lr 0.000741	time 0.7369 (0.7502)	loss 2.6465 (3.3704)	grad_norm 1.3730 (1.4255)	mem 23876MB
[2022-11-12 03:07:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][700/1251]	eta 0:06:53 lr 0.000741	time 0.7372 (0.7500)	loss 4.0253 (3.3724)	grad_norm 1.3302 (1.4246)	mem 23876MB
[2022-11-12 03:08:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][750/1251]	eta 0:06:15 lr 0.000741	time 0.7365 (0.7498)	loss 3.3724 (3.3699)	grad_norm 1.2504 (1.4232)	mem 23876MB
[2022-11-12 03:08:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][800/1251]	eta 0:05:38 lr 0.000741	time 0.7391 (0.7496)	loss 3.6906 (3.3792)	grad_norm 1.3271 (1.4232)	mem 23876MB
[2022-11-12 03:09:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][850/1251]	eta 0:05:00 lr 0.000740	time 0.7339 (0.7494)	loss 2.5215 (3.3825)	grad_norm 1.4735 (1.4258)	mem 23876MB
[2022-11-12 03:10:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][900/1251]	eta 0:04:22 lr 0.000740	time 0.7370 (0.7492)	loss 4.0034 (3.3835)	grad_norm 1.3360 (1.4251)	mem 23876MB
[2022-11-12 03:10:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][950/1251]	eta 0:03:45 lr 0.000740	time 0.7368 (0.7491)	loss 3.6796 (3.3900)	grad_norm 1.3537 (1.4253)	mem 23876MB
[2022-11-12 03:11:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][1000/1251]	eta 0:03:07 lr 0.000740	time 0.7335 (0.7489)	loss 3.8584 (3.3860)	grad_norm 1.4717 (1.4263)	mem 23876MB
[2022-11-12 03:11:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][1050/1251]	eta 0:02:30 lr 0.000740	time 0.7406 (0.7488)	loss 4.0266 (3.3868)	grad_norm 1.4221 (1.4265)	mem 23876MB
[2022-11-12 03:12:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][1100/1251]	eta 0:01:53 lr 0.000739	time 0.7399 (0.7487)	loss 4.0340 (3.3858)	grad_norm 1.4053 (1.4255)	mem 23876MB
[2022-11-12 03:13:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][1150/1251]	eta 0:01:15 lr 0.000739	time 0.7414 (0.7487)	loss 3.1072 (3.3811)	grad_norm 1.3588 (1.4244)	mem 23876MB
[2022-11-12 03:13:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][1200/1251]	eta 0:00:38 lr 0.000739	time 0.7446 (0.7485)	loss 2.9109 (3.3802)	grad_norm 1.2749 (1.4239)	mem 23876MB
[2022-11-12 03:14:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [102/300][1250/1251]	eta 0:00:00 lr 0.000739	time 0.7273 (0.7484)	loss 3.8310 (3.3803)	grad_norm 1.3539 (1.4227)	mem 23876MB
[2022-11-12 03:14:26 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 102 training takes 0:15:36
[2022-11-12 03:14:26 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_102.pth saving......
[2022-11-12 03:14:27 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_102.pth saved !!!
[2022-11-12 03:14:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.698 (1.698)	Loss 1.0274 (1.0274)	Acc@1 75.879 (75.879)	Acc@5 93.164 (93.164)	Mem 23876MB
[2022-11-12 03:14:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.624 Acc@5 94.178
[2022-11-12 03:14:40 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.6%
[2022-11-12 03:14:41 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.765 (1.765)	Loss 0.8204 (0.8204)	Acc@1 80.371 (80.371)	Acc@5 95.410 (95.410)	Mem 23876MB
[2022-11-12 03:14:52 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.838 Acc@5 95.350
[2022-11-12 03:14:52 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.8%
[2022-11-12 03:14:52 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.84% at 102 epoch
[2022-11-12 03:14:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][0/1251]	eta 0:53:26 lr 0.000739	time 2.5632 (2.5632)	loss 3.2430 (3.2430)	grad_norm 1.6310 (1.6310)	mem 23876MB
[2022-11-12 03:15:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][50/1251]	eta 0:15:46 lr 0.000739	time 0.7431 (0.7880)	loss 4.1730 (3.2625)	grad_norm 1.4585 (1.4092)	mem 23876MB
[2022-11-12 03:16:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][100/1251]	eta 0:14:43 lr 0.000739	time 0.7430 (0.7678)	loss 3.7070 (3.3540)	grad_norm 1.5294 (1.4265)	mem 23876MB
[2022-11-12 03:16:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][150/1251]	eta 0:13:57 lr 0.000738	time 0.7418 (0.7609)	loss 3.9456 (3.3371)	grad_norm 1.2948 (1.4178)	mem 23876MB
[2022-11-12 03:17:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][200/1251]	eta 0:13:16 lr 0.000738	time 0.8108 (0.7579)	loss 3.6979 (3.3497)	grad_norm 1.3020 (1.4161)	mem 23876MB
[2022-11-12 03:18:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][250/1251]	eta 0:12:36 lr 0.000738	time 0.8215 (0.7555)	loss 2.5956 (3.3578)	grad_norm 1.3004 (1.4185)	mem 23876MB
[2022-11-12 03:18:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][300/1251]	eta 0:11:57 lr 0.000738	time 0.7347 (0.7544)	loss 2.9050 (3.3450)	grad_norm 1.3707 (1.4212)	mem 23876MB
[2022-11-12 03:19:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][350/1251]	eta 0:11:18 lr 0.000738	time 0.7394 (0.7534)	loss 2.9050 (3.3394)	grad_norm 1.5762 (1.4243)	mem 23876MB
[2022-11-12 03:19:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][400/1251]	eta 0:10:40 lr 0.000737	time 0.7364 (0.7529)	loss 3.9429 (3.3515)	grad_norm 1.4343 (1.4248)	mem 23876MB
[2022-11-12 03:20:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][450/1251]	eta 0:10:02 lr 0.000737	time 0.7361 (0.7521)	loss 3.4033 (3.3609)	grad_norm 1.2901 (1.4235)	mem 23876MB
[2022-11-12 03:21:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][500/1251]	eta 0:09:24 lr 0.000737	time 0.7378 (0.7516)	loss 3.4679 (3.3585)	grad_norm 1.5543 (1.4237)	mem 23876MB
[2022-11-12 03:21:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][550/1251]	eta 0:08:46 lr 0.000737	time 0.7343 (0.7513)	loss 3.3811 (3.3547)	grad_norm 1.3770 (1.4232)	mem 23876MB
[2022-11-12 03:22:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][600/1251]	eta 0:08:08 lr 0.000737	time 0.7361 (0.7510)	loss 2.6349 (3.3619)	grad_norm 1.3509 (1.4244)	mem 23876MB
[2022-11-12 03:23:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][650/1251]	eta 0:07:31 lr 0.000737	time 0.8230 (0.7509)	loss 2.9890 (3.3725)	grad_norm 1.2770 (1.4237)	mem 23876MB
[2022-11-12 03:23:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][700/1251]	eta 0:06:53 lr 0.000736	time 0.7390 (0.7504)	loss 2.2455 (3.3705)	grad_norm 1.4272 (1.4248)	mem 23876MB
[2022-11-12 03:24:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][750/1251]	eta 0:06:15 lr 0.000736	time 0.7379 (0.7502)	loss 3.6123 (3.3745)	grad_norm 1.4781 (1.4282)	mem 23876MB
[2022-11-12 03:24:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][800/1251]	eta 0:05:38 lr 0.000736	time 0.7395 (0.7500)	loss 4.0198 (3.3658)	grad_norm 1.4255 (1.4285)	mem 23876MB
[2022-11-12 03:25:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][850/1251]	eta 0:05:00 lr 0.000736	time 0.7396 (0.7500)	loss 3.8223 (3.3699)	grad_norm 1.4512 (1.4255)	mem 23876MB
[2022-11-12 03:26:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][900/1251]	eta 0:04:23 lr 0.000736	time 0.7398 (0.7498)	loss 3.4283 (3.3673)	grad_norm 1.2560 (1.4258)	mem 23876MB
[2022-11-12 03:26:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][950/1251]	eta 0:03:45 lr 0.000735	time 0.7244 (0.7496)	loss 3.6742 (3.3753)	grad_norm 1.3467 (1.4259)	mem 23876MB
[2022-11-12 03:27:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][1000/1251]	eta 0:03:08 lr 0.000735	time 0.7364 (0.7495)	loss 3.5747 (3.3816)	grad_norm 1.3646 (1.4253)	mem 23876MB
[2022-11-12 03:28:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][1050/1251]	eta 0:02:30 lr 0.000735	time 0.8021 (0.7494)	loss 4.1314 (3.3801)	grad_norm 1.5372 (1.4264)	mem 23876MB
[2022-11-12 03:28:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][1100/1251]	eta 0:01:53 lr 0.000735	time 0.7376 (0.7492)	loss 3.7458 (3.3800)	grad_norm 1.4286 (1.4271)	mem 23876MB
[2022-11-12 03:29:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][1150/1251]	eta 0:01:15 lr 0.000735	time 0.7415 (0.7492)	loss 3.2569 (3.3841)	grad_norm 1.3436 (1.4275)	mem 23876MB
[2022-11-12 03:29:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][1200/1251]	eta 0:00:38 lr 0.000735	time 0.7346 (0.7491)	loss 3.7004 (3.3842)	grad_norm 1.3623 (1.4254)	mem 23876MB
[2022-11-12 03:30:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [103/300][1250/1251]	eta 0:00:00 lr 0.000734	time 0.7284 (0.7489)	loss 3.3075 (3.3848)	grad_norm 1.2540 (1.4255)	mem 23876MB
[2022-11-12 03:30:29 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 103 training takes 0:15:36
[2022-11-12 03:30:29 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_103.pth saving......
[2022-11-12 03:30:30 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_103.pth saved !!!
[2022-11-12 03:30:32 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.600 (1.600)	Loss 0.9227 (0.9227)	Acc@1 77.344 (77.344)	Acc@5 95.410 (95.410)	Mem 23876MB
[2022-11-12 03:30:43 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.298 Acc@5 94.188
[2022-11-12 03:30:43 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.3%
[2022-11-12 03:30:44 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.894 (1.894)	Loss 0.7827 (0.7827)	Acc@1 80.469 (80.469)	Acc@5 95.898 (95.898)	Mem 23876MB
[2022-11-12 03:30:55 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.896 Acc@5 95.360
[2022-11-12 03:30:55 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.9%
[2022-11-12 03:30:55 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.90% at 103 epoch
[2022-11-12 03:30:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][0/1251]	eta 0:51:11 lr 0.000734	time 2.4551 (2.4551)	loss 3.4045 (3.4045)	grad_norm 1.6705 (1.6705)	mem 23876MB
[2022-11-12 03:31:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][50/1251]	eta 0:15:42 lr 0.000734	time 0.7339 (0.7844)	loss 3.5576 (3.2534)	grad_norm 1.2912 (1.4351)	mem 23876MB
[2022-11-12 03:32:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][100/1251]	eta 0:14:40 lr 0.000734	time 0.7403 (0.7652)	loss 3.8141 (3.3469)	grad_norm 1.5116 (1.4418)	mem 23876MB
[2022-11-12 03:32:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][150/1251]	eta 0:13:56 lr 0.000734	time 0.7379 (0.7601)	loss 2.8025 (3.3672)	grad_norm 1.3179 (1.4321)	mem 23876MB
[2022-11-12 03:33:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][200/1251]	eta 0:13:15 lr 0.000734	time 0.7430 (0.7569)	loss 3.2995 (3.3869)	grad_norm 1.5329 (1.4311)	mem 23876MB
[2022-11-12 03:34:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][250/1251]	eta 0:12:35 lr 0.000733	time 0.8104 (0.7547)	loss 3.4098 (3.3702)	grad_norm 1.5786 (1.4313)	mem 23876MB
[2022-11-12 03:34:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][300/1251]	eta 0:11:56 lr 0.000733	time 0.7389 (0.7533)	loss 3.7521 (3.3782)	grad_norm 1.3486 (1.4362)	mem 23876MB
[2022-11-12 03:35:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][350/1251]	eta 0:11:18 lr 0.000733	time 0.7344 (0.7527)	loss 2.8189 (3.3663)	grad_norm 1.4331 (1.4344)	mem 23876MB
[2022-11-12 03:35:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][400/1251]	eta 0:10:40 lr 0.000733	time 0.7369 (0.7523)	loss 3.1067 (3.3783)	grad_norm 1.3583 (1.4336)	mem 23876MB
[2022-11-12 03:36:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][450/1251]	eta 0:10:02 lr 0.000733	time 0.7506 (0.7517)	loss 3.5409 (3.3622)	grad_norm 1.2710 (1.4366)	mem 23876MB
[2022-11-12 03:37:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][500/1251]	eta 0:09:24 lr 0.000732	time 0.7376 (0.7512)	loss 3.6225 (3.3712)	grad_norm 1.4387 (nan)	mem 23876MB
[2022-11-12 03:37:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][550/1251]	eta 0:08:46 lr 0.000732	time 0.7359 (0.7507)	loss 3.5252 (3.3771)	grad_norm 1.2485 (nan)	mem 23876MB
[2022-11-12 03:38:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][600/1251]	eta 0:08:08 lr 0.000732	time 0.7387 (0.7505)	loss 3.5710 (3.3667)	grad_norm 1.3561 (nan)	mem 23876MB
[2022-11-12 03:39:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][650/1251]	eta 0:07:30 lr 0.000732	time 0.7418 (0.7503)	loss 2.3561 (3.3666)	grad_norm 1.4321 (nan)	mem 23876MB
[2022-11-12 03:39:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][700/1251]	eta 0:06:53 lr 0.000732	time 0.7321 (0.7499)	loss 3.5159 (3.3665)	grad_norm 1.3256 (nan)	mem 23876MB
[2022-11-12 03:40:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][750/1251]	eta 0:06:15 lr 0.000732	time 0.8080 (0.7498)	loss 3.4339 (3.3708)	grad_norm 1.3774 (nan)	mem 23876MB
[2022-11-12 03:40:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][800/1251]	eta 0:05:38 lr 0.000731	time 0.7387 (0.7496)	loss 3.8178 (3.3727)	grad_norm 1.5638 (nan)	mem 23876MB
[2022-11-12 03:41:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][850/1251]	eta 0:05:00 lr 0.000731	time 0.7442 (0.7494)	loss 3.7237 (3.3742)	grad_norm 1.3661 (nan)	mem 23876MB
[2022-11-12 03:42:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][900/1251]	eta 0:04:22 lr 0.000731	time 0.7359 (0.7492)	loss 3.4931 (3.3745)	grad_norm 1.3733 (nan)	mem 23876MB
[2022-11-12 03:42:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][950/1251]	eta 0:03:45 lr 0.000731	time 0.8254 (0.7493)	loss 2.5832 (3.3706)	grad_norm 1.7119 (nan)	mem 23876MB
[2022-11-12 03:43:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][1000/1251]	eta 0:03:08 lr 0.000731	time 0.7429 (0.7491)	loss 3.2577 (3.3700)	grad_norm 1.4907 (nan)	mem 23876MB
[2022-11-12 03:44:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][1050/1251]	eta 0:02:30 lr 0.000730	time 0.7368 (0.7490)	loss 3.3331 (3.3719)	grad_norm 1.4491 (nan)	mem 23876MB
[2022-11-12 03:44:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][1100/1251]	eta 0:01:53 lr 0.000730	time 0.7342 (0.7489)	loss 4.0219 (3.3705)	grad_norm 1.5382 (nan)	mem 23876MB
[2022-11-12 03:45:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][1150/1251]	eta 0:01:15 lr 0.000730	time 0.7417 (0.7490)	loss 3.5068 (3.3718)	grad_norm 1.4780 (nan)	mem 23876MB
[2022-11-12 03:45:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][1200/1251]	eta 0:00:38 lr 0.000730	time 0.7355 (0.7490)	loss 2.3236 (3.3724)	grad_norm 1.4902 (nan)	mem 23876MB
[2022-11-12 03:46:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [104/300][1250/1251]	eta 0:00:00 lr 0.000730	time 0.7276 (0.7487)	loss 3.6657 (3.3737)	grad_norm 1.4960 (nan)	mem 23876MB
[2022-11-12 03:46:32 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 104 training takes 0:15:36
[2022-11-12 03:46:32 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_104.pth saving......
[2022-11-12 03:46:33 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_104.pth saved !!!
[2022-11-12 03:46:35 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.771 (1.771)	Loss 0.9076 (0.9076)	Acc@1 78.125 (78.125)	Acc@5 94.922 (94.922)	Mem 23876MB
[2022-11-12 03:46:46 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.280 Acc@5 94.054
[2022-11-12 03:46:46 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.3%
[2022-11-12 03:46:48 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.760 (1.760)	Loss 0.9145 (0.9145)	Acc@1 78.320 (78.320)	Acc@5 95.020 (95.020)	Mem 23876MB
[2022-11-12 03:46:58 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.936 Acc@5 95.372
[2022-11-12 03:46:58 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 79.9%
[2022-11-12 03:46:58 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.94% at 104 epoch
[2022-11-12 03:47:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][0/1251]	eta 0:49:51 lr 0.000730	time 2.3909 (2.3909)	loss 3.6134 (3.6134)	grad_norm 1.3615 (1.3615)	mem 23876MB
[2022-11-12 03:47:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][50/1251]	eta 0:15:39 lr 0.000730	time 0.7376 (0.7822)	loss 3.6864 (3.4092)	grad_norm 1.3645 (1.4456)	mem 23876MB
[2022-11-12 03:48:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][100/1251]	eta 0:14:41 lr 0.000729	time 0.7411 (0.7656)	loss 3.8414 (3.3004)	grad_norm 1.4332 (1.4344)	mem 23876MB
[2022-11-12 03:48:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][150/1251]	eta 0:13:57 lr 0.000729	time 0.7650 (0.7604)	loss 3.5031 (3.3035)	grad_norm 1.2834 (1.4393)	mem 23876MB
[2022-11-12 03:49:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][200/1251]	eta 0:13:15 lr 0.000729	time 0.8163 (0.7568)	loss 3.8990 (3.2964)	grad_norm 1.3720 (1.4266)	mem 23876MB
[2022-11-12 03:50:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][250/1251]	eta 0:12:35 lr 0.000729	time 0.7433 (0.7545)	loss 3.4451 (3.3098)	grad_norm 1.7071 (1.4445)	mem 23876MB
[2022-11-12 03:50:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][300/1251]	eta 0:11:56 lr 0.000729	time 0.7393 (0.7538)	loss 3.4717 (3.3313)	grad_norm 1.4966 (1.4424)	mem 23876MB
[2022-11-12 03:51:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][350/1251]	eta 0:11:18 lr 0.000728	time 0.7476 (0.7525)	loss 3.1602 (3.3238)	grad_norm 1.5131 (1.4418)	mem 23876MB
[2022-11-12 03:52:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][400/1251]	eta 0:10:39 lr 0.000728	time 0.7405 (0.7520)	loss 2.9637 (3.3239)	grad_norm 1.4685 (1.4398)	mem 23876MB
[2022-11-12 03:52:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][450/1251]	eta 0:10:01 lr 0.000728	time 0.8243 (0.7514)	loss 3.5158 (3.3271)	grad_norm 1.3291 (1.4397)	mem 23876MB
[2022-11-12 03:53:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][500/1251]	eta 0:09:23 lr 0.000728	time 0.7413 (0.7507)	loss 2.7166 (3.3329)	grad_norm 1.3372 (1.4408)	mem 23876MB
[2022-11-12 03:53:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][550/1251]	eta 0:08:46 lr 0.000728	time 0.7379 (0.7504)	loss 3.6865 (3.3319)	grad_norm 1.3889 (1.4400)	mem 23876MB
[2022-11-12 03:54:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][600/1251]	eta 0:08:08 lr 0.000728	time 0.8166 (0.7500)	loss 2.8406 (3.3277)	grad_norm 1.3371 (1.4353)	mem 23876MB
[2022-11-12 03:55:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][650/1251]	eta 0:07:30 lr 0.000727	time 0.7368 (0.7496)	loss 3.8690 (3.3337)	grad_norm 1.6406 (1.4339)	mem 23876MB
[2022-11-12 03:55:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][700/1251]	eta 0:06:52 lr 0.000727	time 0.7414 (0.7494)	loss 3.7972 (3.3385)	grad_norm 1.3509 (1.4372)	mem 23876MB
[2022-11-12 03:56:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][750/1251]	eta 0:06:15 lr 0.000727	time 0.7354 (0.7492)	loss 3.9628 (3.3485)	grad_norm 1.8935 (1.4374)	mem 23876MB
[2022-11-12 03:56:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][800/1251]	eta 0:05:37 lr 0.000727	time 0.7438 (0.7491)	loss 3.6747 (3.3474)	grad_norm 1.4987 (1.4383)	mem 23876MB
[2022-11-12 03:57:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][850/1251]	eta 0:05:00 lr 0.000727	time 0.7526 (0.7490)	loss 2.5675 (3.3522)	grad_norm 1.2835 (1.4397)	mem 23876MB
[2022-11-12 03:58:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][900/1251]	eta 0:04:22 lr 0.000726	time 0.7375 (0.7489)	loss 3.2602 (3.3552)	grad_norm 1.3958 (1.4399)	mem 23876MB
[2022-11-12 03:58:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][950/1251]	eta 0:03:45 lr 0.000726	time 0.7388 (0.7488)	loss 3.6306 (3.3595)	grad_norm 1.2746 (1.4387)	mem 23876MB
[2022-11-12 03:59:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][1000/1251]	eta 0:03:07 lr 0.000726	time 0.8106 (0.7488)	loss 3.4908 (3.3567)	grad_norm 1.3340 (1.4386)	mem 23876MB
[2022-11-12 04:00:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][1050/1251]	eta 0:02:30 lr 0.000726	time 0.7408 (0.7485)	loss 3.7374 (3.3610)	grad_norm 1.4778 (1.4390)	mem 23876MB
[2022-11-12 04:00:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][1100/1251]	eta 0:01:53 lr 0.000726	time 0.7428 (0.7485)	loss 2.2110 (3.3629)	grad_norm 1.4751 (1.4386)	mem 23876MB
[2022-11-12 04:01:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][1150/1251]	eta 0:01:15 lr 0.000725	time 0.7382 (0.7483)	loss 3.7819 (3.3674)	grad_norm 1.5391 (1.4385)	mem 23876MB
[2022-11-12 04:01:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][1200/1251]	eta 0:00:38 lr 0.000725	time 0.7437 (0.7483)	loss 3.7130 (3.3706)	grad_norm 1.3827 (1.4375)	mem 23876MB
[2022-11-12 04:02:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [105/300][1250/1251]	eta 0:00:00 lr 0.000725	time 0.7328 (0.7481)	loss 2.6760 (3.3714)	grad_norm 1.2763 (1.4370)	mem 23876MB
[2022-11-12 04:02:34 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 105 training takes 0:15:36
[2022-11-12 04:02:35 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_105.pth saving......
[2022-11-12 04:02:36 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_105.pth saved !!!
[2022-11-12 04:02:37 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.717 (1.717)	Loss 1.0538 (1.0538)	Acc@1 74.805 (74.805)	Acc@5 94.141 (94.141)	Mem 23876MB
[2022-11-12 04:02:48 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.062 Acc@5 94.086
[2022-11-12 04:02:48 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.1%
[2022-11-12 04:02:50 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.875 (1.875)	Loss 0.8569 (0.8569)	Acc@1 79.199 (79.199)	Acc@5 94.336 (94.336)	Mem 23876MB
[2022-11-12 04:03:01 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.990 Acc@5 95.390
[2022-11-12 04:03:01 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.0%
[2022-11-12 04:03:01 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 79.99% at 105 epoch
[2022-11-12 04:03:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][0/1251]	eta 0:49:32 lr 0.000725	time 2.3765 (2.3765)	loss 3.2608 (3.2608)	grad_norm 1.7758 (1.7758)	mem 23876MB
[2022-11-12 04:03:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][50/1251]	eta 0:15:41 lr 0.000725	time 0.7446 (0.7838)	loss 3.8258 (3.3275)	grad_norm 1.6145 (1.5012)	mem 23876MB
[2022-11-12 04:04:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][100/1251]	eta 0:14:39 lr 0.000725	time 0.7365 (0.7642)	loss 3.3181 (3.3128)	grad_norm 1.3747 (1.4600)	mem 23876MB
[2022-11-12 04:04:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][150/1251]	eta 0:13:55 lr 0.000725	time 0.7357 (0.7585)	loss 3.0200 (3.2696)	grad_norm 1.3357 (1.4447)	mem 23876MB
[2022-11-12 04:05:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][200/1251]	eta 0:13:14 lr 0.000724	time 0.7375 (0.7560)	loss 3.6858 (3.3135)	grad_norm 1.3403 (1.4415)	mem 23876MB
[2022-11-12 04:06:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][250/1251]	eta 0:12:34 lr 0.000724	time 0.7363 (0.7540)	loss 3.2425 (3.3521)	grad_norm 1.2983 (1.4382)	mem 23876MB
[2022-11-12 04:06:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][300/1251]	eta 0:11:55 lr 0.000724	time 0.7416 (0.7527)	loss 3.4047 (3.3636)	grad_norm 1.4865 (1.4317)	mem 23876MB
[2022-11-12 04:07:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][350/1251]	eta 0:11:17 lr 0.000724	time 0.7403 (0.7522)	loss 3.2698 (3.3714)	grad_norm 1.6911 (1.4337)	mem 23876MB
[2022-11-12 04:08:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][400/1251]	eta 0:10:39 lr 0.000724	time 0.7405 (0.7516)	loss 3.4837 (3.3723)	grad_norm 1.4166 (1.4347)	mem 23876MB
[2022-11-12 04:08:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][450/1251]	eta 0:10:01 lr 0.000723	time 0.7378 (0.7510)	loss 3.5801 (3.3750)	grad_norm 1.3978 (1.4311)	mem 23876MB
[2022-11-12 04:09:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][500/1251]	eta 0:09:23 lr 0.000723	time 0.7361 (0.7507)	loss 3.5660 (3.3815)	grad_norm 1.5846 (1.4348)	mem 23876MB
[2022-11-12 04:09:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][550/1251]	eta 0:08:46 lr 0.000723	time 0.7381 (0.7505)	loss 2.9968 (3.3825)	grad_norm 1.3537 (1.4363)	mem 23876MB
[2022-11-12 04:10:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][600/1251]	eta 0:08:08 lr 0.000723	time 0.7397 (0.7504)	loss 2.8980 (3.3770)	grad_norm 1.2476 (1.4359)	mem 23876MB
[2022-11-12 04:11:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][650/1251]	eta 0:07:30 lr 0.000723	time 0.7365 (0.7501)	loss 2.9800 (3.3854)	grad_norm 1.3115 (inf)	mem 23876MB
[2022-11-12 04:11:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][700/1251]	eta 0:06:53 lr 0.000722	time 0.7421 (0.7499)	loss 2.7231 (3.3854)	grad_norm 1.5699 (inf)	mem 23876MB
[2022-11-12 04:12:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][750/1251]	eta 0:06:15 lr 0.000722	time 0.7360 (0.7498)	loss 3.7715 (3.3905)	grad_norm 1.4319 (inf)	mem 23876MB
[2022-11-12 04:13:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][800/1251]	eta 0:05:38 lr 0.000722	time 0.7396 (0.7495)	loss 3.6391 (3.3899)	grad_norm 1.3352 (inf)	mem 23876MB
[2022-11-12 04:13:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][850/1251]	eta 0:05:00 lr 0.000722	time 0.7455 (0.7495)	loss 3.5821 (3.3899)	grad_norm 1.3801 (inf)	mem 23876MB
[2022-11-12 04:14:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][900/1251]	eta 0:04:22 lr 0.000722	time 0.7387 (0.7492)	loss 3.6173 (3.3927)	grad_norm 1.4000 (inf)	mem 23876MB
[2022-11-12 04:14:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][950/1251]	eta 0:03:45 lr 0.000722	time 0.7391 (0.7491)	loss 3.9732 (3.3886)	grad_norm 1.4419 (inf)	mem 23876MB
[2022-11-12 04:15:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][1000/1251]	eta 0:03:08 lr 0.000721	time 0.7367 (0.7492)	loss 3.6891 (3.3912)	grad_norm 1.3610 (inf)	mem 23876MB
[2022-11-12 04:16:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][1050/1251]	eta 0:02:30 lr 0.000721	time 0.7382 (0.7490)	loss 2.6183 (3.3885)	grad_norm 1.2072 (inf)	mem 23876MB
[2022-11-12 04:16:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][1100/1251]	eta 0:01:53 lr 0.000721	time 0.7568 (0.7490)	loss 3.9390 (3.3890)	grad_norm 1.3511 (inf)	mem 23876MB
[2022-11-12 04:17:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][1150/1251]	eta 0:01:15 lr 0.000721	time 0.7365 (0.7489)	loss 3.6812 (3.3854)	grad_norm 1.4485 (inf)	mem 23876MB
[2022-11-12 04:18:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][1200/1251]	eta 0:00:38 lr 0.000721	time 0.7395 (0.7488)	loss 2.3253 (3.3811)	grad_norm 1.6563 (inf)	mem 23876MB
[2022-11-12 04:18:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [106/300][1250/1251]	eta 0:00:00 lr 0.000720	time 0.7271 (0.7485)	loss 3.7210 (3.3826)	grad_norm 1.3375 (inf)	mem 23876MB
[2022-11-12 04:18:37 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 106 training takes 0:15:36
[2022-11-12 04:18:37 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_106.pth saving......
[2022-11-12 04:18:38 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_106.pth saved !!!
[2022-11-12 04:18:40 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.609 (1.609)	Loss 0.9834 (0.9834)	Acc@1 78.223 (78.223)	Acc@5 93.359 (93.359)	Mem 23876MB
[2022-11-12 04:18:51 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.260 Acc@5 94.126
[2022-11-12 04:18:51 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.3%
[2022-11-12 04:18:53 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.789 (1.789)	Loss 0.8480 (0.8480)	Acc@1 79.688 (79.688)	Acc@5 95.410 (95.410)	Mem 23876MB
[2022-11-12 04:19:03 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.066 Acc@5 95.404
[2022-11-12 04:19:03 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.1%
[2022-11-12 04:19:03 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.07% at 106 epoch
[2022-11-12 04:19:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][0/1251]	eta 0:49:40 lr 0.000720	time 2.3822 (2.3822)	loss 2.4163 (2.4163)	grad_norm 1.3758 (1.3758)	mem 23876MB
[2022-11-12 04:19:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][50/1251]	eta 0:15:36 lr 0.000720	time 0.7465 (0.7799)	loss 3.4750 (3.3095)	grad_norm 1.4301 (1.4347)	mem 23876MB
[2022-11-12 04:20:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][100/1251]	eta 0:14:40 lr 0.000720	time 0.7404 (0.7650)	loss 3.8306 (3.3202)	grad_norm 1.5278 (1.4206)	mem 23876MB
[2022-11-12 04:20:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][150/1251]	eta 0:13:56 lr 0.000720	time 0.7382 (0.7594)	loss 3.4504 (3.3184)	grad_norm 1.5869 (1.4285)	mem 23876MB
[2022-11-12 04:21:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][200/1251]	eta 0:13:14 lr 0.000720	time 0.8040 (0.7562)	loss 2.6273 (3.3089)	grad_norm 1.5494 (1.4337)	mem 23876MB
[2022-11-12 04:22:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][250/1251]	eta 0:12:35 lr 0.000720	time 0.7402 (0.7549)	loss 3.2349 (3.3282)	grad_norm 1.7474 (1.4389)	mem 23876MB
[2022-11-12 04:22:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][300/1251]	eta 0:11:56 lr 0.000719	time 0.7516 (0.7537)	loss 3.6312 (3.3393)	grad_norm 1.4314 (1.4381)	mem 23876MB
[2022-11-12 04:23:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][350/1251]	eta 0:11:18 lr 0.000719	time 0.7371 (0.7535)	loss 3.8323 (3.3604)	grad_norm 1.5668 (1.4410)	mem 23876MB
[2022-11-12 04:24:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][400/1251]	eta 0:10:40 lr 0.000719	time 0.7422 (0.7525)	loss 3.9642 (3.3615)	grad_norm 1.2792 (1.4378)	mem 23876MB
[2022-11-12 04:24:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][450/1251]	eta 0:10:02 lr 0.000719	time 0.7440 (0.7521)	loss 3.2198 (3.3597)	grad_norm 1.5659 (1.4397)	mem 23876MB
[2022-11-12 04:25:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][500/1251]	eta 0:09:24 lr 0.000719	time 0.7359 (0.7514)	loss 2.7606 (3.3618)	grad_norm 1.3433 (1.4387)	mem 23876MB
[2022-11-12 04:25:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][550/1251]	eta 0:08:46 lr 0.000718	time 0.7377 (0.7514)	loss 3.9329 (3.3608)	grad_norm 1.4794 (1.4356)	mem 23876MB
[2022-11-12 04:26:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][600/1251]	eta 0:08:08 lr 0.000718	time 0.7437 (0.7510)	loss 2.5281 (3.3559)	grad_norm 1.2738 (1.4379)	mem 23876MB
[2022-11-12 04:27:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][650/1251]	eta 0:07:31 lr 0.000718	time 0.7375 (0.7507)	loss 2.6446 (3.3603)	grad_norm 1.3735 (1.4364)	mem 23876MB
[2022-11-12 04:27:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][700/1251]	eta 0:06:53 lr 0.000718	time 0.7371 (0.7504)	loss 3.6552 (3.3561)	grad_norm 1.3263 (1.4350)	mem 23876MB
[2022-11-12 04:28:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][750/1251]	eta 0:06:15 lr 0.000718	time 0.7365 (0.7502)	loss 2.6930 (3.3480)	grad_norm 1.3544 (1.4368)	mem 23876MB
[2022-11-12 04:29:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][800/1251]	eta 0:05:38 lr 0.000717	time 0.7361 (0.7501)	loss 3.8228 (3.3527)	grad_norm 1.2066 (1.4395)	mem 23876MB
[2022-11-12 04:29:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][850/1251]	eta 0:05:00 lr 0.000717	time 0.7367 (0.7501)	loss 4.1026 (3.3604)	grad_norm 1.3898 (1.4421)	mem 23876MB
[2022-11-12 04:30:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][900/1251]	eta 0:04:23 lr 0.000717	time 0.7459 (0.7499)	loss 3.0685 (3.3601)	grad_norm 1.3695 (1.4427)	mem 23876MB
[2022-11-12 04:30:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][950/1251]	eta 0:03:45 lr 0.000717	time 0.8136 (0.7499)	loss 2.5304 (3.3668)	grad_norm 1.2176 (1.4425)	mem 23876MB
[2022-11-12 04:31:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][1000/1251]	eta 0:03:08 lr 0.000717	time 0.8188 (0.7497)	loss 3.9404 (3.3667)	grad_norm 1.3900 (1.4400)	mem 23876MB
[2022-11-12 04:32:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][1050/1251]	eta 0:02:30 lr 0.000717	time 0.7361 (0.7496)	loss 3.0600 (3.3690)	grad_norm 1.4906 (1.4413)	mem 23876MB
[2022-11-12 04:32:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][1100/1251]	eta 0:01:53 lr 0.000716	time 0.7378 (0.7495)	loss 3.7218 (3.3706)	grad_norm 1.6265 (1.4421)	mem 23876MB
[2022-11-12 04:33:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][1150/1251]	eta 0:01:15 lr 0.000716	time 0.7402 (0.7496)	loss 3.0642 (3.3673)	grad_norm 1.4395 (1.4424)	mem 23876MB
[2022-11-12 04:34:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][1200/1251]	eta 0:00:38 lr 0.000716	time 0.7389 (0.7494)	loss 3.6177 (3.3673)	grad_norm 1.4793 (1.4420)	mem 23876MB
[2022-11-12 04:34:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [107/300][1250/1251]	eta 0:00:00 lr 0.000716	time 0.7264 (0.7492)	loss 2.7060 (3.3611)	grad_norm 1.2974 (1.4415)	mem 23876MB
[2022-11-12 04:34:41 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 107 training takes 0:15:37
[2022-11-12 04:34:41 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_107.pth saving......
[2022-11-12 04:34:42 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_107.pth saved !!!
[2022-11-12 04:34:44 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.742 (1.742)	Loss 0.8033 (0.8033)	Acc@1 81.445 (81.445)	Acc@5 94.922 (94.922)	Mem 23876MB
[2022-11-12 04:34:55 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.496 Acc@5 94.294
[2022-11-12 04:34:55 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.5%
[2022-11-12 04:34:56 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.902 (1.902)	Loss 0.8203 (0.8203)	Acc@1 80.566 (80.566)	Acc@5 95.605 (95.605)	Mem 23876MB
[2022-11-12 04:35:07 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.142 Acc@5 95.406
[2022-11-12 04:35:07 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.1%
[2022-11-12 04:35:07 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.14% at 107 epoch
[2022-11-12 04:35:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][0/1251]	eta 0:49:35 lr 0.000716	time 2.3782 (2.3782)	loss 3.7085 (3.7085)	grad_norm 1.2090 (1.2090)	mem 23876MB
[2022-11-12 04:35:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][50/1251]	eta 0:15:39 lr 0.000716	time 0.8226 (0.7821)	loss 2.5896 (3.4230)	grad_norm 1.3864 (1.4893)	mem 23876MB
[2022-11-12 04:36:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][100/1251]	eta 0:14:38 lr 0.000715	time 0.7482 (0.7635)	loss 3.9240 (3.3851)	grad_norm 1.2652 (1.4790)	mem 23876MB
[2022-11-12 04:37:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][150/1251]	eta 0:13:54 lr 0.000715	time 0.7370 (0.7577)	loss 2.4581 (3.3860)	grad_norm 1.7538 (nan)	mem 23876MB
[2022-11-12 04:37:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][200/1251]	eta 0:13:13 lr 0.000715	time 0.7353 (0.7551)	loss 3.9521 (3.3813)	grad_norm 1.6300 (nan)	mem 23876MB
[2022-11-12 04:38:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][250/1251]	eta 0:12:33 lr 0.000715	time 0.7358 (0.7530)	loss 2.7999 (3.3843)	grad_norm 1.2917 (nan)	mem 23876MB
[2022-11-12 04:38:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][300/1251]	eta 0:11:55 lr 0.000715	time 0.7388 (0.7526)	loss 3.5691 (3.3901)	grad_norm 1.4987 (nan)	mem 23876MB
[2022-11-12 04:39:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][350/1251]	eta 0:11:16 lr 0.000714	time 0.7530 (0.7513)	loss 3.8181 (3.3959)	grad_norm 1.4818 (nan)	mem 23876MB
[2022-11-12 04:40:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][400/1251]	eta 0:10:38 lr 0.000714	time 0.7395 (0.7508)	loss 3.8019 (3.3705)	grad_norm 1.3238 (nan)	mem 23876MB
[2022-11-12 04:40:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][450/1251]	eta 0:10:00 lr 0.000714	time 0.7382 (0.7503)	loss 3.7516 (3.3855)	grad_norm 1.2295 (nan)	mem 23876MB
[2022-11-12 04:41:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][500/1251]	eta 0:09:22 lr 0.000714	time 0.7377 (0.7496)	loss 2.6401 (3.3844)	grad_norm 1.5125 (nan)	mem 23876MB
[2022-11-12 04:42:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][550/1251]	eta 0:08:45 lr 0.000714	time 0.7411 (0.7495)	loss 3.4936 (3.3893)	grad_norm 1.2947 (nan)	mem 23876MB
[2022-11-12 04:42:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][600/1251]	eta 0:08:07 lr 0.000714	time 0.8115 (0.7494)	loss 2.7254 (3.3904)	grad_norm 1.2645 (nan)	mem 23876MB
[2022-11-12 04:43:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][650/1251]	eta 0:07:30 lr 0.000713	time 0.7360 (0.7490)	loss 4.0277 (3.3945)	grad_norm 1.4935 (nan)	mem 23876MB
[2022-11-12 04:43:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][700/1251]	eta 0:06:52 lr 0.000713	time 0.7359 (0.7490)	loss 2.5143 (3.3881)	grad_norm 1.2340 (nan)	mem 23876MB
[2022-11-12 04:44:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][750/1251]	eta 0:06:15 lr 0.000713	time 0.7421 (0.7487)	loss 3.8986 (3.3866)	grad_norm 1.3673 (nan)	mem 23876MB
[2022-11-12 04:45:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][800/1251]	eta 0:05:37 lr 0.000713	time 0.7408 (0.7486)	loss 3.6054 (3.3864)	grad_norm 1.3179 (nan)	mem 23876MB
[2022-11-12 04:45:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][850/1251]	eta 0:05:00 lr 0.000713	time 0.7438 (0.7486)	loss 3.6654 (3.3909)	grad_norm 1.3272 (nan)	mem 23876MB
[2022-11-12 04:46:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][900/1251]	eta 0:04:22 lr 0.000712	time 0.7366 (0.7483)	loss 2.6771 (3.3880)	grad_norm 1.3642 (nan)	mem 23876MB
[2022-11-12 04:46:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][950/1251]	eta 0:03:45 lr 0.000712	time 0.7352 (0.7482)	loss 3.5877 (3.3885)	grad_norm 1.4353 (nan)	mem 23876MB
[2022-11-12 04:47:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][1000/1251]	eta 0:03:07 lr 0.000712	time 0.7378 (0.7479)	loss 3.7391 (3.3907)	grad_norm 1.5289 (nan)	mem 23876MB
[2022-11-12 04:48:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][1050/1251]	eta 0:02:30 lr 0.000712	time 0.7396 (0.7479)	loss 2.9948 (3.3883)	grad_norm 1.4120 (nan)	mem 23876MB
[2022-11-12 04:48:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][1100/1251]	eta 0:01:52 lr 0.000712	time 0.7395 (0.7479)	loss 2.7899 (3.3888)	grad_norm 1.5043 (nan)	mem 23876MB
[2022-11-12 04:49:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][1150/1251]	eta 0:01:15 lr 0.000711	time 0.7362 (0.7478)	loss 3.4345 (3.3878)	grad_norm 1.4308 (nan)	mem 23876MB
[2022-11-12 04:50:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][1200/1251]	eta 0:00:38 lr 0.000711	time 0.7394 (0.7478)	loss 2.9598 (3.3943)	grad_norm 1.4504 (nan)	mem 23876MB
[2022-11-12 04:50:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [108/300][1250/1251]	eta 0:00:00 lr 0.000711	time 0.7266 (0.7476)	loss 3.8101 (3.3968)	grad_norm 1.4812 (nan)	mem 23876MB
[2022-11-12 04:50:43 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 108 training takes 0:15:35
[2022-11-12 04:50:43 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_108.pth saving......
[2022-11-12 04:50:44 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_108.pth saved !!!
[2022-11-12 04:50:46 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.721 (1.721)	Loss 0.9308 (0.9308)	Acc@1 76.270 (76.270)	Acc@5 94.629 (94.629)	Mem 23876MB
[2022-11-12 04:50:56 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.608 Acc@5 94.170
[2022-11-12 04:50:56 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.6%
[2022-11-12 04:50:58 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.803 (1.803)	Loss 0.8023 (0.8023)	Acc@1 80.273 (80.273)	Acc@5 94.922 (94.922)	Mem 23876MB
[2022-11-12 04:51:09 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.146 Acc@5 95.426
[2022-11-12 04:51:09 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.1%
[2022-11-12 04:51:09 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.15% at 108 epoch
[2022-11-12 04:51:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][0/1251]	eta 0:52:09 lr 0.000711	time 2.5013 (2.5013)	loss 3.2224 (3.2224)	grad_norm 1.6248 (1.6248)	mem 23876MB
[2022-11-12 04:51:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][50/1251]	eta 0:15:45 lr 0.000711	time 0.7404 (0.7870)	loss 3.3338 (3.3362)	grad_norm 1.3813 (1.4888)	mem 23876MB
[2022-11-12 04:52:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][100/1251]	eta 0:14:44 lr 0.000711	time 0.7448 (0.7681)	loss 2.9707 (3.3056)	grad_norm 1.3550 (1.4486)	mem 23876MB
[2022-11-12 04:53:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][150/1251]	eta 0:13:58 lr 0.000710	time 0.7420 (0.7614)	loss 3.5619 (3.2855)	grad_norm 1.3718 (1.4544)	mem 23876MB
[2022-11-12 04:53:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][200/1251]	eta 0:13:17 lr 0.000710	time 0.7406 (0.7585)	loss 3.1900 (3.2907)	grad_norm 1.7447 (1.4569)	mem 23876MB
[2022-11-12 04:54:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][250/1251]	eta 0:12:37 lr 0.000710	time 0.7322 (0.7566)	loss 2.5302 (3.3239)	grad_norm 1.3977 (1.4549)	mem 23876MB
[2022-11-12 04:54:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][300/1251]	eta 0:11:57 lr 0.000710	time 0.7377 (0.7547)	loss 3.2942 (3.3133)	grad_norm 1.3972 (1.4637)	mem 23876MB
[2022-11-12 04:55:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][350/1251]	eta 0:11:19 lr 0.000710	time 0.7409 (0.7543)	loss 3.6151 (3.3096)	grad_norm 1.4698 (1.4599)	mem 23876MB
[2022-11-12 04:56:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][400/1251]	eta 0:10:41 lr 0.000710	time 0.7524 (0.7537)	loss 2.1791 (3.3197)	grad_norm 1.4034 (1.4586)	mem 23876MB
[2022-11-12 04:56:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][450/1251]	eta 0:10:03 lr 0.000709	time 0.7391 (0.7530)	loss 3.5626 (3.3187)	grad_norm 1.4179 (1.4554)	mem 23876MB
[2022-11-12 04:57:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][500/1251]	eta 0:09:25 lr 0.000709	time 0.7392 (0.7525)	loss 3.9758 (3.3277)	grad_norm 1.4108 (1.4548)	mem 23876MB
[2022-11-12 04:58:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][550/1251]	eta 0:08:47 lr 0.000709	time 0.7414 (0.7522)	loss 3.8488 (3.3329)	grad_norm 1.3903 (1.4551)	mem 23876MB
[2022-11-12 04:58:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][600/1251]	eta 0:08:09 lr 0.000709	time 0.7372 (0.7523)	loss 3.3280 (3.3340)	grad_norm 1.5149 (1.4524)	mem 23876MB
[2022-11-12 04:59:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][650/1251]	eta 0:07:31 lr 0.000709	time 0.7376 (0.7519)	loss 3.7467 (3.3431)	grad_norm 1.3093 (1.4550)	mem 23876MB
[2022-11-12 04:59:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][700/1251]	eta 0:06:54 lr 0.000708	time 0.7360 (0.7514)	loss 3.7068 (3.3479)	grad_norm 1.6991 (1.4545)	mem 23876MB
[2022-11-12 05:00:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][750/1251]	eta 0:06:16 lr 0.000708	time 0.8169 (0.7513)	loss 3.7595 (3.3518)	grad_norm 1.4554 (1.4564)	mem 23876MB
[2022-11-12 05:01:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][800/1251]	eta 0:05:38 lr 0.000708	time 0.7435 (0.7510)	loss 4.0985 (3.3484)	grad_norm 1.6407 (1.4554)	mem 23876MB
[2022-11-12 05:01:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][850/1251]	eta 0:05:01 lr 0.000708	time 0.7395 (0.7510)	loss 3.2778 (3.3448)	grad_norm 1.8482 (1.4560)	mem 23876MB
[2022-11-12 05:02:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][900/1251]	eta 0:04:23 lr 0.000708	time 0.7430 (0.7508)	loss 2.8934 (3.3442)	grad_norm 1.4433 (1.4542)	mem 23876MB
[2022-11-12 05:03:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][950/1251]	eta 0:03:45 lr 0.000707	time 0.8088 (0.7507)	loss 3.4115 (3.3499)	grad_norm 1.4229 (1.4533)	mem 23876MB
[2022-11-12 05:03:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][1000/1251]	eta 0:03:08 lr 0.000707	time 0.7418 (0.7506)	loss 2.7642 (3.3493)	grad_norm 1.3048 (1.4517)	mem 23876MB
[2022-11-12 05:04:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][1050/1251]	eta 0:02:30 lr 0.000707	time 0.7393 (0.7505)	loss 3.0584 (3.3437)	grad_norm 1.5187 (1.4520)	mem 23876MB
[2022-11-12 05:04:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][1100/1251]	eta 0:01:53 lr 0.000707	time 0.7466 (0.7505)	loss 3.5871 (3.3481)	grad_norm 1.3890 (1.4505)	mem 23876MB
[2022-11-12 05:05:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][1150/1251]	eta 0:01:15 lr 0.000707	time 0.7413 (0.7505)	loss 3.6626 (3.3510)	grad_norm 1.4918 (1.4501)	mem 23876MB
[2022-11-12 05:06:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][1200/1251]	eta 0:00:38 lr 0.000707	time 0.7362 (0.7504)	loss 2.4979 (3.3495)	grad_norm 1.4467 (1.4508)	mem 23876MB
[2022-11-12 05:06:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [109/300][1250/1251]	eta 0:00:00 lr 0.000706	time 0.7302 (0.7502)	loss 2.8695 (3.3497)	grad_norm 1.1807 (1.4489)	mem 23876MB
[2022-11-12 05:06:47 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 109 training takes 0:15:38
[2022-11-12 05:06:48 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_109.pth saving......
[2022-11-12 05:06:49 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_109.pth saved !!!
[2022-11-12 05:06:50 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.671 (1.671)	Loss 0.9406 (0.9406)	Acc@1 77.734 (77.734)	Acc@5 93.945 (93.945)	Mem 23876MB
[2022-11-12 05:07:01 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.652 Acc@5 94.210
[2022-11-12 05:07:01 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.7%
[2022-11-12 05:07:03 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.948 (1.948)	Loss 0.8642 (0.8642)	Acc@1 78.711 (78.711)	Acc@5 94.629 (94.629)	Mem 23876MB
[2022-11-12 05:07:14 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.136 Acc@5 95.450
[2022-11-12 05:07:14 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.1%
[2022-11-12 05:07:14 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.15% at 108 epoch
[2022-11-12 05:07:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][0/1251]	eta 0:49:39 lr 0.000706	time 2.3820 (2.3820)	loss 3.7224 (3.7224)	grad_norm 1.3290 (1.3290)	mem 23876MB
[2022-11-12 05:07:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][50/1251]	eta 0:15:38 lr 0.000706	time 0.7375 (0.7814)	loss 3.2893 (3.3523)	grad_norm 1.4120 (1.4886)	mem 23876MB
[2022-11-12 05:08:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][100/1251]	eta 0:14:39 lr 0.000706	time 0.7503 (0.7645)	loss 3.2474 (3.3350)	grad_norm 1.5286 (1.4707)	mem 23876MB
[2022-11-12 05:09:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][150/1251]	eta 0:13:55 lr 0.000706	time 0.7456 (0.7592)	loss 3.5385 (3.3506)	grad_norm 1.5473 (1.4799)	mem 23876MB
[2022-11-12 05:09:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][200/1251]	eta 0:13:14 lr 0.000706	time 0.7409 (0.7556)	loss 3.4441 (3.3555)	grad_norm 1.4398 (1.4787)	mem 23876MB
[2022-11-12 05:10:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][250/1251]	eta 0:12:34 lr 0.000705	time 0.7346 (0.7539)	loss 2.3062 (3.3583)	grad_norm 1.6199 (1.4791)	mem 23876MB
[2022-11-12 05:11:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][300/1251]	eta 0:11:55 lr 0.000705	time 0.7381 (0.7528)	loss 2.3398 (3.3525)	grad_norm 1.4360 (1.4803)	mem 23876MB
[2022-11-12 05:11:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][350/1251]	eta 0:11:17 lr 0.000705	time 0.7437 (0.7520)	loss 3.4268 (3.3452)	grad_norm 1.6210 (1.4767)	mem 23876MB
[2022-11-12 05:12:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][400/1251]	eta 0:10:39 lr 0.000705	time 0.7388 (0.7517)	loss 3.2343 (3.3569)	grad_norm 1.3970 (1.4760)	mem 23876MB
[2022-11-12 05:12:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][450/1251]	eta 0:10:01 lr 0.000705	time 0.7415 (0.7511)	loss 2.2103 (3.3560)	grad_norm 1.4736 (1.4739)	mem 23876MB
[2022-11-12 05:13:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][500/1251]	eta 0:09:23 lr 0.000704	time 0.7311 (0.7507)	loss 3.5778 (3.3602)	grad_norm 1.3912 (1.4679)	mem 23876MB
[2022-11-12 05:14:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][550/1251]	eta 0:08:46 lr 0.000704	time 0.7398 (0.7505)	loss 3.0116 (3.3657)	grad_norm 1.3904 (1.4668)	mem 23876MB
[2022-11-12 05:14:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][600/1251]	eta 0:08:08 lr 0.000704	time 0.8205 (0.7502)	loss 3.9248 (3.3640)	grad_norm 1.8167 (1.4703)	mem 23876MB
[2022-11-12 05:15:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][650/1251]	eta 0:07:30 lr 0.000704	time 0.7380 (0.7499)	loss 3.5536 (3.3664)	grad_norm 1.4281 (1.4720)	mem 23876MB
[2022-11-12 05:15:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][700/1251]	eta 0:06:53 lr 0.000704	time 0.7383 (0.7497)	loss 3.7112 (3.3600)	grad_norm 1.6136 (1.4712)	mem 23876MB
[2022-11-12 05:16:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][750/1251]	eta 0:06:15 lr 0.000703	time 0.7408 (0.7495)	loss 3.5401 (3.3566)	grad_norm 1.2925 (1.4711)	mem 23876MB
[2022-11-12 05:17:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][800/1251]	eta 0:05:38 lr 0.000703	time 0.7399 (0.7495)	loss 2.5364 (3.3526)	grad_norm 1.3007 (1.4701)	mem 23876MB
[2022-11-12 05:17:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][850/1251]	eta 0:05:00 lr 0.000703	time 0.7407 (0.7493)	loss 3.7555 (3.3535)	grad_norm 1.5338 (1.4726)	mem 23876MB
[2022-11-12 05:18:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][900/1251]	eta 0:04:22 lr 0.000703	time 0.7362 (0.7492)	loss 3.8995 (3.3495)	grad_norm 1.2488 (1.4715)	mem 23876MB
[2022-11-12 05:19:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][950/1251]	eta 0:03:45 lr 0.000703	time 0.7393 (0.7492)	loss 2.5986 (3.3480)	grad_norm 1.4192 (1.4703)	mem 23876MB
[2022-11-12 05:19:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][1000/1251]	eta 0:03:07 lr 0.000703	time 0.8138 (0.7489)	loss 3.6543 (3.3444)	grad_norm 1.2781 (1.4710)	mem 23876MB
[2022-11-12 05:20:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][1050/1251]	eta 0:02:30 lr 0.000702	time 0.7424 (0.7489)	loss 4.0808 (3.3397)	grad_norm 1.6196 (1.4711)	mem 23876MB
[2022-11-12 05:20:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][1100/1251]	eta 0:01:53 lr 0.000702	time 0.7416 (0.7488)	loss 4.0087 (3.3385)	grad_norm 1.4664 (1.4707)	mem 23876MB
[2022-11-12 05:21:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][1150/1251]	eta 0:01:15 lr 0.000702	time 0.7340 (0.7488)	loss 2.8238 (3.3411)	grad_norm 1.5913 (1.4704)	mem 23876MB
[2022-11-12 05:22:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][1200/1251]	eta 0:00:38 lr 0.000702	time 0.7397 (0.7487)	loss 3.7050 (3.3379)	grad_norm 1.3049 (1.4683)	mem 23876MB
[2022-11-12 05:22:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [110/300][1250/1251]	eta 0:00:00 lr 0.000702	time 0.7266 (0.7485)	loss 3.9247 (3.3408)	grad_norm 1.3006 (1.4678)	mem 23876MB
[2022-11-12 05:22:50 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 110 training takes 0:15:36
[2022-11-12 05:22:50 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_110.pth saving......
[2022-11-12 05:22:51 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_110.pth saved !!!
[2022-11-12 05:22:53 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.667 (1.667)	Loss 0.9424 (0.9424)	Acc@1 77.246 (77.246)	Acc@5 94.434 (94.434)	Mem 23876MB
[2022-11-12 05:23:04 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.518 Acc@5 94.162
[2022-11-12 05:23:04 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.5%
[2022-11-12 05:23:06 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.808 (1.808)	Loss 0.8559 (0.8559)	Acc@1 79.883 (79.883)	Acc@5 94.727 (94.727)	Mem 23876MB
[2022-11-12 05:23:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.234 Acc@5 95.470
[2022-11-12 05:23:17 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.2%
[2022-11-12 05:23:17 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.23% at 110 epoch
[2022-11-12 05:23:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][0/1251]	eta 0:49:57 lr 0.000702	time 2.3962 (2.3962)	loss 3.2895 (3.2895)	grad_norm 1.4158 (1.4158)	mem 23876MB
[2022-11-12 05:23:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][50/1251]	eta 0:15:38 lr 0.000701	time 0.8097 (0.7817)	loss 4.0067 (3.3281)	grad_norm 1.4552 (1.4797)	mem 23876MB
[2022-11-12 05:24:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][100/1251]	eta 0:14:40 lr 0.000701	time 0.7468 (0.7650)	loss 3.4202 (3.3450)	grad_norm 1.7430 (1.4747)	mem 23876MB
[2022-11-12 05:25:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][150/1251]	eta 0:13:54 lr 0.000701	time 0.7428 (0.7580)	loss 3.0961 (3.3091)	grad_norm 1.5775 (1.4653)	mem 23876MB
[2022-11-12 05:25:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][200/1251]	eta 0:13:15 lr 0.000701	time 0.8484 (0.7566)	loss 3.6809 (3.3328)	grad_norm 1.4506 (1.4643)	mem 23876MB
[2022-11-12 05:26:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][250/1251]	eta 0:12:34 lr 0.000701	time 0.7350 (0.7541)	loss 3.8464 (3.3432)	grad_norm 1.3595 (1.4669)	mem 23876MB
[2022-11-12 05:27:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][300/1251]	eta 0:11:56 lr 0.000700	time 0.7369 (0.7531)	loss 2.3202 (3.3369)	grad_norm 1.4988 (1.4658)	mem 23876MB
[2022-11-12 05:27:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][350/1251]	eta 0:11:17 lr 0.000700	time 0.7383 (0.7517)	loss 3.8381 (3.3333)	grad_norm 1.4972 (inf)	mem 23876MB
[2022-11-12 05:28:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][400/1251]	eta 0:10:39 lr 0.000700	time 0.7390 (0.7512)	loss 2.6011 (3.3284)	grad_norm 1.4702 (inf)	mem 23876MB
[2022-11-12 05:28:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][450/1251]	eta 0:10:01 lr 0.000700	time 0.7368 (0.7506)	loss 2.5937 (3.3350)	grad_norm 1.6051 (inf)	mem 23876MB
[2022-11-12 05:29:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][500/1251]	eta 0:09:23 lr 0.000700	time 0.7410 (0.7503)	loss 3.3675 (3.3416)	grad_norm 1.4813 (inf)	mem 23876MB
[2022-11-12 05:30:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][550/1251]	eta 0:08:45 lr 0.000699	time 0.7318 (0.7499)	loss 3.3187 (3.3402)	grad_norm 1.3644 (inf)	mem 23876MB
[2022-11-12 05:30:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][600/1251]	eta 0:08:08 lr 0.000699	time 0.8263 (0.7500)	loss 3.3891 (3.3426)	grad_norm 1.5477 (inf)	mem 23876MB
[2022-11-12 05:31:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][650/1251]	eta 0:07:30 lr 0.000699	time 0.7395 (0.7496)	loss 3.8099 (3.3451)	grad_norm 1.4051 (inf)	mem 23876MB
[2022-11-12 05:32:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][700/1251]	eta 0:06:52 lr 0.000699	time 0.7463 (0.7493)	loss 3.6566 (3.3512)	grad_norm 1.4663 (inf)	mem 23876MB
[2022-11-12 05:32:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][750/1251]	eta 0:06:15 lr 0.000699	time 0.7397 (0.7491)	loss 3.7564 (3.3460)	grad_norm 1.4944 (inf)	mem 23876MB
[2022-11-12 05:33:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][800/1251]	eta 0:05:37 lr 0.000699	time 0.7338 (0.7490)	loss 2.7114 (3.3451)	grad_norm 2.0767 (inf)	mem 23876MB
[2022-11-12 05:33:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][850/1251]	eta 0:05:00 lr 0.000698	time 0.7417 (0.7489)	loss 3.4119 (3.3468)	grad_norm 1.4775 (inf)	mem 23876MB
[2022-11-12 05:34:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][900/1251]	eta 0:04:22 lr 0.000698	time 0.7456 (0.7487)	loss 3.9835 (3.3462)	grad_norm 1.3941 (inf)	mem 23876MB
[2022-11-12 05:35:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][950/1251]	eta 0:03:45 lr 0.000698	time 0.7380 (0.7487)	loss 2.9178 (3.3487)	grad_norm 1.4176 (inf)	mem 23876MB
[2022-11-12 05:35:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][1000/1251]	eta 0:03:07 lr 0.000698	time 0.8327 (0.7487)	loss 3.0118 (3.3558)	grad_norm 1.2902 (inf)	mem 23876MB
[2022-11-12 05:36:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][1050/1251]	eta 0:02:30 lr 0.000698	time 0.7376 (0.7485)	loss 2.7607 (3.3600)	grad_norm 1.4074 (inf)	mem 23876MB
[2022-11-12 05:37:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][1100/1251]	eta 0:01:52 lr 0.000697	time 0.8085 (0.7483)	loss 2.5646 (3.3572)	grad_norm 1.3609 (inf)	mem 23876MB
[2022-11-12 05:37:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][1150/1251]	eta 0:01:15 lr 0.000697	time 0.7345 (0.7483)	loss 3.8678 (3.3544)	grad_norm 1.4227 (inf)	mem 23876MB
[2022-11-12 05:38:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][1200/1251]	eta 0:00:38 lr 0.000697	time 0.7401 (0.7481)	loss 3.8153 (3.3595)	grad_norm 1.3938 (inf)	mem 23876MB
[2022-11-12 05:38:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [111/300][1250/1251]	eta 0:00:00 lr 0.000697	time 0.7284 (0.7480)	loss 2.7801 (3.3592)	grad_norm 1.5915 (inf)	mem 23876MB
[2022-11-12 05:38:53 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 111 training takes 0:15:35
[2022-11-12 05:38:53 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_111.pth saving......
[2022-11-12 05:38:54 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_111.pth saved !!!
[2022-11-12 05:38:55 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.684 (1.684)	Loss 0.8532 (0.8532)	Acc@1 79.883 (79.883)	Acc@5 95.312 (95.312)	Mem 23876MB
[2022-11-12 05:39:06 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.546 Acc@5 94.282
[2022-11-12 05:39:06 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.5%
[2022-11-12 05:39:08 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.824 (1.824)	Loss 0.8034 (0.8034)	Acc@1 80.762 (80.762)	Acc@5 95.312 (95.312)	Mem 23876MB
[2022-11-12 05:39:19 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.234 Acc@5 95.480
[2022-11-12 05:39:19 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.2%
[2022-11-12 05:39:19 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.23% at 110 epoch
[2022-11-12 05:39:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][0/1251]	eta 0:50:32 lr 0.000697	time 2.4237 (2.4237)	loss 3.7259 (3.7259)	grad_norm 1.4474 (1.4474)	mem 23876MB
[2022-11-12 05:39:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][50/1251]	eta 0:15:38 lr 0.000697	time 0.7396 (0.7816)	loss 3.3525 (3.3345)	grad_norm 1.5680 (1.4742)	mem 23876MB
[2022-11-12 05:40:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][100/1251]	eta 0:14:41 lr 0.000696	time 0.7398 (0.7659)	loss 3.7014 (3.3566)	grad_norm 1.2167 (1.4673)	mem 23876MB
[2022-11-12 05:41:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][150/1251]	eta 0:13:56 lr 0.000696	time 0.7383 (0.7596)	loss 3.6789 (3.3412)	grad_norm 1.6634 (1.4693)	mem 23876MB
[2022-11-12 05:41:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][200/1251]	eta 0:13:15 lr 0.000696	time 0.7435 (0.7569)	loss 3.8944 (3.3381)	grad_norm 1.5037 (1.4707)	mem 23876MB
[2022-11-12 05:42:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][250/1251]	eta 0:12:35 lr 0.000696	time 0.7366 (0.7550)	loss 3.1386 (3.3191)	grad_norm 1.5102 (1.4730)	mem 23876MB
[2022-11-12 05:43:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][300/1251]	eta 0:11:56 lr 0.000696	time 0.7398 (0.7536)	loss 3.6695 (3.3280)	grad_norm 1.3221 (1.4688)	mem 23876MB
[2022-11-12 05:43:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][350/1251]	eta 0:11:18 lr 0.000695	time 0.7326 (0.7531)	loss 4.0224 (3.3301)	grad_norm 1.4762 (1.4686)	mem 23876MB
[2022-11-12 05:44:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][400/1251]	eta 0:10:40 lr 0.000695	time 0.7452 (0.7523)	loss 3.4882 (3.3502)	grad_norm 1.4914 (1.4708)	mem 23876MB
[2022-11-12 05:44:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][450/1251]	eta 0:10:02 lr 0.000695	time 0.7387 (0.7521)	loss 3.7906 (3.3489)	grad_norm 1.3707 (1.4688)	mem 23876MB
[2022-11-12 05:45:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][500/1251]	eta 0:09:24 lr 0.000695	time 0.7412 (0.7514)	loss 3.4303 (3.3478)	grad_norm 1.4111 (1.4657)	mem 23876MB
[2022-11-12 05:46:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][550/1251]	eta 0:08:46 lr 0.000695	time 0.7384 (0.7511)	loss 3.7473 (3.3547)	grad_norm 1.2743 (1.4684)	mem 23876MB
[2022-11-12 05:46:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][600/1251]	eta 0:08:08 lr 0.000695	time 0.7373 (0.7510)	loss 3.8873 (3.3488)	grad_norm 1.3396 (1.4661)	mem 23876MB
[2022-11-12 05:47:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][650/1251]	eta 0:07:31 lr 0.000694	time 0.7402 (0.7508)	loss 3.3644 (3.3476)	grad_norm 1.4225 (1.4669)	mem 23876MB
[2022-11-12 05:48:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][700/1251]	eta 0:06:53 lr 0.000694	time 0.7384 (0.7506)	loss 2.9911 (3.3498)	grad_norm 1.4096 (1.4666)	mem 23876MB
[2022-11-12 05:48:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][750/1251]	eta 0:06:15 lr 0.000694	time 0.7511 (0.7505)	loss 3.8556 (3.3495)	grad_norm 1.6100 (1.4705)	mem 23876MB
[2022-11-12 05:49:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][800/1251]	eta 0:05:38 lr 0.000694	time 0.7411 (0.7502)	loss 3.0407 (3.3496)	grad_norm 1.6413 (1.4715)	mem 23876MB
[2022-11-12 05:49:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][850/1251]	eta 0:05:00 lr 0.000694	time 0.7348 (0.7501)	loss 3.5443 (3.3528)	grad_norm 1.3937 (1.4709)	mem 23876MB
[2022-11-12 05:50:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][900/1251]	eta 0:04:23 lr 0.000693	time 0.7352 (0.7500)	loss 2.6207 (3.3491)	grad_norm 1.5331 (1.4692)	mem 23876MB
[2022-11-12 05:51:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][950/1251]	eta 0:03:45 lr 0.000693	time 0.7388 (0.7498)	loss 1.9831 (3.3455)	grad_norm 1.4087 (1.4686)	mem 23876MB
[2022-11-12 05:51:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][1000/1251]	eta 0:03:08 lr 0.000693	time 0.8169 (0.7496)	loss 4.1389 (3.3513)	grad_norm 1.7240 (1.4690)	mem 23876MB
[2022-11-12 05:52:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][1050/1251]	eta 0:02:30 lr 0.000693	time 0.7388 (0.7494)	loss 2.8707 (3.3464)	grad_norm 1.4676 (1.4691)	mem 23876MB
[2022-11-12 05:53:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][1100/1251]	eta 0:01:53 lr 0.000693	time 0.7357 (0.7493)	loss 3.3576 (3.3463)	grad_norm 1.7191 (1.4689)	mem 23876MB
[2022-11-12 05:53:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][1150/1251]	eta 0:01:15 lr 0.000692	time 0.7378 (0.7494)	loss 3.1660 (3.3384)	grad_norm 1.5074 (1.4692)	mem 23876MB
[2022-11-12 05:54:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][1200/1251]	eta 0:00:38 lr 0.000692	time 0.7400 (0.7492)	loss 3.4519 (3.3391)	grad_norm 1.6153 (1.4687)	mem 23876MB
[2022-11-12 05:54:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [112/300][1250/1251]	eta 0:00:00 lr 0.000692	time 0.7270 (0.7490)	loss 3.3896 (3.3381)	grad_norm 1.4868 (1.4681)	mem 23876MB
[2022-11-12 05:54:56 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 112 training takes 0:15:37
[2022-11-12 05:54:56 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_112.pth saving......
[2022-11-12 05:54:57 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_112.pth saved !!!
[2022-11-12 05:54:59 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.647 (1.647)	Loss 1.0239 (1.0239)	Acc@1 75.000 (75.000)	Acc@5 93.164 (93.164)	Mem 23876MB
[2022-11-12 05:55:10 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.936 Acc@5 94.400
[2022-11-12 05:55:10 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.9%
[2022-11-12 05:55:11 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.897 (1.897)	Loss 0.8541 (0.8541)	Acc@1 78.809 (78.809)	Acc@5 94.824 (94.824)	Mem 23876MB
[2022-11-12 05:55:22 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.288 Acc@5 95.482
[2022-11-12 05:55:22 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.3%
[2022-11-12 05:55:22 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.29% at 112 epoch
[2022-11-12 05:55:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][0/1251]	eta 0:51:39 lr 0.000692	time 2.4778 (2.4778)	loss 2.8395 (2.8395)	grad_norm 1.6056 (1.6056)	mem 23876MB
[2022-11-12 05:56:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][50/1251]	eta 0:15:44 lr 0.000692	time 0.8349 (0.7862)	loss 3.4133 (3.2406)	grad_norm 1.7666 (1.4484)	mem 23876MB
[2022-11-12 05:56:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][100/1251]	eta 0:14:42 lr 0.000692	time 0.7400 (0.7669)	loss 3.5361 (3.2979)	grad_norm 1.6018 (1.4728)	mem 23876MB
[2022-11-12 05:57:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][150/1251]	eta 0:13:57 lr 0.000691	time 0.7413 (0.7604)	loss 2.2733 (3.3195)	grad_norm 1.3754 (1.4695)	mem 23876MB
[2022-11-12 05:57:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][200/1251]	eta 0:13:16 lr 0.000691	time 0.8236 (0.7577)	loss 3.9397 (3.3209)	grad_norm 1.6880 (1.4767)	mem 23876MB
[2022-11-12 05:58:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][250/1251]	eta 0:12:36 lr 0.000691	time 0.7387 (0.7554)	loss 2.9828 (3.3242)	grad_norm 1.4709 (1.4797)	mem 23876MB
[2022-11-12 05:59:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][300/1251]	eta 0:11:57 lr 0.000691	time 0.7465 (0.7546)	loss 3.6365 (3.3315)	grad_norm 1.4122 (1.4841)	mem 23876MB
[2022-11-12 05:59:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][350/1251]	eta 0:11:19 lr 0.000691	time 0.7443 (0.7537)	loss 2.7039 (3.3405)	grad_norm 1.3102 (1.4839)	mem 23876MB
[2022-11-12 06:00:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][400/1251]	eta 0:10:41 lr 0.000690	time 0.7382 (0.7536)	loss 3.5623 (3.3548)	grad_norm 1.5700 (1.4791)	mem 23876MB
[2022-11-12 06:01:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][450/1251]	eta 0:10:03 lr 0.000690	time 0.7423 (0.7531)	loss 3.6172 (3.3516)	grad_norm 1.5625 (1.4783)	mem 23876MB
[2022-11-12 06:01:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][500/1251]	eta 0:09:25 lr 0.000690	time 0.7464 (0.7525)	loss 3.9595 (3.3544)	grad_norm 1.3780 (1.4738)	mem 23876MB
[2022-11-12 06:02:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][550/1251]	eta 0:08:47 lr 0.000690	time 0.7449 (0.7519)	loss 2.9740 (3.3545)	grad_norm 1.4318 (1.4705)	mem 23876MB
[2022-11-12 06:02:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][600/1251]	eta 0:08:09 lr 0.000690	time 0.7479 (0.7516)	loss 3.6237 (3.3495)	grad_norm 1.5276 (1.4708)	mem 23876MB
[2022-11-12 06:03:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][650/1251]	eta 0:07:31 lr 0.000690	time 0.7421 (0.7515)	loss 3.7740 (3.3418)	grad_norm 1.6225 (1.4705)	mem 23876MB
[2022-11-12 06:04:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][700/1251]	eta 0:06:53 lr 0.000689	time 0.7388 (0.7514)	loss 4.0738 (3.3398)	grad_norm 1.7462 (1.4707)	mem 23876MB
[2022-11-12 06:04:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][750/1251]	eta 0:06:16 lr 0.000689	time 0.7361 (0.7510)	loss 3.8443 (3.3384)	grad_norm 1.4620 (1.4717)	mem 23876MB
[2022-11-12 06:05:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][800/1251]	eta 0:05:38 lr 0.000689	time 0.7302 (0.7508)	loss 3.2571 (3.3418)	grad_norm 1.4618 (1.4716)	mem 23876MB
[2022-11-12 06:06:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][850/1251]	eta 0:05:01 lr 0.000689	time 0.7403 (0.7506)	loss 2.8365 (3.3372)	grad_norm 1.2360 (1.4722)	mem 23876MB
[2022-11-12 06:06:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][900/1251]	eta 0:04:23 lr 0.000689	time 0.7311 (0.7507)	loss 3.4943 (3.3372)	grad_norm 1.2494 (1.4696)	mem 23876MB
[2022-11-12 06:07:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][950/1251]	eta 0:03:45 lr 0.000688	time 0.7356 (0.7504)	loss 3.0911 (3.3391)	grad_norm 1.3754 (1.4697)	mem 23876MB
[2022-11-12 06:07:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][1000/1251]	eta 0:03:08 lr 0.000688	time 0.7423 (0.7504)	loss 3.3384 (3.3358)	grad_norm 1.5294 (1.4688)	mem 23876MB
[2022-11-12 06:08:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][1050/1251]	eta 0:02:30 lr 0.000688	time 0.7345 (0.7501)	loss 2.9903 (3.3332)	grad_norm 1.4978 (1.4692)	mem 23876MB
[2022-11-12 06:09:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][1100/1251]	eta 0:01:53 lr 0.000688	time 0.7413 (0.7501)	loss 3.6194 (3.3348)	grad_norm 1.8553 (1.4708)	mem 23876MB
[2022-11-12 06:09:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][1150/1251]	eta 0:01:15 lr 0.000688	time 0.7413 (0.7500)	loss 2.8649 (3.3325)	grad_norm 1.5048 (1.4697)	mem 23876MB
[2022-11-12 06:10:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][1200/1251]	eta 0:00:38 lr 0.000687	time 0.7458 (0.7500)	loss 3.4642 (3.3364)	grad_norm 1.6734 (1.4693)	mem 23876MB
[2022-11-12 06:11:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [113/300][1250/1251]	eta 0:00:00 lr 0.000687	time 0.7252 (0.7498)	loss 3.4264 (3.3349)	grad_norm 1.5911 (1.4671)	mem 23876MB
[2022-11-12 06:11:00 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 113 training takes 0:15:38
[2022-11-12 06:11:00 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_113.pth saving......
[2022-11-12 06:11:02 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_113.pth saved !!!
[2022-11-12 06:11:03 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.743 (1.743)	Loss 0.8808 (0.8808)	Acc@1 79.395 (79.395)	Acc@5 95.703 (95.703)	Mem 23876MB
[2022-11-12 06:11:14 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.828 Acc@5 94.358
[2022-11-12 06:11:14 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.8%
[2022-11-12 06:11:16 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.820 (1.820)	Loss 0.7839 (0.7839)	Acc@1 80.078 (80.078)	Acc@5 95.605 (95.605)	Mem 23876MB
[2022-11-12 06:11:27 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.310 Acc@5 95.512
[2022-11-12 06:11:27 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.3%
[2022-11-12 06:11:27 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.31% at 113 epoch
[2022-11-12 06:11:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][0/1251]	eta 0:51:21 lr 0.000687	time 2.4630 (2.4630)	loss 3.9683 (3.9683)	grad_norm 1.3615 (1.3615)	mem 23876MB
[2022-11-12 06:12:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][50/1251]	eta 0:15:44 lr 0.000687	time 0.7356 (0.7864)	loss 3.8678 (3.2871)	grad_norm 1.4528 (1.4425)	mem 23876MB
[2022-11-12 06:12:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][100/1251]	eta 0:14:40 lr 0.000687	time 0.7320 (0.7651)	loss 4.2361 (3.3005)	grad_norm 1.5691 (1.4481)	mem 23876MB
[2022-11-12 06:13:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][150/1251]	eta 0:13:55 lr 0.000687	time 0.7341 (0.7593)	loss 2.8411 (3.3237)	grad_norm 1.7479 (1.4632)	mem 23876MB
[2022-11-12 06:13:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][200/1251]	eta 0:13:14 lr 0.000686	time 0.7361 (0.7559)	loss 3.7057 (3.3379)	grad_norm 1.4147 (1.4669)	mem 23876MB
[2022-11-12 06:14:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][250/1251]	eta 0:12:34 lr 0.000686	time 0.7407 (0.7542)	loss 2.9608 (3.3244)	grad_norm 1.5179 (1.4675)	mem 23876MB
[2022-11-12 06:15:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][300/1251]	eta 0:11:55 lr 0.000686	time 0.8046 (0.7528)	loss 3.3366 (3.3347)	grad_norm 1.3995 (inf)	mem 23876MB
[2022-11-12 06:15:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][350/1251]	eta 0:11:17 lr 0.000686	time 0.7384 (0.7517)	loss 4.0199 (3.3321)	grad_norm 1.4614 (inf)	mem 23876MB
[2022-11-12 06:16:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][400/1251]	eta 0:10:39 lr 0.000686	time 0.7396 (0.7511)	loss 3.8476 (3.3377)	grad_norm 1.5034 (inf)	mem 23876MB
[2022-11-12 06:17:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][450/1251]	eta 0:10:01 lr 0.000685	time 0.7451 (0.7505)	loss 2.9573 (3.3315)	grad_norm 1.5693 (inf)	mem 23876MB
[2022-11-12 06:17:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][500/1251]	eta 0:09:23 lr 0.000685	time 0.7588 (0.7501)	loss 2.3842 (3.3302)	grad_norm 1.4729 (inf)	mem 23876MB
[2022-11-12 06:18:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][550/1251]	eta 0:08:45 lr 0.000685	time 0.8126 (0.7499)	loss 2.7792 (3.3208)	grad_norm 1.5356 (inf)	mem 23876MB
[2022-11-12 06:18:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][600/1251]	eta 0:08:07 lr 0.000685	time 0.7318 (0.7494)	loss 3.6570 (3.3169)	grad_norm 1.4052 (inf)	mem 23876MB
[2022-11-12 06:19:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][650/1251]	eta 0:07:30 lr 0.000685	time 0.8230 (0.7493)	loss 2.2599 (3.3308)	grad_norm 1.6006 (inf)	mem 23876MB
[2022-11-12 06:20:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][700/1251]	eta 0:06:52 lr 0.000685	time 0.8147 (0.7491)	loss 2.6326 (3.3314)	grad_norm 1.5215 (inf)	mem 23876MB
[2022-11-12 06:20:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][750/1251]	eta 0:06:15 lr 0.000684	time 0.7350 (0.7487)	loss 2.9523 (3.3326)	grad_norm 1.6126 (inf)	mem 23876MB
[2022-11-12 06:21:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][800/1251]	eta 0:05:37 lr 0.000684	time 0.7432 (0.7486)	loss 3.9831 (3.3340)	grad_norm 1.6061 (inf)	mem 23876MB
[2022-11-12 06:22:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][850/1251]	eta 0:05:00 lr 0.000684	time 0.7415 (0.7485)	loss 3.6357 (3.3338)	grad_norm 1.5586 (inf)	mem 23876MB
[2022-11-12 06:22:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][900/1251]	eta 0:04:22 lr 0.000684	time 0.7429 (0.7485)	loss 3.3759 (3.3386)	grad_norm 1.5083 (inf)	mem 23876MB
[2022-11-12 06:23:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][950/1251]	eta 0:03:45 lr 0.000684	time 0.7369 (0.7484)	loss 3.8069 (3.3426)	grad_norm 1.5482 (inf)	mem 23876MB
[2022-11-12 06:23:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][1000/1251]	eta 0:03:07 lr 0.000683	time 0.7471 (0.7483)	loss 3.1467 (3.3372)	grad_norm 1.4414 (inf)	mem 23876MB
[2022-11-12 06:24:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][1050/1251]	eta 0:02:30 lr 0.000683	time 0.7378 (0.7482)	loss 3.5916 (3.3324)	grad_norm 1.2460 (inf)	mem 23876MB
[2022-11-12 06:25:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][1100/1251]	eta 0:01:52 lr 0.000683	time 0.7996 (0.7482)	loss 3.6676 (3.3304)	grad_norm 1.4078 (inf)	mem 23876MB
[2022-11-12 06:25:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][1150/1251]	eta 0:01:15 lr 0.000683	time 0.7418 (0.7480)	loss 3.2747 (3.3331)	grad_norm 1.3379 (inf)	mem 23876MB
[2022-11-12 06:26:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][1200/1251]	eta 0:00:38 lr 0.000683	time 0.7350 (0.7481)	loss 3.7642 (3.3304)	grad_norm 1.2443 (inf)	mem 23876MB
[2022-11-12 06:27:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [114/300][1250/1251]	eta 0:00:00 lr 0.000682	time 0.7231 (0.7477)	loss 3.8363 (3.3242)	grad_norm 1.4833 (inf)	mem 23876MB
[2022-11-12 06:27:02 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 114 training takes 0:15:35
[2022-11-12 06:27:02 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_114.pth saving......
[2022-11-12 06:27:03 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_114.pth saved !!!
[2022-11-12 06:27:05 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.732 (1.732)	Loss 0.8970 (0.8970)	Acc@1 79.980 (79.980)	Acc@5 94.629 (94.629)	Mem 23876MB
[2022-11-12 06:27:16 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.796 Acc@5 94.360
[2022-11-12 06:27:16 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.8%
[2022-11-12 06:27:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.921 (1.921)	Loss 0.8259 (0.8259)	Acc@1 79.004 (79.004)	Acc@5 96.094 (96.094)	Mem 23876MB
[2022-11-12 06:27:29 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.398 Acc@5 95.538
[2022-11-12 06:27:29 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.4%
[2022-11-12 06:27:29 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.40% at 114 epoch
[2022-11-12 06:27:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][0/1251]	eta 0:50:50 lr 0.000682	time 2.4382 (2.4382)	loss 4.0574 (4.0574)	grad_norm 1.5584 (1.5584)	mem 23876MB
[2022-11-12 06:28:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][50/1251]	eta 0:15:35 lr 0.000682	time 0.7402 (0.7790)	loss 2.4113 (3.3206)	grad_norm 1.3486 (1.4570)	mem 23876MB
[2022-11-12 06:28:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][100/1251]	eta 0:14:39 lr 0.000682	time 0.7356 (0.7643)	loss 2.2196 (3.2994)	grad_norm 1.3696 (1.4615)	mem 23876MB
[2022-11-12 06:29:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][150/1251]	eta 0:13:55 lr 0.000682	time 0.7388 (0.7590)	loss 3.7725 (3.2998)	grad_norm 1.4619 (1.4656)	mem 23876MB
[2022-11-12 06:30:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][200/1251]	eta 0:13:14 lr 0.000682	time 0.7407 (0.7558)	loss 3.6054 (3.2953)	grad_norm 1.3361 (1.4726)	mem 23876MB
[2022-11-12 06:30:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][250/1251]	eta 0:12:35 lr 0.000681	time 0.7390 (0.7545)	loss 3.7021 (3.3357)	grad_norm 1.5186 (1.4798)	mem 23876MB
[2022-11-12 06:31:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][300/1251]	eta 0:11:55 lr 0.000681	time 0.7391 (0.7529)	loss 3.9029 (3.3286)	grad_norm 1.6148 (1.4761)	mem 23876MB
[2022-11-12 06:31:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][350/1251]	eta 0:11:17 lr 0.000681	time 0.7412 (0.7521)	loss 3.2038 (3.3374)	grad_norm 1.3922 (1.4746)	mem 23876MB
[2022-11-12 06:32:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][400/1251]	eta 0:10:39 lr 0.000681	time 0.7420 (0.7517)	loss 2.4580 (3.3270)	grad_norm 1.3868 (1.4706)	mem 23876MB
[2022-11-12 06:33:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][450/1251]	eta 0:10:01 lr 0.000681	time 0.7363 (0.7512)	loss 3.8373 (3.3265)	grad_norm 1.4715 (1.4700)	mem 23876MB
[2022-11-12 06:33:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][500/1251]	eta 0:09:24 lr 0.000680	time 0.7512 (0.7510)	loss 2.3452 (3.3183)	grad_norm 1.1968 (1.4657)	mem 23876MB
[2022-11-12 06:34:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][550/1251]	eta 0:08:46 lr 0.000680	time 0.8058 (0.7505)	loss 3.3093 (3.3210)	grad_norm 1.5277 (1.4694)	mem 23876MB
[2022-11-12 06:34:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][600/1251]	eta 0:08:08 lr 0.000680	time 0.7397 (0.7499)	loss 3.6425 (3.3167)	grad_norm 1.4417 (1.4668)	mem 23876MB
[2022-11-12 06:35:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][650/1251]	eta 0:07:30 lr 0.000680	time 0.7416 (0.7500)	loss 3.7679 (3.3083)	grad_norm 1.5009 (1.4659)	mem 23876MB
[2022-11-12 06:36:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][700/1251]	eta 0:06:52 lr 0.000680	time 0.7363 (0.7495)	loss 3.2904 (3.3186)	grad_norm 1.7316 (1.4667)	mem 23876MB
[2022-11-12 06:36:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][750/1251]	eta 0:06:15 lr 0.000679	time 0.7403 (0.7497)	loss 3.2023 (3.3214)	grad_norm 1.6058 (1.4696)	mem 23876MB
[2022-11-12 06:37:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][800/1251]	eta 0:05:38 lr 0.000679	time 0.7402 (0.7496)	loss 3.9209 (3.3251)	grad_norm 1.4549 (1.4693)	mem 23876MB
[2022-11-12 06:38:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][850/1251]	eta 0:05:00 lr 0.000679	time 0.7414 (0.7494)	loss 3.2467 (3.3192)	grad_norm 1.3705 (1.4690)	mem 23876MB
[2022-11-12 06:38:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][900/1251]	eta 0:04:23 lr 0.000679	time 0.7346 (0.7494)	loss 3.9436 (3.3149)	grad_norm 1.4871 (1.4693)	mem 23876MB
[2022-11-12 06:39:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][950/1251]	eta 0:03:45 lr 0.000679	time 0.7374 (0.7493)	loss 3.4950 (3.3163)	grad_norm 1.4920 (1.4680)	mem 23876MB
[2022-11-12 06:39:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][1000/1251]	eta 0:03:08 lr 0.000679	time 0.7411 (0.7491)	loss 3.3136 (3.3159)	grad_norm 1.2302 (1.4698)	mem 23876MB
[2022-11-12 06:40:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][1050/1251]	eta 0:02:30 lr 0.000678	time 0.7415 (0.7491)	loss 3.5360 (3.3182)	grad_norm 1.6723 (1.4712)	mem 23876MB
[2022-11-12 06:41:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][1100/1251]	eta 0:01:53 lr 0.000678	time 0.7437 (0.7489)	loss 2.8775 (3.3154)	grad_norm 1.4170 (1.4714)	mem 23876MB
[2022-11-12 06:41:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][1150/1251]	eta 0:01:15 lr 0.000678	time 0.7407 (0.7489)	loss 3.1778 (3.3157)	grad_norm 1.3251 (1.4715)	mem 23876MB
[2022-11-12 06:42:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][1200/1251]	eta 0:00:38 lr 0.000678	time 0.7512 (0.7488)	loss 3.0093 (3.3132)	grad_norm 1.3715 (1.4706)	mem 23876MB
[2022-11-12 06:43:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [115/300][1250/1251]	eta 0:00:00 lr 0.000678	time 0.7276 (0.7486)	loss 3.3478 (3.3129)	grad_norm 1.2726 (1.4695)	mem 23876MB
[2022-11-12 06:43:05 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 115 training takes 0:15:36
[2022-11-12 06:43:06 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_115.pth saving......
[2022-11-12 06:43:07 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_115.pth saved !!!
[2022-11-12 06:43:08 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.743 (1.743)	Loss 0.8825 (0.8825)	Acc@1 78.711 (78.711)	Acc@5 94.629 (94.629)	Mem 23876MB
[2022-11-12 06:43:19 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.088 Acc@5 94.436
[2022-11-12 06:43:19 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.1%
[2022-11-12 06:43:21 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.818 (1.818)	Loss 0.7343 (0.7343)	Acc@1 81.934 (81.934)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-12 06:43:32 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.422 Acc@5 95.538
[2022-11-12 06:43:32 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.4%
[2022-11-12 06:43:32 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.42% at 115 epoch
[2022-11-12 06:43:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][0/1251]	eta 0:49:30 lr 0.000678	time 2.3742 (2.3742)	loss 3.3869 (3.3869)	grad_norm 1.5431 (1.5431)	mem 23876MB
[2022-11-12 06:44:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][50/1251]	eta 0:15:37 lr 0.000677	time 0.7404 (0.7809)	loss 2.9374 (3.3615)	grad_norm 1.4867 (1.4810)	mem 23876MB
[2022-11-12 06:44:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][100/1251]	eta 0:14:40 lr 0.000677	time 0.7420 (0.7649)	loss 3.8886 (3.3548)	grad_norm 1.7431 (1.4911)	mem 23876MB
[2022-11-12 06:45:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][150/1251]	eta 0:13:55 lr 0.000677	time 0.7460 (0.7589)	loss 3.6557 (3.3113)	grad_norm 1.5324 (1.4712)	mem 23876MB
[2022-11-12 06:46:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][200/1251]	eta 0:13:14 lr 0.000677	time 0.7362 (0.7557)	loss 3.2317 (3.3289)	grad_norm 1.5734 (1.4812)	mem 23876MB
[2022-11-12 06:46:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][250/1251]	eta 0:12:34 lr 0.000677	time 0.7408 (0.7538)	loss 2.7691 (3.3258)	grad_norm 1.3206 (1.4809)	mem 23876MB
[2022-11-12 06:47:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][300/1251]	eta 0:11:55 lr 0.000676	time 0.7381 (0.7529)	loss 3.3043 (3.3248)	grad_norm 1.6996 (1.4810)	mem 23876MB
[2022-11-12 06:47:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][350/1251]	eta 0:11:17 lr 0.000676	time 0.7431 (0.7517)	loss 2.4398 (3.3401)	grad_norm 1.3587 (1.4795)	mem 23876MB
[2022-11-12 06:48:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][400/1251]	eta 0:10:39 lr 0.000676	time 0.7388 (0.7510)	loss 2.7471 (3.3341)	grad_norm 1.2697 (1.4801)	mem 23876MB
[2022-11-12 06:49:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][450/1251]	eta 0:10:01 lr 0.000676	time 0.8173 (0.7506)	loss 3.0986 (3.3323)	grad_norm 1.2776 (1.4817)	mem 23876MB
[2022-11-12 06:49:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][500/1251]	eta 0:09:23 lr 0.000676	time 0.7455 (0.7500)	loss 3.3046 (3.3390)	grad_norm 1.5754 (1.4785)	mem 23876MB
[2022-11-12 06:50:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][550/1251]	eta 0:08:45 lr 0.000675	time 0.7346 (0.7497)	loss 2.1916 (3.3240)	grad_norm 1.6198 (1.4801)	mem 23876MB
[2022-11-12 06:51:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][600/1251]	eta 0:08:07 lr 0.000675	time 0.7339 (0.7493)	loss 3.3149 (3.3359)	grad_norm 1.5615 (1.4822)	mem 23876MB
[2022-11-12 06:51:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][650/1251]	eta 0:07:30 lr 0.000675	time 0.7420 (0.7490)	loss 3.4223 (3.3330)	grad_norm 1.4656 (1.4805)	mem 23876MB
[2022-11-12 06:52:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][700/1251]	eta 0:06:52 lr 0.000675	time 0.7426 (0.7489)	loss 2.8789 (3.3340)	grad_norm 1.2969 (1.4821)	mem 23876MB
[2022-11-12 06:52:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][750/1251]	eta 0:06:15 lr 0.000675	time 0.7373 (0.7486)	loss 3.9497 (3.3379)	grad_norm 1.8258 (1.4818)	mem 23876MB
[2022-11-12 06:53:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][800/1251]	eta 0:05:37 lr 0.000674	time 0.7425 (0.7485)	loss 3.5813 (3.3345)	grad_norm 1.6375 (1.4831)	mem 23876MB
[2022-11-12 06:54:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][850/1251]	eta 0:05:00 lr 0.000674	time 0.7413 (0.7483)	loss 2.9786 (3.3333)	grad_norm 1.5787 (1.4842)	mem 23876MB
[2022-11-12 06:54:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][900/1251]	eta 0:04:22 lr 0.000674	time 0.7434 (0.7481)	loss 3.4561 (3.3380)	grad_norm 1.4301 (1.4831)	mem 23876MB
[2022-11-12 06:55:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][950/1251]	eta 0:03:45 lr 0.000674	time 0.7398 (0.7480)	loss 2.7914 (3.3394)	grad_norm 1.6726 (1.4821)	mem 23876MB
[2022-11-12 06:56:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][1000/1251]	eta 0:03:07 lr 0.000674	time 0.7628 (0.7480)	loss 3.3511 (3.3403)	grad_norm 1.3564 (1.4799)	mem 23876MB
[2022-11-12 06:56:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][1050/1251]	eta 0:02:30 lr 0.000673	time 0.7370 (0.7479)	loss 3.1292 (3.3397)	grad_norm 1.3973 (1.4788)	mem 23876MB
[2022-11-12 06:57:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][1100/1251]	eta 0:01:52 lr 0.000673	time 0.7368 (0.7479)	loss 3.4222 (3.3437)	grad_norm 1.4419 (inf)	mem 23876MB
[2022-11-12 06:57:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][1150/1251]	eta 0:01:15 lr 0.000673	time 0.7379 (0.7478)	loss 3.0817 (3.3418)	grad_norm 1.6542 (inf)	mem 23876MB
[2022-11-12 06:58:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][1200/1251]	eta 0:00:38 lr 0.000673	time 0.7362 (0.7477)	loss 3.5005 (3.3392)	grad_norm 1.4627 (inf)	mem 23876MB
[2022-11-12 06:59:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [116/300][1250/1251]	eta 0:00:00 lr 0.000673	time 0.7250 (0.7475)	loss 2.2946 (3.3382)	grad_norm 1.3659 (inf)	mem 23876MB
[2022-11-12 06:59:07 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 116 training takes 0:15:35
[2022-11-12 06:59:07 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_116.pth saving......
[2022-11-12 06:59:08 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_116.pth saved !!!
[2022-11-12 06:59:10 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.706 (1.706)	Loss 0.8618 (0.8618)	Acc@1 79.883 (79.883)	Acc@5 95.117 (95.117)	Mem 23876MB
[2022-11-12 06:59:21 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.942 Acc@5 94.362
[2022-11-12 06:59:21 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.9%
[2022-11-12 06:59:22 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.847 (1.847)	Loss 0.8693 (0.8693)	Acc@1 79.785 (79.785)	Acc@5 94.824 (94.824)	Mem 23876MB
[2022-11-12 06:59:33 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.506 Acc@5 95.562
[2022-11-12 06:59:33 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.5%
[2022-11-12 06:59:33 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.51% at 116 epoch
[2022-11-12 06:59:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][0/1251]	eta 0:50:06 lr 0.000673	time 2.4037 (2.4037)	loss 3.7569 (3.7569)	grad_norm 1.4507 (1.4507)	mem 23876MB
[2022-11-12 07:00:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][50/1251]	eta 0:15:41 lr 0.000672	time 0.7390 (0.7837)	loss 3.1432 (3.4223)	grad_norm 1.9053 (1.4870)	mem 23876MB
[2022-11-12 07:00:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][100/1251]	eta 0:14:40 lr 0.000672	time 0.7374 (0.7647)	loss 3.8641 (3.3791)	grad_norm 1.5539 (1.4574)	mem 23876MB
[2022-11-12 07:01:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][150/1251]	eta 0:13:54 lr 0.000672	time 0.7430 (0.7584)	loss 3.5370 (3.3590)	grad_norm 1.5225 (1.4681)	mem 23876MB
[2022-11-12 07:02:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][200/1251]	eta 0:13:14 lr 0.000672	time 0.7383 (0.7556)	loss 3.1760 (3.3643)	grad_norm 1.5975 (1.4844)	mem 23876MB
[2022-11-12 07:02:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][250/1251]	eta 0:12:34 lr 0.000672	time 0.7391 (0.7540)	loss 2.5637 (3.3438)	grad_norm 1.5170 (1.4893)	mem 23876MB
[2022-11-12 07:03:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][300/1251]	eta 0:11:55 lr 0.000672	time 0.7391 (0.7527)	loss 2.2466 (3.3241)	grad_norm 1.5372 (1.4916)	mem 23876MB
[2022-11-12 07:03:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][350/1251]	eta 0:11:17 lr 0.000671	time 0.7383 (0.7521)	loss 3.5317 (3.3284)	grad_norm 1.8111 (1.4911)	mem 23876MB
[2022-11-12 07:04:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][400/1251]	eta 0:10:39 lr 0.000671	time 0.7425 (0.7514)	loss 2.3248 (3.3249)	grad_norm 1.3242 (1.4947)	mem 23876MB
[2022-11-12 07:05:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][450/1251]	eta 0:10:01 lr 0.000671	time 0.7347 (0.7507)	loss 4.0791 (3.3341)	grad_norm 1.4681 (1.4904)	mem 23876MB
[2022-11-12 07:05:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][500/1251]	eta 0:09:23 lr 0.000671	time 0.7583 (0.7505)	loss 3.3236 (3.3412)	grad_norm 1.3936 (1.4921)	mem 23876MB
[2022-11-12 07:06:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][550/1251]	eta 0:08:45 lr 0.000671	time 0.7396 (0.7502)	loss 3.0014 (3.3377)	grad_norm 1.7779 (1.4945)	mem 23876MB
[2022-11-12 07:07:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][600/1251]	eta 0:08:08 lr 0.000670	time 0.8031 (0.7500)	loss 3.2299 (3.3387)	grad_norm 1.4774 (1.4971)	mem 23876MB
[2022-11-12 07:07:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][650/1251]	eta 0:07:30 lr 0.000670	time 0.7411 (0.7496)	loss 3.3102 (3.3413)	grad_norm 1.3748 (1.4954)	mem 23876MB
[2022-11-12 07:08:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][700/1251]	eta 0:06:52 lr 0.000670	time 0.7422 (0.7492)	loss 3.2898 (3.3332)	grad_norm 1.3926 (1.4914)	mem 23876MB
[2022-11-12 07:08:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][750/1251]	eta 0:06:15 lr 0.000670	time 0.7405 (0.7491)	loss 3.3609 (3.3296)	grad_norm 1.5265 (1.4910)	mem 23876MB
[2022-11-12 07:09:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][800/1251]	eta 0:05:37 lr 0.000670	time 0.7453 (0.7488)	loss 3.7632 (3.3341)	grad_norm 1.4478 (1.4930)	mem 23876MB
[2022-11-12 07:10:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][850/1251]	eta 0:05:00 lr 0.000669	time 0.7395 (0.7487)	loss 3.6939 (3.3332)	grad_norm 1.6361 (1.4923)	mem 23876MB
[2022-11-12 07:10:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][900/1251]	eta 0:04:22 lr 0.000669	time 0.7380 (0.7486)	loss 3.7679 (3.3296)	grad_norm 1.4767 (1.4880)	mem 23876MB
[2022-11-12 07:11:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][950/1251]	eta 0:03:45 lr 0.000669	time 0.8187 (0.7486)	loss 3.6252 (3.3290)	grad_norm 1.4956 (1.4871)	mem 23876MB
[2022-11-12 07:12:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][1000/1251]	eta 0:03:07 lr 0.000669	time 0.7385 (0.7486)	loss 2.7919 (3.3348)	grad_norm 1.5444 (1.4888)	mem 23876MB
[2022-11-12 07:12:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][1050/1251]	eta 0:02:30 lr 0.000669	time 0.7376 (0.7484)	loss 3.4110 (3.3326)	grad_norm 1.4861 (1.4882)	mem 23876MB
[2022-11-12 07:13:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][1100/1251]	eta 0:01:53 lr 0.000668	time 0.7360 (0.7484)	loss 3.8420 (3.3346)	grad_norm 1.4699 (1.4885)	mem 23876MB
[2022-11-12 07:13:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][1150/1251]	eta 0:01:15 lr 0.000668	time 0.7330 (0.7484)	loss 3.1905 (3.3354)	grad_norm 1.4013 (1.4884)	mem 23876MB
[2022-11-12 07:14:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][1200/1251]	eta 0:00:38 lr 0.000668	time 0.7492 (0.7483)	loss 3.4769 (3.3349)	grad_norm 1.5799 (1.4863)	mem 23876MB
[2022-11-12 07:15:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [117/300][1250/1251]	eta 0:00:00 lr 0.000668	time 0.7248 (0.7481)	loss 3.4861 (3.3423)	grad_norm 1.4182 (1.4872)	mem 23876MB
[2022-11-12 07:15:09 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 117 training takes 0:15:36
[2022-11-12 07:15:09 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_117.pth saving......
[2022-11-12 07:15:11 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_117.pth saved !!!
[2022-11-12 07:15:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.657 (1.657)	Loss 0.9309 (0.9309)	Acc@1 78.613 (78.613)	Acc@5 94.531 (94.531)	Mem 23876MB
[2022-11-12 07:15:23 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.758 Acc@5 94.366
[2022-11-12 07:15:23 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.8%
[2022-11-12 07:15:25 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.883 (1.883)	Loss 0.7335 (0.7335)	Acc@1 82.715 (82.715)	Acc@5 96.680 (96.680)	Mem 23876MB
[2022-11-12 07:15:35 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.508 Acc@5 95.570
[2022-11-12 07:15:35 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.5%
[2022-11-12 07:15:35 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.51% at 117 epoch
[2022-11-12 07:15:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][0/1251]	eta 0:49:38 lr 0.000668	time 2.3811 (2.3811)	loss 2.3402 (2.3402)	grad_norm 1.7617 (1.7617)	mem 23876MB
[2022-11-12 07:16:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][50/1251]	eta 0:15:39 lr 0.000668	time 0.7404 (0.7821)	loss 3.8950 (3.4120)	grad_norm 1.7401 (1.4846)	mem 23876MB
[2022-11-12 07:16:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][100/1251]	eta 0:14:40 lr 0.000667	time 0.7445 (0.7647)	loss 3.4627 (3.3239)	grad_norm 1.4161 (1.4924)	mem 23876MB
[2022-11-12 07:17:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][150/1251]	eta 0:13:55 lr 0.000667	time 0.7490 (0.7591)	loss 3.2924 (3.3268)	grad_norm 1.3968 (1.5048)	mem 23876MB
[2022-11-12 07:18:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][200/1251]	eta 0:13:15 lr 0.000667	time 0.8177 (0.7568)	loss 2.7422 (3.3417)	grad_norm 1.4715 (1.4972)	mem 23876MB
[2022-11-12 07:18:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][250/1251]	eta 0:12:34 lr 0.000667	time 0.7365 (0.7542)	loss 2.7297 (3.3171)	grad_norm 1.4199 (1.4889)	mem 23876MB
[2022-11-12 07:19:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][300/1251]	eta 0:11:56 lr 0.000667	time 0.7375 (0.7532)	loss 3.4663 (3.2972)	grad_norm 1.3709 (1.4852)	mem 23876MB
[2022-11-12 07:19:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][350/1251]	eta 0:11:17 lr 0.000666	time 0.7400 (0.7524)	loss 3.5909 (3.2936)	grad_norm 1.3656 (1.4838)	mem 23876MB
[2022-11-12 07:20:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][400/1251]	eta 0:10:39 lr 0.000666	time 0.7495 (0.7518)	loss 2.9985 (3.3026)	grad_norm 1.3493 (1.4818)	mem 23876MB
[2022-11-12 07:21:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][450/1251]	eta 0:10:01 lr 0.000666	time 0.7432 (0.7512)	loss 2.6726 (3.3058)	grad_norm 1.3729 (1.4774)	mem 23876MB
[2022-11-12 07:21:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][500/1251]	eta 0:09:23 lr 0.000666	time 0.7380 (0.7505)	loss 3.1175 (3.3120)	grad_norm 1.3182 (1.4851)	mem 23876MB
[2022-11-12 07:22:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][550/1251]	eta 0:08:45 lr 0.000666	time 0.7385 (0.7503)	loss 3.5645 (3.3087)	grad_norm 1.5288 (1.4885)	mem 23876MB
[2022-11-12 07:23:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][600/1251]	eta 0:08:08 lr 0.000665	time 0.8482 (0.7502)	loss 3.9288 (3.3203)	grad_norm 1.5438 (1.4881)	mem 23876MB
[2022-11-12 07:23:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][650/1251]	eta 0:07:30 lr 0.000665	time 0.7317 (0.7498)	loss 3.7289 (3.3235)	grad_norm 1.3464 (1.4913)	mem 23876MB
[2022-11-12 07:24:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][700/1251]	eta 0:06:52 lr 0.000665	time 0.7380 (0.7495)	loss 3.3861 (3.3222)	grad_norm 1.3823 (1.4946)	mem 23876MB
[2022-11-12 07:24:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][750/1251]	eta 0:06:15 lr 0.000665	time 0.7349 (0.7495)	loss 3.6708 (3.3181)	grad_norm 1.5664 (1.4922)	mem 23876MB
[2022-11-12 07:25:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][800/1251]	eta 0:05:37 lr 0.000665	time 0.7496 (0.7492)	loss 3.6318 (3.3294)	grad_norm 1.5498 (inf)	mem 23876MB
[2022-11-12 07:26:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][850/1251]	eta 0:05:00 lr 0.000664	time 0.7429 (0.7492)	loss 2.8974 (3.3293)	grad_norm 1.3390 (inf)	mem 23876MB
[2022-11-12 07:26:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][900/1251]	eta 0:04:22 lr 0.000664	time 0.7420 (0.7490)	loss 3.4093 (3.3270)	grad_norm 1.5205 (inf)	mem 23876MB
[2022-11-12 07:27:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][950/1251]	eta 0:03:45 lr 0.000664	time 0.7416 (0.7488)	loss 3.2767 (3.3270)	grad_norm 1.3811 (inf)	mem 23876MB
[2022-11-12 07:28:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][1000/1251]	eta 0:03:07 lr 0.000664	time 0.7377 (0.7488)	loss 3.5947 (3.3245)	grad_norm 1.5931 (inf)	mem 23876MB
[2022-11-12 07:28:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][1050/1251]	eta 0:02:30 lr 0.000664	time 0.7372 (0.7487)	loss 3.6268 (3.3286)	grad_norm 1.5655 (inf)	mem 23876MB
[2022-11-12 07:29:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][1100/1251]	eta 0:01:53 lr 0.000663	time 0.7440 (0.7485)	loss 2.8387 (3.3241)	grad_norm 1.4129 (inf)	mem 23876MB
[2022-11-12 07:29:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][1150/1251]	eta 0:01:15 lr 0.000663	time 0.7336 (0.7486)	loss 3.9697 (3.3299)	grad_norm 1.4644 (inf)	mem 23876MB
[2022-11-12 07:30:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][1200/1251]	eta 0:00:38 lr 0.000663	time 0.7410 (0.7484)	loss 3.6251 (3.3282)	grad_norm 1.4571 (inf)	mem 23876MB
[2022-11-12 07:31:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [118/300][1250/1251]	eta 0:00:00 lr 0.000663	time 0.7249 (0.7482)	loss 3.1403 (3.3270)	grad_norm 1.4023 (inf)	mem 23876MB
[2022-11-12 07:31:11 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 118 training takes 0:15:36
[2022-11-12 07:31:12 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_118.pth saving......
[2022-11-12 07:31:13 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_118.pth saved !!!
[2022-11-12 07:31:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.632 (1.632)	Loss 0.9529 (0.9529)	Acc@1 79.199 (79.199)	Acc@5 93.945 (93.945)	Mem 23876MB
[2022-11-12 07:31:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.074 Acc@5 94.446
[2022-11-12 07:31:25 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.1%
[2022-11-12 07:31:27 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.840 (1.840)	Loss 0.8137 (0.8137)	Acc@1 79.004 (79.004)	Acc@5 96.094 (96.094)	Mem 23876MB
[2022-11-12 07:31:38 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.526 Acc@5 95.582
[2022-11-12 07:31:38 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.5%
[2022-11-12 07:31:38 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.53% at 118 epoch
[2022-11-12 07:31:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][0/1251]	eta 0:50:33 lr 0.000663	time 2.4245 (2.4245)	loss 3.6047 (3.6047)	grad_norm 1.3781 (1.3781)	mem 23876MB
[2022-11-12 07:32:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][50/1251]	eta 0:15:34 lr 0.000663	time 0.7447 (0.7782)	loss 3.8036 (3.3123)	grad_norm 1.5750 (1.4771)	mem 23876MB
[2022-11-12 07:32:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][100/1251]	eta 0:14:38 lr 0.000662	time 0.7447 (0.7630)	loss 3.2220 (3.2455)	grad_norm 1.3368 (1.5123)	mem 23876MB
[2022-11-12 07:33:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][150/1251]	eta 0:13:55 lr 0.000662	time 0.7380 (0.7584)	loss 2.9268 (3.2631)	grad_norm 1.4860 (1.5077)	mem 23876MB
[2022-11-12 07:34:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][200/1251]	eta 0:13:14 lr 0.000662	time 0.7399 (0.7555)	loss 3.8428 (3.2650)	grad_norm 1.5000 (1.4915)	mem 23876MB
[2022-11-12 07:34:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][250/1251]	eta 0:12:34 lr 0.000662	time 0.7483 (0.7536)	loss 3.2486 (3.2872)	grad_norm 1.7501 (1.4929)	mem 23876MB
[2022-11-12 07:35:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][300/1251]	eta 0:11:55 lr 0.000662	time 0.7339 (0.7528)	loss 3.1400 (3.2711)	grad_norm 1.5728 (1.5022)	mem 23876MB
[2022-11-12 07:36:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][350/1251]	eta 0:11:17 lr 0.000662	time 0.7398 (0.7521)	loss 3.4635 (3.2705)	grad_norm 1.5053 (1.5062)	mem 23876MB
[2022-11-12 07:36:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][400/1251]	eta 0:10:39 lr 0.000661	time 0.7405 (0.7515)	loss 2.8232 (3.2770)	grad_norm 1.3966 (1.5048)	mem 23876MB
[2022-11-12 07:37:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][450/1251]	eta 0:10:01 lr 0.000661	time 0.7412 (0.7508)	loss 3.6509 (3.2698)	grad_norm 1.4158 (1.5049)	mem 23876MB
[2022-11-12 07:37:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][500/1251]	eta 0:09:23 lr 0.000661	time 0.7500 (0.7503)	loss 3.2052 (3.2818)	grad_norm 1.4481 (1.4997)	mem 23876MB
[2022-11-12 07:38:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][550/1251]	eta 0:08:45 lr 0.000661	time 0.7389 (0.7503)	loss 3.0732 (3.2838)	grad_norm 1.4882 (1.4983)	mem 23876MB
[2022-11-12 07:39:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][600/1251]	eta 0:08:08 lr 0.000661	time 0.7235 (0.7500)	loss 2.8564 (3.2913)	grad_norm 1.5217 (1.4985)	mem 23876MB
[2022-11-12 07:39:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][650/1251]	eta 0:07:30 lr 0.000660	time 0.7385 (0.7497)	loss 3.6949 (3.2994)	grad_norm 1.4638 (1.4983)	mem 23876MB
[2022-11-12 07:40:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][700/1251]	eta 0:06:53 lr 0.000660	time 0.7265 (0.7497)	loss 2.4321 (3.3050)	grad_norm 1.4318 (1.5003)	mem 23876MB
[2022-11-12 07:41:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][750/1251]	eta 0:06:15 lr 0.000660	time 0.7366 (0.7496)	loss 2.7353 (3.2965)	grad_norm 1.4678 (1.4979)	mem 23876MB
[2022-11-12 07:41:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][800/1251]	eta 0:05:37 lr 0.000660	time 0.7365 (0.7493)	loss 3.5518 (3.2971)	grad_norm 1.3796 (1.4978)	mem 23876MB
[2022-11-12 07:42:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][850/1251]	eta 0:05:00 lr 0.000660	time 0.7389 (0.7492)	loss 3.3983 (3.2978)	grad_norm 1.4446 (1.4964)	mem 23876MB
[2022-11-12 07:42:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][900/1251]	eta 0:04:22 lr 0.000659	time 0.7368 (0.7491)	loss 2.2618 (3.3022)	grad_norm 1.4148 (1.4960)	mem 23876MB
[2022-11-12 07:43:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][950/1251]	eta 0:03:45 lr 0.000659	time 0.7397 (0.7490)	loss 2.8648 (3.3019)	grad_norm 1.7169 (1.4961)	mem 23876MB
[2022-11-12 07:44:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][1000/1251]	eta 0:03:07 lr 0.000659	time 0.7405 (0.7488)	loss 3.3195 (3.3031)	grad_norm 1.2775 (1.4976)	mem 23876MB
[2022-11-12 07:44:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][1050/1251]	eta 0:02:30 lr 0.000659	time 0.7378 (0.7489)	loss 3.8388 (3.3092)	grad_norm 1.7300 (1.4953)	mem 23876MB
[2022-11-12 07:45:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][1100/1251]	eta 0:01:53 lr 0.000659	time 0.7445 (0.7488)	loss 3.7250 (3.3128)	grad_norm 1.3531 (1.4968)	mem 23876MB
[2022-11-12 07:46:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][1150/1251]	eta 0:01:15 lr 0.000658	time 0.7398 (0.7488)	loss 1.9715 (3.3169)	grad_norm 1.7218 (1.4968)	mem 23876MB
[2022-11-12 07:46:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][1200/1251]	eta 0:00:38 lr 0.000658	time 0.7473 (0.7487)	loss 4.0958 (3.3190)	grad_norm 1.5253 (1.4973)	mem 23876MB
[2022-11-12 07:47:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [119/300][1250/1251]	eta 0:00:00 lr 0.000658	time 0.7264 (0.7484)	loss 3.3322 (3.3195)	grad_norm 1.6229 (1.4983)	mem 23876MB
[2022-11-12 07:47:14 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 119 training takes 0:15:36
[2022-11-12 07:47:14 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_119.pth saving......
[2022-11-12 07:47:16 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_119.pth saved !!!
[2022-11-12 07:47:17 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.657 (1.657)	Loss 0.9570 (0.9570)	Acc@1 77.832 (77.832)	Acc@5 93.652 (93.652)	Mem 23876MB
[2022-11-12 07:47:28 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.200 Acc@5 94.482
[2022-11-12 07:47:28 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.2%
[2022-11-12 07:47:30 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.878 (1.878)	Loss 0.8214 (0.8214)	Acc@1 79.492 (79.492)	Acc@5 96.094 (96.094)	Mem 23876MB
[2022-11-12 07:47:41 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.530 Acc@5 95.600
[2022-11-12 07:47:41 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.5%
[2022-11-12 07:47:41 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.53% at 119 epoch
[2022-11-12 07:47:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][0/1251]	eta 0:51:51 lr 0.000658	time 2.4868 (2.4868)	loss 2.9673 (2.9673)	grad_norm 1.5130 (1.5130)	mem 23876MB
[2022-11-12 07:48:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][50/1251]	eta 0:15:40 lr 0.000658	time 0.7385 (0.7834)	loss 2.4643 (3.3717)	grad_norm 1.5569 (1.4960)	mem 23876MB
[2022-11-12 07:48:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][100/1251]	eta 0:14:41 lr 0.000658	time 0.7390 (0.7656)	loss 3.8492 (3.2983)	grad_norm 1.2849 (1.4942)	mem 23876MB
[2022-11-12 07:49:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][150/1251]	eta 0:13:56 lr 0.000657	time 0.7381 (0.7595)	loss 3.1523 (3.2823)	grad_norm 1.3662 (1.4917)	mem 23876MB
[2022-11-12 07:50:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][200/1251]	eta 0:13:15 lr 0.000657	time 0.8114 (0.7569)	loss 3.3500 (3.3038)	grad_norm 1.3648 (1.4896)	mem 23876MB
[2022-11-12 07:50:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][250/1251]	eta 0:12:35 lr 0.000657	time 0.7381 (0.7544)	loss 2.7450 (3.3093)	grad_norm 1.6609 (1.4918)	mem 23876MB
[2022-11-12 07:51:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][300/1251]	eta 0:11:56 lr 0.000657	time 0.7378 (0.7538)	loss 2.5412 (3.3251)	grad_norm 1.4929 (1.4928)	mem 23876MB
[2022-11-12 07:52:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][350/1251]	eta 0:11:17 lr 0.000657	time 0.7410 (0.7523)	loss 4.0525 (3.3339)	grad_norm 1.5069 (1.4949)	mem 23876MB
[2022-11-12 07:52:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][400/1251]	eta 0:10:39 lr 0.000656	time 0.7403 (0.7515)	loss 2.3056 (3.3363)	grad_norm 1.5003 (nan)	mem 23876MB
[2022-11-12 07:53:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][450/1251]	eta 0:10:01 lr 0.000656	time 0.7415 (0.7511)	loss 2.7004 (3.3367)	grad_norm 1.5054 (nan)	mem 23876MB
[2022-11-12 07:53:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][500/1251]	eta 0:09:23 lr 0.000656	time 0.7369 (0.7504)	loss 3.0665 (3.3362)	grad_norm 1.3706 (nan)	mem 23876MB
[2022-11-12 07:54:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][550/1251]	eta 0:08:46 lr 0.000656	time 0.7337 (0.7504)	loss 3.4328 (3.3418)	grad_norm 1.3919 (nan)	mem 23876MB
[2022-11-12 07:55:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][600/1251]	eta 0:08:08 lr 0.000656	time 0.8193 (0.7502)	loss 3.4407 (3.3410)	grad_norm 1.5990 (nan)	mem 23876MB
[2022-11-12 07:55:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][650/1251]	eta 0:07:30 lr 0.000655	time 0.7402 (0.7498)	loss 3.5006 (3.3335)	grad_norm 1.5037 (nan)	mem 23876MB
[2022-11-12 07:56:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][700/1251]	eta 0:06:52 lr 0.000655	time 0.7348 (0.7495)	loss 2.2606 (3.3345)	grad_norm 1.4962 (nan)	mem 23876MB
[2022-11-12 07:57:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][750/1251]	eta 0:06:15 lr 0.000655	time 0.7385 (0.7493)	loss 4.1178 (3.3345)	grad_norm 1.5324 (nan)	mem 23876MB
[2022-11-12 07:57:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][800/1251]	eta 0:05:37 lr 0.000655	time 0.7367 (0.7491)	loss 2.7900 (3.3327)	grad_norm 1.6498 (nan)	mem 23876MB
[2022-11-12 07:58:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][850/1251]	eta 0:05:00 lr 0.000655	time 0.7440 (0.7491)	loss 2.5919 (3.3352)	grad_norm 1.3473 (nan)	mem 23876MB
[2022-11-12 07:58:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][900/1251]	eta 0:04:22 lr 0.000654	time 0.7473 (0.7487)	loss 3.3796 (3.3348)	grad_norm 1.4010 (nan)	mem 23876MB
[2022-11-12 07:59:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][950/1251]	eta 0:03:45 lr 0.000654	time 0.7390 (0.7488)	loss 2.5958 (3.3327)	grad_norm 1.4459 (nan)	mem 23876MB
[2022-11-12 08:00:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][1000/1251]	eta 0:03:07 lr 0.000654	time 0.8188 (0.7487)	loss 2.9161 (3.3361)	grad_norm 1.5700 (nan)	mem 23876MB
[2022-11-12 08:00:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][1050/1251]	eta 0:02:30 lr 0.000654	time 0.7364 (0.7484)	loss 3.1061 (3.3349)	grad_norm 1.3968 (nan)	mem 23876MB
[2022-11-12 08:01:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][1100/1251]	eta 0:01:52 lr 0.000654	time 0.7363 (0.7483)	loss 3.5312 (3.3346)	grad_norm 1.4681 (nan)	mem 23876MB
[2022-11-12 08:02:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][1150/1251]	eta 0:01:15 lr 0.000653	time 0.7443 (0.7483)	loss 3.4983 (3.3333)	grad_norm 1.4204 (nan)	mem 23876MB
[2022-11-12 08:02:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][1200/1251]	eta 0:00:38 lr 0.000653	time 0.7363 (0.7482)	loss 3.9261 (3.3330)	grad_norm 1.6132 (nan)	mem 23876MB
[2022-11-12 08:03:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [120/300][1250/1251]	eta 0:00:00 lr 0.000653	time 0.7410 (0.7482)	loss 2.3777 (3.3351)	grad_norm 1.4773 (nan)	mem 23876MB
[2022-11-12 08:03:17 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 120 training takes 0:15:36
[2022-11-12 08:03:17 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_120.pth saving......
[2022-11-12 08:03:18 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_120.pth saved !!!
[2022-11-12 08:03:20 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.679 (1.679)	Loss 0.8871 (0.8871)	Acc@1 78.027 (78.027)	Acc@5 95.020 (95.020)	Mem 23876MB
[2022-11-12 08:03:30 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.976 Acc@5 94.494
[2022-11-12 08:03:30 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.0%
[2022-11-12 08:03:32 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.865 (1.865)	Loss 0.8174 (0.8174)	Acc@1 79.980 (79.980)	Acc@5 95.312 (95.312)	Mem 23876MB
[2022-11-12 08:03:43 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.570 Acc@5 95.638
[2022-11-12 08:03:43 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.6%
[2022-11-12 08:03:43 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.57% at 120 epoch
[2022-11-12 08:03:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][0/1251]	eta 0:50:36 lr 0.000653	time 2.4269 (2.4269)	loss 2.7483 (2.7483)	grad_norm 1.4527 (1.4527)	mem 23876MB
[2022-11-12 08:04:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][50/1251]	eta 0:15:46 lr 0.000653	time 0.8437 (0.7877)	loss 2.5570 (3.3622)	grad_norm 1.4146 (1.4550)	mem 23876MB
[2022-11-12 08:05:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][100/1251]	eta 0:14:44 lr 0.000653	time 0.7424 (0.7686)	loss 2.8629 (3.2844)	grad_norm 1.3031 (1.4610)	mem 23876MB
[2022-11-12 08:05:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][150/1251]	eta 0:13:59 lr 0.000652	time 0.7330 (0.7628)	loss 3.7500 (3.2546)	grad_norm 1.4605 (1.4754)	mem 23876MB
[2022-11-12 08:06:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][200/1251]	eta 0:13:17 lr 0.000652	time 0.7402 (0.7593)	loss 2.9942 (3.2508)	grad_norm 1.3737 (1.4841)	mem 23876MB
[2022-11-12 08:06:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][250/1251]	eta 0:12:37 lr 0.000652	time 0.7367 (0.7571)	loss 3.9082 (3.2635)	grad_norm 1.6587 (1.4976)	mem 23876MB
[2022-11-12 08:07:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][300/1251]	eta 0:11:59 lr 0.000652	time 0.7406 (0.7561)	loss 3.0344 (3.2603)	grad_norm 1.5154 (1.5058)	mem 23876MB
[2022-11-12 08:08:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][350/1251]	eta 0:11:19 lr 0.000652	time 0.7461 (0.7546)	loss 2.6141 (3.2681)	grad_norm 1.4561 (1.5002)	mem 23876MB
[2022-11-12 08:08:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][400/1251]	eta 0:10:41 lr 0.000651	time 0.7404 (0.7542)	loss 3.4119 (3.2753)	grad_norm 1.6621 (1.5028)	mem 23876MB
[2022-11-12 08:09:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][450/1251]	eta 0:10:03 lr 0.000651	time 0.7381 (0.7533)	loss 3.8017 (3.2885)	grad_norm 1.4924 (1.4984)	mem 23876MB
[2022-11-12 08:10:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][500/1251]	eta 0:09:25 lr 0.000651	time 0.7365 (0.7526)	loss 3.7460 (3.2955)	grad_norm 1.8246 (1.4995)	mem 23876MB
[2022-11-12 08:10:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][550/1251]	eta 0:08:47 lr 0.000651	time 0.7387 (0.7521)	loss 2.2812 (3.2946)	grad_norm 1.5327 (1.4985)	mem 23876MB
[2022-11-12 08:11:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][600/1251]	eta 0:08:09 lr 0.000651	time 0.7454 (0.7518)	loss 3.0327 (3.2938)	grad_norm 1.3500 (1.4980)	mem 23876MB
[2022-11-12 08:11:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][650/1251]	eta 0:07:31 lr 0.000650	time 0.7382 (0.7515)	loss 3.4184 (3.2974)	grad_norm 1.4560 (1.5008)	mem 23876MB
[2022-11-12 08:12:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][700/1251]	eta 0:06:53 lr 0.000650	time 0.7372 (0.7511)	loss 3.5956 (3.2966)	grad_norm 1.2640 (1.5019)	mem 23876MB
[2022-11-12 08:13:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][750/1251]	eta 0:06:16 lr 0.000650	time 0.7448 (0.7507)	loss 3.6916 (3.3074)	grad_norm 1.4596 (1.5014)	mem 23876MB
[2022-11-12 08:13:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][800/1251]	eta 0:05:38 lr 0.000650	time 0.7411 (0.7504)	loss 2.6852 (3.3082)	grad_norm 1.3966 (1.5022)	mem 23876MB
[2022-11-12 08:14:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][850/1251]	eta 0:05:00 lr 0.000650	time 0.7367 (0.7503)	loss 3.1876 (3.3058)	grad_norm 1.3833 (1.4998)	mem 23876MB
[2022-11-12 08:14:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][900/1251]	eta 0:04:23 lr 0.000649	time 0.7440 (0.7500)	loss 3.7516 (3.3075)	grad_norm 1.5663 (1.4984)	mem 23876MB
[2022-11-12 08:15:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][950/1251]	eta 0:03:45 lr 0.000649	time 0.7382 (0.7499)	loss 3.0197 (3.3056)	grad_norm 1.3434 (1.5012)	mem 23876MB
[2022-11-12 08:16:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][1000/1251]	eta 0:03:08 lr 0.000649	time 0.7357 (0.7498)	loss 3.2513 (3.3087)	grad_norm 1.4971 (1.5015)	mem 23876MB
[2022-11-12 08:16:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][1050/1251]	eta 0:02:30 lr 0.000649	time 0.7393 (0.7495)	loss 3.8459 (3.3127)	grad_norm 1.5703 (1.5007)	mem 23876MB
[2022-11-12 08:17:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][1100/1251]	eta 0:01:53 lr 0.000649	time 0.7433 (0.7496)	loss 2.9905 (3.3098)	grad_norm 1.5036 (1.5005)	mem 23876MB
[2022-11-12 08:18:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][1150/1251]	eta 0:01:15 lr 0.000648	time 0.7359 (0.7494)	loss 3.5857 (3.3046)	grad_norm 1.3140 (1.4993)	mem 23876MB
[2022-11-12 08:18:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][1200/1251]	eta 0:00:38 lr 0.000648	time 0.7307 (0.7494)	loss 2.2735 (3.3051)	grad_norm 1.4205 (1.4981)	mem 23876MB
[2022-11-12 08:19:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [121/300][1250/1251]	eta 0:00:00 lr 0.000648	time 0.7252 (0.7491)	loss 2.4341 (3.3087)	grad_norm 1.3995 (1.4991)	mem 23876MB
[2022-11-12 08:19:20 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 121 training takes 0:15:37
[2022-11-12 08:19:20 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_121.pth saving......
[2022-11-12 08:19:21 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_121.pth saved !!!
[2022-11-12 08:19:23 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.711 (1.711)	Loss 0.8552 (0.8552)	Acc@1 79.688 (79.688)	Acc@5 95.215 (95.215)	Mem 23876MB
[2022-11-12 08:19:34 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.206 Acc@5 94.474
[2022-11-12 08:19:34 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.2%
[2022-11-12 08:19:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.983 (1.983)	Loss 0.8164 (0.8164)	Acc@1 80.371 (80.371)	Acc@5 95.215 (95.215)	Mem 23876MB
[2022-11-12 08:19:46 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.602 Acc@5 95.666
[2022-11-12 08:19:46 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.6%
[2022-11-12 08:19:46 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.60% at 121 epoch
[2022-11-12 08:19:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][0/1251]	eta 0:49:54 lr 0.000648	time 2.3934 (2.3934)	loss 3.6129 (3.6129)	grad_norm 1.5185 (1.5185)	mem 23876MB
[2022-11-12 08:20:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][50/1251]	eta 0:15:39 lr 0.000648	time 0.7313 (0.7822)	loss 2.4628 (3.2940)	grad_norm 1.3797 (1.5357)	mem 23876MB
[2022-11-12 08:21:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][100/1251]	eta 0:14:42 lr 0.000648	time 0.7427 (0.7668)	loss 2.3425 (3.1943)	grad_norm 1.7039 (1.5064)	mem 23876MB
[2022-11-12 08:21:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][150/1251]	eta 0:13:56 lr 0.000647	time 0.7408 (0.7599)	loss 3.2062 (3.2126)	grad_norm 1.6480 (1.5121)	mem 23876MB
[2022-11-12 08:22:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][200/1251]	eta 0:13:15 lr 0.000647	time 0.7424 (0.7573)	loss 3.1468 (3.2224)	grad_norm 1.6110 (1.5164)	mem 23876MB
[2022-11-12 08:22:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][250/1251]	eta 0:12:36 lr 0.000647	time 0.7484 (0.7561)	loss 3.6269 (3.2419)	grad_norm 1.6841 (1.5111)	mem 23876MB
[2022-11-12 08:23:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][300/1251]	eta 0:11:57 lr 0.000647	time 0.7445 (0.7547)	loss 3.1603 (3.2487)	grad_norm 1.2578 (nan)	mem 23876MB
[2022-11-12 08:24:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][350/1251]	eta 0:11:18 lr 0.000647	time 0.7455 (0.7535)	loss 3.1676 (3.2500)	grad_norm 1.5939 (nan)	mem 23876MB
[2022-11-12 08:24:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][400/1251]	eta 0:10:40 lr 0.000646	time 0.7380 (0.7530)	loss 3.8513 (3.2447)	grad_norm 1.3732 (nan)	mem 23876MB
[2022-11-12 08:25:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][450/1251]	eta 0:10:02 lr 0.000646	time 0.7356 (0.7524)	loss 2.5471 (3.2539)	grad_norm 1.2790 (nan)	mem 23876MB
[2022-11-12 08:26:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][500/1251]	eta 0:09:24 lr 0.000646	time 0.7510 (0.7523)	loss 3.6309 (3.2493)	grad_norm 1.2971 (nan)	mem 23876MB
[2022-11-12 08:26:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][550/1251]	eta 0:08:46 lr 0.000646	time 0.7424 (0.7518)	loss 2.8588 (3.2543)	grad_norm 1.3892 (nan)	mem 23876MB
[2022-11-12 08:27:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][600/1251]	eta 0:08:09 lr 0.000646	time 0.7370 (0.7517)	loss 2.4153 (3.2515)	grad_norm 1.4981 (nan)	mem 23876MB
[2022-11-12 08:27:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][650/1251]	eta 0:07:31 lr 0.000645	time 0.7368 (0.7513)	loss 4.1454 (3.2663)	grad_norm 1.5157 (nan)	mem 23876MB
[2022-11-12 08:28:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][700/1251]	eta 0:06:53 lr 0.000645	time 0.7376 (0.7511)	loss 3.5250 (3.2745)	grad_norm 1.3761 (nan)	mem 23876MB
[2022-11-12 08:29:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][750/1251]	eta 0:06:16 lr 0.000645	time 0.7446 (0.7508)	loss 3.2352 (3.2792)	grad_norm 1.6753 (nan)	mem 23876MB
[2022-11-12 08:29:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][800/1251]	eta 0:05:38 lr 0.000645	time 0.7377 (0.7506)	loss 3.5601 (3.2851)	grad_norm 1.4100 (nan)	mem 23876MB
[2022-11-12 08:30:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][850/1251]	eta 0:05:01 lr 0.000645	time 0.7440 (0.7507)	loss 3.4388 (3.2914)	grad_norm 1.3411 (nan)	mem 23876MB
[2022-11-12 08:31:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][900/1251]	eta 0:04:23 lr 0.000644	time 0.7387 (0.7505)	loss 3.4991 (3.2917)	grad_norm 1.4091 (nan)	mem 23876MB
[2022-11-12 08:31:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][950/1251]	eta 0:03:45 lr 0.000644	time 0.8138 (0.7506)	loss 3.0582 (3.2891)	grad_norm 1.4465 (nan)	mem 23876MB
[2022-11-12 08:32:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][1000/1251]	eta 0:03:08 lr 0.000644	time 0.7356 (0.7504)	loss 3.5317 (3.2898)	grad_norm 1.3060 (nan)	mem 23876MB
[2022-11-12 08:32:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][1050/1251]	eta 0:02:30 lr 0.000644	time 0.7339 (0.7504)	loss 3.3070 (3.2907)	grad_norm 1.2287 (nan)	mem 23876MB
[2022-11-12 08:33:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][1100/1251]	eta 0:01:53 lr 0.000644	time 0.7410 (0.7503)	loss 3.2161 (3.2900)	grad_norm 1.3941 (nan)	mem 23876MB
[2022-11-12 08:34:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][1150/1251]	eta 0:01:15 lr 0.000644	time 0.7390 (0.7501)	loss 3.7572 (3.2940)	grad_norm 1.7693 (nan)	mem 23876MB
[2022-11-12 08:34:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][1200/1251]	eta 0:00:38 lr 0.000643	time 0.7362 (0.7501)	loss 3.7158 (3.2954)	grad_norm 1.9136 (nan)	mem 23876MB
[2022-11-12 08:35:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [122/300][1250/1251]	eta 0:00:00 lr 0.000643	time 0.7547 (0.7498)	loss 3.5411 (3.2966)	grad_norm 1.6505 (nan)	mem 23876MB
[2022-11-12 08:35:25 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 122 training takes 0:15:38
[2022-11-12 08:35:25 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_122.pth saving......
[2022-11-12 08:35:26 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_122.pth saved !!!
[2022-11-12 08:35:28 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.674 (1.674)	Loss 0.9459 (0.9459)	Acc@1 78.223 (78.223)	Acc@5 93.945 (93.945)	Mem 23876MB
[2022-11-12 08:35:38 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 77.910 Acc@5 94.278
[2022-11-12 08:35:38 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 77.9%
[2022-11-12 08:35:40 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.815 (1.815)	Loss 0.8114 (0.8114)	Acc@1 79.785 (79.785)	Acc@5 95.117 (95.117)	Mem 23876MB
[2022-11-12 08:35:51 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.634 Acc@5 95.658
[2022-11-12 08:35:51 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.6%
[2022-11-12 08:35:51 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.63% at 122 epoch
[2022-11-12 08:35:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][0/1251]	eta 0:52:16 lr 0.000643	time 2.5075 (2.5075)	loss 3.5259 (3.5259)	grad_norm 1.3484 (1.3484)	mem 23876MB
[2022-11-12 08:36:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][50/1251]	eta 0:15:34 lr 0.000643	time 0.7499 (0.7779)	loss 4.0966 (3.2672)	grad_norm 1.6090 (1.4996)	mem 23876MB
[2022-11-12 08:37:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][100/1251]	eta 0:14:40 lr 0.000643	time 0.7452 (0.7646)	loss 3.4199 (3.2598)	grad_norm 2.5947 (1.5331)	mem 23876MB
[2022-11-12 08:37:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][150/1251]	eta 0:13:54 lr 0.000643	time 0.7332 (0.7583)	loss 3.1690 (3.2924)	grad_norm 1.4031 (1.5319)	mem 23876MB
[2022-11-12 08:38:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][200/1251]	eta 0:13:14 lr 0.000642	time 0.7363 (0.7557)	loss 3.4344 (3.3319)	grad_norm 1.5108 (1.5277)	mem 23876MB
[2022-11-12 08:39:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][250/1251]	eta 0:12:34 lr 0.000642	time 0.7395 (0.7540)	loss 3.3606 (3.3226)	grad_norm 1.4911 (1.5141)	mem 23876MB
[2022-11-12 08:39:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][300/1251]	eta 0:11:55 lr 0.000642	time 0.7378 (0.7527)	loss 3.5264 (3.2935)	grad_norm 1.4993 (1.5102)	mem 23876MB
[2022-11-12 08:40:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][350/1251]	eta 0:11:17 lr 0.000642	time 0.7398 (0.7521)	loss 3.7180 (3.3152)	grad_norm 1.5852 (1.5065)	mem 23876MB
[2022-11-12 08:40:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][400/1251]	eta 0:10:39 lr 0.000642	time 0.7366 (0.7516)	loss 3.6775 (3.3152)	grad_norm 1.4396 (1.5078)	mem 23876MB
[2022-11-12 08:41:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][450/1251]	eta 0:10:01 lr 0.000641	time 0.7432 (0.7512)	loss 3.4008 (3.3173)	grad_norm 1.4827 (1.5131)	mem 23876MB
[2022-11-12 08:42:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][500/1251]	eta 0:09:23 lr 0.000641	time 0.7407 (0.7510)	loss 3.6503 (3.3135)	grad_norm 1.3079 (1.5117)	mem 23876MB
[2022-11-12 08:42:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][550/1251]	eta 0:08:45 lr 0.000641	time 0.7349 (0.7502)	loss 3.1200 (3.3184)	grad_norm 1.4511 (1.5119)	mem 23876MB
[2022-11-12 08:43:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][600/1251]	eta 0:08:08 lr 0.000641	time 0.8186 (0.7500)	loss 3.8781 (3.3245)	grad_norm 1.5136 (1.5114)	mem 23876MB
[2022-11-12 08:43:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][650/1251]	eta 0:07:30 lr 0.000641	time 0.8341 (0.7498)	loss 2.5662 (3.3295)	grad_norm 1.4699 (1.5118)	mem 23876MB
[2022-11-12 08:44:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][700/1251]	eta 0:06:52 lr 0.000640	time 0.7427 (0.7495)	loss 3.3123 (3.3270)	grad_norm 1.4483 (1.5122)	mem 23876MB
[2022-11-12 08:45:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][750/1251]	eta 0:06:15 lr 0.000640	time 0.7380 (0.7492)	loss 3.8075 (3.3242)	grad_norm 1.5650 (1.5153)	mem 23876MB
[2022-11-12 08:45:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][800/1251]	eta 0:05:37 lr 0.000640	time 0.7334 (0.7493)	loss 3.5303 (3.3259)	grad_norm 1.5641 (1.5164)	mem 23876MB
[2022-11-12 08:46:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][850/1251]	eta 0:05:00 lr 0.000640	time 0.7362 (0.7488)	loss 3.2128 (3.3267)	grad_norm 1.2715 (1.5187)	mem 23876MB
[2022-11-12 08:47:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][900/1251]	eta 0:04:22 lr 0.000640	time 0.7371 (0.7488)	loss 3.5102 (3.3231)	grad_norm 1.2235 (1.5173)	mem 23876MB
[2022-11-12 08:47:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][950/1251]	eta 0:03:45 lr 0.000639	time 0.7376 (0.7486)	loss 3.4617 (3.3228)	grad_norm 1.4373 (1.5160)	mem 23876MB
[2022-11-12 08:48:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][1000/1251]	eta 0:03:07 lr 0.000639	time 0.7316 (0.7485)	loss 3.3567 (3.3230)	grad_norm 1.4709 (1.5169)	mem 23876MB
[2022-11-12 08:48:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][1050/1251]	eta 0:02:30 lr 0.000639	time 0.7406 (0.7483)	loss 3.4312 (3.3226)	grad_norm 1.6684 (1.5206)	mem 23876MB
[2022-11-12 08:49:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][1100/1251]	eta 0:01:52 lr 0.000639	time 0.7384 (0.7482)	loss 3.4110 (3.3239)	grad_norm 1.5114 (1.5180)	mem 23876MB
[2022-11-12 08:50:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][1150/1251]	eta 0:01:15 lr 0.000639	time 0.7464 (0.7480)	loss 3.6877 (3.3216)	grad_norm 1.5802 (1.5185)	mem 23876MB
[2022-11-12 08:50:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][1200/1251]	eta 0:00:38 lr 0.000638	time 0.8150 (0.7481)	loss 3.6513 (3.3233)	grad_norm 1.4439 (1.5172)	mem 23876MB
[2022-11-12 08:51:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [123/300][1250/1251]	eta 0:00:00 lr 0.000638	time 0.7328 (0.7478)	loss 3.5067 (3.3189)	grad_norm 1.4688 (1.5161)	mem 23876MB
[2022-11-12 08:51:27 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 123 training takes 0:15:35
[2022-11-12 08:51:27 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_123.pth saving......
[2022-11-12 08:51:28 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_123.pth saved !!!
[2022-11-12 08:51:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.581 (1.581)	Loss 0.9161 (0.9161)	Acc@1 77.441 (77.441)	Acc@5 94.336 (94.336)	Mem 23876MB
[2022-11-12 08:51:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.368 Acc@5 94.646
[2022-11-12 08:51:40 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.4%
[2022-11-12 08:51:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.873 (1.873)	Loss 0.7807 (0.7807)	Acc@1 83.008 (83.008)	Acc@5 95.215 (95.215)	Mem 23876MB
[2022-11-12 08:51:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.688 Acc@5 95.674
[2022-11-12 08:51:53 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.7%
[2022-11-12 08:51:53 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.69% at 123 epoch
[2022-11-12 08:51:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][0/1251]	eta 0:51:37 lr 0.000638	time 2.4762 (2.4762)	loss 3.1990 (3.1990)	grad_norm 1.3549 (1.3549)	mem 23876MB
[2022-11-12 08:52:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][50/1251]	eta 0:15:37 lr 0.000638	time 0.7356 (0.7808)	loss 3.7987 (3.3036)	grad_norm 1.2850 (1.4709)	mem 23876MB
[2022-11-12 08:53:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][100/1251]	eta 0:14:40 lr 0.000638	time 0.7384 (0.7648)	loss 3.9932 (3.2700)	grad_norm 1.4269 (inf)	mem 23876MB
[2022-11-12 08:53:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][150/1251]	eta 0:13:55 lr 0.000638	time 0.7360 (0.7591)	loss 3.5914 (3.2827)	grad_norm 1.4753 (inf)	mem 23876MB
[2022-11-12 08:54:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][200/1251]	eta 0:13:14 lr 0.000637	time 0.7478 (0.7562)	loss 3.9117 (3.2813)	grad_norm 1.6226 (inf)	mem 23876MB
[2022-11-12 08:55:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][250/1251]	eta 0:12:35 lr 0.000637	time 0.7288 (0.7544)	loss 3.4832 (3.3124)	grad_norm 1.3489 (inf)	mem 23876MB
[2022-11-12 08:55:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][300/1251]	eta 0:11:55 lr 0.000637	time 0.7403 (0.7528)	loss 3.1627 (3.3081)	grad_norm 1.7541 (inf)	mem 23876MB
[2022-11-12 08:56:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][350/1251]	eta 0:11:17 lr 0.000637	time 0.7394 (0.7522)	loss 2.6363 (3.3162)	grad_norm 1.5903 (inf)	mem 23876MB
[2022-11-12 08:56:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][400/1251]	eta 0:10:39 lr 0.000637	time 0.7424 (0.7514)	loss 3.2875 (3.3140)	grad_norm 1.3733 (inf)	mem 23876MB
[2022-11-12 08:57:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][450/1251]	eta 0:10:01 lr 0.000636	time 0.7403 (0.7507)	loss 2.2917 (3.3167)	grad_norm 1.4266 (inf)	mem 23876MB
[2022-11-12 08:58:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][500/1251]	eta 0:09:23 lr 0.000636	time 0.7430 (0.7502)	loss 3.2245 (3.3194)	grad_norm 1.4668 (inf)	mem 23876MB
[2022-11-12 08:58:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][550/1251]	eta 0:08:45 lr 0.000636	time 0.7404 (0.7500)	loss 3.2753 (3.3148)	grad_norm 1.4579 (inf)	mem 23876MB
[2022-11-12 08:59:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][600/1251]	eta 0:08:08 lr 0.000636	time 0.7399 (0.7497)	loss 3.6457 (3.3177)	grad_norm 1.3773 (inf)	mem 23876MB
[2022-11-12 09:00:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][650/1251]	eta 0:07:30 lr 0.000636	time 0.7438 (0.7494)	loss 2.9307 (3.3212)	grad_norm 1.5096 (inf)	mem 23876MB
[2022-11-12 09:00:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][700/1251]	eta 0:06:52 lr 0.000635	time 0.7379 (0.7492)	loss 3.4225 (3.3159)	grad_norm 1.4691 (inf)	mem 23876MB
[2022-11-12 09:01:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][750/1251]	eta 0:06:15 lr 0.000635	time 0.8100 (0.7492)	loss 2.9502 (3.3210)	grad_norm 1.3932 (inf)	mem 23876MB
[2022-11-12 09:01:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][800/1251]	eta 0:05:37 lr 0.000635	time 0.7384 (0.7490)	loss 4.2534 (3.3165)	grad_norm 1.5884 (inf)	mem 23876MB
[2022-11-12 09:02:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][850/1251]	eta 0:05:00 lr 0.000635	time 0.7387 (0.7486)	loss 2.2760 (3.3145)	grad_norm 1.3372 (inf)	mem 23876MB
[2022-11-12 09:03:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][900/1251]	eta 0:04:22 lr 0.000635	time 0.7463 (0.7486)	loss 3.6782 (3.3137)	grad_norm 1.4968 (inf)	mem 23876MB
[2022-11-12 09:03:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][950/1251]	eta 0:03:45 lr 0.000634	time 0.7337 (0.7485)	loss 3.6026 (3.3165)	grad_norm 1.4436 (inf)	mem 23876MB
[2022-11-12 09:04:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][1000/1251]	eta 0:03:07 lr 0.000634	time 0.7370 (0.7483)	loss 3.3065 (3.3162)	grad_norm 1.4679 (inf)	mem 23876MB
[2022-11-12 09:04:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][1050/1251]	eta 0:02:30 lr 0.000634	time 0.7340 (0.7483)	loss 3.5882 (3.3124)	grad_norm 1.4800 (inf)	mem 23876MB
[2022-11-12 09:05:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][1100/1251]	eta 0:01:52 lr 0.000634	time 0.7382 (0.7481)	loss 3.5095 (3.3185)	grad_norm 1.4280 (inf)	mem 23876MB
[2022-11-12 09:06:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][1150/1251]	eta 0:01:15 lr 0.000634	time 0.7358 (0.7482)	loss 2.8734 (3.3187)	grad_norm 1.5304 (inf)	mem 23876MB
[2022-11-12 09:06:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][1200/1251]	eta 0:00:38 lr 0.000633	time 0.7372 (0.7481)	loss 2.7275 (3.3215)	grad_norm 1.6245 (inf)	mem 23876MB
[2022-11-12 09:07:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [124/300][1250/1251]	eta 0:00:00 lr 0.000633	time 0.7267 (0.7478)	loss 2.7539 (3.3206)	grad_norm 1.3690 (inf)	mem 23876MB
[2022-11-12 09:07:28 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 124 training takes 0:15:35
[2022-11-12 09:07:28 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_124.pth saving......
[2022-11-12 09:07:30 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_124.pth saved !!!
[2022-11-12 09:07:31 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.600 (1.600)	Loss 0.9120 (0.9120)	Acc@1 78.809 (78.809)	Acc@5 94.629 (94.629)	Mem 23876MB
[2022-11-12 09:07:42 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.254 Acc@5 94.502
[2022-11-12 09:07:42 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.3%
[2022-11-12 09:07:44 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.914 (1.914)	Loss 0.8435 (0.8435)	Acc@1 80.371 (80.371)	Acc@5 94.922 (94.922)	Mem 23876MB
[2022-11-12 09:07:55 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.748 Acc@5 95.688
[2022-11-12 09:07:55 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.7%
[2022-11-12 09:07:55 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.75% at 124 epoch
[2022-11-12 09:07:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][0/1251]	eta 0:48:47 lr 0.000633	time 2.3398 (2.3398)	loss 3.4363 (3.4363)	grad_norm 1.4260 (1.4260)	mem 23876MB
[2022-11-12 09:08:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][50/1251]	eta 0:15:38 lr 0.000633	time 0.7393 (0.7811)	loss 3.6586 (3.1948)	grad_norm 1.6275 (1.5382)	mem 23876MB
[2022-11-12 09:09:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][100/1251]	eta 0:14:41 lr 0.000633	time 0.7422 (0.7659)	loss 3.3522 (3.2423)	grad_norm 1.4289 (1.5201)	mem 23876MB
[2022-11-12 09:09:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][150/1251]	eta 0:13:57 lr 0.000633	time 0.7381 (0.7609)	loss 3.4197 (3.2949)	grad_norm 1.5039 (1.5141)	mem 23876MB
[2022-11-12 09:10:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][200/1251]	eta 0:13:16 lr 0.000632	time 0.7367 (0.7581)	loss 3.8200 (3.2793)	grad_norm 1.4472 (1.5093)	mem 23876MB
[2022-11-12 09:11:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][250/1251]	eta 0:12:37 lr 0.000632	time 0.8300 (0.7563)	loss 3.0272 (3.2862)	grad_norm 1.5951 (1.5141)	mem 23876MB
[2022-11-12 09:11:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][300/1251]	eta 0:11:57 lr 0.000632	time 0.7402 (0.7548)	loss 3.1818 (3.3105)	grad_norm 1.4903 (1.5140)	mem 23876MB
[2022-11-12 09:12:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][350/1251]	eta 0:11:18 lr 0.000632	time 0.7350 (0.7535)	loss 2.7657 (3.3051)	grad_norm 1.2969 (1.5122)	mem 23876MB
[2022-11-12 09:12:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][400/1251]	eta 0:10:40 lr 0.000632	time 0.7432 (0.7527)	loss 3.8511 (3.2986)	grad_norm 1.7317 (1.5124)	mem 23876MB
[2022-11-12 09:13:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][450/1251]	eta 0:10:02 lr 0.000631	time 0.7352 (0.7522)	loss 2.8080 (3.2948)	grad_norm 1.3796 (1.5102)	mem 23876MB
[2022-11-12 09:14:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][500/1251]	eta 0:09:24 lr 0.000631	time 0.7360 (0.7517)	loss 3.8191 (3.2810)	grad_norm 1.5798 (1.5138)	mem 23876MB
[2022-11-12 09:14:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][550/1251]	eta 0:08:46 lr 0.000631	time 0.7345 (0.7512)	loss 3.1690 (3.2885)	grad_norm 1.4562 (1.5138)	mem 23876MB
[2022-11-12 09:15:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][600/1251]	eta 0:08:08 lr 0.000631	time 0.7279 (0.7509)	loss 2.6194 (3.2892)	grad_norm 1.6220 (1.5146)	mem 23876MB
[2022-11-12 09:16:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][650/1251]	eta 0:07:31 lr 0.000631	time 0.7393 (0.7506)	loss 2.0872 (3.2827)	grad_norm 1.5627 (1.5142)	mem 23876MB
[2022-11-12 09:16:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][700/1251]	eta 0:06:53 lr 0.000630	time 0.7358 (0.7504)	loss 3.4804 (3.2816)	grad_norm 1.4512 (1.5161)	mem 23876MB
[2022-11-12 09:17:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][750/1251]	eta 0:06:15 lr 0.000630	time 0.7282 (0.7502)	loss 2.4625 (3.2852)	grad_norm 1.4521 (1.5174)	mem 23876MB
[2022-11-12 09:17:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][800/1251]	eta 0:05:38 lr 0.000630	time 0.7397 (0.7500)	loss 3.1199 (3.2857)	grad_norm 1.4940 (1.5155)	mem 23876MB
[2022-11-12 09:18:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][850/1251]	eta 0:05:00 lr 0.000630	time 0.7382 (0.7499)	loss 3.6813 (3.2841)	grad_norm 1.5581 (1.5129)	mem 23876MB
[2022-11-12 09:19:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][900/1251]	eta 0:04:23 lr 0.000630	time 0.7377 (0.7497)	loss 3.5044 (3.2907)	grad_norm 1.5411 (1.5130)	mem 23876MB
[2022-11-12 09:19:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][950/1251]	eta 0:03:45 lr 0.000629	time 0.7402 (0.7496)	loss 2.7677 (3.2887)	grad_norm 1.2921 (1.5129)	mem 23876MB
[2022-11-12 09:20:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][1000/1251]	eta 0:03:08 lr 0.000629	time 0.8227 (0.7494)	loss 3.4633 (3.2910)	grad_norm 1.4423 (1.5132)	mem 23876MB
[2022-11-12 09:21:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][1050/1251]	eta 0:02:30 lr 0.000629	time 0.7424 (0.7494)	loss 3.5207 (3.2899)	grad_norm 1.2873 (1.5127)	mem 23876MB
[2022-11-12 09:21:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][1100/1251]	eta 0:01:53 lr 0.000629	time 0.7357 (0.7491)	loss 3.6848 (3.2877)	grad_norm 1.2977 (1.5113)	mem 23876MB
[2022-11-12 09:22:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][1150/1251]	eta 0:01:15 lr 0.000629	time 0.7471 (0.7491)	loss 3.4528 (3.2910)	grad_norm 1.3798 (1.5123)	mem 23876MB
[2022-11-12 09:22:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][1200/1251]	eta 0:00:38 lr 0.000628	time 0.7407 (0.7489)	loss 3.4702 (3.2957)	grad_norm 1.8755 (1.5143)	mem 23876MB
[2022-11-12 09:23:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [125/300][1250/1251]	eta 0:00:00 lr 0.000628	time 0.7251 (0.7488)	loss 3.3189 (3.2998)	grad_norm 1.3638 (1.5139)	mem 23876MB
[2022-11-12 09:23:32 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 125 training takes 0:15:36
[2022-11-12 09:23:32 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_125.pth saving......
[2022-11-12 09:23:33 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_125.pth saved !!!
[2022-11-12 09:23:35 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.659 (1.659)	Loss 0.9211 (0.9211)	Acc@1 78.516 (78.516)	Acc@5 94.238 (94.238)	Mem 23876MB
[2022-11-12 09:23:45 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.632 Acc@5 94.768
[2022-11-12 09:23:45 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.6%
[2022-11-12 09:23:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.742 (1.742)	Loss 0.7411 (0.7411)	Acc@1 82.812 (82.812)	Acc@5 95.508 (95.508)	Mem 23876MB
[2022-11-12 09:23:58 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.750 Acc@5 95.664
[2022-11-12 09:23:58 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.7%
[2022-11-12 09:23:58 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.75% at 125 epoch
[2022-11-12 09:24:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][0/1251]	eta 0:49:31 lr 0.000628	time 2.3757 (2.3757)	loss 3.4294 (3.4294)	grad_norm 1.4797 (1.4797)	mem 23876MB
[2022-11-12 09:24:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][50/1251]	eta 0:15:43 lr 0.000628	time 0.8258 (0.7859)	loss 3.1510 (3.2887)	grad_norm 1.4734 (1.5520)	mem 23876MB
[2022-11-12 09:25:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][100/1251]	eta 0:14:41 lr 0.000628	time 0.7455 (0.7655)	loss 3.6013 (3.2854)	grad_norm 1.4867 (1.5393)	mem 23876MB
[2022-11-12 09:25:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][150/1251]	eta 0:13:56 lr 0.000627	time 0.7396 (0.7599)	loss 2.7766 (3.2844)	grad_norm 1.6154 (1.5212)	mem 23876MB
[2022-11-12 09:26:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][200/1251]	eta 0:13:16 lr 0.000627	time 0.7357 (0.7576)	loss 3.5555 (3.2963)	grad_norm 1.6472 (1.5293)	mem 23876MB
[2022-11-12 09:27:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][250/1251]	eta 0:12:35 lr 0.000627	time 0.7386 (0.7550)	loss 3.6143 (3.2920)	grad_norm 1.6697 (1.5273)	mem 23876MB
[2022-11-12 09:27:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][300/1251]	eta 0:11:56 lr 0.000627	time 0.7416 (0.7536)	loss 2.1454 (3.3155)	grad_norm 1.5348 (1.5273)	mem 23876MB
[2022-11-12 09:28:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][350/1251]	eta 0:11:17 lr 0.000627	time 0.7394 (0.7524)	loss 3.6868 (3.3160)	grad_norm 1.5103 (1.5249)	mem 23876MB
[2022-11-12 09:28:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][400/1251]	eta 0:10:39 lr 0.000626	time 0.7302 (0.7518)	loss 3.4007 (3.3164)	grad_norm 1.3565 (1.5333)	mem 23876MB
[2022-11-12 09:29:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][450/1251]	eta 0:10:01 lr 0.000626	time 0.7375 (0.7513)	loss 3.6039 (3.3099)	grad_norm 1.6608 (1.5295)	mem 23876MB
[2022-11-12 09:30:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][500/1251]	eta 0:09:23 lr 0.000626	time 0.7497 (0.7510)	loss 2.9958 (3.3063)	grad_norm 1.7396 (1.5324)	mem 23876MB
[2022-11-12 09:30:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][550/1251]	eta 0:08:46 lr 0.000626	time 0.7368 (0.7504)	loss 3.2960 (3.2991)	grad_norm 1.5330 (1.5286)	mem 23876MB
[2022-11-12 09:31:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][600/1251]	eta 0:08:08 lr 0.000626	time 0.7386 (0.7504)	loss 2.9748 (3.2997)	grad_norm 1.5758 (1.5283)	mem 23876MB
[2022-11-12 09:32:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][650/1251]	eta 0:07:30 lr 0.000625	time 0.7382 (0.7499)	loss 3.4727 (3.3019)	grad_norm 1.7244 (1.5275)	mem 23876MB
[2022-11-12 09:32:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][700/1251]	eta 0:06:53 lr 0.000625	time 0.7431 (0.7498)	loss 3.2005 (3.3024)	grad_norm 1.5728 (1.5281)	mem 23876MB
[2022-11-12 09:33:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][750/1251]	eta 0:06:15 lr 0.000625	time 0.7394 (0.7497)	loss 3.2226 (3.2989)	grad_norm 1.4292 (1.5259)	mem 23876MB
[2022-11-12 09:33:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][800/1251]	eta 0:05:37 lr 0.000625	time 0.7377 (0.7494)	loss 3.7913 (3.2960)	grad_norm 1.4136 (1.5251)	mem 23876MB
[2022-11-12 09:34:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][850/1251]	eta 0:05:00 lr 0.000625	time 0.7583 (0.7493)	loss 3.1969 (3.3022)	grad_norm 1.6333 (1.5261)	mem 23876MB
[2022-11-12 09:35:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][900/1251]	eta 0:04:22 lr 0.000624	time 0.7375 (0.7490)	loss 3.6229 (3.3018)	grad_norm 1.6302 (1.5286)	mem 23876MB
[2022-11-12 09:35:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][950/1251]	eta 0:03:45 lr 0.000624	time 0.7394 (0.7488)	loss 2.2532 (3.2974)	grad_norm 1.9358 (1.5270)	mem 23876MB
[2022-11-12 09:36:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][1000/1251]	eta 0:03:07 lr 0.000624	time 0.7368 (0.7489)	loss 3.6060 (3.2979)	grad_norm 1.5106 (1.5264)	mem 23876MB
[2022-11-12 09:37:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][1050/1251]	eta 0:02:30 lr 0.000624	time 0.7321 (0.7487)	loss 2.9197 (3.2969)	grad_norm 1.5206 (1.5280)	mem 23876MB
[2022-11-12 09:37:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][1100/1251]	eta 0:01:53 lr 0.000624	time 0.7336 (0.7487)	loss 2.8102 (3.2937)	grad_norm 1.9130 (1.5292)	mem 23876MB
[2022-11-12 09:38:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][1150/1251]	eta 0:01:15 lr 0.000623	time 0.7371 (0.7486)	loss 2.9624 (3.2941)	grad_norm 1.5529 (1.5289)	mem 23876MB
[2022-11-12 09:38:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][1200/1251]	eta 0:00:38 lr 0.000623	time 0.7470 (0.7485)	loss 3.6014 (3.2905)	grad_norm 1.3777 (1.5285)	mem 23876MB
[2022-11-12 09:39:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [126/300][1250/1251]	eta 0:00:00 lr 0.000623	time 0.7271 (0.7482)	loss 3.9258 (3.2987)	grad_norm 1.3895 (1.5279)	mem 23876MB
[2022-11-12 09:39:34 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 126 training takes 0:15:36
[2022-11-12 09:39:34 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_126.pth saving......
[2022-11-12 09:39:35 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_126.pth saved !!!
[2022-11-12 09:39:37 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.587 (1.587)	Loss 0.8288 (0.8288)	Acc@1 78.711 (78.711)	Acc@5 95.703 (95.703)	Mem 23876MB
[2022-11-12 09:39:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.366 Acc@5 94.576
[2022-11-12 09:39:47 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.4%
[2022-11-12 09:39:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.845 (1.845)	Loss 0.8114 (0.8114)	Acc@1 79.980 (79.980)	Acc@5 95.801 (95.801)	Mem 23876MB
[2022-11-12 09:40:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.812 Acc@5 95.666
[2022-11-12 09:40:00 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.8%
[2022-11-12 09:40:00 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.81% at 126 epoch
[2022-11-12 09:40:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][0/1251]	eta 0:48:57 lr 0.000623	time 2.3485 (2.3485)	loss 3.8588 (3.8588)	grad_norm 1.9082 (1.9082)	mem 23876MB
[2022-11-12 09:40:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][50/1251]	eta 0:15:36 lr 0.000623	time 0.7440 (0.7797)	loss 2.2110 (3.2498)	grad_norm 1.3375 (1.5507)	mem 23876MB
[2022-11-12 09:41:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][100/1251]	eta 0:14:39 lr 0.000623	time 0.7348 (0.7639)	loss 3.1603 (3.2292)	grad_norm 1.3556 (1.5415)	mem 23876MB
[2022-11-12 09:41:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][150/1251]	eta 0:13:55 lr 0.000622	time 0.7417 (0.7592)	loss 2.8823 (3.2345)	grad_norm 1.4870 (1.5345)	mem 23876MB
[2022-11-12 09:42:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][200/1251]	eta 0:13:14 lr 0.000622	time 0.8227 (0.7562)	loss 3.1449 (3.2761)	grad_norm 1.3781 (1.5524)	mem 23876MB
[2022-11-12 09:43:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][250/1251]	eta 0:12:35 lr 0.000622	time 0.7413 (0.7544)	loss 3.7107 (3.2853)	grad_norm 1.5963 (1.5484)	mem 23876MB
[2022-11-12 09:43:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][300/1251]	eta 0:11:55 lr 0.000622	time 0.7390 (0.7525)	loss 3.4781 (3.2742)	grad_norm 1.4525 (1.5399)	mem 23876MB
[2022-11-12 09:44:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][350/1251]	eta 0:11:17 lr 0.000622	time 0.7439 (0.7520)	loss 3.4964 (3.2751)	grad_norm 2.2262 (1.5393)	mem 23876MB
[2022-11-12 09:45:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][400/1251]	eta 0:10:39 lr 0.000621	time 0.7356 (0.7513)	loss 3.3251 (3.2804)	grad_norm 1.3683 (inf)	mem 23876MB
[2022-11-12 09:45:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][450/1251]	eta 0:10:01 lr 0.000621	time 0.7381 (0.7508)	loss 2.9013 (3.2740)	grad_norm 1.4255 (inf)	mem 23876MB
[2022-11-12 09:46:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][500/1251]	eta 0:09:23 lr 0.000621	time 0.7404 (0.7502)	loss 3.3476 (3.2747)	grad_norm 1.4990 (inf)	mem 23876MB
[2022-11-12 09:46:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][550/1251]	eta 0:08:45 lr 0.000621	time 0.7411 (0.7498)	loss 4.0480 (3.2654)	grad_norm 1.5094 (inf)	mem 23876MB
[2022-11-12 09:47:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][600/1251]	eta 0:08:08 lr 0.000621	time 0.7369 (0.7498)	loss 3.7694 (3.2667)	grad_norm 1.3989 (inf)	mem 23876MB
[2022-11-12 09:48:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][650/1251]	eta 0:07:30 lr 0.000620	time 0.7875 (0.7497)	loss 3.5252 (3.2724)	grad_norm inf (inf)	mem 23876MB
[2022-11-12 09:48:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][700/1251]	eta 0:06:52 lr 0.000620	time 0.7416 (0.7494)	loss 3.5392 (3.2742)	grad_norm 1.5355 (inf)	mem 23876MB
[2022-11-12 09:49:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][750/1251]	eta 0:06:15 lr 0.000620	time 0.8122 (0.7492)	loss 3.1242 (3.2784)	grad_norm 1.4177 (inf)	mem 23876MB
[2022-11-12 09:50:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][800/1251]	eta 0:05:37 lr 0.000620	time 0.7441 (0.7489)	loss 2.6642 (3.2699)	grad_norm 1.5650 (inf)	mem 23876MB
[2022-11-12 09:50:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][850/1251]	eta 0:05:00 lr 0.000620	time 0.7364 (0.7488)	loss 3.0292 (3.2737)	grad_norm 1.5875 (inf)	mem 23876MB
[2022-11-12 09:51:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][900/1251]	eta 0:04:22 lr 0.000619	time 0.7396 (0.7488)	loss 3.4924 (3.2653)	grad_norm 1.3738 (inf)	mem 23876MB
[2022-11-12 09:51:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][950/1251]	eta 0:03:45 lr 0.000619	time 0.7393 (0.7487)	loss 4.1132 (3.2723)	grad_norm 1.7601 (inf)	mem 23876MB
[2022-11-12 09:52:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][1000/1251]	eta 0:03:07 lr 0.000619	time 0.7385 (0.7486)	loss 3.3374 (3.2751)	grad_norm 1.4672 (inf)	mem 23876MB
[2022-11-12 09:53:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][1050/1251]	eta 0:02:30 lr 0.000619	time 0.7439 (0.7486)	loss 2.5012 (3.2742)	grad_norm 1.4214 (inf)	mem 23876MB
[2022-11-12 09:53:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][1100/1251]	eta 0:01:53 lr 0.000619	time 0.7384 (0.7485)	loss 3.8034 (3.2718)	grad_norm 1.5552 (inf)	mem 23876MB
[2022-11-12 09:54:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][1150/1251]	eta 0:01:15 lr 0.000618	time 0.7321 (0.7485)	loss 3.6178 (3.2688)	grad_norm 1.6893 (inf)	mem 23876MB
[2022-11-12 09:54:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][1200/1251]	eta 0:00:38 lr 0.000618	time 0.7402 (0.7484)	loss 2.8365 (3.2704)	grad_norm 1.4770 (inf)	mem 23876MB
[2022-11-12 09:55:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [127/300][1250/1251]	eta 0:00:00 lr 0.000618	time 0.7259 (0.7481)	loss 3.9220 (3.2701)	grad_norm 2.2600 (inf)	mem 23876MB
[2022-11-12 09:55:36 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 127 training takes 0:15:36
[2022-11-12 09:55:36 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_127.pth saving......
[2022-11-12 09:55:38 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_127.pth saved !!!
[2022-11-12 09:55:39 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.590 (1.590)	Loss 0.8825 (0.8825)	Acc@1 78.809 (78.809)	Acc@5 95.508 (95.508)	Mem 23876MB
[2022-11-12 09:55:50 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.594 Acc@5 94.798
[2022-11-12 09:55:50 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.6%
[2022-11-12 09:55:52 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.957 (1.957)	Loss 0.7757 (0.7757)	Acc@1 81.543 (81.543)	Acc@5 95.898 (95.898)	Mem 23876MB
[2022-11-12 09:56:03 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.818 Acc@5 95.690
[2022-11-12 09:56:03 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.8%
[2022-11-12 09:56:03 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.82% at 127 epoch
[2022-11-12 09:56:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][0/1251]	eta 0:52:23 lr 0.000618	time 2.5130 (2.5130)	loss 3.0341 (3.0341)	grad_norm 1.4788 (1.4788)	mem 23876MB
[2022-11-12 09:56:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][50/1251]	eta 0:15:40 lr 0.000618	time 0.7453 (0.7828)	loss 3.5687 (3.2444)	grad_norm 1.6087 (1.5403)	mem 23876MB
[2022-11-12 09:57:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][100/1251]	eta 0:14:38 lr 0.000618	time 0.7456 (0.7634)	loss 3.7127 (3.3076)	grad_norm 1.4838 (1.5444)	mem 23876MB
[2022-11-12 09:57:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][150/1251]	eta 0:13:55 lr 0.000617	time 0.7372 (0.7586)	loss 3.6011 (3.3013)	grad_norm 1.5590 (1.5298)	mem 23876MB
[2022-11-12 09:58:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][200/1251]	eta 0:13:13 lr 0.000617	time 0.8172 (0.7554)	loss 3.8107 (3.3118)	grad_norm 1.7883 (1.5263)	mem 23876MB
[2022-11-12 09:59:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][250/1251]	eta 0:12:34 lr 0.000617	time 0.7385 (0.7537)	loss 3.9174 (3.2961)	grad_norm 1.4071 (1.5252)	mem 23876MB
[2022-11-12 09:59:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][300/1251]	eta 0:11:55 lr 0.000617	time 0.7389 (0.7523)	loss 3.1213 (3.2770)	grad_norm 1.4387 (1.5259)	mem 23876MB
[2022-11-12 10:00:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][350/1251]	eta 0:11:17 lr 0.000617	time 0.7376 (0.7514)	loss 3.1783 (3.2677)	grad_norm 1.3798 (1.5170)	mem 23876MB
[2022-11-12 10:01:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][400/1251]	eta 0:10:39 lr 0.000616	time 0.7419 (0.7512)	loss 3.8222 (3.2852)	grad_norm 1.5128 (1.5187)	mem 23876MB
[2022-11-12 10:01:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][450/1251]	eta 0:10:01 lr 0.000616	time 0.7348 (0.7506)	loss 2.2361 (3.2732)	grad_norm 1.5818 (1.5198)	mem 23876MB
[2022-11-12 10:02:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][500/1251]	eta 0:09:23 lr 0.000616	time 0.7460 (0.7502)	loss 3.4471 (3.2793)	grad_norm 1.6373 (1.5194)	mem 23876MB
[2022-11-12 10:02:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][550/1251]	eta 0:08:45 lr 0.000616	time 0.7355 (0.7497)	loss 3.4401 (3.2798)	grad_norm 2.1372 (1.5256)	mem 23876MB
[2022-11-12 10:03:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][600/1251]	eta 0:08:07 lr 0.000616	time 0.8126 (0.7495)	loss 2.9607 (3.2753)	grad_norm 1.6228 (1.5260)	mem 23876MB
[2022-11-12 10:04:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][650/1251]	eta 0:07:30 lr 0.000615	time 0.7443 (0.7493)	loss 3.7820 (3.2696)	grad_norm 1.6540 (1.5234)	mem 23876MB
[2022-11-12 10:04:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][700/1251]	eta 0:06:52 lr 0.000615	time 0.7380 (0.7491)	loss 3.6540 (3.2721)	grad_norm 1.6223 (1.5222)	mem 23876MB
[2022-11-12 10:05:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][750/1251]	eta 0:06:15 lr 0.000615	time 0.7356 (0.7488)	loss 3.2505 (3.2724)	grad_norm 1.3788 (1.5241)	mem 23876MB
[2022-11-12 10:06:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][800/1251]	eta 0:05:37 lr 0.000615	time 0.7420 (0.7487)	loss 3.0705 (3.2693)	grad_norm 1.3709 (1.5206)	mem 23876MB
[2022-11-12 10:06:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][850/1251]	eta 0:05:00 lr 0.000615	time 0.7419 (0.7485)	loss 3.2876 (3.2711)	grad_norm 1.5937 (1.5215)	mem 23876MB
[2022-11-12 10:07:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][900/1251]	eta 0:04:22 lr 0.000614	time 0.7429 (0.7484)	loss 3.7981 (3.2687)	grad_norm 1.5006 (1.5223)	mem 23876MB
[2022-11-12 10:07:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][950/1251]	eta 0:03:45 lr 0.000614	time 0.7425 (0.7482)	loss 3.2893 (3.2688)	grad_norm 1.4274 (1.5271)	mem 23876MB
[2022-11-12 10:08:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][1000/1251]	eta 0:03:07 lr 0.000614	time 0.8220 (0.7481)	loss 3.6262 (3.2720)	grad_norm 1.5926 (1.5256)	mem 23876MB
[2022-11-12 10:09:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][1050/1251]	eta 0:02:30 lr 0.000614	time 0.7443 (0.7479)	loss 2.9520 (3.2722)	grad_norm 1.4736 (1.5271)	mem 23876MB
[2022-11-12 10:09:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][1100/1251]	eta 0:01:52 lr 0.000614	time 0.7386 (0.7478)	loss 3.4594 (3.2729)	grad_norm 1.3619 (1.5248)	mem 23876MB
[2022-11-12 10:10:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][1150/1251]	eta 0:01:15 lr 0.000613	time 0.7400 (0.7477)	loss 3.5066 (3.2727)	grad_norm 1.6677 (1.5245)	mem 23876MB
[2022-11-12 10:11:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][1200/1251]	eta 0:00:38 lr 0.000613	time 0.7351 (0.7477)	loss 3.3078 (3.2720)	grad_norm 1.3720 (1.5253)	mem 23876MB
[2022-11-12 10:11:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [128/300][1250/1251]	eta 0:00:00 lr 0.000613	time 0.7290 (0.7475)	loss 2.4800 (3.2708)	grad_norm 1.9064 (1.5269)	mem 23876MB
[2022-11-12 10:11:38 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 128 training takes 0:15:35
[2022-11-12 10:11:38 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_128.pth saving......
[2022-11-12 10:11:39 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_128.pth saved !!!
[2022-11-12 10:11:41 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.689 (1.689)	Loss 0.8864 (0.8864)	Acc@1 78.711 (78.711)	Acc@5 94.824 (94.824)	Mem 23876MB
[2022-11-12 10:11:51 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.282 Acc@5 94.730
[2022-11-12 10:11:51 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.3%
[2022-11-12 10:11:53 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.907 (1.907)	Loss 0.7475 (0.7475)	Acc@1 81.445 (81.445)	Acc@5 96.191 (96.191)	Mem 23876MB
[2022-11-12 10:12:04 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.852 Acc@5 95.712
[2022-11-12 10:12:04 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.9%
[2022-11-12 10:12:04 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.85% at 128 epoch
[2022-11-12 10:12:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][0/1251]	eta 0:51:02 lr 0.000613	time 2.4478 (2.4478)	loss 4.0964 (4.0964)	grad_norm 1.6620 (1.6620)	mem 23876MB
[2022-11-12 10:12:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][50/1251]	eta 0:15:35 lr 0.000613	time 0.7369 (0.7790)	loss 3.0122 (3.2934)	grad_norm 1.4366 (1.5252)	mem 23876MB
[2022-11-12 10:13:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][100/1251]	eta 0:14:39 lr 0.000613	time 0.7414 (0.7641)	loss 3.3279 (3.2282)	grad_norm 1.4949 (1.5096)	mem 23876MB
[2022-11-12 10:13:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][150/1251]	eta 0:13:55 lr 0.000612	time 0.7395 (0.7588)	loss 3.5154 (3.2359)	grad_norm 2.1562 (1.5175)	mem 23876MB
[2022-11-12 10:14:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][200/1251]	eta 0:13:14 lr 0.000612	time 0.7420 (0.7561)	loss 3.7786 (3.2525)	grad_norm 1.5815 (1.5350)	mem 23876MB
[2022-11-12 10:15:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][250/1251]	eta 0:12:35 lr 0.000612	time 0.7475 (0.7548)	loss 3.4863 (3.2671)	grad_norm 1.4237 (nan)	mem 23876MB
[2022-11-12 10:15:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][300/1251]	eta 0:11:56 lr 0.000612	time 0.7418 (0.7534)	loss 3.4330 (3.2713)	grad_norm 1.4114 (nan)	mem 23876MB
[2022-11-12 10:16:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][350/1251]	eta 0:11:18 lr 0.000612	time 0.7445 (0.7526)	loss 3.3768 (3.2840)	grad_norm 1.3151 (nan)	mem 23876MB
[2022-11-12 10:17:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][400/1251]	eta 0:10:40 lr 0.000611	time 0.7398 (0.7521)	loss 2.8748 (3.2958)	grad_norm 1.3827 (nan)	mem 23876MB
[2022-11-12 10:17:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][450/1251]	eta 0:10:02 lr 0.000611	time 0.7421 (0.7517)	loss 2.5890 (3.2858)	grad_norm 1.8068 (nan)	mem 23876MB
[2022-11-12 10:18:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][500/1251]	eta 0:09:24 lr 0.000611	time 0.7601 (0.7513)	loss 3.7380 (3.2892)	grad_norm 1.4569 (nan)	mem 23876MB
[2022-11-12 10:18:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][550/1251]	eta 0:08:46 lr 0.000611	time 0.7438 (0.7512)	loss 3.4437 (3.2827)	grad_norm 1.5529 (nan)	mem 23876MB
[2022-11-12 10:19:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][600/1251]	eta 0:08:08 lr 0.000611	time 0.7394 (0.7507)	loss 3.0169 (3.2908)	grad_norm 1.7161 (nan)	mem 23876MB
[2022-11-12 10:20:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][650/1251]	eta 0:07:31 lr 0.000610	time 0.7408 (0.7505)	loss 3.3450 (3.2911)	grad_norm 1.6113 (nan)	mem 23876MB
[2022-11-12 10:20:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][700/1251]	eta 0:06:53 lr 0.000610	time 0.7351 (0.7503)	loss 3.5247 (3.2853)	grad_norm 1.5022 (nan)	mem 23876MB
[2022-11-12 10:21:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][750/1251]	eta 0:06:15 lr 0.000610	time 0.7339 (0.7500)	loss 3.4207 (3.2863)	grad_norm 1.5886 (nan)	mem 23876MB
[2022-11-12 10:22:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][800/1251]	eta 0:05:38 lr 0.000610	time 0.7397 (0.7499)	loss 3.8492 (3.2898)	grad_norm 1.6355 (nan)	mem 23876MB
[2022-11-12 10:22:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][850/1251]	eta 0:05:00 lr 0.000610	time 0.7505 (0.7495)	loss 3.7010 (3.2914)	grad_norm 1.3501 (nan)	mem 23876MB
[2022-11-12 10:23:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][900/1251]	eta 0:04:23 lr 0.000609	time 0.7423 (0.7494)	loss 3.2888 (3.2959)	grad_norm 1.3109 (nan)	mem 23876MB
[2022-11-12 10:23:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][950/1251]	eta 0:03:45 lr 0.000609	time 0.7384 (0.7493)	loss 2.1256 (3.2968)	grad_norm 1.6156 (nan)	mem 23876MB
[2022-11-12 10:24:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][1000/1251]	eta 0:03:07 lr 0.000609	time 0.7387 (0.7489)	loss 3.1241 (3.2982)	grad_norm 1.3417 (nan)	mem 23876MB
[2022-11-12 10:25:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][1050/1251]	eta 0:02:30 lr 0.000609	time 0.7400 (0.7490)	loss 3.1519 (3.3024)	grad_norm 1.5333 (nan)	mem 23876MB
[2022-11-12 10:25:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][1100/1251]	eta 0:01:53 lr 0.000609	time 0.7391 (0.7487)	loss 3.4451 (3.3027)	grad_norm 1.6747 (nan)	mem 23876MB
[2022-11-12 10:26:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][1150/1251]	eta 0:01:15 lr 0.000608	time 0.7398 (0.7486)	loss 2.0164 (3.2952)	grad_norm 2.0054 (nan)	mem 23876MB
[2022-11-12 10:27:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][1200/1251]	eta 0:00:38 lr 0.000608	time 0.7377 (0.7485)	loss 3.6174 (3.3002)	grad_norm 1.6150 (nan)	mem 23876MB
[2022-11-12 10:27:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [129/300][1250/1251]	eta 0:00:00 lr 0.000608	time 0.7247 (0.7482)	loss 3.3950 (3.2996)	grad_norm 1.4250 (nan)	mem 23876MB
[2022-11-12 10:27:40 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 129 training takes 0:15:36
[2022-11-12 10:27:41 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_129.pth saving......
[2022-11-12 10:27:42 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_129.pth saved !!!
[2022-11-12 10:27:43 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.646 (1.646)	Loss 0.8842 (0.8842)	Acc@1 78.223 (78.223)	Acc@5 94.922 (94.922)	Mem 23876MB
[2022-11-12 10:27:54 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.542 Acc@5 94.610
[2022-11-12 10:27:54 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.5%
[2022-11-12 10:27:56 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.799 (1.799)	Loss 0.8625 (0.8625)	Acc@1 79.102 (79.102)	Acc@5 95.020 (95.020)	Mem 23876MB
[2022-11-12 10:28:06 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.928 Acc@5 95.710
[2022-11-12 10:28:06 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 80.9%
[2022-11-12 10:28:06 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.93% at 129 epoch
[2022-11-12 10:28:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][0/1251]	eta 0:51:08 lr 0.000608	time 2.4525 (2.4525)	loss 3.4315 (3.4315)	grad_norm 1.3611 (1.3611)	mem 23876MB
[2022-11-12 10:28:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][50/1251]	eta 0:15:42 lr 0.000608	time 0.7410 (0.7851)	loss 2.1493 (3.3867)	grad_norm 1.5754 (1.5374)	mem 23876MB
[2022-11-12 10:29:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][100/1251]	eta 0:14:41 lr 0.000608	time 0.7421 (0.7658)	loss 2.8102 (3.2825)	grad_norm 1.6984 (1.5269)	mem 23876MB
[2022-11-12 10:30:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][150/1251]	eta 0:13:56 lr 0.000607	time 0.7431 (0.7596)	loss 3.6067 (3.3104)	grad_norm 1.4105 (1.5586)	mem 23876MB
[2022-11-12 10:30:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][200/1251]	eta 0:13:14 lr 0.000607	time 0.7388 (0.7563)	loss 4.1417 (3.3177)	grad_norm 1.3253 (1.5493)	mem 23876MB
[2022-11-12 10:31:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][250/1251]	eta 0:12:35 lr 0.000607	time 0.7406 (0.7546)	loss 2.3428 (3.2986)	grad_norm 1.6527 (1.5512)	mem 23876MB
[2022-11-12 10:31:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][300/1251]	eta 0:11:56 lr 0.000607	time 0.7371 (0.7534)	loss 3.6575 (3.3192)	grad_norm 1.4014 (1.5428)	mem 23876MB
[2022-11-12 10:32:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][350/1251]	eta 0:11:18 lr 0.000606	time 0.7404 (0.7525)	loss 2.8030 (3.3025)	grad_norm 1.4443 (1.5410)	mem 23876MB
[2022-11-12 10:33:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][400/1251]	eta 0:10:39 lr 0.000606	time 0.7402 (0.7518)	loss 3.2631 (3.3074)	grad_norm 1.3611 (1.5384)	mem 23876MB
[2022-11-12 10:33:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][450/1251]	eta 0:10:01 lr 0.000606	time 0.7377 (0.7511)	loss 3.0404 (3.3094)	grad_norm 1.3977 (1.5406)	mem 23876MB
[2022-11-12 10:34:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][500/1251]	eta 0:09:23 lr 0.000606	time 0.7419 (0.7506)	loss 3.9432 (3.3091)	grad_norm 1.7691 (1.5408)	mem 23876MB
[2022-11-12 10:35:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][550/1251]	eta 0:08:45 lr 0.000606	time 0.7359 (0.7503)	loss 3.7523 (3.3129)	grad_norm 1.5527 (1.5459)	mem 23876MB
[2022-11-12 10:35:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][600/1251]	eta 0:08:08 lr 0.000605	time 0.7377 (0.7497)	loss 2.9752 (3.3117)	grad_norm 1.4366 (1.5437)	mem 23876MB
[2022-11-12 10:36:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][650/1251]	eta 0:07:30 lr 0.000605	time 0.7350 (0.7496)	loss 3.2276 (3.3059)	grad_norm 1.4458 (1.5438)	mem 23876MB
[2022-11-12 10:36:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][700/1251]	eta 0:06:52 lr 0.000605	time 0.7328 (0.7493)	loss 3.3004 (3.2964)	grad_norm 1.3756 (1.5436)	mem 23876MB
[2022-11-12 10:37:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][750/1251]	eta 0:06:15 lr 0.000605	time 0.7358 (0.7492)	loss 3.5128 (3.3009)	grad_norm 1.8140 (1.5454)	mem 23876MB
[2022-11-12 10:38:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][800/1251]	eta 0:05:37 lr 0.000605	time 0.7368 (0.7489)	loss 3.9145 (3.3000)	grad_norm 1.6255 (1.5451)	mem 23876MB
[2022-11-12 10:38:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][850/1251]	eta 0:05:00 lr 0.000604	time 0.7345 (0.7490)	loss 2.4650 (3.2950)	grad_norm 1.6376 (1.5445)	mem 23876MB
[2022-11-12 10:39:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][900/1251]	eta 0:04:22 lr 0.000604	time 0.7351 (0.7487)	loss 3.2462 (3.3013)	grad_norm 1.6451 (1.5474)	mem 23876MB
[2022-11-12 10:39:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][950/1251]	eta 0:03:45 lr 0.000604	time 0.7411 (0.7485)	loss 3.7124 (3.3059)	grad_norm 1.6221 (1.5466)	mem 23876MB
[2022-11-12 10:40:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][1000/1251]	eta 0:03:07 lr 0.000604	time 0.7374 (0.7484)	loss 3.5233 (3.3079)	grad_norm 1.5260 (1.5493)	mem 23876MB
[2022-11-12 10:41:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][1050/1251]	eta 0:02:30 lr 0.000604	time 0.7376 (0.7481)	loss 3.4734 (3.3072)	grad_norm 1.6211 (1.5517)	mem 23876MB
[2022-11-12 10:41:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][1100/1251]	eta 0:01:52 lr 0.000603	time 0.7384 (0.7481)	loss 3.4455 (3.3033)	grad_norm 1.4766 (1.5514)	mem 23876MB
[2022-11-12 10:42:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][1150/1251]	eta 0:01:15 lr 0.000603	time 0.7365 (0.7479)	loss 3.7746 (3.3038)	grad_norm 1.4367 (inf)	mem 23876MB
[2022-11-12 10:43:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][1200/1251]	eta 0:00:38 lr 0.000603	time 0.7330 (0.7478)	loss 3.9620 (3.3073)	grad_norm 1.5405 (inf)	mem 23876MB
[2022-11-12 10:43:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [130/300][1250/1251]	eta 0:00:00 lr 0.000603	time 0.7269 (0.7476)	loss 3.2500 (3.3094)	grad_norm 1.5345 (inf)	mem 23876MB
[2022-11-12 10:43:42 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 130 training takes 0:15:35
[2022-11-12 10:43:42 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_130.pth saving......
[2022-11-12 10:43:43 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_130.pth saved !!!
[2022-11-12 10:43:45 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.720 (1.720)	Loss 0.8420 (0.8420)	Acc@1 79.785 (79.785)	Acc@5 95.117 (95.117)	Mem 23876MB
[2022-11-12 10:43:55 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.320 Acc@5 94.704
[2022-11-12 10:43:55 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.3%
[2022-11-12 10:43:57 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.752 (1.752)	Loss 0.7719 (0.7719)	Acc@1 81.934 (81.934)	Acc@5 95.410 (95.410)	Mem 23876MB
[2022-11-12 10:44:08 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.962 Acc@5 95.708
[2022-11-12 10:44:08 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.0%
[2022-11-12 10:44:08 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.96% at 130 epoch
[2022-11-12 10:44:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][0/1251]	eta 0:48:50 lr 0.000603	time 2.3422 (2.3422)	loss 3.3161 (3.3161)	grad_norm 1.6616 (1.6616)	mem 23876MB
[2022-11-12 10:44:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][50/1251]	eta 0:15:37 lr 0.000603	time 0.7374 (0.7808)	loss 3.8585 (3.2586)	grad_norm 1.5065 (1.5571)	mem 23876MB
[2022-11-12 10:45:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][100/1251]	eta 0:14:39 lr 0.000602	time 0.7384 (0.7643)	loss 3.5485 (3.2364)	grad_norm 1.5722 (1.5492)	mem 23876MB
[2022-11-12 10:46:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][150/1251]	eta 0:13:55 lr 0.000602	time 0.7381 (0.7586)	loss 3.6986 (3.2351)	grad_norm 1.4351 (1.5472)	mem 23876MB
[2022-11-12 10:46:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][200/1251]	eta 0:13:14 lr 0.000602	time 0.7413 (0.7563)	loss 2.5737 (3.2294)	grad_norm 1.3392 (1.5466)	mem 23876MB
[2022-11-12 10:47:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][250/1251]	eta 0:12:34 lr 0.000602	time 0.7493 (0.7542)	loss 3.2801 (3.2213)	grad_norm 1.4184 (1.5496)	mem 23876MB
[2022-11-12 10:47:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][300/1251]	eta 0:11:56 lr 0.000602	time 0.7444 (0.7534)	loss 3.3215 (3.2468)	grad_norm 1.5109 (1.5563)	mem 23876MB
[2022-11-12 10:48:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][350/1251]	eta 0:11:18 lr 0.000601	time 0.7398 (0.7526)	loss 3.9841 (3.2447)	grad_norm 1.3665 (1.5530)	mem 23876MB
[2022-11-12 10:49:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][400/1251]	eta 0:10:39 lr 0.000601	time 0.7481 (0.7518)	loss 3.2100 (3.2405)	grad_norm 1.5346 (1.5504)	mem 23876MB
[2022-11-12 10:49:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][450/1251]	eta 0:10:01 lr 0.000601	time 0.7408 (0.7512)	loss 3.1572 (3.2335)	grad_norm 1.5984 (1.5502)	mem 23876MB
[2022-11-12 10:50:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][500/1251]	eta 0:09:23 lr 0.000601	time 0.7406 (0.7505)	loss 3.8555 (3.2225)	grad_norm 1.6433 (1.5474)	mem 23876MB
[2022-11-12 10:51:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][550/1251]	eta 0:08:46 lr 0.000601	time 0.7432 (0.7505)	loss 3.8722 (3.2287)	grad_norm 1.5685 (1.5471)	mem 23876MB
[2022-11-12 10:51:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][600/1251]	eta 0:08:08 lr 0.000600	time 0.7359 (0.7499)	loss 3.6928 (3.2328)	grad_norm 1.8483 (1.5473)	mem 23876MB
[2022-11-12 10:52:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][650/1251]	eta 0:07:30 lr 0.000600	time 0.7401 (0.7498)	loss 3.0621 (3.2350)	grad_norm 1.5784 (1.5471)	mem 23876MB
[2022-11-12 10:52:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][700/1251]	eta 0:06:52 lr 0.000600	time 0.7431 (0.7494)	loss 3.4103 (3.2363)	grad_norm 1.3135 (1.5450)	mem 23876MB
[2022-11-12 10:53:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][750/1251]	eta 0:06:15 lr 0.000600	time 0.7265 (0.7492)	loss 2.3135 (3.2447)	grad_norm 1.3466 (inf)	mem 23876MB
[2022-11-12 10:54:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][800/1251]	eta 0:05:37 lr 0.000600	time 0.7427 (0.7490)	loss 3.0052 (3.2442)	grad_norm 1.5143 (inf)	mem 23876MB
[2022-11-12 10:54:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][850/1251]	eta 0:05:00 lr 0.000599	time 0.7342 (0.7488)	loss 3.6982 (3.2505)	grad_norm 1.4657 (inf)	mem 23876MB
[2022-11-12 10:55:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][900/1251]	eta 0:04:22 lr 0.000599	time 0.7408 (0.7487)	loss 3.8429 (3.2488)	grad_norm 1.5477 (inf)	mem 23876MB
[2022-11-12 10:56:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][950/1251]	eta 0:03:45 lr 0.000599	time 0.8173 (0.7487)	loss 3.6427 (3.2491)	grad_norm 1.6232 (inf)	mem 23876MB
[2022-11-12 10:56:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][1000/1251]	eta 0:03:07 lr 0.000599	time 0.7446 (0.7485)	loss 2.8644 (3.2490)	grad_norm 1.5075 (inf)	mem 23876MB
[2022-11-12 10:57:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][1050/1251]	eta 0:02:30 lr 0.000599	time 0.7335 (0.7484)	loss 3.5428 (3.2543)	grad_norm 1.4621 (inf)	mem 23876MB
[2022-11-12 10:57:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][1100/1251]	eta 0:01:52 lr 0.000598	time 0.7434 (0.7482)	loss 3.6867 (3.2587)	grad_norm 1.6273 (inf)	mem 23876MB
[2022-11-12 10:58:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][1150/1251]	eta 0:01:15 lr 0.000598	time 0.7413 (0.7482)	loss 3.4297 (3.2573)	grad_norm 1.4413 (inf)	mem 23876MB
[2022-11-12 10:59:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][1200/1251]	eta 0:00:38 lr 0.000598	time 0.7503 (0.7481)	loss 2.3699 (3.2614)	grad_norm 1.8314 (inf)	mem 23876MB
[2022-11-12 10:59:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [131/300][1250/1251]	eta 0:00:00 lr 0.000598	time 0.7262 (0.7479)	loss 2.3036 (3.2626)	grad_norm 1.5293 (inf)	mem 23876MB
[2022-11-12 10:59:44 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 131 training takes 0:15:35
[2022-11-12 10:59:44 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_131.pth saving......
[2022-11-12 10:59:45 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_131.pth saved !!!
[2022-11-12 10:59:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.693 (1.693)	Loss 0.9330 (0.9330)	Acc@1 77.832 (77.832)	Acc@5 95.020 (95.020)	Mem 23876MB
[2022-11-12 10:59:57 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.476 Acc@5 94.710
[2022-11-12 10:59:57 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.5%
[2022-11-12 10:59:59 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.824 (1.824)	Loss 0.8004 (0.8004)	Acc@1 81.641 (81.641)	Acc@5 95.312 (95.312)	Mem 23876MB
[2022-11-12 11:00:10 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.954 Acc@5 95.752
[2022-11-12 11:00:10 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.0%
[2022-11-12 11:00:10 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 80.96% at 130 epoch
[2022-11-12 11:00:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][0/1251]	eta 0:50:32 lr 0.000598	time 2.4239 (2.4239)	loss 3.6577 (3.6577)	grad_norm 1.6809 (1.6809)	mem 23876MB
[2022-11-12 11:00:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][50/1251]	eta 0:15:39 lr 0.000598	time 0.7376 (0.7819)	loss 3.8263 (3.1891)	grad_norm 1.5987 (1.5716)	mem 23876MB
[2022-11-12 11:01:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][100/1251]	eta 0:14:40 lr 0.000597	time 0.7374 (0.7648)	loss 2.3186 (3.2026)	grad_norm 1.5626 (1.5555)	mem 23876MB
[2022-11-12 11:02:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][150/1251]	eta 0:13:54 lr 0.000597	time 0.7382 (0.7579)	loss 2.2246 (3.2091)	grad_norm 1.3560 (1.5480)	mem 23876MB
[2022-11-12 11:02:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][200/1251]	eta 0:13:13 lr 0.000597	time 0.7401 (0.7554)	loss 2.7443 (3.2471)	grad_norm 1.7085 (1.5543)	mem 23876MB
[2022-11-12 11:03:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][250/1251]	eta 0:12:33 lr 0.000597	time 0.7433 (0.7530)	loss 3.7043 (3.2516)	grad_norm 1.5822 (1.5512)	mem 23876MB
[2022-11-12 11:03:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][300/1251]	eta 0:11:55 lr 0.000597	time 0.7414 (0.7521)	loss 2.7156 (3.2471)	grad_norm 1.6674 (1.5462)	mem 23876MB
[2022-11-12 11:04:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][350/1251]	eta 0:11:16 lr 0.000596	time 0.7607 (0.7509)	loss 3.9404 (3.2625)	grad_norm 1.6313 (1.5488)	mem 23876MB
[2022-11-12 11:05:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][400/1251]	eta 0:10:38 lr 0.000596	time 0.7428 (0.7507)	loss 4.1873 (3.2749)	grad_norm 1.5782 (1.5559)	mem 23876MB
[2022-11-12 11:05:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][450/1251]	eta 0:10:00 lr 0.000596	time 0.7384 (0.7502)	loss 3.6833 (3.2710)	grad_norm 1.4220 (1.5567)	mem 23876MB
[2022-11-12 11:06:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][500/1251]	eta 0:09:23 lr 0.000596	time 0.7460 (0.7499)	loss 3.6416 (3.2738)	grad_norm 1.3037 (1.5554)	mem 23876MB
[2022-11-12 11:07:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][550/1251]	eta 0:08:45 lr 0.000596	time 0.7364 (0.7495)	loss 3.5567 (3.2860)	grad_norm 1.5300 (1.5542)	mem 23876MB
[2022-11-12 11:07:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][600/1251]	eta 0:08:07 lr 0.000595	time 0.8159 (0.7494)	loss 3.5812 (3.2864)	grad_norm 1.2449 (1.5522)	mem 23876MB
[2022-11-12 11:08:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][650/1251]	eta 0:07:30 lr 0.000595	time 0.7391 (0.7489)	loss 3.8030 (3.2860)	grad_norm 1.7643 (1.5548)	mem 23876MB
[2022-11-12 11:08:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][700/1251]	eta 0:06:52 lr 0.000595	time 0.7388 (0.7486)	loss 3.7764 (3.2763)	grad_norm 1.4454 (1.5544)	mem 23876MB
[2022-11-12 11:09:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][750/1251]	eta 0:06:15 lr 0.000595	time 0.8205 (0.7487)	loss 3.0472 (3.2749)	grad_norm 1.5660 (1.5571)	mem 23876MB
[2022-11-12 11:10:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][800/1251]	eta 0:05:37 lr 0.000594	time 0.7399 (0.7484)	loss 2.3019 (3.2750)	grad_norm 1.4405 (1.5635)	mem 23876MB
[2022-11-12 11:10:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][850/1251]	eta 0:05:00 lr 0.000594	time 0.7364 (0.7482)	loss 3.9045 (3.2785)	grad_norm 1.5453 (1.5626)	mem 23876MB
[2022-11-12 11:11:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][900/1251]	eta 0:04:22 lr 0.000594	time 0.7380 (0.7480)	loss 3.1796 (3.2780)	grad_norm 1.6557 (1.5628)	mem 23876MB
[2022-11-12 11:12:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][950/1251]	eta 0:03:45 lr 0.000594	time 0.7356 (0.7478)	loss 3.0458 (3.2822)	grad_norm 1.5829 (1.5601)	mem 23876MB
[2022-11-12 11:12:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][1000/1251]	eta 0:03:07 lr 0.000594	time 0.7391 (0.7478)	loss 3.2378 (3.2837)	grad_norm 1.4582 (1.5607)	mem 23876MB
[2022-11-12 11:13:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][1050/1251]	eta 0:02:30 lr 0.000593	time 0.7407 (0.7477)	loss 3.8721 (3.2833)	grad_norm 1.5009 (1.5600)	mem 23876MB
[2022-11-12 11:13:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][1100/1251]	eta 0:01:52 lr 0.000593	time 0.7385 (0.7476)	loss 2.6477 (3.2843)	grad_norm 1.4878 (1.5611)	mem 23876MB
[2022-11-12 11:14:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][1150/1251]	eta 0:01:15 lr 0.000593	time 0.7375 (0.7476)	loss 3.5189 (3.2842)	grad_norm 1.3923 (1.5591)	mem 23876MB
[2022-11-12 11:15:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][1200/1251]	eta 0:00:38 lr 0.000593	time 0.7435 (0.7473)	loss 3.1794 (3.2891)	grad_norm 1.4317 (1.5589)	mem 23876MB
[2022-11-12 11:15:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [132/300][1250/1251]	eta 0:00:00 lr 0.000593	time 0.7269 (0.7472)	loss 3.5751 (3.2863)	grad_norm 1.4982 (1.5586)	mem 23876MB
[2022-11-12 11:15:45 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 132 training takes 0:15:34
[2022-11-12 11:15:45 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_132.pth saving......
[2022-11-12 11:15:46 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_132.pth saved !!!
[2022-11-12 11:15:48 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.646 (1.646)	Loss 0.8599 (0.8599)	Acc@1 79.785 (79.785)	Acc@5 95.898 (95.898)	Mem 23876MB
[2022-11-12 11:15:58 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.602 Acc@5 94.810
[2022-11-12 11:15:58 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.6%
[2022-11-12 11:16:00 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.909 (1.909)	Loss 0.7546 (0.7546)	Acc@1 82.617 (82.617)	Acc@5 96.289 (96.289)	Mem 23876MB
[2022-11-12 11:16:11 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.044 Acc@5 95.768
[2022-11-12 11:16:11 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.0%
[2022-11-12 11:16:11 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.04% at 132 epoch
[2022-11-12 11:16:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][0/1251]	eta 0:50:26 lr 0.000593	time 2.4191 (2.4191)	loss 3.6432 (3.6432)	grad_norm 1.3940 (1.3940)	mem 23876MB
[2022-11-12 11:16:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][50/1251]	eta 0:15:38 lr 0.000592	time 0.7370 (0.7816)	loss 3.6384 (3.1964)	grad_norm 1.4745 (1.6054)	mem 23876MB
[2022-11-12 11:17:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][100/1251]	eta 0:14:37 lr 0.000592	time 0.7363 (0.7624)	loss 3.6876 (3.2117)	grad_norm 1.4680 (nan)	mem 23876MB
[2022-11-12 11:18:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][150/1251]	eta 0:13:54 lr 0.000592	time 0.7382 (0.7581)	loss 3.9044 (3.2656)	grad_norm 2.0999 (nan)	mem 23876MB
[2022-11-12 11:18:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][200/1251]	eta 0:13:14 lr 0.000592	time 0.7399 (0.7559)	loss 3.7412 (3.2798)	grad_norm 1.5652 (nan)	mem 23876MB
[2022-11-12 11:19:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][250/1251]	eta 0:12:34 lr 0.000592	time 0.7429 (0.7533)	loss 3.1759 (3.2748)	grad_norm 1.5635 (nan)	mem 23876MB
[2022-11-12 11:19:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][300/1251]	eta 0:11:55 lr 0.000591	time 0.7397 (0.7525)	loss 2.1555 (3.2552)	grad_norm 1.5694 (nan)	mem 23876MB
[2022-11-12 11:20:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][350/1251]	eta 0:11:17 lr 0.000591	time 0.7374 (0.7518)	loss 2.9239 (3.2654)	grad_norm 1.4043 (nan)	mem 23876MB
[2022-11-12 11:21:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][400/1251]	eta 0:10:39 lr 0.000591	time 0.7371 (0.7511)	loss 2.4318 (3.2638)	grad_norm 1.6196 (nan)	mem 23876MB
[2022-11-12 11:21:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][450/1251]	eta 0:10:01 lr 0.000591	time 0.8287 (0.7505)	loss 2.6551 (3.2643)	grad_norm 1.5377 (nan)	mem 23876MB
[2022-11-12 11:22:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][500/1251]	eta 0:09:23 lr 0.000591	time 0.7415 (0.7501)	loss 2.9064 (3.2682)	grad_norm 1.6562 (nan)	mem 23876MB
[2022-11-12 11:23:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][550/1251]	eta 0:08:45 lr 0.000590	time 0.8435 (0.7498)	loss 3.8194 (3.2731)	grad_norm 1.3685 (nan)	mem 23876MB
[2022-11-12 11:23:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][600/1251]	eta 0:08:08 lr 0.000590	time 0.7422 (0.7497)	loss 4.0112 (3.2762)	grad_norm 1.6990 (nan)	mem 23876MB
[2022-11-12 11:24:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][650/1251]	eta 0:07:30 lr 0.000590	time 0.7355 (0.7493)	loss 2.4498 (3.2690)	grad_norm 1.5417 (nan)	mem 23876MB
[2022-11-12 11:24:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][700/1251]	eta 0:06:52 lr 0.000590	time 0.7443 (0.7491)	loss 3.3762 (3.2589)	grad_norm 1.3952 (nan)	mem 23876MB
[2022-11-12 11:25:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][750/1251]	eta 0:06:15 lr 0.000590	time 0.7377 (0.7489)	loss 3.2716 (3.2652)	grad_norm 1.6179 (nan)	mem 23876MB
[2022-11-12 11:26:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][800/1251]	eta 0:05:37 lr 0.000589	time 0.7359 (0.7488)	loss 3.8777 (3.2646)	grad_norm 1.6144 (nan)	mem 23876MB
[2022-11-12 11:26:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][850/1251]	eta 0:05:00 lr 0.000589	time 0.7403 (0.7486)	loss 3.5476 (3.2669)	grad_norm 1.4528 (nan)	mem 23876MB
[2022-11-12 11:27:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][900/1251]	eta 0:04:22 lr 0.000589	time 0.7419 (0.7484)	loss 3.9263 (3.2627)	grad_norm 1.6349 (nan)	mem 23876MB
[2022-11-12 11:28:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][950/1251]	eta 0:03:45 lr 0.000589	time 0.7360 (0.7482)	loss 3.1227 (3.2607)	grad_norm 1.5087 (nan)	mem 23876MB
[2022-11-12 11:28:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][1000/1251]	eta 0:03:07 lr 0.000589	time 0.7414 (0.7483)	loss 3.6104 (3.2617)	grad_norm 1.8507 (nan)	mem 23876MB
[2022-11-12 11:29:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][1050/1251]	eta 0:02:30 lr 0.000588	time 0.7447 (0.7480)	loss 2.8539 (3.2620)	grad_norm 1.7104 (nan)	mem 23876MB
[2022-11-12 11:29:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][1100/1251]	eta 0:01:52 lr 0.000588	time 0.7392 (0.7481)	loss 3.5584 (3.2615)	grad_norm 1.7190 (nan)	mem 23876MB
[2022-11-12 11:30:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][1150/1251]	eta 0:01:15 lr 0.000588	time 0.7365 (0.7479)	loss 3.2177 (3.2627)	grad_norm 1.3613 (nan)	mem 23876MB
[2022-11-12 11:31:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][1200/1251]	eta 0:00:38 lr 0.000588	time 0.7382 (0.7480)	loss 3.0729 (3.2630)	grad_norm 1.3679 (nan)	mem 23876MB
[2022-11-12 11:31:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [133/300][1250/1251]	eta 0:00:00 lr 0.000588	time 0.7264 (0.7476)	loss 2.9775 (3.2630)	grad_norm 1.9431 (nan)	mem 23876MB
[2022-11-12 11:31:46 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 133 training takes 0:15:35
[2022-11-12 11:31:46 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_133.pth saving......
[2022-11-12 11:31:48 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_133.pth saved !!!
[2022-11-12 11:31:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.583 (1.583)	Loss 0.9481 (0.9481)	Acc@1 77.832 (77.832)	Acc@5 93.945 (93.945)	Mem 23876MB
[2022-11-12 11:32:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.740 Acc@5 94.728
[2022-11-12 11:32:00 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.7%
[2022-11-12 11:32:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.956 (1.956)	Loss 0.8126 (0.8126)	Acc@1 79.590 (79.590)	Acc@5 95.996 (95.996)	Mem 23876MB
[2022-11-12 11:32:13 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.008 Acc@5 95.764
[2022-11-12 11:32:13 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.0%
[2022-11-12 11:32:13 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.04% at 132 epoch
[2022-11-12 11:32:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][0/1251]	eta 0:51:10 lr 0.000588	time 2.4545 (2.4545)	loss 3.3878 (3.3878)	grad_norm 1.6359 (1.6359)	mem 23876MB
[2022-11-12 11:32:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][50/1251]	eta 0:15:39 lr 0.000587	time 0.7380 (0.7819)	loss 3.3838 (3.2544)	grad_norm 1.6829 (1.5522)	mem 23876MB
[2022-11-12 11:33:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][100/1251]	eta 0:14:43 lr 0.000587	time 0.7418 (0.7675)	loss 2.3617 (3.1987)	grad_norm 1.3343 (1.5371)	mem 23876MB
[2022-11-12 11:34:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][150/1251]	eta 0:13:57 lr 0.000587	time 0.7371 (0.7608)	loss 2.7804 (3.2044)	grad_norm 1.3798 (1.5384)	mem 23876MB
[2022-11-12 11:34:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][200/1251]	eta 0:13:17 lr 0.000587	time 0.7481 (0.7585)	loss 3.6135 (3.1926)	grad_norm 1.5260 (1.5562)	mem 23876MB
[2022-11-12 11:35:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][250/1251]	eta 0:12:37 lr 0.000587	time 0.7371 (0.7564)	loss 2.7318 (3.2032)	grad_norm 1.7090 (1.5566)	mem 23876MB
[2022-11-12 11:36:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][300/1251]	eta 0:11:57 lr 0.000586	time 0.7397 (0.7545)	loss 3.5621 (3.2400)	grad_norm 1.7098 (1.5597)	mem 23876MB
[2022-11-12 11:36:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][350/1251]	eta 0:11:19 lr 0.000586	time 0.7375 (0.7540)	loss 2.4449 (3.2293)	grad_norm 1.4694 (1.5608)	mem 23876MB
[2022-11-12 11:37:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][400/1251]	eta 0:10:41 lr 0.000586	time 0.7464 (0.7535)	loss 3.8270 (3.2494)	grad_norm 1.5570 (1.5657)	mem 23876MB
[2022-11-12 11:37:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][450/1251]	eta 0:10:03 lr 0.000586	time 0.7515 (0.7532)	loss 3.4385 (3.2266)	grad_norm 1.5285 (1.5608)	mem 23876MB
[2022-11-12 11:38:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][500/1251]	eta 0:09:25 lr 0.000586	time 0.7353 (0.7526)	loss 3.4842 (3.2308)	grad_norm 1.5521 (1.5570)	mem 23876MB
[2022-11-12 11:39:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][550/1251]	eta 0:08:47 lr 0.000585	time 0.7385 (0.7518)	loss 3.3118 (3.2378)	grad_norm 1.3447 (1.5585)	mem 23876MB
[2022-11-12 11:39:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][600/1251]	eta 0:08:09 lr 0.000585	time 0.7396 (0.7516)	loss 2.5741 (3.2411)	grad_norm 1.3802 (1.5617)	mem 23876MB
[2022-11-12 11:40:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][650/1251]	eta 0:07:31 lr 0.000585	time 0.7371 (0.7514)	loss 2.9144 (3.2560)	grad_norm 1.4873 (1.5631)	mem 23876MB
[2022-11-12 11:40:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][700/1251]	eta 0:06:53 lr 0.000585	time 0.7396 (0.7513)	loss 2.3363 (3.2471)	grad_norm 1.5259 (1.5603)	mem 23876MB
[2022-11-12 11:41:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][750/1251]	eta 0:06:16 lr 0.000584	time 0.7384 (0.7509)	loss 2.5230 (3.2505)	grad_norm 1.7482 (1.5609)	mem 23876MB
[2022-11-12 11:42:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][800/1251]	eta 0:05:38 lr 0.000584	time 0.7353 (0.7508)	loss 2.4453 (3.2573)	grad_norm 1.5477 (1.5615)	mem 23876MB
[2022-11-12 11:42:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][850/1251]	eta 0:05:00 lr 0.000584	time 0.7425 (0.7504)	loss 3.3398 (3.2631)	grad_norm 1.5025 (1.5639)	mem 23876MB
[2022-11-12 11:43:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][900/1251]	eta 0:04:23 lr 0.000584	time 0.7383 (0.7503)	loss 3.8283 (3.2649)	grad_norm 1.3684 (1.5621)	mem 23876MB
[2022-11-12 11:44:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][950/1251]	eta 0:03:45 lr 0.000584	time 0.7403 (0.7502)	loss 2.9820 (3.2603)	grad_norm 1.8475 (1.5640)	mem 23876MB
[2022-11-12 11:44:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][1000/1251]	eta 0:03:08 lr 0.000583	time 0.8173 (0.7502)	loss 3.6016 (3.2581)	grad_norm 1.6703 (1.5622)	mem 23876MB
[2022-11-12 11:45:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][1050/1251]	eta 0:02:30 lr 0.000583	time 0.7379 (0.7501)	loss 2.7962 (3.2608)	grad_norm 1.4888 (1.5640)	mem 23876MB
[2022-11-12 11:45:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][1100/1251]	eta 0:01:53 lr 0.000583	time 0.7382 (0.7498)	loss 3.4826 (3.2613)	grad_norm 1.6781 (1.5672)	mem 23876MB
[2022-11-12 11:46:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][1150/1251]	eta 0:01:15 lr 0.000583	time 0.7351 (0.7498)	loss 3.5514 (3.2659)	grad_norm 1.4919 (1.5674)	mem 23876MB
[2022-11-12 11:47:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][1200/1251]	eta 0:00:38 lr 0.000583	time 0.7406 (0.7498)	loss 3.0174 (3.2639)	grad_norm 1.4730 (1.5658)	mem 23876MB
[2022-11-12 11:47:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [134/300][1250/1251]	eta 0:00:00 lr 0.000582	time 0.7267 (0.7495)	loss 3.4230 (3.2648)	grad_norm 1.2395 (1.5649)	mem 23876MB
[2022-11-12 11:47:50 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 134 training takes 0:15:37
[2022-11-12 11:47:51 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_134.pth saving......
[2022-11-12 11:47:52 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_134.pth saved !!!
[2022-11-12 11:47:53 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.660 (1.660)	Loss 0.8081 (0.8081)	Acc@1 79.980 (79.980)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-12 11:48:04 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.700 Acc@5 94.778
[2022-11-12 11:48:04 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.7%
[2022-11-12 11:48:06 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.948 (1.948)	Loss 0.8790 (0.8790)	Acc@1 79.883 (79.883)	Acc@5 95.020 (95.020)	Mem 23876MB
[2022-11-12 11:48:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.058 Acc@5 95.802
[2022-11-12 11:48:17 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.1%
[2022-11-12 11:48:17 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.06% at 134 epoch
[2022-11-12 11:48:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][0/1251]	eta 0:48:42 lr 0.000582	time 2.3361 (2.3361)	loss 3.5056 (3.5056)	grad_norm 1.5447 (1.5447)	mem 23876MB
[2022-11-12 11:48:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][50/1251]	eta 0:15:36 lr 0.000582	time 0.8252 (0.7798)	loss 3.7759 (3.2549)	grad_norm 1.5188 (1.5475)	mem 23876MB
[2022-11-12 11:49:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][100/1251]	eta 0:14:37 lr 0.000582	time 0.7363 (0.7625)	loss 3.6383 (3.2733)	grad_norm 1.3081 (1.5488)	mem 23876MB
[2022-11-12 11:50:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][150/1251]	eta 0:13:54 lr 0.000582	time 0.7377 (0.7578)	loss 3.9751 (3.3101)	grad_norm 1.4199 (1.5558)	mem 23876MB
[2022-11-12 11:50:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][200/1251]	eta 0:13:13 lr 0.000582	time 0.7367 (0.7548)	loss 3.4553 (3.3283)	grad_norm 1.4091 (1.5512)	mem 23876MB
[2022-11-12 11:51:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][250/1251]	eta 0:12:34 lr 0.000581	time 0.7499 (0.7536)	loss 2.7610 (3.3065)	grad_norm 1.5519 (1.5542)	mem 23876MB
[2022-11-12 11:52:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][300/1251]	eta 0:11:55 lr 0.000581	time 0.7421 (0.7528)	loss 2.9702 (3.2858)	grad_norm 1.5733 (1.5489)	mem 23876MB
[2022-11-12 11:52:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][350/1251]	eta 0:11:17 lr 0.000581	time 0.7356 (0.7521)	loss 3.4129 (3.2862)	grad_norm 1.4665 (1.5523)	mem 23876MB
[2022-11-12 11:53:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][400/1251]	eta 0:10:39 lr 0.000581	time 0.7387 (0.7511)	loss 3.4381 (3.2819)	grad_norm 1.4007 (1.5510)	mem 23876MB
[2022-11-12 11:53:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][450/1251]	eta 0:10:01 lr 0.000581	time 0.7406 (0.7507)	loss 2.3345 (3.2860)	grad_norm 1.6144 (1.5549)	mem 23876MB
[2022-11-12 11:54:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][500/1251]	eta 0:09:23 lr 0.000580	time 0.7356 (0.7504)	loss 3.2692 (3.2937)	grad_norm 1.4503 (1.5627)	mem 23876MB
[2022-11-12 11:55:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][550/1251]	eta 0:08:45 lr 0.000580	time 0.7409 (0.7501)	loss 3.6743 (3.2982)	grad_norm 1.4463 (1.5608)	mem 23876MB
[2022-11-12 11:55:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][600/1251]	eta 0:08:08 lr 0.000580	time 0.7427 (0.7498)	loss 3.4192 (3.2972)	grad_norm 1.6093 (1.5620)	mem 23876MB
[2022-11-12 11:56:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][650/1251]	eta 0:07:30 lr 0.000580	time 0.7367 (0.7495)	loss 3.2329 (3.2950)	grad_norm 1.4950 (1.5615)	mem 23876MB
[2022-11-12 11:57:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][700/1251]	eta 0:06:52 lr 0.000580	time 0.7357 (0.7495)	loss 3.6829 (3.2913)	grad_norm 1.5754 (1.5621)	mem 23876MB
[2022-11-12 11:57:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][750/1251]	eta 0:06:15 lr 0.000579	time 0.7361 (0.7492)	loss 3.8982 (3.2944)	grad_norm 1.6890 (1.5608)	mem 23876MB
[2022-11-12 11:58:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][800/1251]	eta 0:05:37 lr 0.000579	time 0.7392 (0.7490)	loss 3.8106 (3.2935)	grad_norm 1.4885 (1.5621)	mem 23876MB
[2022-11-12 11:58:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][850/1251]	eta 0:05:00 lr 0.000579	time 0.7344 (0.7489)	loss 3.4376 (3.2914)	grad_norm 1.4584 (1.5649)	mem 23876MB
[2022-11-12 11:59:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][900/1251]	eta 0:04:22 lr 0.000579	time 0.7415 (0.7487)	loss 2.8596 (3.2907)	grad_norm 1.5643 (1.5689)	mem 23876MB
[2022-11-12 12:00:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][950/1251]	eta 0:03:45 lr 0.000579	time 0.7550 (0.7486)	loss 3.7174 (3.2885)	grad_norm 1.5313 (1.5674)	mem 23876MB
[2022-11-12 12:00:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][1000/1251]	eta 0:03:07 lr 0.000578	time 0.7466 (0.7485)	loss 2.5642 (3.2900)	grad_norm 1.7748 (1.5687)	mem 23876MB
[2022-11-12 12:01:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][1050/1251]	eta 0:02:30 lr 0.000578	time 0.7429 (0.7483)	loss 3.9465 (3.2868)	grad_norm 1.5374 (1.5675)	mem 23876MB
[2022-11-12 12:02:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][1100/1251]	eta 0:01:52 lr 0.000578	time 0.7381 (0.7483)	loss 3.5228 (3.2882)	grad_norm 1.5172 (1.5710)	mem 23876MB
[2022-11-12 12:02:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][1150/1251]	eta 0:01:15 lr 0.000578	time 0.7349 (0.7482)	loss 3.7703 (3.2849)	grad_norm 1.5531 (1.5689)	mem 23876MB
[2022-11-12 12:03:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][1200/1251]	eta 0:00:38 lr 0.000578	time 0.7386 (0.7480)	loss 2.8410 (3.2832)	grad_norm 1.5305 (1.5695)	mem 23876MB
[2022-11-12 12:03:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [135/300][1250/1251]	eta 0:00:00 lr 0.000577	time 0.7260 (0.7478)	loss 2.2426 (3.2831)	grad_norm 1.5305 (1.5685)	mem 23876MB
[2022-11-12 12:03:53 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 135 training takes 0:15:35
[2022-11-12 12:03:53 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_135.pth saving......
[2022-11-12 12:03:54 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_135.pth saved !!!
[2022-11-12 12:03:55 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.617 (1.617)	Loss 0.8821 (0.8821)	Acc@1 79.102 (79.102)	Acc@5 94.824 (94.824)	Mem 23876MB
[2022-11-12 12:04:06 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.908 Acc@5 94.814
[2022-11-12 12:04:06 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.9%
[2022-11-12 12:04:08 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.876 (1.876)	Loss 0.8628 (0.8628)	Acc@1 79.102 (79.102)	Acc@5 95.312 (95.312)	Mem 23876MB
[2022-11-12 12:04:19 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.084 Acc@5 95.832
[2022-11-12 12:04:19 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.1%
[2022-11-12 12:04:19 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.08% at 135 epoch
[2022-11-12 12:04:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][0/1251]	eta 0:49:30 lr 0.000577	time 2.3743 (2.3743)	loss 3.1667 (3.1667)	grad_norm 1.5559 (1.5559)	mem 23876MB
[2022-11-12 12:04:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][50/1251]	eta 0:15:43 lr 0.000577	time 0.7325 (0.7860)	loss 3.1694 (3.2202)	grad_norm 1.5007 (1.6062)	mem 23876MB
[2022-11-12 12:05:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][100/1251]	eta 0:14:42 lr 0.000577	time 0.7421 (0.7668)	loss 3.4517 (3.1696)	grad_norm 1.5528 (1.5687)	mem 23876MB
[2022-11-12 12:06:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][150/1251]	eta 0:13:57 lr 0.000577	time 0.7423 (0.7607)	loss 3.4202 (3.2513)	grad_norm 1.6978 (1.5751)	mem 23876MB
[2022-11-12 12:06:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][200/1251]	eta 0:13:16 lr 0.000576	time 0.7385 (0.7581)	loss 3.3405 (3.2880)	grad_norm 1.4681 (1.5706)	mem 23876MB
[2022-11-12 12:07:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][250/1251]	eta 0:12:36 lr 0.000576	time 0.7335 (0.7559)	loss 3.7545 (3.2815)	grad_norm 1.6178 (1.5625)	mem 23876MB
[2022-11-12 12:08:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][300/1251]	eta 0:11:57 lr 0.000576	time 0.7364 (0.7546)	loss 3.4970 (3.2745)	grad_norm 1.5516 (1.5714)	mem 23876MB
[2022-11-12 12:08:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][350/1251]	eta 0:11:18 lr 0.000576	time 0.7410 (0.7533)	loss 3.8919 (3.2785)	grad_norm 1.7956 (1.5775)	mem 23876MB
[2022-11-12 12:09:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][400/1251]	eta 0:10:40 lr 0.000576	time 0.7360 (0.7527)	loss 2.0623 (3.2734)	grad_norm 1.4659 (1.5745)	mem 23876MB
[2022-11-12 12:09:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][450/1251]	eta 0:10:02 lr 0.000575	time 0.7435 (0.7522)	loss 2.1694 (3.2715)	grad_norm 1.6215 (1.5796)	mem 23876MB
[2022-11-12 12:10:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][500/1251]	eta 0:09:24 lr 0.000575	time 0.7382 (0.7518)	loss 3.7650 (3.2693)	grad_norm 1.7152 (1.5856)	mem 23876MB
[2022-11-12 12:11:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][550/1251]	eta 0:08:46 lr 0.000575	time 0.7427 (0.7516)	loss 2.1666 (3.2755)	grad_norm 1.4875 (1.5795)	mem 23876MB
[2022-11-12 12:11:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][600/1251]	eta 0:08:08 lr 0.000575	time 0.7485 (0.7511)	loss 3.6026 (3.2765)	grad_norm 1.4916 (1.5808)	mem 23876MB
[2022-11-12 12:12:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][650/1251]	eta 0:07:31 lr 0.000575	time 0.7373 (0.7510)	loss 2.2390 (3.2818)	grad_norm 1.6625 (1.5810)	mem 23876MB
[2022-11-12 12:13:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][700/1251]	eta 0:06:53 lr 0.000574	time 0.7381 (0.7507)	loss 3.7435 (3.2829)	grad_norm 2.0780 (1.5822)	mem 23876MB
[2022-11-12 12:13:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][750/1251]	eta 0:06:16 lr 0.000574	time 0.7396 (0.7505)	loss 3.6448 (3.2836)	grad_norm 2.1787 (1.5825)	mem 23876MB
[2022-11-12 12:14:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][800/1251]	eta 0:05:38 lr 0.000574	time 0.7533 (0.7502)	loss 3.7615 (3.2840)	grad_norm 1.5770 (1.5840)	mem 23876MB
[2022-11-12 12:14:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][850/1251]	eta 0:05:00 lr 0.000574	time 0.7381 (0.7502)	loss 2.5851 (3.2845)	grad_norm 1.7653 (1.5844)	mem 23876MB
[2022-11-12 12:15:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][900/1251]	eta 0:04:23 lr 0.000574	time 0.7418 (0.7498)	loss 3.9104 (3.2806)	grad_norm 1.6517 (1.5827)	mem 23876MB
[2022-11-12 12:16:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][950/1251]	eta 0:03:45 lr 0.000573	time 0.7378 (0.7498)	loss 2.0946 (3.2722)	grad_norm 1.4581 (1.5837)	mem 23876MB
[2022-11-12 12:16:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][1000/1251]	eta 0:03:08 lr 0.000573	time 0.7534 (0.7496)	loss 3.3912 (3.2704)	grad_norm 1.5021 (1.5813)	mem 23876MB
[2022-11-12 12:17:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][1050/1251]	eta 0:02:30 lr 0.000573	time 0.7385 (0.7495)	loss 2.3875 (3.2685)	grad_norm 1.4249 (1.5793)	mem 23876MB
[2022-11-12 12:18:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][1100/1251]	eta 0:01:53 lr 0.000573	time 0.7375 (0.7493)	loss 3.6225 (3.2686)	grad_norm 1.5071 (1.5778)	mem 23876MB
[2022-11-12 12:18:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][1150/1251]	eta 0:01:15 lr 0.000573	time 0.7608 (0.7493)	loss 3.5821 (3.2724)	grad_norm 1.5185 (1.5753)	mem 23876MB
[2022-11-12 12:19:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][1200/1251]	eta 0:00:38 lr 0.000572	time 0.7457 (0.7491)	loss 3.7440 (3.2713)	grad_norm 1.4422 (1.5730)	mem 23876MB
[2022-11-12 12:19:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [136/300][1250/1251]	eta 0:00:00 lr 0.000572	time 0.7264 (0.7491)	loss 3.4403 (3.2724)	grad_norm 1.6441 (1.5734)	mem 23876MB
[2022-11-12 12:19:56 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 136 training takes 0:15:37
[2022-11-12 12:19:56 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_136.pth saving......
[2022-11-12 12:19:57 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_136.pth saved !!!
[2022-11-12 12:19:59 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.749 (1.749)	Loss 0.8151 (0.8151)	Acc@1 80.078 (80.078)	Acc@5 96.680 (96.680)	Mem 23876MB
[2022-11-12 12:20:10 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.800 Acc@5 94.912
[2022-11-12 12:20:10 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.8%
[2022-11-12 12:20:11 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.911 (1.911)	Loss 0.7780 (0.7780)	Acc@1 82.227 (82.227)	Acc@5 96.191 (96.191)	Mem 23876MB
[2022-11-12 12:20:22 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.184 Acc@5 95.860
[2022-11-12 12:20:22 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.2%
[2022-11-12 12:20:22 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.18% at 136 epoch
[2022-11-12 12:20:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][0/1251]	eta 0:53:20 lr 0.000572	time 2.5587 (2.5587)	loss 2.9310 (2.9310)	grad_norm 1.4073 (1.4073)	mem 23876MB
[2022-11-12 12:21:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][50/1251]	eta 0:15:46 lr 0.000572	time 0.7438 (0.7883)	loss 3.2215 (3.1863)	grad_norm 1.4092 (1.6053)	mem 23876MB
[2022-11-12 12:21:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][100/1251]	eta 0:14:44 lr 0.000572	time 0.7428 (0.7683)	loss 2.2332 (3.2296)	grad_norm 1.5855 (1.6324)	mem 23876MB
[2022-11-12 12:22:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][150/1251]	eta 0:13:57 lr 0.000572	time 0.7383 (0.7610)	loss 3.8357 (3.2343)	grad_norm 1.6241 (1.6155)	mem 23876MB
[2022-11-12 12:22:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][200/1251]	eta 0:13:16 lr 0.000571	time 0.7334 (0.7582)	loss 3.7057 (3.2537)	grad_norm 1.5872 (1.6070)	mem 23876MB
[2022-11-12 12:23:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][250/1251]	eta 0:12:36 lr 0.000571	time 0.7365 (0.7556)	loss 3.0810 (3.2346)	grad_norm 1.5806 (1.5971)	mem 23876MB
[2022-11-12 12:24:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][300/1251]	eta 0:11:57 lr 0.000571	time 0.7391 (0.7543)	loss 2.8978 (3.2440)	grad_norm 1.3754 (1.5912)	mem 23876MB
[2022-11-12 12:24:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][350/1251]	eta 0:11:18 lr 0.000571	time 0.7498 (0.7532)	loss 2.2671 (3.2293)	grad_norm 1.6683 (1.5926)	mem 23876MB
[2022-11-12 12:25:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][400/1251]	eta 0:10:40 lr 0.000571	time 0.7337 (0.7525)	loss 3.5367 (3.2290)	grad_norm 1.6165 (1.5941)	mem 23876MB
[2022-11-12 12:26:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][450/1251]	eta 0:10:02 lr 0.000570	time 0.7483 (0.7519)	loss 2.3545 (3.2225)	grad_norm 1.4129 (1.5954)	mem 23876MB
[2022-11-12 12:26:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][500/1251]	eta 0:09:24 lr 0.000570	time 0.7444 (0.7513)	loss 2.9334 (3.2325)	grad_norm 1.4401 (1.5920)	mem 23876MB
[2022-11-12 12:27:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][550/1251]	eta 0:08:46 lr 0.000570	time 0.7413 (0.7509)	loss 3.6766 (3.2179)	grad_norm 1.4430 (1.5873)	mem 23876MB
[2022-11-12 12:27:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][600/1251]	eta 0:08:08 lr 0.000570	time 0.7509 (0.7508)	loss 3.9252 (3.2210)	grad_norm 1.6285 (1.5858)	mem 23876MB
[2022-11-12 12:28:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][650/1251]	eta 0:07:31 lr 0.000570	time 0.7479 (0.7504)	loss 2.9910 (3.2280)	grad_norm 1.7338 (nan)	mem 23876MB
[2022-11-12 12:29:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][700/1251]	eta 0:06:53 lr 0.000569	time 0.7379 (0.7502)	loss 2.4953 (3.2284)	grad_norm 1.4233 (nan)	mem 23876MB
[2022-11-12 12:29:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][750/1251]	eta 0:06:15 lr 0.000569	time 0.7282 (0.7500)	loss 2.6280 (3.2211)	grad_norm 1.6359 (nan)	mem 23876MB
[2022-11-12 12:30:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][800/1251]	eta 0:05:38 lr 0.000569	time 0.7382 (0.7499)	loss 2.9348 (3.2234)	grad_norm 1.5378 (nan)	mem 23876MB
[2022-11-12 12:31:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][850/1251]	eta 0:05:00 lr 0.000569	time 0.7404 (0.7496)	loss 3.7077 (3.2220)	grad_norm 1.4301 (nan)	mem 23876MB
[2022-11-12 12:31:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][900/1251]	eta 0:04:23 lr 0.000568	time 0.7370 (0.7494)	loss 3.2492 (3.2279)	grad_norm 1.4639 (nan)	mem 23876MB
[2022-11-12 12:32:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][950/1251]	eta 0:03:45 lr 0.000568	time 0.7370 (0.7493)	loss 3.1741 (3.2316)	grad_norm 1.4088 (nan)	mem 23876MB
[2022-11-12 12:32:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][1000/1251]	eta 0:03:08 lr 0.000568	time 0.7446 (0.7493)	loss 3.0687 (3.2400)	grad_norm 1.6723 (nan)	mem 23876MB
[2022-11-12 12:33:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][1050/1251]	eta 0:02:30 lr 0.000568	time 0.7399 (0.7490)	loss 3.5860 (3.2409)	grad_norm 1.6037 (nan)	mem 23876MB
[2022-11-12 12:34:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][1100/1251]	eta 0:01:53 lr 0.000568	time 0.7397 (0.7490)	loss 2.8868 (3.2453)	grad_norm 1.7900 (nan)	mem 23876MB
[2022-11-12 12:34:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][1150/1251]	eta 0:01:15 lr 0.000567	time 0.7398 (0.7490)	loss 3.7627 (3.2455)	grad_norm 1.8193 (nan)	mem 23876MB
[2022-11-12 12:35:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][1200/1251]	eta 0:00:38 lr 0.000567	time 0.7372 (0.7489)	loss 3.6354 (3.2418)	grad_norm 1.5154 (nan)	mem 23876MB
[2022-11-12 12:35:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [137/300][1250/1251]	eta 0:00:00 lr 0.000567	time 0.7259 (0.7487)	loss 2.9860 (3.2418)	grad_norm 1.5564 (nan)	mem 23876MB
[2022-11-12 12:35:59 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 137 training takes 0:15:36
[2022-11-12 12:35:59 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_137.pth saving......
[2022-11-12 12:36:00 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_137.pth saved !!!
[2022-11-12 12:36:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.675 (1.675)	Loss 0.9685 (0.9685)	Acc@1 77.246 (77.246)	Acc@5 93.652 (93.652)	Mem 23876MB
[2022-11-12 12:36:13 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.854 Acc@5 94.918
[2022-11-12 12:36:13 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.9%
[2022-11-12 12:36:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.782 (1.782)	Loss 0.9161 (0.9161)	Acc@1 78.223 (78.223)	Acc@5 94.922 (94.922)	Mem 23876MB
[2022-11-12 12:36:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.214 Acc@5 95.882
[2022-11-12 12:36:25 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.2%
[2022-11-12 12:36:25 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.21% at 137 epoch
[2022-11-12 12:36:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][0/1251]	eta 0:48:35 lr 0.000567	time 2.3304 (2.3304)	loss 3.4707 (3.4707)	grad_norm 1.6198 (1.6198)	mem 23876MB
[2022-11-12 12:37:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][50/1251]	eta 0:15:35 lr 0.000567	time 0.7354 (0.7786)	loss 3.1925 (3.1500)	grad_norm 1.3092 (1.5283)	mem 23876MB
[2022-11-12 12:37:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][100/1251]	eta 0:14:40 lr 0.000567	time 0.7443 (0.7646)	loss 2.4442 (3.2384)	grad_norm 1.5154 (1.5439)	mem 23876MB
[2022-11-12 12:38:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][150/1251]	eta 0:13:55 lr 0.000566	time 0.7427 (0.7592)	loss 3.3696 (3.2557)	grad_norm 1.4891 (1.5593)	mem 23876MB
[2022-11-12 12:38:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][200/1251]	eta 0:13:14 lr 0.000566	time 0.7344 (0.7556)	loss 3.2977 (3.2601)	grad_norm 1.4481 (1.5547)	mem 23876MB
[2022-11-12 12:39:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][250/1251]	eta 0:12:34 lr 0.000566	time 0.7365 (0.7539)	loss 2.8799 (3.2533)	grad_norm 1.5931 (1.5615)	mem 23876MB
[2022-11-12 12:40:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][300/1251]	eta 0:11:55 lr 0.000566	time 0.7414 (0.7529)	loss 2.4041 (3.2587)	grad_norm 1.4171 (1.5711)	mem 23876MB
[2022-11-12 12:40:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][350/1251]	eta 0:11:17 lr 0.000566	time 0.7474 (0.7521)	loss 3.2249 (3.2379)	grad_norm 1.3913 (1.5682)	mem 23876MB
[2022-11-12 12:41:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][400/1251]	eta 0:10:39 lr 0.000565	time 0.7377 (0.7516)	loss 2.5611 (3.2258)	grad_norm 1.5732 (1.5790)	mem 23876MB
[2022-11-12 12:42:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][450/1251]	eta 0:10:01 lr 0.000565	time 0.7378 (0.7508)	loss 3.3838 (3.2210)	grad_norm 1.4469 (1.5761)	mem 23876MB
[2022-11-12 12:42:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][500/1251]	eta 0:09:23 lr 0.000565	time 0.7373 (0.7504)	loss 3.6627 (3.2181)	grad_norm 1.5931 (1.5730)	mem 23876MB
[2022-11-12 12:43:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][550/1251]	eta 0:08:45 lr 0.000565	time 0.7389 (0.7498)	loss 2.1339 (3.2178)	grad_norm 1.4012 (1.5736)	mem 23876MB
[2022-11-12 12:43:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][600/1251]	eta 0:08:07 lr 0.000565	time 0.7363 (0.7496)	loss 3.4406 (3.2048)	grad_norm 2.0180 (1.5730)	mem 23876MB
[2022-11-12 12:44:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][650/1251]	eta 0:07:30 lr 0.000564	time 0.7341 (0.7495)	loss 3.7891 (3.2031)	grad_norm 1.5063 (1.5734)	mem 23876MB
[2022-11-12 12:45:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][700/1251]	eta 0:06:52 lr 0.000564	time 0.7406 (0.7493)	loss 3.3777 (3.2118)	grad_norm 1.5019 (1.5726)	mem 23876MB
[2022-11-12 12:45:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][750/1251]	eta 0:06:15 lr 0.000564	time 0.7403 (0.7490)	loss 2.5914 (3.2113)	grad_norm 1.4016 (1.5755)	mem 23876MB
[2022-11-12 12:46:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][800/1251]	eta 0:05:37 lr 0.000564	time 0.7476 (0.7489)	loss 2.8450 (3.2155)	grad_norm 1.6703 (1.5771)	mem 23876MB
[2022-11-12 12:47:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][850/1251]	eta 0:05:00 lr 0.000564	time 0.7391 (0.7487)	loss 4.0074 (3.2219)	grad_norm 1.5443 (1.5783)	mem 23876MB
[2022-11-12 12:47:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][900/1251]	eta 0:04:22 lr 0.000563	time 0.7412 (0.7487)	loss 2.5309 (3.2288)	grad_norm 1.5774 (1.5799)	mem 23876MB
[2022-11-12 12:48:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][950/1251]	eta 0:03:45 lr 0.000563	time 0.7395 (0.7484)	loss 3.5133 (3.2351)	grad_norm 1.7170 (1.5816)	mem 23876MB
[2022-11-12 12:48:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][1000/1251]	eta 0:03:07 lr 0.000563	time 0.7344 (0.7483)	loss 4.0657 (3.2365)	grad_norm 1.6678 (1.5838)	mem 23876MB
[2022-11-12 12:49:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][1050/1251]	eta 0:02:30 lr 0.000563	time 0.7366 (0.7483)	loss 2.4500 (3.2380)	grad_norm 1.9248 (1.5840)	mem 23876MB
[2022-11-12 12:50:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][1100/1251]	eta 0:01:52 lr 0.000563	time 0.7367 (0.7481)	loss 3.4298 (3.2398)	grad_norm 1.5369 (1.5860)	mem 23876MB
[2022-11-12 12:50:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][1150/1251]	eta 0:01:15 lr 0.000562	time 0.7507 (0.7480)	loss 3.9897 (3.2420)	grad_norm 1.7786 (1.5869)	mem 23876MB
[2022-11-12 12:51:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][1200/1251]	eta 0:00:38 lr 0.000562	time 0.7396 (0.7480)	loss 3.6729 (3.2396)	grad_norm 1.7627 (1.5861)	mem 23876MB
[2022-11-12 12:52:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [138/300][1250/1251]	eta 0:00:00 lr 0.000562	time 0.7265 (0.7477)	loss 3.5187 (3.2444)	grad_norm 2.0208 (1.5859)	mem 23876MB
[2022-11-12 12:52:01 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 138 training takes 0:15:35
[2022-11-12 12:52:01 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_138.pth saving......
[2022-11-12 12:52:02 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_138.pth saved !!!
[2022-11-12 12:52:03 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.568 (1.568)	Loss 0.8847 (0.8847)	Acc@1 79.102 (79.102)	Acc@5 95.410 (95.410)	Mem 23876MB
[2022-11-12 12:52:14 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.060 Acc@5 95.074
[2022-11-12 12:52:14 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.1%
[2022-11-12 12:52:16 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.914 (1.914)	Loss 0.7814 (0.7814)	Acc@1 80.664 (80.664)	Acc@5 95.801 (95.801)	Mem 23876MB
[2022-11-12 12:52:27 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.226 Acc@5 95.920
[2022-11-12 12:52:27 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.2%
[2022-11-12 12:52:27 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.23% at 138 epoch
[2022-11-12 12:52:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][0/1251]	eta 0:50:13 lr 0.000562	time 2.4091 (2.4091)	loss 2.3370 (2.3370)	grad_norm 1.4620 (1.4620)	mem 23876MB
[2022-11-12 12:53:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][50/1251]	eta 0:15:38 lr 0.000562	time 0.7421 (0.7815)	loss 3.8129 (3.1885)	grad_norm 1.5475 (1.6710)	mem 23876MB
[2022-11-12 12:53:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][100/1251]	eta 0:14:38 lr 0.000561	time 0.8231 (0.7635)	loss 3.4305 (3.1798)	grad_norm 1.4626 (1.6174)	mem 23876MB
[2022-11-12 12:54:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][150/1251]	eta 0:13:53 lr 0.000561	time 0.7404 (0.7569)	loss 3.6293 (3.2258)	grad_norm 1.4930 (1.6073)	mem 23876MB
[2022-11-12 12:54:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][200/1251]	eta 0:13:13 lr 0.000561	time 0.7376 (0.7551)	loss 2.3901 (3.2242)	grad_norm 1.5854 (1.6085)	mem 23876MB
[2022-11-12 12:55:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][250/1251]	eta 0:12:34 lr 0.000561	time 0.7445 (0.7535)	loss 3.3920 (3.2367)	grad_norm 1.5142 (1.6075)	mem 23876MB
[2022-11-12 12:56:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][300/1251]	eta 0:11:55 lr 0.000561	time 0.7493 (0.7524)	loss 3.7539 (3.2265)	grad_norm 1.8857 (1.6081)	mem 23876MB
[2022-11-12 12:56:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][350/1251]	eta 0:11:17 lr 0.000560	time 0.7396 (0.7518)	loss 3.3983 (3.2180)	grad_norm 1.5808 (1.5997)	mem 23876MB
[2022-11-12 12:57:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][400/1251]	eta 0:10:38 lr 0.000560	time 0.7346 (0.7509)	loss 2.9796 (3.2325)	grad_norm 1.7327 (1.5976)	mem 23876MB
[2022-11-12 12:58:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][450/1251]	eta 0:10:01 lr 0.000560	time 0.7403 (0.7504)	loss 2.6870 (3.2361)	grad_norm 1.4573 (1.5988)	mem 23876MB
[2022-11-12 12:58:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][500/1251]	eta 0:09:23 lr 0.000560	time 0.7360 (0.7499)	loss 3.0985 (3.2375)	grad_norm 1.4481 (1.5996)	mem 23876MB
[2022-11-12 12:59:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][550/1251]	eta 0:08:45 lr 0.000560	time 0.7373 (0.7497)	loss 3.1192 (3.2374)	grad_norm 1.3315 (1.6005)	mem 23876MB
[2022-11-12 12:59:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][600/1251]	eta 0:08:07 lr 0.000559	time 0.7385 (0.7495)	loss 3.5506 (3.2397)	grad_norm 1.5668 (1.6002)	mem 23876MB
[2022-11-12 13:00:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][650/1251]	eta 0:07:30 lr 0.000559	time 0.7549 (0.7493)	loss 2.3263 (3.2412)	grad_norm 1.4863 (1.5971)	mem 23876MB
[2022-11-12 13:01:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][700/1251]	eta 0:06:52 lr 0.000559	time 0.7579 (0.7491)	loss 3.8437 (3.2430)	grad_norm 1.4814 (1.5956)	mem 23876MB
[2022-11-12 13:01:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][750/1251]	eta 0:06:15 lr 0.000559	time 0.7378 (0.7490)	loss 3.6641 (3.2375)	grad_norm 1.5043 (1.5937)	mem 23876MB
[2022-11-12 13:02:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][800/1251]	eta 0:05:37 lr 0.000559	time 0.7380 (0.7488)	loss 2.9383 (3.2351)	grad_norm 1.4957 (1.5947)	mem 23876MB
[2022-11-12 13:03:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][850/1251]	eta 0:05:00 lr 0.000558	time 0.7382 (0.7487)	loss 3.5625 (3.2316)	grad_norm 1.7623 (1.5922)	mem 23876MB
[2022-11-12 13:03:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][900/1251]	eta 0:04:22 lr 0.000558	time 0.7418 (0.7486)	loss 3.7770 (3.2313)	grad_norm 1.7343 (1.5920)	mem 23876MB
[2022-11-12 13:04:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][950/1251]	eta 0:03:45 lr 0.000558	time 0.7381 (0.7484)	loss 3.7677 (3.2314)	grad_norm 1.7403 (1.5929)	mem 23876MB
[2022-11-12 13:04:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][1000/1251]	eta 0:03:07 lr 0.000558	time 0.7359 (0.7484)	loss 3.7453 (3.2278)	grad_norm 1.4163 (1.5912)	mem 23876MB
[2022-11-12 13:05:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][1050/1251]	eta 0:02:30 lr 0.000558	time 0.7450 (0.7483)	loss 2.9533 (3.2244)	grad_norm 1.4417 (1.5914)	mem 23876MB
[2022-11-12 13:06:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][1100/1251]	eta 0:01:52 lr 0.000557	time 0.7401 (0.7483)	loss 3.3153 (3.2261)	grad_norm 1.8156 (1.5913)	mem 23876MB
[2022-11-12 13:06:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][1150/1251]	eta 0:01:15 lr 0.000557	time 0.8337 (0.7483)	loss 3.3023 (3.2269)	grad_norm 1.7542 (1.5913)	mem 23876MB
[2022-11-12 13:07:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][1200/1251]	eta 0:00:38 lr 0.000557	time 0.8054 (0.7482)	loss 3.7906 (3.2271)	grad_norm 1.6904 (1.5926)	mem 23876MB
[2022-11-12 13:08:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [139/300][1250/1251]	eta 0:00:00 lr 0.000557	time 0.7250 (0.7480)	loss 3.7012 (3.2311)	grad_norm 1.7586 (1.5915)	mem 23876MB
[2022-11-12 13:08:03 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 139 training takes 0:15:35
[2022-11-12 13:08:03 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_139.pth saving......
[2022-11-12 13:08:04 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_139.pth saved !!!
[2022-11-12 13:08:06 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.671 (1.671)	Loss 0.8954 (0.8954)	Acc@1 79.199 (79.199)	Acc@5 95.020 (95.020)	Mem 23876MB
[2022-11-12 13:08:16 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.002 Acc@5 94.910
[2022-11-12 13:08:16 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.0%
[2022-11-12 13:08:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.939 (1.939)	Loss 0.7286 (0.7286)	Acc@1 83.301 (83.301)	Acc@5 96.680 (96.680)	Mem 23876MB
[2022-11-12 13:08:29 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.276 Acc@5 95.944
[2022-11-12 13:08:29 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.3%
[2022-11-12 13:08:29 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.28% at 139 epoch
[2022-11-12 13:08:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][0/1251]	eta 0:50:18 lr 0.000557	time 2.4125 (2.4125)	loss 3.6088 (3.6088)	grad_norm 1.5036 (1.5036)	mem 23876MB
[2022-11-12 13:09:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][50/1251]	eta 0:15:40 lr 0.000557	time 0.7409 (0.7827)	loss 3.6773 (3.2550)	grad_norm 1.5061 (1.5710)	mem 23876MB
[2022-11-12 13:09:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][100/1251]	eta 0:14:39 lr 0.000556	time 0.7418 (0.7645)	loss 2.7319 (3.2509)	grad_norm 1.3710 (1.5873)	mem 23876MB
[2022-11-12 13:10:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][150/1251]	eta 0:13:56 lr 0.000556	time 0.7376 (0.7593)	loss 3.7355 (3.2132)	grad_norm 1.5327 (1.5672)	mem 23876MB
[2022-11-12 13:11:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][200/1251]	eta 0:13:15 lr 0.000556	time 0.8139 (0.7568)	loss 3.2130 (3.2397)	grad_norm 1.4515 (inf)	mem 23876MB
[2022-11-12 13:11:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][250/1251]	eta 0:12:35 lr 0.000556	time 0.7356 (0.7545)	loss 2.9144 (3.2318)	grad_norm 1.7379 (inf)	mem 23876MB
[2022-11-12 13:12:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][300/1251]	eta 0:11:56 lr 0.000556	time 0.7374 (0.7533)	loss 3.7681 (3.2389)	grad_norm 1.6249 (inf)	mem 23876MB
[2022-11-12 13:12:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][350/1251]	eta 0:11:17 lr 0.000555	time 0.7373 (0.7522)	loss 3.0038 (3.2347)	grad_norm 1.9379 (inf)	mem 23876MB
[2022-11-12 13:13:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][400/1251]	eta 0:10:39 lr 0.000555	time 0.7424 (0.7517)	loss 3.1122 (3.2491)	grad_norm 1.6360 (inf)	mem 23876MB
[2022-11-12 13:14:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][450/1251]	eta 0:10:01 lr 0.000555	time 0.7426 (0.7514)	loss 2.9676 (3.2512)	grad_norm 1.6155 (inf)	mem 23876MB
[2022-11-12 13:14:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][500/1251]	eta 0:09:23 lr 0.000555	time 0.7364 (0.7507)	loss 3.3080 (3.2568)	grad_norm 1.3868 (inf)	mem 23876MB
[2022-11-12 13:15:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][550/1251]	eta 0:08:46 lr 0.000554	time 0.7357 (0.7505)	loss 3.6467 (3.2571)	grad_norm 1.7800 (inf)	mem 23876MB
[2022-11-12 13:16:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][600/1251]	eta 0:08:08 lr 0.000554	time 0.7385 (0.7502)	loss 2.1076 (3.2539)	grad_norm 1.5501 (inf)	mem 23876MB
[2022-11-12 13:16:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][650/1251]	eta 0:07:30 lr 0.000554	time 0.7440 (0.7502)	loss 3.4909 (3.2598)	grad_norm 1.7153 (inf)	mem 23876MB
[2022-11-12 13:17:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][700/1251]	eta 0:06:53 lr 0.000554	time 0.7370 (0.7499)	loss 3.1243 (3.2578)	grad_norm 1.7967 (inf)	mem 23876MB
[2022-11-12 13:17:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][750/1251]	eta 0:06:15 lr 0.000554	time 0.7437 (0.7497)	loss 3.7310 (3.2539)	grad_norm 1.6745 (inf)	mem 23876MB
[2022-11-12 13:18:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][800/1251]	eta 0:05:38 lr 0.000553	time 0.7376 (0.7496)	loss 3.5441 (3.2482)	grad_norm 1.6351 (inf)	mem 23876MB
[2022-11-12 13:19:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][850/1251]	eta 0:05:00 lr 0.000553	time 0.7542 (0.7496)	loss 3.1669 (3.2510)	grad_norm 2.0489 (inf)	mem 23876MB
[2022-11-12 13:19:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][900/1251]	eta 0:04:23 lr 0.000553	time 0.7360 (0.7497)	loss 2.8259 (3.2536)	grad_norm 1.8875 (inf)	mem 23876MB
[2022-11-12 13:20:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][950/1251]	eta 0:03:45 lr 0.000553	time 0.7445 (0.7495)	loss 3.4004 (3.2472)	grad_norm 1.5161 (inf)	mem 23876MB
[2022-11-12 13:20:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][1000/1251]	eta 0:03:08 lr 0.000553	time 0.7388 (0.7495)	loss 3.0573 (3.2481)	grad_norm 2.1476 (inf)	mem 23876MB
[2022-11-12 13:21:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][1050/1251]	eta 0:02:30 lr 0.000552	time 0.7419 (0.7494)	loss 3.5711 (3.2472)	grad_norm 1.8055 (inf)	mem 23876MB
[2022-11-12 13:22:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][1100/1251]	eta 0:01:53 lr 0.000552	time 0.7375 (0.7495)	loss 3.8254 (3.2492)	grad_norm 1.5626 (inf)	mem 23876MB
[2022-11-12 13:22:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][1150/1251]	eta 0:01:15 lr 0.000552	time 0.7374 (0.7495)	loss 3.5767 (3.2517)	grad_norm 1.4734 (inf)	mem 23876MB
[2022-11-12 13:23:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][1200/1251]	eta 0:00:38 lr 0.000552	time 0.7398 (0.7493)	loss 3.6307 (3.2545)	grad_norm 1.4748 (inf)	mem 23876MB
[2022-11-12 13:24:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [140/300][1250/1251]	eta 0:00:00 lr 0.000552	time 0.7279 (0.7492)	loss 3.1301 (3.2587)	grad_norm 1.7665 (inf)	mem 23876MB
[2022-11-12 13:24:06 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 140 training takes 0:15:37
[2022-11-12 13:24:07 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_140.pth saving......
[2022-11-12 13:24:08 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_140.pth saved !!!
[2022-11-12 13:24:09 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.641 (1.641)	Loss 0.8480 (0.8480)	Acc@1 78.223 (78.223)	Acc@5 95.117 (95.117)	Mem 23876MB
[2022-11-12 13:24:20 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.114 Acc@5 94.962
[2022-11-12 13:24:20 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.1%
[2022-11-12 13:24:22 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.924 (1.924)	Loss 0.7736 (0.7736)	Acc@1 80.664 (80.664)	Acc@5 95.703 (95.703)	Mem 23876MB
[2022-11-12 13:24:33 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.312 Acc@5 95.982
[2022-11-12 13:24:33 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.3%
[2022-11-12 13:24:33 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.31% at 140 epoch
[2022-11-12 13:24:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][0/1251]	eta 0:50:08 lr 0.000552	time 2.4051 (2.4051)	loss 2.3991 (2.3991)	grad_norm 1.4818 (1.4818)	mem 23876MB
[2022-11-12 13:25:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][50/1251]	eta 0:15:41 lr 0.000551	time 0.7385 (0.7841)	loss 3.5943 (3.2315)	grad_norm 1.4746 (1.5559)	mem 23876MB
[2022-11-12 13:25:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][100/1251]	eta 0:14:39 lr 0.000551	time 0.7398 (0.7642)	loss 2.5946 (3.1774)	grad_norm 1.4261 (1.5469)	mem 23876MB
[2022-11-12 13:26:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][150/1251]	eta 0:13:56 lr 0.000551	time 0.7391 (0.7594)	loss 2.3809 (3.1303)	grad_norm 1.7483 (1.5548)	mem 23876MB
[2022-11-12 13:27:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][200/1251]	eta 0:13:15 lr 0.000551	time 0.7337 (0.7570)	loss 3.3957 (3.1224)	grad_norm 1.4655 (1.5674)	mem 23876MB
[2022-11-12 13:27:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][250/1251]	eta 0:12:35 lr 0.000551	time 0.8132 (0.7549)	loss 3.0218 (3.1360)	grad_norm 1.7397 (1.5716)	mem 23876MB
[2022-11-12 13:28:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][300/1251]	eta 0:11:56 lr 0.000550	time 0.7363 (0.7535)	loss 2.5323 (3.1477)	grad_norm 1.4374 (1.5764)	mem 23876MB
[2022-11-12 13:28:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][350/1251]	eta 0:11:17 lr 0.000550	time 0.7417 (0.7523)	loss 3.4579 (3.1468)	grad_norm 1.4858 (1.5699)	mem 23876MB
[2022-11-12 13:29:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][400/1251]	eta 0:10:39 lr 0.000550	time 0.7417 (0.7519)	loss 3.7282 (3.1545)	grad_norm 1.3960 (1.5721)	mem 23876MB
[2022-11-12 13:30:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][450/1251]	eta 0:10:02 lr 0.000550	time 0.7420 (0.7516)	loss 3.7680 (3.1511)	grad_norm 1.6105 (1.5814)	mem 23876MB
[2022-11-12 13:30:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][500/1251]	eta 0:09:24 lr 0.000550	time 0.7402 (0.7512)	loss 2.9690 (3.1572)	grad_norm 1.5187 (1.5898)	mem 23876MB
[2022-11-12 13:31:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][550/1251]	eta 0:08:46 lr 0.000549	time 0.7369 (0.7507)	loss 3.7193 (3.1661)	grad_norm 1.6911 (1.5904)	mem 23876MB
[2022-11-12 13:32:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][600/1251]	eta 0:08:08 lr 0.000549	time 0.7412 (0.7508)	loss 3.7934 (3.1722)	grad_norm 1.4936 (1.5862)	mem 23876MB
[2022-11-12 13:32:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][650/1251]	eta 0:07:31 lr 0.000549	time 0.7362 (0.7504)	loss 2.8090 (3.1837)	grad_norm 1.5887 (1.5896)	mem 23876MB
[2022-11-12 13:33:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][700/1251]	eta 0:06:53 lr 0.000549	time 0.7352 (0.7500)	loss 3.6352 (3.1892)	grad_norm 2.1314 (1.5901)	mem 23876MB
[2022-11-12 13:33:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][750/1251]	eta 0:06:15 lr 0.000548	time 0.7397 (0.7500)	loss 2.4589 (3.1943)	grad_norm 1.7186 (1.5911)	mem 23876MB
[2022-11-12 13:34:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][800/1251]	eta 0:05:38 lr 0.000548	time 0.7409 (0.7497)	loss 3.2744 (3.1911)	grad_norm 1.5819 (1.5896)	mem 23876MB
[2022-11-12 13:35:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][850/1251]	eta 0:05:00 lr 0.000548	time 0.7341 (0.7496)	loss 2.6543 (3.2033)	grad_norm 1.5876 (1.5894)	mem 23876MB
[2022-11-12 13:35:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][900/1251]	eta 0:04:23 lr 0.000548	time 0.7381 (0.7495)	loss 2.9092 (3.2061)	grad_norm 1.4649 (1.5900)	mem 23876MB
[2022-11-12 13:36:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][950/1251]	eta 0:03:45 lr 0.000548	time 0.7380 (0.7492)	loss 2.7019 (3.2093)	grad_norm 1.5928 (1.5880)	mem 23876MB
[2022-11-12 13:37:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][1000/1251]	eta 0:03:08 lr 0.000547	time 0.7411 (0.7491)	loss 3.5761 (3.2164)	grad_norm 1.5667 (1.5908)	mem 23876MB
[2022-11-12 13:37:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][1050/1251]	eta 0:02:30 lr 0.000547	time 0.7351 (0.7491)	loss 3.5507 (3.2251)	grad_norm 1.5390 (1.5931)	mem 23876MB
[2022-11-12 13:38:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][1100/1251]	eta 0:01:53 lr 0.000547	time 0.7439 (0.7490)	loss 3.0476 (3.2276)	grad_norm 1.6710 (1.5910)	mem 23876MB
[2022-11-12 13:38:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][1150/1251]	eta 0:01:15 lr 0.000547	time 0.7402 (0.7490)	loss 2.5354 (3.2350)	grad_norm 1.7285 (1.5913)	mem 23876MB
[2022-11-12 13:39:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][1200/1251]	eta 0:00:38 lr 0.000547	time 0.7360 (0.7488)	loss 3.0526 (3.2334)	grad_norm 1.6074 (1.5914)	mem 23876MB
[2022-11-12 13:40:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [141/300][1250/1251]	eta 0:00:00 lr 0.000546	time 0.7256 (0.7487)	loss 3.7959 (3.2364)	grad_norm 1.4660 (1.5924)	mem 23876MB
[2022-11-12 13:40:09 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 141 training takes 0:15:36
[2022-11-12 13:40:10 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_141.pth saving......
[2022-11-12 13:40:11 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_141.pth saved !!!
[2022-11-12 13:40:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.607 (1.607)	Loss 0.8907 (0.8907)	Acc@1 79.980 (79.980)	Acc@5 95.117 (95.117)	Mem 23876MB
[2022-11-12 13:40:23 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.142 Acc@5 94.956
[2022-11-12 13:40:23 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.1%
[2022-11-12 13:40:25 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.884 (1.884)	Loss 0.7604 (0.7604)	Acc@1 80.664 (80.664)	Acc@5 97.168 (97.168)	Mem 23876MB
[2022-11-12 13:40:36 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.292 Acc@5 95.988
[2022-11-12 13:40:36 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.3%
[2022-11-12 13:40:36 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.31% at 140 epoch
[2022-11-12 13:40:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][0/1251]	eta 0:50:20 lr 0.000546	time 2.4146 (2.4146)	loss 2.2515 (2.2515)	grad_norm 1.4904 (1.4904)	mem 23876MB
[2022-11-12 13:41:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][50/1251]	eta 0:15:38 lr 0.000546	time 0.7397 (0.7816)	loss 3.2897 (3.2368)	grad_norm 1.4133 (1.6142)	mem 23876MB
[2022-11-12 13:41:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][100/1251]	eta 0:14:40 lr 0.000546	time 0.7410 (0.7653)	loss 3.5342 (3.2062)	grad_norm 1.6722 (1.6121)	mem 23876MB
[2022-11-12 13:42:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][150/1251]	eta 0:13:57 lr 0.000546	time 0.7463 (0.7605)	loss 2.9643 (3.1952)	grad_norm 1.5314 (1.6116)	mem 23876MB
[2022-11-12 13:43:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][200/1251]	eta 0:13:16 lr 0.000546	time 0.7424 (0.7581)	loss 3.6605 (3.1926)	grad_norm 1.2963 (1.6154)	mem 23876MB
[2022-11-12 13:43:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][250/1251]	eta 0:12:37 lr 0.000545	time 0.7394 (0.7563)	loss 3.6009 (3.2030)	grad_norm 1.6058 (1.6153)	mem 23876MB
[2022-11-12 13:44:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][300/1251]	eta 0:11:57 lr 0.000545	time 0.7419 (0.7543)	loss 3.3318 (3.2140)	grad_norm 1.6369 (1.6193)	mem 23876MB
[2022-11-12 13:45:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][350/1251]	eta 0:11:18 lr 0.000545	time 0.7364 (0.7535)	loss 2.5284 (3.2117)	grad_norm 1.5366 (1.6180)	mem 23876MB
[2022-11-12 13:45:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][400/1251]	eta 0:10:40 lr 0.000545	time 0.7353 (0.7528)	loss 3.5646 (3.2061)	grad_norm 1.6122 (1.6205)	mem 23876MB
[2022-11-12 13:46:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][450/1251]	eta 0:10:02 lr 0.000545	time 0.7358 (0.7522)	loss 2.9916 (3.2009)	grad_norm 1.7786 (1.6140)	mem 23876MB
[2022-11-12 13:46:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][500/1251]	eta 0:09:24 lr 0.000544	time 0.7524 (0.7518)	loss 3.0875 (3.2079)	grad_norm 2.2194 (1.6145)	mem 23876MB
[2022-11-12 13:47:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][550/1251]	eta 0:08:46 lr 0.000544	time 0.7407 (0.7512)	loss 3.4891 (3.2019)	grad_norm 1.7102 (1.6133)	mem 23876MB
[2022-11-12 13:48:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][600/1251]	eta 0:08:08 lr 0.000544	time 0.7342 (0.7509)	loss 3.6864 (3.2048)	grad_norm 1.5862 (1.6104)	mem 23876MB
[2022-11-12 13:48:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][650/1251]	eta 0:07:31 lr 0.000544	time 0.7369 (0.7508)	loss 3.1188 (3.2047)	grad_norm 1.6523 (1.6114)	mem 23876MB
[2022-11-12 13:49:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][700/1251]	eta 0:06:53 lr 0.000544	time 0.7434 (0.7504)	loss 2.8894 (3.2044)	grad_norm 1.5830 (1.6109)	mem 23876MB
[2022-11-12 13:49:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][750/1251]	eta 0:06:15 lr 0.000543	time 0.7370 (0.7504)	loss 2.7856 (3.2066)	grad_norm 1.4024 (1.6140)	mem 23876MB
[2022-11-12 13:50:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][800/1251]	eta 0:05:38 lr 0.000543	time 0.7345 (0.7500)	loss 3.5145 (3.2111)	grad_norm 1.5639 (1.6168)	mem 23876MB
[2022-11-12 13:51:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][850/1251]	eta 0:05:00 lr 0.000543	time 0.7332 (0.7498)	loss 3.4983 (3.2097)	grad_norm 1.5396 (1.6170)	mem 23876MB
[2022-11-12 13:51:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][900/1251]	eta 0:04:23 lr 0.000543	time 0.7417 (0.7497)	loss 2.9655 (3.2176)	grad_norm 1.5166 (1.6147)	mem 23876MB
[2022-11-12 13:52:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][950/1251]	eta 0:03:45 lr 0.000542	time 0.7435 (0.7495)	loss 3.8733 (3.2233)	grad_norm 1.6670 (1.6127)	mem 23876MB
[2022-11-12 13:53:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][1000/1251]	eta 0:03:08 lr 0.000542	time 0.7412 (0.7494)	loss 3.5720 (3.2262)	grad_norm 1.5485 (1.6137)	mem 23876MB
[2022-11-12 13:53:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][1050/1251]	eta 0:02:30 lr 0.000542	time 0.7392 (0.7493)	loss 2.6334 (3.2262)	grad_norm 1.9489 (1.6169)	mem 23876MB
[2022-11-12 13:54:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][1100/1251]	eta 0:01:53 lr 0.000542	time 0.7377 (0.7491)	loss 3.5372 (3.2260)	grad_norm 1.6804 (1.6170)	mem 23876MB
[2022-11-12 13:54:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][1150/1251]	eta 0:01:15 lr 0.000542	time 0.7367 (0.7491)	loss 3.1247 (3.2222)	grad_norm 1.4919 (1.6187)	mem 23876MB
[2022-11-12 13:55:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][1200/1251]	eta 0:00:38 lr 0.000541	time 0.7325 (0.7489)	loss 2.1534 (3.2237)	grad_norm 1.5413 (1.6167)	mem 23876MB
[2022-11-12 13:56:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [142/300][1250/1251]	eta 0:00:00 lr 0.000541	time 0.7312 (0.7487)	loss 3.8438 (3.2259)	grad_norm 1.3938 (1.6178)	mem 23876MB
[2022-11-12 13:56:13 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 142 training takes 0:15:36
[2022-11-12 13:56:13 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_142.pth saving......
[2022-11-12 13:56:14 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_142.pth saved !!!
[2022-11-12 13:56:15 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.714 (1.714)	Loss 0.9201 (0.9201)	Acc@1 77.539 (77.539)	Acc@5 94.238 (94.238)	Mem 23876MB
[2022-11-12 13:56:26 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.084 Acc@5 94.924
[2022-11-12 13:56:26 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.1%
[2022-11-12 13:56:28 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.858 (1.858)	Loss 0.8335 (0.8335)	Acc@1 81.543 (81.543)	Acc@5 95.312 (95.312)	Mem 23876MB
[2022-11-12 13:56:39 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.342 Acc@5 95.992
[2022-11-12 13:56:39 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.3%
[2022-11-12 13:56:39 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.34% at 142 epoch
[2022-11-12 13:56:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][0/1251]	eta 0:48:57 lr 0.000541	time 2.3479 (2.3479)	loss 3.0667 (3.0667)	grad_norm 1.6763 (1.6763)	mem 23876MB
[2022-11-12 13:57:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][50/1251]	eta 0:15:39 lr 0.000541	time 0.7453 (0.7820)	loss 3.2772 (3.2567)	grad_norm 1.6697 (1.6440)	mem 23876MB
[2022-11-12 13:57:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][100/1251]	eta 0:14:39 lr 0.000541	time 0.7368 (0.7637)	loss 3.2879 (3.2821)	grad_norm 1.5244 (1.6417)	mem 23876MB
[2022-11-12 13:58:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][150/1251]	eta 0:13:56 lr 0.000541	time 0.7373 (0.7595)	loss 3.3390 (3.2374)	grad_norm 1.4675 (1.6146)	mem 23876MB
[2022-11-12 13:59:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][200/1251]	eta 0:13:15 lr 0.000540	time 0.8196 (0.7566)	loss 2.9110 (3.2510)	grad_norm 1.4201 (1.6186)	mem 23876MB
[2022-11-12 13:59:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][250/1251]	eta 0:12:35 lr 0.000540	time 0.7400 (0.7545)	loss 3.6539 (3.2521)	grad_norm 1.4995 (1.6121)	mem 23876MB
[2022-11-12 14:00:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][300/1251]	eta 0:11:56 lr 0.000540	time 0.7387 (0.7534)	loss 3.1552 (3.2298)	grad_norm 1.6273 (1.6074)	mem 23876MB
[2022-11-12 14:01:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][350/1251]	eta 0:11:17 lr 0.000540	time 0.7410 (0.7522)	loss 2.2408 (3.2275)	grad_norm 1.7090 (1.6130)	mem 23876MB
[2022-11-12 14:01:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][400/1251]	eta 0:10:39 lr 0.000540	time 0.7362 (0.7519)	loss 3.7201 (3.2238)	grad_norm 1.5160 (1.6088)	mem 23876MB
[2022-11-12 14:02:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][450/1251]	eta 0:10:01 lr 0.000539	time 0.7384 (0.7513)	loss 3.7515 (3.2106)	grad_norm 1.4774 (1.6016)	mem 23876MB
[2022-11-12 14:02:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][500/1251]	eta 0:09:23 lr 0.000539	time 0.7369 (0.7508)	loss 3.3809 (3.2034)	grad_norm 1.6491 (1.6019)	mem 23876MB
[2022-11-12 14:03:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][550/1251]	eta 0:08:46 lr 0.000539	time 0.7387 (0.7505)	loss 3.2781 (3.2087)	grad_norm 1.5631 (1.6011)	mem 23876MB
[2022-11-12 14:04:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][600/1251]	eta 0:08:08 lr 0.000539	time 0.8282 (0.7504)	loss 3.3886 (3.2118)	grad_norm 1.8457 (1.6054)	mem 23876MB
[2022-11-12 14:04:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][650/1251]	eta 0:07:30 lr 0.000539	time 0.7374 (0.7503)	loss 3.5622 (3.2064)	grad_norm 1.6804 (1.6074)	mem 23876MB
[2022-11-12 14:05:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][700/1251]	eta 0:06:53 lr 0.000538	time 0.7397 (0.7500)	loss 3.8362 (3.2038)	grad_norm 1.5903 (1.6057)	mem 23876MB
[2022-11-12 14:06:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][750/1251]	eta 0:06:15 lr 0.000538	time 0.7439 (0.7499)	loss 2.6223 (3.1980)	grad_norm 1.5786 (1.6057)	mem 23876MB
[2022-11-12 14:06:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][800/1251]	eta 0:05:38 lr 0.000538	time 0.7402 (0.7496)	loss 3.7448 (3.1948)	grad_norm 1.5623 (nan)	mem 23876MB
[2022-11-12 14:07:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][850/1251]	eta 0:05:00 lr 0.000538	time 0.8182 (0.7495)	loss 3.0941 (3.2004)	grad_norm 1.5502 (nan)	mem 23876MB
[2022-11-12 14:07:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][900/1251]	eta 0:04:22 lr 0.000538	time 0.7454 (0.7492)	loss 3.6684 (3.1986)	grad_norm 1.5263 (nan)	mem 23876MB
[2022-11-12 14:08:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][950/1251]	eta 0:03:45 lr 0.000537	time 0.7405 (0.7494)	loss 2.3369 (3.1995)	grad_norm 1.4536 (nan)	mem 23876MB
[2022-11-12 14:09:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][1000/1251]	eta 0:03:08 lr 0.000537	time 0.8191 (0.7493)	loss 2.3544 (3.1941)	grad_norm 1.5168 (nan)	mem 23876MB
[2022-11-12 14:09:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][1050/1251]	eta 0:02:30 lr 0.000537	time 0.7371 (0.7491)	loss 3.0819 (3.1907)	grad_norm 1.4283 (nan)	mem 23876MB
[2022-11-12 14:10:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][1100/1251]	eta 0:01:53 lr 0.000537	time 0.7398 (0.7491)	loss 3.5850 (3.1930)	grad_norm 1.6335 (nan)	mem 23876MB
[2022-11-12 14:11:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][1150/1251]	eta 0:01:15 lr 0.000536	time 0.7344 (0.7490)	loss 2.5702 (3.1954)	grad_norm 1.5605 (nan)	mem 23876MB
[2022-11-12 14:11:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][1200/1251]	eta 0:00:38 lr 0.000536	time 0.7410 (0.7489)	loss 3.7358 (3.2004)	grad_norm 1.8087 (nan)	mem 23876MB
[2022-11-12 14:12:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [143/300][1250/1251]	eta 0:00:00 lr 0.000536	time 0.7260 (0.7488)	loss 2.8153 (3.2040)	grad_norm 1.7398 (nan)	mem 23876MB
[2022-11-12 14:12:16 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 143 training takes 0:15:36
[2022-11-12 14:12:16 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_143.pth saving......
[2022-11-12 14:12:17 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_143.pth saved !!!
[2022-11-12 14:12:19 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.715 (1.715)	Loss 0.9885 (0.9885)	Acc@1 75.488 (75.488)	Acc@5 94.238 (94.238)	Mem 23876MB
[2022-11-12 14:12:29 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.004 Acc@5 94.878
[2022-11-12 14:12:29 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.0%
[2022-11-12 14:12:31 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.862 (1.862)	Loss 0.6986 (0.6986)	Acc@1 83.691 (83.691)	Acc@5 96.680 (96.680)	Mem 23876MB
[2022-11-12 14:12:42 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.358 Acc@5 96.010
[2022-11-12 14:12:42 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.4%
[2022-11-12 14:12:42 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.36% at 143 epoch
[2022-11-12 14:12:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][0/1251]	eta 0:50:31 lr 0.000536	time 2.4234 (2.4234)	loss 3.2338 (3.2338)	grad_norm 1.6728 (1.6728)	mem 23876MB
[2022-11-12 14:13:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][50/1251]	eta 0:15:47 lr 0.000536	time 0.7427 (0.7892)	loss 3.1632 (3.0745)	grad_norm 1.5496 (1.6832)	mem 23876MB
[2022-11-12 14:13:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][100/1251]	eta 0:14:44 lr 0.000536	time 0.7429 (0.7683)	loss 3.7308 (3.1728)	grad_norm 1.5481 (1.6561)	mem 23876MB
[2022-11-12 14:14:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][150/1251]	eta 0:13:58 lr 0.000535	time 0.7386 (0.7618)	loss 3.6633 (3.1492)	grad_norm 1.7429 (1.6493)	mem 23876MB
[2022-11-12 14:15:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][200/1251]	eta 0:13:17 lr 0.000535	time 0.8144 (0.7586)	loss 3.2233 (3.1561)	grad_norm 1.7412 (1.6398)	mem 23876MB
[2022-11-12 14:15:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][250/1251]	eta 0:12:37 lr 0.000535	time 0.7383 (0.7566)	loss 3.6296 (3.1666)	grad_norm 1.4838 (1.6472)	mem 23876MB
[2022-11-12 14:16:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][300/1251]	eta 0:11:57 lr 0.000535	time 0.7353 (0.7549)	loss 3.5021 (3.1754)	grad_norm 1.5218 (1.6434)	mem 23876MB
[2022-11-12 14:17:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][350/1251]	eta 0:11:19 lr 0.000535	time 0.7455 (0.7542)	loss 3.2925 (3.1901)	grad_norm 1.7207 (1.6445)	mem 23876MB
[2022-11-12 14:17:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][400/1251]	eta 0:10:41 lr 0.000534	time 0.8115 (0.7533)	loss 3.7089 (3.1951)	grad_norm 1.4711 (1.6362)	mem 23876MB
[2022-11-12 14:18:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][450/1251]	eta 0:10:02 lr 0.000534	time 0.7363 (0.7526)	loss 3.9521 (3.2014)	grad_norm 1.7760 (1.6379)	mem 23876MB
[2022-11-12 14:18:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][500/1251]	eta 0:09:24 lr 0.000534	time 0.7347 (0.7519)	loss 2.6504 (3.1920)	grad_norm 1.7558 (1.6328)	mem 23876MB
[2022-11-12 14:19:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][550/1251]	eta 0:08:46 lr 0.000534	time 0.7220 (0.7517)	loss 3.3540 (3.1977)	grad_norm 1.9637 (1.6316)	mem 23876MB
[2022-11-12 14:20:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][600/1251]	eta 0:08:09 lr 0.000534	time 0.8034 (0.7514)	loss 2.8100 (3.2050)	grad_norm 1.8046 (1.6315)	mem 23876MB
[2022-11-12 14:20:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][650/1251]	eta 0:07:31 lr 0.000533	time 0.7451 (0.7510)	loss 3.7017 (3.2003)	grad_norm 1.7989 (1.6337)	mem 23876MB
[2022-11-12 14:21:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][700/1251]	eta 0:06:53 lr 0.000533	time 0.7390 (0.7507)	loss 3.2627 (3.1935)	grad_norm 1.5273 (1.6324)	mem 23876MB
[2022-11-12 14:22:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][750/1251]	eta 0:06:16 lr 0.000533	time 0.7347 (0.7507)	loss 1.9930 (3.1913)	grad_norm 1.4761 (1.6329)	mem 23876MB
[2022-11-12 14:22:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][800/1251]	eta 0:05:38 lr 0.000533	time 0.7449 (0.7505)	loss 3.8125 (3.2012)	grad_norm 1.5475 (1.6321)	mem 23876MB
[2022-11-12 14:23:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][850/1251]	eta 0:05:00 lr 0.000533	time 0.7359 (0.7506)	loss 2.3329 (3.1985)	grad_norm 1.4928 (1.6329)	mem 23876MB
[2022-11-12 14:23:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][900/1251]	eta 0:04:23 lr 0.000532	time 0.7438 (0.7503)	loss 3.6028 (3.2011)	grad_norm 1.7424 (1.6328)	mem 23876MB
[2022-11-12 14:24:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][950/1251]	eta 0:03:45 lr 0.000532	time 0.7325 (0.7502)	loss 2.3022 (3.2073)	grad_norm 1.5551 (1.6328)	mem 23876MB
[2022-11-12 14:25:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][1000/1251]	eta 0:03:08 lr 0.000532	time 0.8041 (0.7501)	loss 3.3147 (3.2011)	grad_norm 1.5434 (1.6335)	mem 23876MB
[2022-11-12 14:25:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][1050/1251]	eta 0:02:30 lr 0.000532	time 0.7461 (0.7499)	loss 3.4081 (3.2020)	grad_norm 1.5511 (1.6349)	mem 23876MB
[2022-11-12 14:26:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][1100/1251]	eta 0:01:53 lr 0.000532	time 0.7416 (0.7500)	loss 3.1329 (3.2047)	grad_norm 1.5452 (1.6352)	mem 23876MB
[2022-11-12 14:27:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][1150/1251]	eta 0:01:15 lr 0.000531	time 0.7370 (0.7498)	loss 3.3362 (3.2024)	grad_norm 1.3551 (1.6337)	mem 23876MB
[2022-11-12 14:27:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][1200/1251]	eta 0:00:38 lr 0.000531	time 0.8082 (0.7498)	loss 3.6162 (3.2047)	grad_norm 1.6057 (1.6343)	mem 23876MB
[2022-11-12 14:28:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [144/300][1250/1251]	eta 0:00:00 lr 0.000531	time 0.7281 (0.7494)	loss 2.8410 (3.2049)	grad_norm 1.9952 (1.6349)	mem 23876MB
[2022-11-12 14:28:20 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 144 training takes 0:15:37
[2022-11-12 14:28:20 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_144.pth saving......
[2022-11-12 14:28:21 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_144.pth saved !!!
[2022-11-12 14:28:23 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.680 (1.680)	Loss 0.9221 (0.9221)	Acc@1 77.832 (77.832)	Acc@5 95.020 (95.020)	Mem 23876MB
[2022-11-12 14:28:33 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.248 Acc@5 94.940
[2022-11-12 14:28:33 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.2%
[2022-11-12 14:28:35 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.821 (1.821)	Loss 0.7172 (0.7172)	Acc@1 82.520 (82.520)	Acc@5 97.363 (97.363)	Mem 23876MB
[2022-11-12 14:28:46 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.378 Acc@5 96.006
[2022-11-12 14:28:46 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.4%
[2022-11-12 14:28:46 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.38% at 144 epoch
[2022-11-12 14:28:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][0/1251]	eta 0:51:30 lr 0.000531	time 2.4702 (2.4702)	loss 2.4196 (2.4196)	grad_norm 1.7162 (1.7162)	mem 23876MB
[2022-11-12 14:29:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][50/1251]	eta 0:15:34 lr 0.000531	time 0.7335 (0.7783)	loss 3.1038 (3.1249)	grad_norm 1.6959 (1.6289)	mem 23876MB
[2022-11-12 14:30:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][100/1251]	eta 0:14:40 lr 0.000530	time 0.7351 (0.7648)	loss 2.6171 (3.1631)	grad_norm 1.7794 (1.6420)	mem 23876MB
[2022-11-12 14:30:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][150/1251]	eta 0:13:56 lr 0.000530	time 0.7254 (0.7594)	loss 3.0589 (3.2123)	grad_norm 1.7478 (1.6352)	mem 23876MB
[2022-11-12 14:31:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][200/1251]	eta 0:13:14 lr 0.000530	time 0.7368 (0.7564)	loss 2.6430 (3.2124)	grad_norm 1.7298 (1.6359)	mem 23876MB
[2022-11-12 14:31:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][250/1251]	eta 0:12:35 lr 0.000530	time 0.7493 (0.7548)	loss 2.3010 (3.2089)	grad_norm 1.5271 (1.6261)	mem 23876MB
[2022-11-12 14:32:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][300/1251]	eta 0:11:56 lr 0.000530	time 0.7430 (0.7533)	loss 3.7426 (3.2259)	grad_norm 1.3962 (nan)	mem 23876MB
[2022-11-12 14:33:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][350/1251]	eta 0:11:18 lr 0.000529	time 0.7377 (0.7525)	loss 3.4020 (3.2192)	grad_norm 1.4654 (nan)	mem 23876MB
[2022-11-12 14:33:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][400/1251]	eta 0:10:39 lr 0.000529	time 0.7396 (0.7520)	loss 3.4283 (3.2246)	grad_norm 1.5050 (nan)	mem 23876MB
[2022-11-12 14:34:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][450/1251]	eta 0:10:01 lr 0.000529	time 0.7393 (0.7512)	loss 2.3130 (3.2129)	grad_norm 1.6592 (nan)	mem 23876MB
[2022-11-12 14:35:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][500/1251]	eta 0:09:23 lr 0.000529	time 0.7409 (0.7508)	loss 2.9961 (3.2085)	grad_norm 1.6216 (nan)	mem 23876MB
[2022-11-12 14:35:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][550/1251]	eta 0:08:46 lr 0.000529	time 0.7443 (0.7507)	loss 3.4676 (3.2166)	grad_norm 1.6077 (nan)	mem 23876MB
[2022-11-12 14:36:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][600/1251]	eta 0:08:08 lr 0.000528	time 0.7388 (0.7501)	loss 3.6529 (3.2125)	grad_norm 1.5051 (nan)	mem 23876MB
[2022-11-12 14:36:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][650/1251]	eta 0:07:30 lr 0.000528	time 0.7403 (0.7501)	loss 3.8845 (3.2142)	grad_norm 1.6538 (nan)	mem 23876MB
[2022-11-12 14:37:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][700/1251]	eta 0:06:53 lr 0.000528	time 0.7375 (0.7496)	loss 3.4450 (3.2151)	grad_norm 1.6295 (nan)	mem 23876MB
[2022-11-12 14:38:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][750/1251]	eta 0:06:15 lr 0.000528	time 0.7407 (0.7493)	loss 3.6412 (3.2106)	grad_norm 1.9623 (nan)	mem 23876MB
[2022-11-12 14:38:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][800/1251]	eta 0:05:37 lr 0.000528	time 0.7395 (0.7493)	loss 2.5646 (3.2139)	grad_norm 1.5468 (nan)	mem 23876MB
[2022-11-12 14:39:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][850/1251]	eta 0:05:00 lr 0.000527	time 0.7457 (0.7490)	loss 3.1662 (3.2111)	grad_norm 1.4595 (nan)	mem 23876MB
[2022-11-12 14:40:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][900/1251]	eta 0:04:22 lr 0.000527	time 0.7370 (0.7489)	loss 3.4175 (3.2068)	grad_norm 1.6087 (nan)	mem 23876MB
[2022-11-12 14:40:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][950/1251]	eta 0:03:45 lr 0.000527	time 0.7393 (0.7489)	loss 3.5755 (3.2058)	grad_norm 1.6839 (nan)	mem 23876MB
[2022-11-12 14:41:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][1000/1251]	eta 0:03:07 lr 0.000527	time 0.7388 (0.7488)	loss 2.4975 (3.2051)	grad_norm 1.3790 (nan)	mem 23876MB
[2022-11-12 14:41:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][1050/1251]	eta 0:02:30 lr 0.000527	time 0.7361 (0.7489)	loss 3.4927 (3.2011)	grad_norm 1.5608 (nan)	mem 23876MB
[2022-11-12 14:42:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][1100/1251]	eta 0:01:53 lr 0.000526	time 0.7378 (0.7485)	loss 3.7550 (3.2027)	grad_norm 1.6584 (nan)	mem 23876MB
[2022-11-12 14:43:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][1150/1251]	eta 0:01:15 lr 0.000526	time 0.7369 (0.7487)	loss 3.4947 (3.2020)	grad_norm 1.5030 (nan)	mem 23876MB
[2022-11-12 14:43:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][1200/1251]	eta 0:00:38 lr 0.000526	time 0.7396 (0.7486)	loss 3.6482 (3.2071)	grad_norm 1.6211 (nan)	mem 23876MB
[2022-11-12 14:44:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [145/300][1250/1251]	eta 0:00:00 lr 0.000526	time 0.7309 (0.7482)	loss 3.4866 (3.2094)	grad_norm 1.5662 (nan)	mem 23876MB
[2022-11-12 14:44:22 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 145 training takes 0:15:36
[2022-11-12 14:44:22 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_145.pth saving......
[2022-11-12 14:44:23 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_145.pth saved !!!
[2022-11-12 14:44:25 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.726 (1.726)	Loss 0.8366 (0.8366)	Acc@1 79.688 (79.688)	Acc@5 95.703 (95.703)	Mem 23876MB
[2022-11-12 14:44:36 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 78.844 Acc@5 94.966
[2022-11-12 14:44:36 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 78.8%
[2022-11-12 14:44:38 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.964 (1.964)	Loss 0.7526 (0.7526)	Acc@1 82.520 (82.520)	Acc@5 96.191 (96.191)	Mem 23876MB
[2022-11-12 14:44:49 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.364 Acc@5 96.020
[2022-11-12 14:44:49 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.4%
[2022-11-12 14:44:49 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.38% at 144 epoch
[2022-11-12 14:44:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][0/1251]	eta 0:51:40 lr 0.000526	time 2.4782 (2.4782)	loss 3.7999 (3.7999)	grad_norm 1.7934 (1.7934)	mem 23876MB
[2022-11-12 14:45:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][50/1251]	eta 0:15:37 lr 0.000526	time 0.7454 (0.7809)	loss 3.3244 (3.1190)	grad_norm 1.6026 (1.5763)	mem 23876MB
[2022-11-12 14:46:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][100/1251]	eta 0:14:40 lr 0.000525	time 0.7420 (0.7647)	loss 2.6140 (3.1202)	grad_norm 1.7105 (1.6242)	mem 23876MB
[2022-11-12 14:46:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][150/1251]	eta 0:13:54 lr 0.000525	time 0.7375 (0.7579)	loss 2.8673 (3.1377)	grad_norm 1.7139 (1.6258)	mem 23876MB
[2022-11-12 14:47:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][200/1251]	eta 0:13:13 lr 0.000525	time 0.7391 (0.7552)	loss 1.9799 (3.1501)	grad_norm 1.5671 (1.6145)	mem 23876MB
[2022-11-12 14:47:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][250/1251]	eta 0:12:33 lr 0.000525	time 0.7430 (0.7527)	loss 3.2622 (3.1609)	grad_norm 1.7576 (1.6146)	mem 23876MB
[2022-11-12 14:48:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][300/1251]	eta 0:11:54 lr 0.000524	time 0.7351 (0.7518)	loss 2.6471 (3.1575)	grad_norm 1.4257 (1.6221)	mem 23876MB
[2022-11-12 14:49:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][350/1251]	eta 0:11:16 lr 0.000524	time 0.7371 (0.7510)	loss 1.9154 (3.1619)	grad_norm 1.5098 (1.6233)	mem 23876MB
[2022-11-12 14:49:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][400/1251]	eta 0:10:38 lr 0.000524	time 0.7391 (0.7501)	loss 3.9416 (3.1649)	grad_norm 1.7277 (1.6267)	mem 23876MB
[2022-11-12 14:50:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][450/1251]	eta 0:10:00 lr 0.000524	time 0.7385 (0.7494)	loss 2.8124 (3.1706)	grad_norm 1.5602 (1.6253)	mem 23876MB
[2022-11-12 14:51:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][500/1251]	eta 0:09:22 lr 0.000524	time 0.7386 (0.7491)	loss 2.2666 (3.1832)	grad_norm 1.5269 (1.6279)	mem 23876MB
[2022-11-12 14:51:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][550/1251]	eta 0:08:44 lr 0.000523	time 0.7373 (0.7488)	loss 2.7690 (3.1822)	grad_norm 1.9216 (1.6267)	mem 23876MB
[2022-11-12 14:52:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][600/1251]	eta 0:08:07 lr 0.000523	time 0.7388 (0.7485)	loss 3.6701 (3.1828)	grad_norm 1.9694 (1.6267)	mem 23876MB
[2022-11-12 14:52:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][650/1251]	eta 0:07:29 lr 0.000523	time 0.7491 (0.7484)	loss 3.3455 (3.1817)	grad_norm 1.6569 (1.6256)	mem 23876MB
[2022-11-12 14:53:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][700/1251]	eta 0:06:52 lr 0.000523	time 0.7376 (0.7482)	loss 2.1240 (3.1823)	grad_norm 1.8754 (1.6267)	mem 23876MB
[2022-11-12 14:54:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][750/1251]	eta 0:06:14 lr 0.000523	time 0.7344 (0.7481)	loss 3.5099 (3.1841)	grad_norm 1.4254 (1.6236)	mem 23876MB
[2022-11-12 14:54:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][800/1251]	eta 0:05:37 lr 0.000522	time 0.7445 (0.7480)	loss 3.4152 (3.1870)	grad_norm 1.7406 (1.6261)	mem 23876MB
[2022-11-12 14:55:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][850/1251]	eta 0:04:59 lr 0.000522	time 0.7371 (0.7480)	loss 2.9565 (3.1834)	grad_norm 1.7195 (1.6268)	mem 23876MB
[2022-11-12 14:56:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][900/1251]	eta 0:04:22 lr 0.000522	time 0.7371 (0.7479)	loss 2.7769 (3.1887)	grad_norm 1.4927 (1.6290)	mem 23876MB
[2022-11-12 14:56:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][950/1251]	eta 0:03:45 lr 0.000522	time 0.7375 (0.7479)	loss 2.1679 (3.1910)	grad_norm 1.5913 (1.6263)	mem 23876MB
[2022-11-12 14:57:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][1000/1251]	eta 0:03:07 lr 0.000522	time 0.7426 (0.7479)	loss 3.7801 (3.1901)	grad_norm 1.6139 (1.6259)	mem 23876MB
[2022-11-12 14:57:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][1050/1251]	eta 0:02:30 lr 0.000521	time 0.7408 (0.7480)	loss 3.5449 (3.1920)	grad_norm 2.1648 (1.6299)	mem 23876MB
[2022-11-12 14:58:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][1100/1251]	eta 0:01:52 lr 0.000521	time 0.7375 (0.7479)	loss 2.1178 (3.1919)	grad_norm 1.7457 (1.6312)	mem 23876MB
[2022-11-12 14:59:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][1150/1251]	eta 0:01:15 lr 0.000521	time 0.7440 (0.7479)	loss 3.1658 (3.1951)	grad_norm 1.5691 (nan)	mem 23876MB
[2022-11-12 14:59:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][1200/1251]	eta 0:00:38 lr 0.000521	time 0.7359 (0.7477)	loss 3.7474 (3.1957)	grad_norm 1.4855 (nan)	mem 23876MB
[2022-11-12 15:00:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [146/300][1250/1251]	eta 0:00:00 lr 0.000521	time 0.7294 (0.7477)	loss 3.3095 (3.1961)	grad_norm 1.5980 (nan)	mem 23876MB
[2022-11-12 15:00:24 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 146 training takes 0:15:35
[2022-11-12 15:00:24 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_146.pth saving......
[2022-11-12 15:00:25 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_146.pth saved !!!
[2022-11-12 15:00:27 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.692 (1.692)	Loss 0.8838 (0.8838)	Acc@1 80.664 (80.664)	Acc@5 94.629 (94.629)	Mem 23876MB
[2022-11-12 15:00:38 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.092 Acc@5 94.924
[2022-11-12 15:00:38 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.1%
[2022-11-12 15:00:40 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.883 (1.883)	Loss 0.8479 (0.8479)	Acc@1 78.906 (78.906)	Acc@5 95.605 (95.605)	Mem 23876MB
[2022-11-12 15:00:50 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.442 Acc@5 96.034
[2022-11-12 15:00:50 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.4%
[2022-11-12 15:00:50 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.44% at 146 epoch
[2022-11-12 15:00:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][0/1251]	eta 0:51:08 lr 0.000521	time 2.4527 (2.4527)	loss 3.2714 (3.2714)	grad_norm 1.9153 (1.9153)	mem 23876MB
[2022-11-12 15:01:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][50/1251]	eta 0:15:45 lr 0.000520	time 0.7424 (0.7871)	loss 2.5121 (3.1109)	grad_norm 1.5304 (1.5956)	mem 23876MB
[2022-11-12 15:02:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][100/1251]	eta 0:14:42 lr 0.000520	time 0.7371 (0.7671)	loss 3.3456 (3.1878)	grad_norm 1.6940 (1.5951)	mem 23876MB
[2022-11-12 15:02:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][150/1251]	eta 0:13:58 lr 0.000520	time 0.7403 (0.7615)	loss 3.2000 (3.1387)	grad_norm 1.5958 (1.5991)	mem 23876MB
[2022-11-12 15:03:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][200/1251]	eta 0:13:17 lr 0.000520	time 0.8142 (0.7583)	loss 2.9647 (3.1758)	grad_norm 1.6409 (1.6117)	mem 23876MB
[2022-11-12 15:04:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][250/1251]	eta 0:12:36 lr 0.000520	time 0.7379 (0.7557)	loss 2.9614 (3.1776)	grad_norm 1.6911 (1.6131)	mem 23876MB
[2022-11-12 15:04:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][300/1251]	eta 0:11:57 lr 0.000519	time 0.7509 (0.7543)	loss 3.0547 (3.1628)	grad_norm 1.7471 (1.6157)	mem 23876MB
[2022-11-12 15:05:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][350/1251]	eta 0:11:19 lr 0.000519	time 0.7404 (0.7538)	loss 2.8097 (3.1617)	grad_norm 1.5974 (1.6182)	mem 23876MB
[2022-11-12 15:05:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][400/1251]	eta 0:10:40 lr 0.000519	time 0.7395 (0.7527)	loss 1.9736 (3.1503)	grad_norm 1.4176 (1.6137)	mem 23876MB
[2022-11-12 15:06:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][450/1251]	eta 0:10:02 lr 0.000519	time 0.7424 (0.7527)	loss 3.5135 (3.1702)	grad_norm 1.6698 (1.6186)	mem 23876MB
[2022-11-12 15:07:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][500/1251]	eta 0:09:24 lr 0.000518	time 0.7413 (0.7518)	loss 3.3106 (3.1674)	grad_norm 1.7913 (1.6233)	mem 23876MB
[2022-11-12 15:07:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][550/1251]	eta 0:08:46 lr 0.000518	time 0.7431 (0.7514)	loss 3.4105 (3.1726)	grad_norm 1.9530 (1.6257)	mem 23876MB
[2022-11-12 15:08:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][600/1251]	eta 0:08:09 lr 0.000518	time 0.8196 (0.7513)	loss 2.1397 (3.1743)	grad_norm 1.4931 (1.6232)	mem 23876MB
[2022-11-12 15:08:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][650/1251]	eta 0:07:31 lr 0.000518	time 0.8114 (0.7506)	loss 2.4226 (3.1762)	grad_norm 1.6657 (1.6262)	mem 23876MB
[2022-11-12 15:09:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][700/1251]	eta 0:06:53 lr 0.000518	time 0.7463 (0.7503)	loss 3.7156 (3.1713)	grad_norm 1.4316 (1.6264)	mem 23876MB
[2022-11-12 15:10:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][750/1251]	eta 0:06:15 lr 0.000517	time 0.8328 (0.7500)	loss 3.0061 (3.1843)	grad_norm 1.5422 (1.6271)	mem 23876MB
[2022-11-12 15:10:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][800/1251]	eta 0:05:38 lr 0.000517	time 0.7354 (0.7498)	loss 3.5394 (3.1888)	grad_norm 1.6419 (inf)	mem 23876MB
[2022-11-12 15:11:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][850/1251]	eta 0:05:00 lr 0.000517	time 0.7388 (0.7496)	loss 3.4196 (3.1933)	grad_norm 1.5568 (inf)	mem 23876MB
[2022-11-12 15:12:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][900/1251]	eta 0:04:23 lr 0.000517	time 0.7413 (0.7494)	loss 3.5669 (3.1934)	grad_norm 1.5996 (inf)	mem 23876MB
[2022-11-12 15:12:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][950/1251]	eta 0:03:45 lr 0.000517	time 0.7437 (0.7491)	loss 2.2924 (3.1896)	grad_norm 1.4323 (inf)	mem 23876MB
[2022-11-12 15:13:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][1000/1251]	eta 0:03:08 lr 0.000516	time 0.8210 (0.7491)	loss 2.5840 (3.1868)	grad_norm 1.4159 (inf)	mem 23876MB
[2022-11-12 15:13:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][1050/1251]	eta 0:02:30 lr 0.000516	time 0.8149 (0.7489)	loss 3.3479 (3.1899)	grad_norm 1.7995 (inf)	mem 23876MB
[2022-11-12 15:14:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][1100/1251]	eta 0:01:53 lr 0.000516	time 0.7397 (0.7489)	loss 3.5288 (3.1888)	grad_norm 1.4927 (inf)	mem 23876MB
[2022-11-12 15:15:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][1150/1251]	eta 0:01:15 lr 0.000516	time 0.7400 (0.7488)	loss 3.4253 (3.1886)	grad_norm 1.5449 (inf)	mem 23876MB
[2022-11-12 15:15:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][1200/1251]	eta 0:00:38 lr 0.000516	time 0.7366 (0.7487)	loss 3.2656 (3.1916)	grad_norm 1.6016 (inf)	mem 23876MB
[2022-11-12 15:16:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [147/300][1250/1251]	eta 0:00:00 lr 0.000515	time 0.7248 (0.7484)	loss 3.5779 (3.1935)	grad_norm 1.6677 (inf)	mem 23876MB
[2022-11-12 15:16:27 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 147 training takes 0:15:36
[2022-11-12 15:16:27 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_147.pth saving......
[2022-11-12 15:16:28 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_147.pth saved !!!
[2022-11-12 15:16:30 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.678 (1.678)	Loss 0.8761 (0.8761)	Acc@1 80.078 (80.078)	Acc@5 94.727 (94.727)	Mem 23876MB
[2022-11-12 15:16:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.166 Acc@5 94.960
[2022-11-12 15:16:40 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.2%
[2022-11-12 15:16:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.906 (1.906)	Loss 0.7425 (0.7425)	Acc@1 81.738 (81.738)	Acc@5 96.289 (96.289)	Mem 23876MB
[2022-11-12 15:16:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.506 Acc@5 96.016
[2022-11-12 15:16:53 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.5%
[2022-11-12 15:16:53 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.51% at 147 epoch
[2022-11-12 15:16:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][0/1251]	eta 0:50:25 lr 0.000515	time 2.4188 (2.4188)	loss 3.5305 (3.5305)	grad_norm 1.5420 (1.5420)	mem 23876MB
[2022-11-12 15:17:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][50/1251]	eta 0:15:39 lr 0.000515	time 0.7366 (0.7823)	loss 2.9759 (3.2524)	grad_norm 1.5951 (1.6606)	mem 23876MB
[2022-11-12 15:18:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][100/1251]	eta 0:14:40 lr 0.000515	time 0.7382 (0.7650)	loss 2.2297 (3.2295)	grad_norm 1.7069 (1.6416)	mem 23876MB
[2022-11-12 15:18:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][150/1251]	eta 0:13:56 lr 0.000515	time 0.7439 (0.7594)	loss 2.0118 (3.2005)	grad_norm 1.6292 (1.6477)	mem 23876MB
[2022-11-12 15:19:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][200/1251]	eta 0:13:15 lr 0.000515	time 0.7424 (0.7566)	loss 3.6215 (3.1766)	grad_norm 1.6436 (1.6532)	mem 23876MB
[2022-11-12 15:20:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][250/1251]	eta 0:12:35 lr 0.000514	time 0.8131 (0.7550)	loss 3.5197 (3.1599)	grad_norm 1.6944 (1.6438)	mem 23876MB
[2022-11-12 15:20:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][300/1251]	eta 0:11:56 lr 0.000514	time 0.7438 (0.7537)	loss 3.5867 (3.1739)	grad_norm 1.5156 (1.6512)	mem 23876MB
[2022-11-12 15:21:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][350/1251]	eta 0:11:18 lr 0.000514	time 0.7432 (0.7532)	loss 3.5704 (3.1946)	grad_norm 1.4818 (1.6455)	mem 23876MB
[2022-11-12 15:21:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][400/1251]	eta 0:10:40 lr 0.000514	time 0.7465 (0.7529)	loss 3.4639 (3.1924)	grad_norm 1.6950 (1.6462)	mem 23876MB
[2022-11-12 15:22:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][450/1251]	eta 0:10:02 lr 0.000514	time 0.7378 (0.7521)	loss 3.4796 (3.1814)	grad_norm 2.1827 (1.6504)	mem 23876MB
[2022-11-12 15:23:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][500/1251]	eta 0:09:24 lr 0.000513	time 0.7405 (0.7516)	loss 3.0746 (3.1947)	grad_norm 1.6397 (1.6544)	mem 23876MB
[2022-11-12 15:23:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][550/1251]	eta 0:08:46 lr 0.000513	time 0.7373 (0.7512)	loss 2.2721 (3.1946)	grad_norm 1.7934 (1.6548)	mem 23876MB
[2022-11-12 15:24:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][600/1251]	eta 0:08:08 lr 0.000513	time 0.7393 (0.7510)	loss 3.5009 (3.2071)	grad_norm 1.6933 (1.6599)	mem 23876MB
[2022-11-12 15:25:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][650/1251]	eta 0:07:31 lr 0.000513	time 0.7509 (0.7507)	loss 3.9104 (3.2050)	grad_norm 1.7785 (1.6574)	mem 23876MB
[2022-11-12 15:25:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][700/1251]	eta 0:06:53 lr 0.000512	time 0.7469 (0.7504)	loss 3.3983 (3.2044)	grad_norm 1.6617 (1.6557)	mem 23876MB
[2022-11-12 15:26:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][750/1251]	eta 0:06:15 lr 0.000512	time 0.7371 (0.7502)	loss 3.6364 (3.2025)	grad_norm 1.5955 (1.6543)	mem 23876MB
[2022-11-12 15:26:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][800/1251]	eta 0:05:38 lr 0.000512	time 0.7397 (0.7502)	loss 3.7119 (3.1995)	grad_norm 1.4249 (1.6521)	mem 23876MB
[2022-11-12 15:27:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][850/1251]	eta 0:05:00 lr 0.000512	time 0.7371 (0.7499)	loss 3.4906 (3.2050)	grad_norm 1.5890 (1.6479)	mem 23876MB
[2022-11-12 15:28:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][900/1251]	eta 0:04:23 lr 0.000512	time 0.7419 (0.7499)	loss 2.9383 (3.2037)	grad_norm 1.5070 (1.6447)	mem 23876MB
[2022-11-12 15:28:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][950/1251]	eta 0:03:45 lr 0.000511	time 0.7437 (0.7498)	loss 3.0620 (3.2029)	grad_norm 1.8930 (1.6423)	mem 23876MB
[2022-11-12 15:29:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][1000/1251]	eta 0:03:08 lr 0.000511	time 0.7367 (0.7495)	loss 2.1799 (3.2081)	grad_norm 1.5485 (1.6432)	mem 23876MB
[2022-11-12 15:30:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][1050/1251]	eta 0:02:30 lr 0.000511	time 0.7507 (0.7495)	loss 3.3199 (3.2104)	grad_norm 1.5647 (1.6419)	mem 23876MB
[2022-11-12 15:30:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][1100/1251]	eta 0:01:53 lr 0.000511	time 0.7395 (0.7494)	loss 3.3329 (3.2093)	grad_norm 1.6337 (1.6417)	mem 23876MB
[2022-11-12 15:31:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][1150/1251]	eta 0:01:15 lr 0.000511	time 0.7375 (0.7493)	loss 2.7476 (3.2082)	grad_norm 1.6180 (1.6411)	mem 23876MB
[2022-11-12 15:31:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][1200/1251]	eta 0:00:38 lr 0.000510	time 0.7367 (0.7494)	loss 3.6569 (3.2123)	grad_norm 1.6561 (1.6420)	mem 23876MB
[2022-11-12 15:32:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [148/300][1250/1251]	eta 0:00:00 lr 0.000510	time 0.7258 (0.7490)	loss 3.7063 (3.2131)	grad_norm 1.6273 (1.6430)	mem 23876MB
[2022-11-12 15:32:30 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 148 training takes 0:15:37
[2022-11-12 15:32:30 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_148.pth saving......
[2022-11-12 15:32:31 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_148.pth saved !!!
[2022-11-12 15:32:33 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.660 (1.660)	Loss 0.8329 (0.8329)	Acc@1 79.883 (79.883)	Acc@5 95.020 (95.020)	Mem 23876MB
[2022-11-12 15:32:44 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.460 Acc@5 95.062
[2022-11-12 15:32:44 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.5%
[2022-11-12 15:32:46 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.828 (1.828)	Loss 0.7752 (0.7752)	Acc@1 81.641 (81.641)	Acc@5 95.410 (95.410)	Mem 23876MB
[2022-11-12 15:32:57 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.542 Acc@5 96.040
[2022-11-12 15:32:57 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.5%
[2022-11-12 15:32:57 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.54% at 148 epoch
[2022-11-12 15:32:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][0/1251]	eta 0:49:33 lr 0.000510	time 2.3767 (2.3767)	loss 3.4127 (3.4127)	grad_norm 1.9297 (1.9297)	mem 23876MB
[2022-11-12 15:33:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][50/1251]	eta 0:15:34 lr 0.000510	time 0.7378 (0.7784)	loss 3.4753 (3.1592)	grad_norm 1.5440 (1.6749)	mem 23876MB
[2022-11-12 15:34:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][100/1251]	eta 0:14:39 lr 0.000510	time 0.7356 (0.7639)	loss 3.4998 (3.2161)	grad_norm 1.5471 (1.6551)	mem 23876MB
[2022-11-12 15:34:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][150/1251]	eta 0:13:55 lr 0.000510	time 0.7422 (0.7585)	loss 2.8230 (3.2019)	grad_norm 1.5105 (1.6461)	mem 23876MB
[2022-11-12 15:35:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][200/1251]	eta 0:13:14 lr 0.000509	time 0.7359 (0.7562)	loss 3.1399 (3.2120)	grad_norm 1.8684 (1.6362)	mem 23876MB
[2022-11-12 15:36:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][250/1251]	eta 0:12:35 lr 0.000509	time 0.7377 (0.7547)	loss 3.6061 (3.2180)	grad_norm 1.7051 (1.6396)	mem 23876MB
[2022-11-12 15:36:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][300/1251]	eta 0:11:56 lr 0.000509	time 0.7395 (0.7533)	loss 2.6968 (3.2194)	grad_norm 1.4366 (1.6445)	mem 23876MB
[2022-11-12 15:37:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][350/1251]	eta 0:11:17 lr 0.000509	time 0.7342 (0.7524)	loss 3.4368 (3.2173)	grad_norm 1.5770 (1.6398)	mem 23876MB
[2022-11-12 15:37:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][400/1251]	eta 0:10:39 lr 0.000509	time 0.7366 (0.7517)	loss 3.7090 (3.2253)	grad_norm 1.6810 (1.6542)	mem 23876MB
[2022-11-12 15:38:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][450/1251]	eta 0:10:01 lr 0.000508	time 0.7337 (0.7512)	loss 2.4503 (3.2254)	grad_norm 1.5845 (1.6497)	mem 23876MB
[2022-11-12 15:39:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][500/1251]	eta 0:09:23 lr 0.000508	time 0.7379 (0.7509)	loss 3.6367 (3.2246)	grad_norm 1.7486 (1.6483)	mem 23876MB
[2022-11-12 15:39:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][550/1251]	eta 0:08:46 lr 0.000508	time 0.7381 (0.7506)	loss 3.6831 (3.2160)	grad_norm 1.9479 (1.6526)	mem 23876MB
[2022-11-12 15:40:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][600/1251]	eta 0:08:08 lr 0.000508	time 0.7516 (0.7505)	loss 3.0498 (3.2135)	grad_norm 1.8203 (1.6493)	mem 23876MB
[2022-11-12 15:41:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][650/1251]	eta 0:07:30 lr 0.000507	time 0.7518 (0.7500)	loss 1.9144 (3.2125)	grad_norm 1.6447 (1.6509)	mem 23876MB
[2022-11-12 15:41:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][700/1251]	eta 0:06:53 lr 0.000507	time 0.7429 (0.7499)	loss 2.2269 (3.2081)	grad_norm 1.5116 (1.6519)	mem 23876MB
[2022-11-12 15:42:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][750/1251]	eta 0:06:15 lr 0.000507	time 0.7454 (0.7498)	loss 3.5872 (3.2036)	grad_norm 1.9105 (1.6492)	mem 23876MB
[2022-11-12 15:42:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][800/1251]	eta 0:05:38 lr 0.000507	time 0.7365 (0.7498)	loss 2.9799 (3.1947)	grad_norm 1.4069 (1.6445)	mem 23876MB
[2022-11-12 15:43:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][850/1251]	eta 0:05:00 lr 0.000507	time 0.7279 (0.7497)	loss 2.1088 (3.1935)	grad_norm 2.0484 (1.6445)	mem 23876MB
[2022-11-12 15:44:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][900/1251]	eta 0:04:23 lr 0.000506	time 0.7364 (0.7496)	loss 3.7635 (3.1924)	grad_norm 1.4627 (1.6447)	mem 23876MB
[2022-11-12 15:44:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][950/1251]	eta 0:03:45 lr 0.000506	time 0.8095 (0.7496)	loss 3.0941 (3.1958)	grad_norm 1.6526 (1.6439)	mem 23876MB
[2022-11-12 15:45:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][1000/1251]	eta 0:03:08 lr 0.000506	time 0.7415 (0.7495)	loss 2.9724 (3.1975)	grad_norm 1.4874 (1.6412)	mem 23876MB
[2022-11-12 15:46:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][1050/1251]	eta 0:02:30 lr 0.000506	time 0.7386 (0.7495)	loss 3.6503 (3.1959)	grad_norm 1.5978 (1.6483)	mem 23876MB
[2022-11-12 15:46:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][1100/1251]	eta 0:01:53 lr 0.000506	time 0.7443 (0.7494)	loss 3.1078 (3.1960)	grad_norm 1.5121 (1.6498)	mem 23876MB
[2022-11-12 15:47:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][1150/1251]	eta 0:01:15 lr 0.000505	time 0.7395 (0.7494)	loss 3.6407 (3.1982)	grad_norm 1.5849 (nan)	mem 23876MB
[2022-11-12 15:47:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][1200/1251]	eta 0:00:38 lr 0.000505	time 0.7423 (0.7494)	loss 3.3686 (3.1951)	grad_norm 1.4364 (nan)	mem 23876MB
[2022-11-12 15:48:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [149/300][1250/1251]	eta 0:00:00 lr 0.000505	time 0.7266 (0.7491)	loss 3.4162 (3.1914)	grad_norm 1.6396 (nan)	mem 23876MB
[2022-11-12 15:48:34 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 149 training takes 0:15:37
[2022-11-12 15:48:34 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_149.pth saving......
[2022-11-12 15:48:35 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_149.pth saved !!!
[2022-11-12 15:48:37 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.716 (1.716)	Loss 0.8072 (0.8072)	Acc@1 80.176 (80.176)	Acc@5 95.898 (95.898)	Mem 23876MB
[2022-11-12 15:48:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.432 Acc@5 95.156
[2022-11-12 15:48:47 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.4%
[2022-11-12 15:48:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.872 (1.872)	Loss 0.7566 (0.7566)	Acc@1 82.617 (82.617)	Acc@5 95.898 (95.898)	Mem 23876MB
[2022-11-12 15:49:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.564 Acc@5 96.044
[2022-11-12 15:49:00 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.6%
[2022-11-12 15:49:00 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.56% at 149 epoch
[2022-11-12 15:49:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][0/1251]	eta 0:52:01 lr 0.000505	time 2.4952 (2.4952)	loss 3.8670 (3.8670)	grad_norm 1.6390 (1.6390)	mem 23876MB
[2022-11-12 15:49:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][50/1251]	eta 0:15:38 lr 0.000505	time 0.7369 (0.7816)	loss 3.9427 (3.2821)	grad_norm 1.6903 (1.7056)	mem 23876MB
[2022-11-12 15:50:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][100/1251]	eta 0:14:40 lr 0.000505	time 0.7445 (0.7649)	loss 2.1558 (3.2379)	grad_norm 1.7506 (1.7051)	mem 23876MB
[2022-11-12 15:50:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][150/1251]	eta 0:13:55 lr 0.000504	time 0.7391 (0.7592)	loss 3.3122 (3.1984)	grad_norm 1.6589 (1.7233)	mem 23876MB
[2022-11-12 15:51:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][200/1251]	eta 0:13:14 lr 0.000504	time 0.7374 (0.7559)	loss 2.8883 (3.2073)	grad_norm 1.5420 (1.7023)	mem 23876MB
[2022-11-12 15:52:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][250/1251]	eta 0:12:35 lr 0.000504	time 0.7443 (0.7545)	loss 3.0215 (3.1873)	grad_norm 1.4870 (1.6907)	mem 23876MB
[2022-11-12 15:52:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][300/1251]	eta 0:11:55 lr 0.000504	time 0.7412 (0.7528)	loss 3.4908 (3.1810)	grad_norm 1.5712 (1.6873)	mem 23876MB
[2022-11-12 15:53:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][350/1251]	eta 0:11:18 lr 0.000504	time 0.7416 (0.7525)	loss 3.5466 (3.1900)	grad_norm 1.4298 (1.6830)	mem 23876MB
[2022-11-12 15:54:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][400/1251]	eta 0:10:39 lr 0.000503	time 0.7352 (0.7514)	loss 2.6384 (3.1711)	grad_norm 1.5293 (1.6785)	mem 23876MB
[2022-11-12 15:54:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][450/1251]	eta 0:10:01 lr 0.000503	time 0.7497 (0.7513)	loss 3.0865 (3.1696)	grad_norm 1.7409 (1.6760)	mem 23876MB
[2022-11-12 15:55:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][500/1251]	eta 0:09:23 lr 0.000503	time 0.7434 (0.7505)	loss 3.6205 (3.1544)	grad_norm 1.6020 (1.6666)	mem 23876MB
[2022-11-12 15:55:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][550/1251]	eta 0:08:46 lr 0.000503	time 0.7370 (0.7504)	loss 3.1177 (3.1605)	grad_norm 1.5700 (1.6665)	mem 23876MB
[2022-11-12 15:56:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][600/1251]	eta 0:08:08 lr 0.000503	time 0.7387 (0.7501)	loss 2.1377 (3.1593)	grad_norm 1.5173 (1.6688)	mem 23876MB
[2022-11-12 15:57:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][650/1251]	eta 0:07:30 lr 0.000502	time 0.8203 (0.7500)	loss 3.4152 (3.1633)	grad_norm 1.5125 (1.6668)	mem 23876MB
[2022-11-12 15:57:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][700/1251]	eta 0:06:53 lr 0.000502	time 0.8166 (0.7496)	loss 2.7963 (3.1680)	grad_norm 1.6387 (1.6668)	mem 23876MB
[2022-11-12 15:58:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][750/1251]	eta 0:06:15 lr 0.000502	time 0.7380 (0.7496)	loss 3.0301 (3.1724)	grad_norm 1.7936 (1.6645)	mem 23876MB
[2022-11-12 15:59:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][800/1251]	eta 0:05:37 lr 0.000502	time 0.7546 (0.7493)	loss 2.7099 (3.1723)	grad_norm 1.5447 (1.6643)	mem 23876MB
[2022-11-12 15:59:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][850/1251]	eta 0:05:00 lr 0.000501	time 0.7422 (0.7493)	loss 3.0126 (3.1722)	grad_norm 1.9995 (1.6640)	mem 23876MB
[2022-11-12 16:00:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][900/1251]	eta 0:04:22 lr 0.000501	time 0.7435 (0.7489)	loss 3.5493 (3.1757)	grad_norm 1.4059 (1.6630)	mem 23876MB
[2022-11-12 16:00:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][950/1251]	eta 0:03:45 lr 0.000501	time 0.7387 (0.7489)	loss 2.6160 (3.1738)	grad_norm 1.7309 (1.6648)	mem 23876MB
[2022-11-12 16:01:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][1000/1251]	eta 0:03:07 lr 0.000501	time 0.7348 (0.7487)	loss 3.1656 (3.1718)	grad_norm 1.6941 (1.6621)	mem 23876MB
[2022-11-12 16:02:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][1050/1251]	eta 0:02:30 lr 0.000501	time 0.7417 (0.7486)	loss 2.4396 (3.1735)	grad_norm 1.7446 (1.6596)	mem 23876MB
[2022-11-12 16:02:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][1100/1251]	eta 0:01:53 lr 0.000500	time 0.7379 (0.7485)	loss 3.5811 (3.1787)	grad_norm 1.7619 (1.6630)	mem 23876MB
[2022-11-12 16:03:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][1150/1251]	eta 0:01:15 lr 0.000500	time 0.7380 (0.7484)	loss 2.4399 (3.1706)	grad_norm 1.7364 (1.6638)	mem 23876MB
[2022-11-12 16:03:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][1200/1251]	eta 0:00:38 lr 0.000500	time 0.7388 (0.7483)	loss 3.5348 (3.1750)	grad_norm 1.5738 (1.6636)	mem 23876MB
[2022-11-12 16:04:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [150/300][1250/1251]	eta 0:00:00 lr 0.000500	time 0.7321 (0.7481)	loss 3.5458 (3.1751)	grad_norm 1.6500 (1.6643)	mem 23876MB
[2022-11-12 16:04:36 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 150 training takes 0:15:36
[2022-11-12 16:04:36 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_150.pth saving......
[2022-11-12 16:04:37 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_150.pth saved !!!
[2022-11-12 16:04:39 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.717 (1.717)	Loss 0.9160 (0.9160)	Acc@1 78.125 (78.125)	Acc@5 94.238 (94.238)	Mem 23876MB
[2022-11-12 16:04:50 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.596 Acc@5 95.196
[2022-11-12 16:04:50 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.6%
[2022-11-12 16:04:52 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.796 (1.796)	Loss 0.7563 (0.7563)	Acc@1 83.691 (83.691)	Acc@5 96.484 (96.484)	Mem 23876MB
[2022-11-12 16:05:03 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.596 Acc@5 96.052
[2022-11-12 16:05:03 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.6%
[2022-11-12 16:05:03 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.60% at 150 epoch
[2022-11-12 16:05:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][0/1251]	eta 0:49:02 lr 0.000500	time 2.3525 (2.3525)	loss 3.4677 (3.4677)	grad_norm 1.5622 (1.5622)	mem 23876MB
[2022-11-12 16:05:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][50/1251]	eta 0:15:41 lr 0.000500	time 0.7231 (0.7836)	loss 3.3393 (3.1620)	grad_norm 1.7775 (1.6995)	mem 23876MB
[2022-11-12 16:06:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][100/1251]	eta 0:14:39 lr 0.000499	time 0.7406 (0.7637)	loss 2.9224 (3.1929)	grad_norm 1.9964 (1.6803)	mem 23876MB
[2022-11-12 16:06:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][150/1251]	eta 0:13:56 lr 0.000499	time 0.7397 (0.7601)	loss 2.9741 (3.2220)	grad_norm 1.5254 (1.6738)	mem 23876MB
[2022-11-12 16:07:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][200/1251]	eta 0:13:15 lr 0.000499	time 0.8084 (0.7569)	loss 2.4861 (3.1966)	grad_norm 1.8286 (1.6713)	mem 23876MB
[2022-11-12 16:08:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][250/1251]	eta 0:12:35 lr 0.000499	time 0.7361 (0.7551)	loss 3.6627 (3.2179)	grad_norm 1.6020 (1.6592)	mem 23876MB
[2022-11-12 16:08:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][300/1251]	eta 0:11:57 lr 0.000499	time 0.7381 (0.7543)	loss 3.3300 (3.1978)	grad_norm 1.8183 (1.6562)	mem 23876MB
[2022-11-12 16:09:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][350/1251]	eta 0:11:18 lr 0.000498	time 0.7379 (0.7530)	loss 3.3017 (3.2004)	grad_norm 1.4914 (1.6549)	mem 23876MB
[2022-11-12 16:10:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][400/1251]	eta 0:10:40 lr 0.000498	time 0.7371 (0.7524)	loss 2.3863 (3.2122)	grad_norm 1.5537 (1.6561)	mem 23876MB
[2022-11-12 16:10:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][450/1251]	eta 0:10:02 lr 0.000498	time 0.7330 (0.7522)	loss 3.3751 (3.2077)	grad_norm 1.4010 (1.6508)	mem 23876MB
[2022-11-12 16:11:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][500/1251]	eta 0:09:24 lr 0.000498	time 0.7394 (0.7516)	loss 3.4767 (3.2040)	grad_norm 1.5605 (1.6468)	mem 23876MB
[2022-11-12 16:11:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][550/1251]	eta 0:08:46 lr 0.000498	time 0.7426 (0.7515)	loss 3.6173 (3.2130)	grad_norm 1.6070 (1.6496)	mem 23876MB
[2022-11-12 16:12:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][600/1251]	eta 0:08:08 lr 0.000497	time 0.7444 (0.7511)	loss 2.7217 (3.2093)	grad_norm 1.5739 (1.6495)	mem 23876MB
[2022-11-12 16:13:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][650/1251]	eta 0:07:31 lr 0.000497	time 0.7420 (0.7507)	loss 3.5302 (3.2094)	grad_norm 1.4943 (1.6531)	mem 23876MB
[2022-11-12 16:13:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][700/1251]	eta 0:06:53 lr 0.000497	time 0.7384 (0.7505)	loss 2.6222 (3.2066)	grad_norm 1.4650 (1.6543)	mem 23876MB
[2022-11-12 16:14:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][750/1251]	eta 0:06:15 lr 0.000497	time 0.7380 (0.7502)	loss 3.4310 (3.2103)	grad_norm 1.6159 (1.6517)	mem 23876MB
[2022-11-12 16:15:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][800/1251]	eta 0:05:38 lr 0.000497	time 0.7398 (0.7500)	loss 3.2340 (3.2124)	grad_norm 1.5869 (1.6482)	mem 23876MB
[2022-11-12 16:15:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][850/1251]	eta 0:05:00 lr 0.000496	time 0.7381 (0.7498)	loss 3.0434 (3.2135)	grad_norm 1.5586 (1.6495)	mem 23876MB
[2022-11-12 16:16:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][900/1251]	eta 0:04:23 lr 0.000496	time 0.7402 (0.7495)	loss 3.4788 (3.2154)	grad_norm 1.6293 (1.6521)	mem 23876MB
[2022-11-12 16:16:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][950/1251]	eta 0:03:45 lr 0.000496	time 0.7520 (0.7494)	loss 3.2036 (3.2124)	grad_norm 1.6241 (1.6553)	mem 23876MB
[2022-11-12 16:17:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][1000/1251]	eta 0:03:08 lr 0.000496	time 0.7447 (0.7494)	loss 3.2540 (3.2110)	grad_norm 1.6217 (1.6531)	mem 23876MB
[2022-11-12 16:18:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][1050/1251]	eta 0:02:30 lr 0.000495	time 0.7376 (0.7490)	loss 3.0054 (3.2180)	grad_norm 1.5731 (1.6534)	mem 23876MB
[2022-11-12 16:18:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][1100/1251]	eta 0:01:53 lr 0.000495	time 0.7339 (0.7489)	loss 2.8504 (3.2182)	grad_norm 1.4684 (1.6539)	mem 23876MB
[2022-11-12 16:19:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][1150/1251]	eta 0:01:15 lr 0.000495	time 0.7389 (0.7488)	loss 3.0787 (3.2161)	grad_norm 1.6060 (1.6547)	mem 23876MB
[2022-11-12 16:20:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][1200/1251]	eta 0:00:38 lr 0.000495	time 0.7398 (0.7488)	loss 2.0625 (3.2199)	grad_norm 1.6517 (1.6549)	mem 23876MB
[2022-11-12 16:20:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [151/300][1250/1251]	eta 0:00:00 lr 0.000495	time 0.7231 (0.7487)	loss 3.4736 (3.2202)	grad_norm 1.7962 (1.6537)	mem 23876MB
[2022-11-12 16:20:39 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 151 training takes 0:15:36
[2022-11-12 16:20:39 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_151.pth saving......
[2022-11-12 16:20:41 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_151.pth saved !!!
[2022-11-12 16:20:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.673 (1.673)	Loss 0.9137 (0.9137)	Acc@1 78.906 (78.906)	Acc@5 94.922 (94.922)	Mem 23876MB
[2022-11-12 16:20:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.200 Acc@5 95.052
[2022-11-12 16:20:53 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.2%
[2022-11-12 16:20:55 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.839 (1.839)	Loss 0.8116 (0.8116)	Acc@1 81.250 (81.250)	Acc@5 95.312 (95.312)	Mem 23876MB
[2022-11-12 16:21:06 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.614 Acc@5 96.058
[2022-11-12 16:21:06 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.6%
[2022-11-12 16:21:06 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.61% at 151 epoch
[2022-11-12 16:21:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][0/1251]	eta 0:51:01 lr 0.000495	time 2.4473 (2.4473)	loss 3.5868 (3.5868)	grad_norm 1.8340 (1.8340)	mem 23876MB
[2022-11-12 16:21:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][50/1251]	eta 0:15:42 lr 0.000494	time 0.7362 (0.7847)	loss 2.8219 (3.1841)	grad_norm 1.7673 (1.6663)	mem 23876MB
[2022-11-12 16:22:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][100/1251]	eta 0:14:42 lr 0.000494	time 0.7355 (0.7667)	loss 3.7256 (3.1854)	grad_norm 1.8792 (1.6637)	mem 23876MB
[2022-11-12 16:23:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][150/1251]	eta 0:13:56 lr 0.000494	time 0.7349 (0.7598)	loss 3.3414 (3.1967)	grad_norm 1.6480 (1.6655)	mem 23876MB
[2022-11-12 16:23:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][200/1251]	eta 0:13:15 lr 0.000494	time 0.8046 (0.7566)	loss 2.1106 (3.2025)	grad_norm 1.6139 (1.6588)	mem 23876MB
[2022-11-12 16:24:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][250/1251]	eta 0:12:35 lr 0.000494	time 0.7391 (0.7543)	loss 3.5647 (3.1947)	grad_norm 1.4040 (1.6607)	mem 23876MB
[2022-11-12 16:24:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][300/1251]	eta 0:11:57 lr 0.000493	time 0.7417 (0.7540)	loss 2.5368 (3.2070)	grad_norm 1.7688 (1.6630)	mem 23876MB
[2022-11-12 16:25:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][350/1251]	eta 0:11:18 lr 0.000493	time 0.7399 (0.7528)	loss 2.6110 (3.2058)	grad_norm 1.6254 (1.6577)	mem 23876MB
[2022-11-12 16:26:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][400/1251]	eta 0:10:39 lr 0.000493	time 0.7375 (0.7520)	loss 3.2241 (3.1952)	grad_norm 1.7415 (1.6557)	mem 23876MB
[2022-11-12 16:26:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][450/1251]	eta 0:10:01 lr 0.000493	time 0.8350 (0.7511)	loss 3.2367 (3.1853)	grad_norm 1.6378 (1.6528)	mem 23876MB
[2022-11-12 16:27:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][500/1251]	eta 0:09:23 lr 0.000493	time 0.7411 (0.7509)	loss 3.6319 (3.1861)	grad_norm 1.6061 (1.6477)	mem 23876MB
[2022-11-12 16:27:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][550/1251]	eta 0:08:46 lr 0.000492	time 0.7352 (0.7504)	loss 3.0954 (3.1833)	grad_norm 1.5213 (1.6522)	mem 23876MB
[2022-11-12 16:28:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][600/1251]	eta 0:08:08 lr 0.000492	time 0.7339 (0.7502)	loss 3.2745 (3.1718)	grad_norm 1.4397 (1.6553)	mem 23876MB
[2022-11-12 16:29:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][650/1251]	eta 0:07:30 lr 0.000492	time 0.7366 (0.7499)	loss 2.7990 (3.1733)	grad_norm 1.5068 (1.6569)	mem 23876MB
[2022-11-12 16:29:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][700/1251]	eta 0:06:53 lr 0.000492	time 0.7412 (0.7496)	loss 2.5083 (3.1764)	grad_norm 1.5510 (1.6613)	mem 23876MB
[2022-11-12 16:30:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][750/1251]	eta 0:06:15 lr 0.000492	time 0.7415 (0.7494)	loss 3.0679 (3.1760)	grad_norm 1.8522 (1.6666)	mem 23876MB
[2022-11-12 16:31:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][800/1251]	eta 0:05:37 lr 0.000491	time 0.7532 (0.7492)	loss 3.7943 (3.1739)	grad_norm 1.6767 (1.6668)	mem 23876MB
[2022-11-12 16:31:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][850/1251]	eta 0:05:00 lr 0.000491	time 0.7331 (0.7491)	loss 2.8148 (3.1817)	grad_norm 1.4391 (1.6668)	mem 23876MB
[2022-11-12 16:32:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][900/1251]	eta 0:04:22 lr 0.000491	time 0.7366 (0.7491)	loss 3.0460 (3.1877)	grad_norm 1.5570 (1.6678)	mem 23876MB
[2022-11-12 16:32:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][950/1251]	eta 0:03:45 lr 0.000491	time 0.7369 (0.7490)	loss 3.0447 (3.1891)	grad_norm 1.5995 (1.6692)	mem 23876MB
[2022-11-12 16:33:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][1000/1251]	eta 0:03:07 lr 0.000490	time 0.7390 (0.7487)	loss 3.3159 (3.1912)	grad_norm 1.8280 (1.6706)	mem 23876MB
[2022-11-12 16:34:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][1050/1251]	eta 0:02:30 lr 0.000490	time 0.7357 (0.7486)	loss 3.1209 (3.1823)	grad_norm 1.5148 (1.6712)	mem 23876MB
[2022-11-12 16:34:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][1100/1251]	eta 0:01:53 lr 0.000490	time 0.7432 (0.7485)	loss 3.6414 (3.1857)	grad_norm 1.9007 (1.6719)	mem 23876MB
[2022-11-12 16:35:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][1150/1251]	eta 0:01:15 lr 0.000490	time 0.7390 (0.7485)	loss 3.2407 (3.1877)	grad_norm 1.7580 (1.6694)	mem 23876MB
[2022-11-12 16:36:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][1200/1251]	eta 0:00:38 lr 0.000490	time 0.7369 (0.7484)	loss 3.7379 (3.1925)	grad_norm 1.8639 (1.6704)	mem 23876MB
[2022-11-12 16:36:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [152/300][1250/1251]	eta 0:00:00 lr 0.000489	time 0.7294 (0.7482)	loss 2.7174 (3.1915)	grad_norm 1.5684 (1.6700)	mem 23876MB
[2022-11-12 16:36:42 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 152 training takes 0:15:36
[2022-11-12 16:36:42 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_152.pth saving......
[2022-11-12 16:36:43 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_152.pth saved !!!
[2022-11-12 16:36:45 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.761 (1.761)	Loss 0.7306 (0.7306)	Acc@1 81.934 (81.934)	Acc@5 96.777 (96.777)	Mem 23876MB
[2022-11-12 16:36:55 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.614 Acc@5 95.196
[2022-11-12 16:36:55 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.6%
[2022-11-12 16:36:57 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.818 (1.818)	Loss 0.7751 (0.7751)	Acc@1 80.664 (80.664)	Acc@5 96.289 (96.289)	Mem 23876MB
[2022-11-12 16:37:08 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.622 Acc@5 96.078
[2022-11-12 16:37:08 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.6%
[2022-11-12 16:37:08 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.62% at 152 epoch
[2022-11-12 16:37:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][0/1251]	eta 0:48:31 lr 0.000489	time 2.3271 (2.3271)	loss 2.9791 (2.9791)	grad_norm 1.6243 (1.6243)	mem 23876MB
[2022-11-12 16:37:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][50/1251]	eta 0:15:39 lr 0.000489	time 0.7363 (0.7819)	loss 3.6436 (2.9899)	grad_norm 1.5995 (nan)	mem 23876MB
[2022-11-12 16:38:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][100/1251]	eta 0:14:39 lr 0.000489	time 0.7316 (0.7638)	loss 2.4667 (3.0925)	grad_norm 1.6050 (nan)	mem 23876MB
[2022-11-12 16:39:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][150/1251]	eta 0:13:56 lr 0.000489	time 0.8116 (0.7600)	loss 3.4965 (3.1222)	grad_norm 1.7307 (nan)	mem 23876MB
[2022-11-12 16:39:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][200/1251]	eta 0:13:15 lr 0.000489	time 0.7416 (0.7571)	loss 2.8133 (3.1242)	grad_norm 1.7733 (nan)	mem 23876MB
[2022-11-12 16:40:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][250/1251]	eta 0:12:35 lr 0.000488	time 0.7371 (0.7545)	loss 2.7100 (3.1407)	grad_norm 1.4572 (nan)	mem 23876MB
[2022-11-12 16:40:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][300/1251]	eta 0:11:56 lr 0.000488	time 0.7378 (0.7539)	loss 3.3077 (3.1497)	grad_norm 1.6944 (nan)	mem 23876MB
[2022-11-12 16:41:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][350/1251]	eta 0:11:18 lr 0.000488	time 0.7442 (0.7529)	loss 3.1736 (3.1527)	grad_norm 1.6792 (nan)	mem 23876MB
[2022-11-12 16:42:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][400/1251]	eta 0:10:40 lr 0.000488	time 0.7402 (0.7523)	loss 2.9960 (3.1592)	grad_norm 1.4965 (nan)	mem 23876MB
[2022-11-12 16:42:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][450/1251]	eta 0:10:02 lr 0.000488	time 0.7436 (0.7518)	loss 3.4453 (3.1685)	grad_norm 1.7145 (nan)	mem 23876MB
[2022-11-12 16:43:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][500/1251]	eta 0:09:24 lr 0.000487	time 0.7435 (0.7511)	loss 3.0203 (3.1637)	grad_norm 1.3803 (nan)	mem 23876MB
[2022-11-12 16:44:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][550/1251]	eta 0:08:46 lr 0.000487	time 0.7533 (0.7508)	loss 3.5436 (3.1661)	grad_norm 1.7028 (nan)	mem 23876MB
[2022-11-12 16:44:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][600/1251]	eta 0:08:08 lr 0.000487	time 0.7353 (0.7505)	loss 2.3796 (3.1708)	grad_norm 1.4738 (nan)	mem 23876MB
[2022-11-12 16:45:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][650/1251]	eta 0:07:30 lr 0.000487	time 0.7397 (0.7501)	loss 3.1924 (3.1710)	grad_norm 2.1252 (nan)	mem 23876MB
[2022-11-12 16:45:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][700/1251]	eta 0:06:53 lr 0.000487	time 0.7421 (0.7499)	loss 2.9324 (3.1644)	grad_norm 1.9186 (nan)	mem 23876MB
[2022-11-12 16:46:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][750/1251]	eta 0:06:15 lr 0.000486	time 0.7527 (0.7496)	loss 3.3115 (3.1708)	grad_norm 1.5919 (nan)	mem 23876MB
[2022-11-12 16:47:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][800/1251]	eta 0:05:38 lr 0.000486	time 0.7448 (0.7496)	loss 3.3685 (3.1682)	grad_norm 1.6463 (nan)	mem 23876MB
[2022-11-12 16:47:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][850/1251]	eta 0:05:00 lr 0.000486	time 0.7384 (0.7492)	loss 3.0111 (3.1726)	grad_norm 1.8179 (nan)	mem 23876MB
[2022-11-12 16:48:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][900/1251]	eta 0:04:22 lr 0.000486	time 0.7390 (0.7491)	loss 3.3113 (3.1653)	grad_norm 1.6207 (nan)	mem 23876MB
[2022-11-12 16:49:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][950/1251]	eta 0:03:45 lr 0.000486	time 0.7387 (0.7489)	loss 3.5357 (3.1718)	grad_norm 1.6508 (nan)	mem 23876MB
[2022-11-12 16:49:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][1000/1251]	eta 0:03:07 lr 0.000485	time 0.7413 (0.7489)	loss 3.1716 (3.1729)	grad_norm 1.4701 (nan)	mem 23876MB
[2022-11-12 16:50:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][1050/1251]	eta 0:02:30 lr 0.000485	time 0.7437 (0.7487)	loss 3.3412 (3.1756)	grad_norm 1.7113 (nan)	mem 23876MB
[2022-11-12 16:50:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][1100/1251]	eta 0:01:53 lr 0.000485	time 0.8202 (0.7485)	loss 3.5041 (3.1742)	grad_norm 1.6554 (nan)	mem 23876MB
[2022-11-12 16:51:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][1150/1251]	eta 0:01:15 lr 0.000485	time 0.7365 (0.7483)	loss 3.9244 (3.1729)	grad_norm 1.5968 (nan)	mem 23876MB
[2022-11-12 16:52:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][1200/1251]	eta 0:00:38 lr 0.000484	time 0.7366 (0.7482)	loss 2.9897 (3.1675)	grad_norm 1.6959 (nan)	mem 23876MB
[2022-11-12 16:52:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [153/300][1250/1251]	eta 0:00:00 lr 0.000484	time 0.7282 (0.7480)	loss 3.5134 (3.1718)	grad_norm 1.6145 (nan)	mem 23876MB
[2022-11-12 16:52:44 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 153 training takes 0:15:35
[2022-11-12 16:52:44 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_153.pth saving......
[2022-11-12 16:52:45 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_153.pth saved !!!
[2022-11-12 16:52:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.587 (1.587)	Loss 0.8993 (0.8993)	Acc@1 77.930 (77.930)	Acc@5 94.434 (94.434)	Mem 23876MB
[2022-11-12 16:52:57 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.426 Acc@5 95.108
[2022-11-12 16:52:57 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.4%
[2022-11-12 16:52:59 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.810 (1.810)	Loss 0.8093 (0.8093)	Acc@1 80.664 (80.664)	Acc@5 95.312 (95.312)	Mem 23876MB
[2022-11-12 16:53:10 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.720 Acc@5 96.086
[2022-11-12 16:53:10 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.7%
[2022-11-12 16:53:10 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.72% at 153 epoch
[2022-11-12 16:53:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][0/1251]	eta 0:49:05 lr 0.000484	time 2.3545 (2.3545)	loss 2.9512 (2.9512)	grad_norm 2.0245 (2.0245)	mem 23876MB
[2022-11-12 16:53:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][50/1251]	eta 0:15:40 lr 0.000484	time 0.7374 (0.7833)	loss 3.4510 (3.0209)	grad_norm 1.7872 (1.6211)	mem 23876MB
[2022-11-12 16:54:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][100/1251]	eta 0:14:41 lr 0.000484	time 0.7359 (0.7657)	loss 3.9381 (3.1182)	grad_norm 1.7769 (1.6493)	mem 23876MB
[2022-11-12 16:55:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][150/1251]	eta 0:13:55 lr 0.000484	time 0.7387 (0.7589)	loss 3.5209 (3.1460)	grad_norm 1.5943 (1.6616)	mem 23876MB
[2022-11-12 16:55:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][200/1251]	eta 0:13:14 lr 0.000483	time 0.8025 (0.7563)	loss 3.3490 (3.1593)	grad_norm 1.7971 (1.6579)	mem 23876MB
[2022-11-12 16:56:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][250/1251]	eta 0:12:34 lr 0.000483	time 0.7370 (0.7542)	loss 3.3225 (3.1571)	grad_norm 1.3928 (1.6697)	mem 23876MB
[2022-11-12 16:56:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][300/1251]	eta 0:11:56 lr 0.000483	time 0.7413 (0.7534)	loss 2.8918 (3.1432)	grad_norm 1.6229 (1.6623)	mem 23876MB
[2022-11-12 16:57:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][350/1251]	eta 0:11:18 lr 0.000483	time 0.7399 (0.7527)	loss 2.8857 (3.1372)	grad_norm 1.5298 (1.6575)	mem 23876MB
[2022-11-12 16:58:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][400/1251]	eta 0:10:40 lr 0.000483	time 0.7366 (0.7522)	loss 3.5316 (3.1492)	grad_norm 2.0406 (1.6744)	mem 23876MB
[2022-11-12 16:58:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][450/1251]	eta 0:10:02 lr 0.000482	time 0.7363 (0.7516)	loss 3.6609 (3.1524)	grad_norm 1.6388 (1.6746)	mem 23876MB
[2022-11-12 16:59:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][500/1251]	eta 0:09:24 lr 0.000482	time 0.7526 (0.7511)	loss 3.1792 (3.1436)	grad_norm 1.7132 (1.6722)	mem 23876MB
[2022-11-12 17:00:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][550/1251]	eta 0:08:46 lr 0.000482	time 0.7387 (0.7505)	loss 3.3002 (3.1450)	grad_norm 1.7107 (1.6722)	mem 23876MB
[2022-11-12 17:00:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][600/1251]	eta 0:08:08 lr 0.000482	time 0.7427 (0.7503)	loss 2.3602 (3.1453)	grad_norm 1.5401 (1.6731)	mem 23876MB
[2022-11-12 17:01:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][650/1251]	eta 0:07:30 lr 0.000482	time 0.7390 (0.7502)	loss 2.9283 (3.1533)	grad_norm 1.8285 (1.6755)	mem 23876MB
[2022-11-12 17:01:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][700/1251]	eta 0:06:53 lr 0.000481	time 0.7436 (0.7500)	loss 3.7487 (3.1562)	grad_norm 1.7487 (1.6774)	mem 23876MB
[2022-11-12 17:02:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][750/1251]	eta 0:06:15 lr 0.000481	time 0.7357 (0.7497)	loss 3.7278 (3.1613)	grad_norm 1.9952 (1.6773)	mem 23876MB
[2022-11-12 17:03:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][800/1251]	eta 0:05:38 lr 0.000481	time 0.7396 (0.7496)	loss 3.8719 (3.1582)	grad_norm 1.9676 (1.6830)	mem 23876MB
[2022-11-12 17:03:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][850/1251]	eta 0:05:00 lr 0.000481	time 0.7373 (0.7494)	loss 3.7617 (3.1581)	grad_norm 1.5086 (1.6834)	mem 23876MB
[2022-11-12 17:04:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][900/1251]	eta 0:04:23 lr 0.000481	time 0.7354 (0.7494)	loss 2.7729 (3.1510)	grad_norm 1.4352 (1.6821)	mem 23876MB
[2022-11-12 17:05:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][950/1251]	eta 0:03:45 lr 0.000480	time 0.7401 (0.7492)	loss 3.5998 (3.1495)	grad_norm 1.5687 (1.6824)	mem 23876MB
[2022-11-12 17:05:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][1000/1251]	eta 0:03:08 lr 0.000480	time 0.7405 (0.7490)	loss 3.7781 (3.1476)	grad_norm 1.8012 (1.6805)	mem 23876MB
[2022-11-12 17:06:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][1050/1251]	eta 0:02:30 lr 0.000480	time 0.7462 (0.7490)	loss 3.5074 (3.1509)	grad_norm 1.8695 (1.6806)	mem 23876MB
[2022-11-12 17:06:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][1100/1251]	eta 0:01:53 lr 0.000480	time 0.7462 (0.7488)	loss 3.5781 (3.1533)	grad_norm 1.4847 (1.6809)	mem 23876MB
[2022-11-12 17:07:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][1150/1251]	eta 0:01:15 lr 0.000480	time 0.7359 (0.7489)	loss 2.5508 (3.1567)	grad_norm 1.6984 (1.6798)	mem 23876MB
[2022-11-12 17:08:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][1200/1251]	eta 0:00:38 lr 0.000479	time 0.7424 (0.7489)	loss 2.9814 (3.1586)	grad_norm 1.9820 (1.6786)	mem 23876MB
[2022-11-12 17:08:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [154/300][1250/1251]	eta 0:00:00 lr 0.000479	time 0.7264 (0.7485)	loss 2.3304 (3.1611)	grad_norm 1.7403 (1.6805)	mem 23876MB
[2022-11-12 17:08:46 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 154 training takes 0:15:36
[2022-11-12 17:08:47 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_154.pth saving......
[2022-11-12 17:08:48 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_154.pth saved !!!
[2022-11-12 17:08:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.679 (1.679)	Loss 0.9240 (0.9240)	Acc@1 79.297 (79.297)	Acc@5 94.629 (94.629)	Mem 23876MB
[2022-11-12 17:09:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.716 Acc@5 95.248
[2022-11-12 17:09:00 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.7%
[2022-11-12 17:09:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.809 (1.809)	Loss 0.7611 (0.7611)	Acc@1 82.324 (82.324)	Acc@5 95.508 (95.508)	Mem 23876MB
[2022-11-12 17:09:13 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.800 Acc@5 96.110
[2022-11-12 17:09:13 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.8%
[2022-11-12 17:09:13 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.80% at 154 epoch
[2022-11-12 17:09:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][0/1251]	eta 0:49:35 lr 0.000479	time 2.3789 (2.3789)	loss 3.3598 (3.3598)	grad_norm 1.3643 (1.3643)	mem 23876MB
[2022-11-12 17:09:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][50/1251]	eta 0:15:37 lr 0.000479	time 0.7386 (0.7810)	loss 3.8268 (3.2393)	grad_norm 1.5357 (1.6361)	mem 23876MB
[2022-11-12 17:10:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][100/1251]	eta 0:14:39 lr 0.000479	time 0.7447 (0.7645)	loss 3.5161 (3.1486)	grad_norm 1.4777 (1.6346)	mem 23876MB
[2022-11-12 17:11:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][150/1251]	eta 0:13:55 lr 0.000478	time 0.7348 (0.7591)	loss 3.1239 (3.1641)	grad_norm 1.8249 (1.6387)	mem 23876MB
[2022-11-12 17:11:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][200/1251]	eta 0:13:15 lr 0.000478	time 0.7333 (0.7566)	loss 3.0103 (3.1563)	grad_norm 1.7208 (1.6301)	mem 23876MB
[2022-11-12 17:12:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][250/1251]	eta 0:12:34 lr 0.000478	time 0.7368 (0.7537)	loss 2.5999 (3.1439)	grad_norm 1.8811 (1.6533)	mem 23876MB
[2022-11-12 17:12:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][300/1251]	eta 0:11:56 lr 0.000478	time 0.7402 (0.7531)	loss 2.8534 (3.1668)	grad_norm 1.5304 (1.6576)	mem 23876MB
[2022-11-12 17:13:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][350/1251]	eta 0:11:17 lr 0.000478	time 0.7362 (0.7521)	loss 3.0732 (3.1727)	grad_norm 1.5041 (1.6601)	mem 23876MB
[2022-11-12 17:14:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][400/1251]	eta 0:10:39 lr 0.000477	time 0.7367 (0.7514)	loss 3.1154 (3.1595)	grad_norm 1.9861 (1.6620)	mem 23876MB
[2022-11-12 17:14:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][450/1251]	eta 0:10:01 lr 0.000477	time 0.7357 (0.7510)	loss 3.5711 (3.1569)	grad_norm 1.4351 (1.6645)	mem 23876MB
[2022-11-12 17:15:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][500/1251]	eta 0:09:23 lr 0.000477	time 0.7412 (0.7505)	loss 2.5471 (3.1632)	grad_norm 1.6798 (1.6636)	mem 23876MB
[2022-11-12 17:16:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][550/1251]	eta 0:08:45 lr 0.000477	time 0.7373 (0.7501)	loss 2.6598 (3.1547)	grad_norm 1.3322 (1.6608)	mem 23876MB
[2022-11-12 17:16:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][600/1251]	eta 0:08:08 lr 0.000477	time 0.7522 (0.7499)	loss 3.2756 (3.1556)	grad_norm 1.6342 (1.6602)	mem 23876MB
[2022-11-12 17:17:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][650/1251]	eta 0:07:30 lr 0.000476	time 0.7419 (0.7495)	loss 2.8854 (3.1546)	grad_norm 1.8021 (1.6613)	mem 23876MB
[2022-11-12 17:17:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][700/1251]	eta 0:06:52 lr 0.000476	time 0.7358 (0.7494)	loss 2.6676 (3.1579)	grad_norm 1.5206 (1.6629)	mem 23876MB
[2022-11-12 17:18:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][750/1251]	eta 0:06:15 lr 0.000476	time 0.7356 (0.7492)	loss 2.2422 (3.1661)	grad_norm 2.0624 (1.6645)	mem 23876MB
[2022-11-12 17:19:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][800/1251]	eta 0:05:37 lr 0.000476	time 0.7394 (0.7491)	loss 3.0895 (3.1667)	grad_norm 1.6459 (1.6664)	mem 23876MB
[2022-11-12 17:19:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][850/1251]	eta 0:05:00 lr 0.000476	time 0.7368 (0.7488)	loss 3.2013 (3.1689)	grad_norm 2.1874 (1.6673)	mem 23876MB
[2022-11-12 17:20:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][900/1251]	eta 0:04:22 lr 0.000475	time 0.7425 (0.7487)	loss 1.9569 (3.1712)	grad_norm 1.4063 (inf)	mem 23876MB
[2022-11-12 17:21:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][950/1251]	eta 0:03:45 lr 0.000475	time 0.8160 (0.7485)	loss 3.0682 (3.1692)	grad_norm 1.5228 (inf)	mem 23876MB
[2022-11-12 17:21:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][1000/1251]	eta 0:03:07 lr 0.000475	time 0.7370 (0.7485)	loss 3.4916 (3.1716)	grad_norm 1.6863 (inf)	mem 23876MB
[2022-11-12 17:22:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][1050/1251]	eta 0:02:30 lr 0.000475	time 0.7364 (0.7484)	loss 3.7809 (3.1762)	grad_norm 1.7633 (inf)	mem 23876MB
[2022-11-12 17:22:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][1100/1251]	eta 0:01:53 lr 0.000475	time 0.8293 (0.7484)	loss 2.7601 (3.1725)	grad_norm 1.6690 (inf)	mem 23876MB
[2022-11-12 17:23:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][1150/1251]	eta 0:01:15 lr 0.000474	time 0.7424 (0.7484)	loss 3.7822 (3.1716)	grad_norm 1.5534 (inf)	mem 23876MB
[2022-11-12 17:24:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][1200/1251]	eta 0:00:38 lr 0.000474	time 0.7502 (0.7482)	loss 2.4889 (3.1722)	grad_norm 2.0385 (inf)	mem 23876MB
[2022-11-12 17:24:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [155/300][1250/1251]	eta 0:00:00 lr 0.000474	time 0.7268 (0.7480)	loss 3.4443 (3.1672)	grad_norm 1.5359 (inf)	mem 23876MB
[2022-11-12 17:24:49 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 155 training takes 0:15:35
[2022-11-12 17:24:49 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_155.pth saving......
[2022-11-12 17:24:50 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_155.pth saved !!!
[2022-11-12 17:24:52 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.646 (1.646)	Loss 0.8781 (0.8781)	Acc@1 80.664 (80.664)	Acc@5 95.117 (95.117)	Mem 23876MB
[2022-11-12 17:25:02 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.760 Acc@5 95.126
[2022-11-12 17:25:02 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.8%
[2022-11-12 17:25:04 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.934 (1.934)	Loss 0.8096 (0.8096)	Acc@1 80.957 (80.957)	Acc@5 94.922 (94.922)	Mem 23876MB
[2022-11-12 17:25:15 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.840 Acc@5 96.104
[2022-11-12 17:25:15 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.8%
[2022-11-12 17:25:15 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.84% at 155 epoch
[2022-11-12 17:25:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][0/1251]	eta 0:49:55 lr 0.000474	time 2.3942 (2.3942)	loss 3.4561 (3.4561)	grad_norm 1.5834 (1.5834)	mem 23876MB
[2022-11-12 17:25:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][50/1251]	eta 0:15:40 lr 0.000474	time 0.7377 (0.7831)	loss 3.5085 (3.2540)	grad_norm 1.4998 (1.5940)	mem 23876MB
[2022-11-12 17:26:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][100/1251]	eta 0:14:41 lr 0.000474	time 0.7370 (0.7661)	loss 3.4773 (3.2083)	grad_norm 1.6959 (1.6355)	mem 23876MB
[2022-11-12 17:27:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][150/1251]	eta 0:13:56 lr 0.000473	time 0.7385 (0.7595)	loss 3.0128 (3.2059)	grad_norm 1.5217 (1.6435)	mem 23876MB
[2022-11-12 17:27:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][200/1251]	eta 0:13:15 lr 0.000473	time 0.8054 (0.7569)	loss 3.3152 (3.1795)	grad_norm 2.0432 (1.6495)	mem 23876MB
[2022-11-12 17:28:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][250/1251]	eta 0:12:35 lr 0.000473	time 0.7382 (0.7545)	loss 3.1299 (3.1676)	grad_norm 1.5041 (1.6510)	mem 23876MB
[2022-11-12 17:29:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][300/1251]	eta 0:11:56 lr 0.000473	time 0.7401 (0.7536)	loss 3.3377 (3.1702)	grad_norm 1.7591 (1.6531)	mem 23876MB
[2022-11-12 17:29:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][350/1251]	eta 0:11:17 lr 0.000472	time 0.7433 (0.7525)	loss 3.1436 (3.1640)	grad_norm 2.1280 (1.6748)	mem 23876MB
[2022-11-12 17:30:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][400/1251]	eta 0:10:39 lr 0.000472	time 0.7391 (0.7516)	loss 3.3929 (3.1554)	grad_norm 1.6056 (1.6771)	mem 23876MB
[2022-11-12 17:30:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][450/1251]	eta 0:10:01 lr 0.000472	time 0.7359 (0.7513)	loss 3.4590 (3.1776)	grad_norm 1.6627 (1.6736)	mem 23876MB
[2022-11-12 17:31:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][500/1251]	eta 0:09:23 lr 0.000472	time 0.7413 (0.7505)	loss 3.1935 (3.1628)	grad_norm 1.7199 (1.6741)	mem 23876MB
[2022-11-12 17:32:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][550/1251]	eta 0:08:45 lr 0.000472	time 0.8166 (0.7500)	loss 3.6240 (3.1619)	grad_norm 1.6585 (1.6756)	mem 23876MB
[2022-11-12 17:32:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][600/1251]	eta 0:08:07 lr 0.000471	time 0.8299 (0.7496)	loss 3.1581 (3.1606)	grad_norm 1.4724 (1.6777)	mem 23876MB
[2022-11-12 17:33:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][650/1251]	eta 0:07:30 lr 0.000471	time 0.7361 (0.7493)	loss 3.4859 (3.1555)	grad_norm 1.9252 (1.6779)	mem 23876MB
[2022-11-12 17:34:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][700/1251]	eta 0:06:52 lr 0.000471	time 0.7411 (0.7491)	loss 3.4227 (3.1584)	grad_norm 1.8596 (1.6849)	mem 23876MB
[2022-11-12 17:34:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][750/1251]	eta 0:06:15 lr 0.000471	time 0.7353 (0.7489)	loss 2.5275 (3.1600)	grad_norm 1.6635 (1.6847)	mem 23876MB
[2022-11-12 17:35:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][800/1251]	eta 0:05:37 lr 0.000471	time 0.7487 (0.7485)	loss 3.5727 (3.1596)	grad_norm 1.7502 (1.6847)	mem 23876MB
[2022-11-12 17:35:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][850/1251]	eta 0:05:00 lr 0.000470	time 0.7419 (0.7486)	loss 3.7903 (3.1663)	grad_norm 2.1554 (1.6845)	mem 23876MB
[2022-11-12 17:36:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][900/1251]	eta 0:04:22 lr 0.000470	time 0.7391 (0.7482)	loss 3.8000 (3.1703)	grad_norm 1.5919 (1.6860)	mem 23876MB
[2022-11-12 17:37:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][950/1251]	eta 0:03:45 lr 0.000470	time 0.7473 (0.7482)	loss 2.7085 (3.1776)	grad_norm 1.4562 (1.6856)	mem 23876MB
[2022-11-12 17:37:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][1000/1251]	eta 0:03:07 lr 0.000470	time 0.8069 (0.7481)	loss 3.5016 (3.1725)	grad_norm 1.8645 (1.6901)	mem 23876MB
[2022-11-12 17:38:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][1050/1251]	eta 0:02:30 lr 0.000470	time 0.7456 (0.7480)	loss 3.5023 (3.1744)	grad_norm 1.5575 (1.6922)	mem 23876MB
[2022-11-12 17:38:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][1100/1251]	eta 0:01:52 lr 0.000469	time 0.7404 (0.7479)	loss 3.0455 (3.1739)	grad_norm 1.8887 (1.6949)	mem 23876MB
[2022-11-12 17:39:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][1150/1251]	eta 0:01:15 lr 0.000469	time 0.7490 (0.7479)	loss 2.3858 (3.1696)	grad_norm 1.7647 (1.6952)	mem 23876MB
[2022-11-12 17:40:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][1200/1251]	eta 0:00:38 lr 0.000469	time 0.7420 (0.7478)	loss 2.3176 (3.1711)	grad_norm 1.5028 (1.6969)	mem 23876MB
[2022-11-12 17:40:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [156/300][1250/1251]	eta 0:00:00 lr 0.000469	time 0.7283 (0.7478)	loss 2.8678 (3.1760)	grad_norm 1.5283 (1.6958)	mem 23876MB
[2022-11-12 17:40:50 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 156 training takes 0:15:35
[2022-11-12 17:40:51 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_156.pth saving......
[2022-11-12 17:40:52 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_156.pth saved !!!
[2022-11-12 17:40:53 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.606 (1.606)	Loss 0.8466 (0.8466)	Acc@1 79.395 (79.395)	Acc@5 94.922 (94.922)	Mem 23876MB
[2022-11-12 17:41:04 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.688 Acc@5 95.236
[2022-11-12 17:41:04 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.7%
[2022-11-12 17:41:06 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.911 (1.911)	Loss 0.8270 (0.8270)	Acc@1 79.785 (79.785)	Acc@5 94.727 (94.727)	Mem 23876MB
[2022-11-12 17:41:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.856 Acc@5 96.098
[2022-11-12 17:41:17 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.9%
[2022-11-12 17:41:17 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.86% at 156 epoch
[2022-11-12 17:41:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][0/1251]	eta 0:49:33 lr 0.000469	time 2.3768 (2.3768)	loss 2.2085 (2.2085)	grad_norm 1.9495 (1.9495)	mem 23876MB
[2022-11-12 17:41:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][50/1251]	eta 0:15:39 lr 0.000469	time 0.7341 (0.7825)	loss 3.3582 (3.2033)	grad_norm 1.6837 (1.7115)	mem 23876MB
[2022-11-12 17:42:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][100/1251]	eta 0:14:40 lr 0.000468	time 0.7370 (0.7652)	loss 3.1522 (3.1798)	grad_norm 1.7177 (1.7009)	mem 23876MB
[2022-11-12 17:43:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][150/1251]	eta 0:13:57 lr 0.000468	time 0.7405 (0.7603)	loss 2.6857 (3.1582)	grad_norm 1.6816 (1.6818)	mem 23876MB
[2022-11-12 17:43:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][200/1251]	eta 0:13:16 lr 0.000468	time 0.7390 (0.7574)	loss 3.8501 (3.1683)	grad_norm 1.7179 (1.7179)	mem 23876MB
[2022-11-12 17:44:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][250/1251]	eta 0:12:36 lr 0.000468	time 0.7374 (0.7555)	loss 3.7160 (3.1981)	grad_norm 1.6908 (1.7169)	mem 23876MB
[2022-11-12 17:45:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][300/1251]	eta 0:11:57 lr 0.000468	time 0.7382 (0.7543)	loss 2.3558 (3.1801)	grad_norm 1.5423 (1.7105)	mem 23876MB
[2022-11-12 17:45:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][350/1251]	eta 0:11:18 lr 0.000467	time 0.7463 (0.7533)	loss 3.1915 (3.1729)	grad_norm 1.6851 (1.7112)	mem 23876MB
[2022-11-12 17:46:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][400/1251]	eta 0:10:40 lr 0.000467	time 0.7439 (0.7528)	loss 3.3913 (3.1810)	grad_norm 1.6828 (1.7136)	mem 23876MB
[2022-11-12 17:46:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][450/1251]	eta 0:10:02 lr 0.000467	time 0.7380 (0.7520)	loss 2.4324 (3.1619)	grad_norm 1.9186 (1.7122)	mem 23876MB
[2022-11-12 17:47:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][500/1251]	eta 0:09:24 lr 0.000467	time 0.7386 (0.7512)	loss 3.2988 (3.1678)	grad_norm 1.8816 (1.7152)	mem 23876MB
[2022-11-12 17:48:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][550/1251]	eta 0:08:46 lr 0.000466	time 0.8223 (0.7510)	loss 3.1971 (3.1656)	grad_norm 1.5214 (1.7120)	mem 23876MB
[2022-11-12 17:48:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][600/1251]	eta 0:08:08 lr 0.000466	time 0.7375 (0.7504)	loss 3.3251 (3.1675)	grad_norm 1.4329 (1.7077)	mem 23876MB
[2022-11-12 17:49:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][650/1251]	eta 0:07:30 lr 0.000466	time 0.7453 (0.7501)	loss 3.3107 (3.1679)	grad_norm 1.9370 (1.7076)	mem 23876MB
[2022-11-12 17:50:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][700/1251]	eta 0:06:53 lr 0.000466	time 0.7422 (0.7501)	loss 3.2280 (3.1709)	grad_norm 1.4901 (1.7048)	mem 23876MB
[2022-11-12 17:50:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][750/1251]	eta 0:06:15 lr 0.000466	time 0.7347 (0.7497)	loss 3.5805 (3.1664)	grad_norm 1.5281 (1.7026)	mem 23876MB
[2022-11-12 17:51:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][800/1251]	eta 0:05:38 lr 0.000465	time 0.7464 (0.7497)	loss 2.2063 (3.1629)	grad_norm 1.6557 (1.7012)	mem 23876MB
[2022-11-12 17:51:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][850/1251]	eta 0:05:00 lr 0.000465	time 0.7357 (0.7495)	loss 3.4623 (3.1663)	grad_norm 1.6579 (1.7015)	mem 23876MB
[2022-11-12 17:52:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][900/1251]	eta 0:04:22 lr 0.000465	time 0.7347 (0.7491)	loss 3.4377 (3.1688)	grad_norm 1.6019 (1.7006)	mem 23876MB
[2022-11-12 17:53:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][950/1251]	eta 0:03:45 lr 0.000465	time 0.7475 (0.7492)	loss 3.4433 (3.1695)	grad_norm 1.4504 (1.6997)	mem 23876MB
[2022-11-12 17:53:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][1000/1251]	eta 0:03:07 lr 0.000465	time 0.7350 (0.7490)	loss 3.6009 (3.1645)	grad_norm 1.5972 (nan)	mem 23876MB
[2022-11-12 17:54:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][1050/1251]	eta 0:02:30 lr 0.000464	time 0.7414 (0.7489)	loss 3.2478 (3.1650)	grad_norm 2.5757 (nan)	mem 23876MB
[2022-11-12 17:55:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][1100/1251]	eta 0:01:53 lr 0.000464	time 0.7424 (0.7489)	loss 3.8360 (3.1606)	grad_norm 1.9441 (nan)	mem 23876MB
[2022-11-12 17:55:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][1150/1251]	eta 0:01:15 lr 0.000464	time 0.7558 (0.7487)	loss 2.4516 (3.1602)	grad_norm 1.8121 (nan)	mem 23876MB
[2022-11-12 17:56:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][1200/1251]	eta 0:00:38 lr 0.000464	time 0.7419 (0.7487)	loss 3.2191 (3.1600)	grad_norm 1.4817 (nan)	mem 23876MB
[2022-11-12 17:56:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [157/300][1250/1251]	eta 0:00:00 lr 0.000464	time 0.7263 (0.7485)	loss 2.9153 (3.1600)	grad_norm 1.4957 (nan)	mem 23876MB
[2022-11-12 17:56:53 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 157 training takes 0:15:36
[2022-11-12 17:56:53 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_157.pth saving......
[2022-11-12 17:56:55 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_157.pth saved !!!
[2022-11-12 17:56:56 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.645 (1.645)	Loss 0.8599 (0.8599)	Acc@1 80.078 (80.078)	Acc@5 95.605 (95.605)	Mem 23876MB
[2022-11-12 17:57:07 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.708 Acc@5 95.280
[2022-11-12 17:57:07 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.7%
[2022-11-12 17:57:09 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.748 (1.748)	Loss 0.7349 (0.7349)	Acc@1 81.348 (81.348)	Acc@5 96.289 (96.289)	Mem 23876MB
[2022-11-12 17:57:19 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.892 Acc@5 96.102
[2022-11-12 17:57:19 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.9%
[2022-11-12 17:57:19 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.89% at 157 epoch
[2022-11-12 17:57:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][0/1251]	eta 0:51:23 lr 0.000464	time 2.4648 (2.4648)	loss 2.7102 (2.7102)	grad_norm 1.7271 (1.7271)	mem 23876MB
[2022-11-12 17:57:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][50/1251]	eta 0:15:42 lr 0.000463	time 0.7393 (0.7849)	loss 3.6982 (3.1364)	grad_norm 1.8858 (1.6801)	mem 23876MB
[2022-11-12 17:58:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][100/1251]	eta 0:14:42 lr 0.000463	time 0.7392 (0.7671)	loss 3.6294 (3.2026)	grad_norm 1.7412 (1.6825)	mem 23876MB
[2022-11-12 17:59:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][150/1251]	eta 0:13:57 lr 0.000463	time 0.7368 (0.7608)	loss 3.6583 (3.1810)	grad_norm 2.0418 (1.7155)	mem 23876MB
[2022-11-12 17:59:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][200/1251]	eta 0:13:15 lr 0.000463	time 0.7367 (0.7573)	loss 2.9695 (3.1699)	grad_norm 1.6219 (1.7500)	mem 23876MB
[2022-11-12 18:00:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][250/1251]	eta 0:12:36 lr 0.000463	time 0.7417 (0.7560)	loss 2.8006 (3.1617)	grad_norm 1.7748 (1.7495)	mem 23876MB
[2022-11-12 18:01:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][300/1251]	eta 0:11:57 lr 0.000462	time 0.7410 (0.7547)	loss 3.3569 (3.1491)	grad_norm 1.5099 (1.7474)	mem 23876MB
[2022-11-12 18:01:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][350/1251]	eta 0:11:19 lr 0.000462	time 0.7444 (0.7540)	loss 3.6375 (3.1481)	grad_norm 1.6153 (1.7443)	mem 23876MB
[2022-11-12 18:02:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][400/1251]	eta 0:10:40 lr 0.000462	time 0.7395 (0.7530)	loss 2.7418 (3.1508)	grad_norm 1.5484 (1.7498)	mem 23876MB
[2022-11-12 18:02:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][450/1251]	eta 0:10:02 lr 0.000462	time 0.7358 (0.7521)	loss 4.1282 (3.1422)	grad_norm 1.7269 (1.7419)	mem 23876MB
[2022-11-12 18:03:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][500/1251]	eta 0:09:24 lr 0.000462	time 0.7410 (0.7517)	loss 3.1562 (3.1532)	grad_norm 1.5721 (1.7384)	mem 23876MB
[2022-11-12 18:04:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][550/1251]	eta 0:08:46 lr 0.000461	time 0.7360 (0.7514)	loss 3.4493 (3.1521)	grad_norm 1.6808 (1.7318)	mem 23876MB
[2022-11-12 18:04:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][600/1251]	eta 0:08:09 lr 0.000461	time 0.7354 (0.7512)	loss 3.4256 (3.1643)	grad_norm 1.4065 (1.7319)	mem 23876MB
[2022-11-12 18:05:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][650/1251]	eta 0:07:31 lr 0.000461	time 0.7378 (0.7508)	loss 3.5929 (3.1752)	grad_norm 1.7755 (1.7312)	mem 23876MB
[2022-11-12 18:06:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][700/1251]	eta 0:06:53 lr 0.000461	time 0.7375 (0.7506)	loss 2.9791 (3.1732)	grad_norm 2.0745 (1.7245)	mem 23876MB
[2022-11-12 18:06:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][750/1251]	eta 0:06:15 lr 0.000460	time 0.7370 (0.7504)	loss 2.9554 (3.1676)	grad_norm 1.4701 (1.7234)	mem 23876MB
[2022-11-12 18:07:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][800/1251]	eta 0:05:38 lr 0.000460	time 0.7396 (0.7502)	loss 2.3253 (3.1703)	grad_norm 1.6955 (1.7233)	mem 23876MB
[2022-11-12 18:07:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][850/1251]	eta 0:05:00 lr 0.000460	time 0.7454 (0.7504)	loss 3.8502 (3.1727)	grad_norm 1.9614 (1.7232)	mem 23876MB
[2022-11-12 18:08:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][900/1251]	eta 0:04:23 lr 0.000460	time 0.7427 (0.7502)	loss 3.9348 (3.1739)	grad_norm 2.0146 (1.7280)	mem 23876MB
[2022-11-12 18:09:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][950/1251]	eta 0:03:45 lr 0.000460	time 0.8218 (0.7501)	loss 3.3557 (3.1749)	grad_norm 1.7474 (1.7256)	mem 23876MB
[2022-11-12 18:09:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][1000/1251]	eta 0:03:08 lr 0.000459	time 0.7974 (0.7499)	loss 3.4842 (3.1772)	grad_norm 1.7286 (1.7235)	mem 23876MB
[2022-11-12 18:10:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][1050/1251]	eta 0:02:30 lr 0.000459	time 0.7411 (0.7498)	loss 2.4802 (3.1756)	grad_norm 1.6199 (1.7218)	mem 23876MB
[2022-11-12 18:11:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][1100/1251]	eta 0:01:53 lr 0.000459	time 0.7424 (0.7497)	loss 3.6617 (3.1801)	grad_norm 1.7005 (1.7189)	mem 23876MB
[2022-11-12 18:11:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][1150/1251]	eta 0:01:15 lr 0.000459	time 0.7416 (0.7497)	loss 2.4773 (3.1772)	grad_norm 1.6613 (1.7173)	mem 23876MB
[2022-11-12 18:12:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][1200/1251]	eta 0:00:38 lr 0.000459	time 0.7430 (0.7497)	loss 3.6313 (3.1783)	grad_norm 1.4759 (1.7140)	mem 23876MB
[2022-11-12 18:12:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [158/300][1250/1251]	eta 0:00:00 lr 0.000458	time 0.7259 (0.7494)	loss 3.6529 (3.1748)	grad_norm 2.0746 (1.7139)	mem 23876MB
[2022-11-12 18:12:57 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 158 training takes 0:15:37
[2022-11-12 18:12:57 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_158.pth saving......
[2022-11-12 18:12:58 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_158.pth saved !!!
[2022-11-12 18:13:00 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.708 (1.708)	Loss 0.8286 (0.8286)	Acc@1 80.762 (80.762)	Acc@5 95.801 (95.801)	Mem 23876MB
[2022-11-12 18:13:11 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.036 Acc@5 95.246
[2022-11-12 18:13:11 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.0%
[2022-11-12 18:13:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.788 (1.788)	Loss 0.7242 (0.7242)	Acc@1 82.324 (82.324)	Acc@5 96.191 (96.191)	Mem 23876MB
[2022-11-12 18:13:23 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.904 Acc@5 96.100
[2022-11-12 18:13:23 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 81.9%
[2022-11-12 18:13:23 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.90% at 158 epoch
[2022-11-12 18:13:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][0/1251]	eta 0:52:00 lr 0.000458	time 2.4941 (2.4941)	loss 3.4398 (3.4398)	grad_norm 1.6410 (1.6410)	mem 23876MB
[2022-11-12 18:14:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][50/1251]	eta 0:15:37 lr 0.000458	time 0.8118 (0.7803)	loss 3.2615 (3.2125)	grad_norm 1.6674 (1.6562)	mem 23876MB
[2022-11-12 18:14:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][100/1251]	eta 0:14:39 lr 0.000458	time 0.7412 (0.7642)	loss 3.8294 (3.1630)	grad_norm 1.6159 (1.6916)	mem 23876MB
[2022-11-12 18:15:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][150/1251]	eta 0:13:54 lr 0.000458	time 0.7427 (0.7577)	loss 3.6672 (3.1192)	grad_norm 1.6213 (1.6859)	mem 23876MB
[2022-11-12 18:15:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][200/1251]	eta 0:13:13 lr 0.000458	time 0.7373 (0.7550)	loss 3.2524 (3.1379)	grad_norm 1.6338 (1.6979)	mem 23876MB
[2022-11-12 18:16:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][250/1251]	eta 0:12:33 lr 0.000457	time 0.7366 (0.7527)	loss 2.9402 (3.1370)	grad_norm 1.6499 (1.6938)	mem 23876MB
[2022-11-12 18:17:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][300/1251]	eta 0:11:54 lr 0.000457	time 0.7356 (0.7517)	loss 3.7761 (3.1558)	grad_norm 1.8091 (1.6916)	mem 23876MB
[2022-11-12 18:17:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][350/1251]	eta 0:11:16 lr 0.000457	time 0.7368 (0.7507)	loss 3.4531 (3.1669)	grad_norm 2.0663 (1.6966)	mem 23876MB
[2022-11-12 18:18:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][400/1251]	eta 0:10:38 lr 0.000457	time 0.7493 (0.7503)	loss 2.8019 (3.1469)	grad_norm 1.4964 (1.7114)	mem 23876MB
[2022-11-12 18:19:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][450/1251]	eta 0:10:00 lr 0.000457	time 0.7427 (0.7496)	loss 2.8719 (3.1353)	grad_norm 1.4988 (1.7045)	mem 23876MB
[2022-11-12 18:19:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][500/1251]	eta 0:09:22 lr 0.000456	time 0.7401 (0.7492)	loss 2.2295 (3.1285)	grad_norm 1.4545 (1.7033)	mem 23876MB
[2022-11-12 18:20:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][550/1251]	eta 0:08:44 lr 0.000456	time 0.7333 (0.7487)	loss 3.0448 (3.1281)	grad_norm 1.5425 (1.7040)	mem 23876MB
[2022-11-12 18:20:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][600/1251]	eta 0:08:07 lr 0.000456	time 0.8175 (0.7487)	loss 2.6451 (3.1248)	grad_norm 1.5377 (1.7012)	mem 23876MB
[2022-11-12 18:21:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][650/1251]	eta 0:07:29 lr 0.000456	time 0.8056 (0.7482)	loss 3.1173 (3.1276)	grad_norm 1.7290 (1.7024)	mem 23876MB
[2022-11-12 18:22:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][700/1251]	eta 0:06:52 lr 0.000456	time 0.7423 (0.7480)	loss 2.8917 (3.1330)	grad_norm 1.8331 (1.7024)	mem 23876MB
[2022-11-12 18:22:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][750/1251]	eta 0:06:14 lr 0.000455	time 0.7394 (0.7478)	loss 3.4913 (3.1410)	grad_norm 1.9106 (1.7020)	mem 23876MB
[2022-11-12 18:23:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][800/1251]	eta 0:05:37 lr 0.000455	time 0.7359 (0.7477)	loss 3.5223 (3.1403)	grad_norm 1.7491 (1.7007)	mem 23876MB
[2022-11-12 18:23:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][850/1251]	eta 0:04:59 lr 0.000455	time 0.7369 (0.7475)	loss 3.0184 (3.1370)	grad_norm 1.5621 (1.7052)	mem 23876MB
[2022-11-12 18:24:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][900/1251]	eta 0:04:22 lr 0.000455	time 0.7390 (0.7475)	loss 3.7888 (3.1446)	grad_norm 1.5857 (1.7024)	mem 23876MB
[2022-11-12 18:25:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][950/1251]	eta 0:03:44 lr 0.000454	time 0.7415 (0.7474)	loss 3.7615 (3.1418)	grad_norm 1.9109 (1.7019)	mem 23876MB
[2022-11-12 18:25:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][1000/1251]	eta 0:03:07 lr 0.000454	time 0.7397 (0.7474)	loss 3.5621 (3.1433)	grad_norm 1.5827 (1.7022)	mem 23876MB
[2022-11-12 18:26:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][1050/1251]	eta 0:02:30 lr 0.000454	time 0.7393 (0.7472)	loss 2.0559 (3.1385)	grad_norm 1.6534 (1.7007)	mem 23876MB
[2022-11-12 18:27:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][1100/1251]	eta 0:01:52 lr 0.000454	time 0.8200 (0.7473)	loss 3.3577 (3.1455)	grad_norm 1.8854 (1.7023)	mem 23876MB
[2022-11-12 18:27:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][1150/1251]	eta 0:01:15 lr 0.000454	time 0.7499 (0.7471)	loss 2.2089 (3.1453)	grad_norm 1.7159 (1.7022)	mem 23876MB
[2022-11-12 18:28:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][1200/1251]	eta 0:00:38 lr 0.000453	time 0.7420 (0.7471)	loss 2.7564 (3.1436)	grad_norm 1.6238 (1.7020)	mem 23876MB
[2022-11-12 18:28:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [159/300][1250/1251]	eta 0:00:00 lr 0.000453	time 0.7267 (0.7469)	loss 2.5978 (3.1447)	grad_norm 2.4798 (1.7005)	mem 23876MB
[2022-11-12 18:28:58 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 159 training takes 0:15:34
[2022-11-12 18:28:58 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_159.pth saving......
[2022-11-12 18:28:59 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_159.pth saved !!!
[2022-11-12 18:29:00 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.585 (1.585)	Loss 0.8532 (0.8532)	Acc@1 78.906 (78.906)	Acc@5 95.508 (95.508)	Mem 23876MB
[2022-11-12 18:29:11 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.038 Acc@5 95.316
[2022-11-12 18:29:11 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.0%
[2022-11-12 18:29:13 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.752 (1.752)	Loss 0.7760 (0.7760)	Acc@1 82.227 (82.227)	Acc@5 95.605 (95.605)	Mem 23876MB
[2022-11-12 18:29:24 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.952 Acc@5 96.126
[2022-11-12 18:29:24 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.0%
[2022-11-12 18:29:24 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.95% at 159 epoch
[2022-11-12 18:29:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][0/1251]	eta 0:50:23 lr 0.000453	time 2.4166 (2.4166)	loss 3.7415 (3.7415)	grad_norm 1.8426 (1.8426)	mem 23876MB
[2022-11-12 18:30:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][50/1251]	eta 0:15:40 lr 0.000453	time 0.7521 (0.7834)	loss 3.4260 (3.1825)	grad_norm 1.9009 (1.7213)	mem 23876MB
[2022-11-12 18:30:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][100/1251]	eta 0:14:40 lr 0.000453	time 0.7355 (0.7653)	loss 3.2719 (3.1318)	grad_norm 1.6540 (1.6858)	mem 23876MB
[2022-11-12 18:31:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][150/1251]	eta 0:13:56 lr 0.000453	time 0.7416 (0.7597)	loss 3.5911 (3.1024)	grad_norm 1.6623 (1.7015)	mem 23876MB
[2022-11-12 18:31:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][200/1251]	eta 0:13:14 lr 0.000452	time 0.7385 (0.7557)	loss 3.5942 (3.1291)	grad_norm 1.7535 (1.7088)	mem 23876MB
[2022-11-12 18:32:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][250/1251]	eta 0:12:34 lr 0.000452	time 0.7377 (0.7536)	loss 2.6967 (3.1332)	grad_norm 2.1517 (1.7127)	mem 23876MB
[2022-11-12 18:33:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][300/1251]	eta 0:11:55 lr 0.000452	time 0.7424 (0.7520)	loss 2.4318 (3.1188)	grad_norm 1.7171 (1.7113)	mem 23876MB
[2022-11-12 18:33:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][350/1251]	eta 0:11:17 lr 0.000452	time 0.7426 (0.7514)	loss 3.2227 (3.1226)	grad_norm 1.5867 (1.7154)	mem 23876MB
[2022-11-12 18:34:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][400/1251]	eta 0:10:38 lr 0.000452	time 0.7422 (0.7503)	loss 3.6218 (3.1234)	grad_norm 2.4771 (1.7222)	mem 23876MB
[2022-11-12 18:35:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][450/1251]	eta 0:10:00 lr 0.000451	time 0.7378 (0.7498)	loss 2.6530 (3.1252)	grad_norm 1.9291 (1.7306)	mem 23876MB
[2022-11-12 18:35:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][500/1251]	eta 0:09:22 lr 0.000451	time 0.7411 (0.7492)	loss 2.5814 (3.1150)	grad_norm 1.5717 (1.7254)	mem 23876MB
[2022-11-12 18:36:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][550/1251]	eta 0:08:45 lr 0.000451	time 0.7404 (0.7490)	loss 2.8440 (3.1139)	grad_norm 2.4114 (1.7269)	mem 23876MB
[2022-11-12 18:36:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][600/1251]	eta 0:08:07 lr 0.000451	time 0.7439 (0.7488)	loss 3.6480 (3.1133)	grad_norm 1.7559 (1.7279)	mem 23876MB
[2022-11-12 18:37:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][650/1251]	eta 0:07:29 lr 0.000451	time 0.7427 (0.7484)	loss 2.2494 (3.1171)	grad_norm 2.0224 (1.7278)	mem 23876MB
[2022-11-12 18:38:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][700/1251]	eta 0:06:52 lr 0.000450	time 0.7471 (0.7483)	loss 2.0584 (3.1158)	grad_norm 1.4899 (1.7253)	mem 23876MB
[2022-11-12 18:38:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][750/1251]	eta 0:06:14 lr 0.000450	time 0.7392 (0.7482)	loss 2.8754 (3.1113)	grad_norm 1.7631 (1.7246)	mem 23876MB
[2022-11-12 18:39:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][800/1251]	eta 0:05:37 lr 0.000450	time 0.7408 (0.7480)	loss 3.5649 (3.1132)	grad_norm 1.7530 (1.7248)	mem 23876MB
[2022-11-12 18:40:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][850/1251]	eta 0:04:59 lr 0.000450	time 0.7402 (0.7479)	loss 3.0897 (3.1095)	grad_norm 1.5949 (1.7214)	mem 23876MB
[2022-11-12 18:40:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][900/1251]	eta 0:04:22 lr 0.000450	time 0.7463 (0.7477)	loss 3.7101 (3.1170)	grad_norm 1.7201 (1.7222)	mem 23876MB
[2022-11-12 18:41:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][950/1251]	eta 0:03:45 lr 0.000449	time 0.7412 (0.7477)	loss 3.7234 (3.1169)	grad_norm 1.5580 (1.7216)	mem 23876MB
[2022-11-12 18:41:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][1000/1251]	eta 0:03:07 lr 0.000449	time 0.7470 (0.7477)	loss 3.6993 (3.1210)	grad_norm 2.0048 (1.7219)	mem 23876MB
[2022-11-12 18:42:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][1050/1251]	eta 0:02:30 lr 0.000449	time 0.7429 (0.7475)	loss 3.6537 (3.1155)	grad_norm 1.6014 (1.7179)	mem 23876MB
[2022-11-12 18:43:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][1100/1251]	eta 0:01:52 lr 0.000449	time 0.7350 (0.7474)	loss 3.7112 (3.1130)	grad_norm 1.7498 (1.7157)	mem 23876MB
[2022-11-12 18:43:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][1150/1251]	eta 0:01:15 lr 0.000449	time 0.7402 (0.7475)	loss 2.5338 (3.1171)	grad_norm 1.6112 (1.7166)	mem 23876MB
[2022-11-12 18:44:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][1200/1251]	eta 0:00:38 lr 0.000448	time 0.7418 (0.7472)	loss 3.7688 (3.1190)	grad_norm 1.6164 (1.7155)	mem 23876MB
[2022-11-12 18:44:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [160/300][1250/1251]	eta 0:00:00 lr 0.000448	time 0.7268 (0.7471)	loss 3.3726 (3.1118)	grad_norm 1.9039 (inf)	mem 23876MB
[2022-11-12 18:44:58 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 160 training takes 0:15:34
[2022-11-12 18:44:58 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_160.pth saving......
[2022-11-12 18:45:00 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_160.pth saved !!!
[2022-11-12 18:45:01 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.588 (1.588)	Loss 0.8439 (0.8439)	Acc@1 79.688 (79.688)	Acc@5 95.117 (95.117)	Mem 23876MB
[2022-11-12 18:45:12 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.682 Acc@5 95.246
[2022-11-12 18:45:12 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.7%
[2022-11-12 18:45:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.951 (1.951)	Loss 0.6685 (0.6685)	Acc@1 83.301 (83.301)	Acc@5 96.680 (96.680)	Mem 23876MB
[2022-11-12 18:45:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.992 Acc@5 96.158
[2022-11-12 18:45:25 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.0%
[2022-11-12 18:45:25 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.99% at 160 epoch
[2022-11-12 18:45:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][0/1251]	eta 0:53:19 lr 0.000448	time 2.5575 (2.5575)	loss 3.2104 (3.2104)	grad_norm 1.9616 (1.9616)	mem 23876MB
[2022-11-12 18:46:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][50/1251]	eta 0:15:40 lr 0.000448	time 0.7384 (0.7829)	loss 3.2630 (3.0815)	grad_norm 2.0543 (1.6801)	mem 23876MB
[2022-11-12 18:46:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][100/1251]	eta 0:14:42 lr 0.000448	time 0.8341 (0.7671)	loss 3.0769 (3.0459)	grad_norm 1.7138 (1.7022)	mem 23876MB
[2022-11-12 18:47:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][150/1251]	eta 0:13:57 lr 0.000447	time 0.7397 (0.7604)	loss 3.0086 (3.0533)	grad_norm 1.5102 (1.7193)	mem 23876MB
[2022-11-12 18:47:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][200/1251]	eta 0:13:15 lr 0.000447	time 0.8168 (0.7572)	loss 2.6125 (3.0497)	grad_norm 1.7175 (1.7153)	mem 23876MB
[2022-11-12 18:48:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][250/1251]	eta 0:12:35 lr 0.000447	time 0.7366 (0.7545)	loss 2.8299 (3.0827)	grad_norm 1.5344 (1.7201)	mem 23876MB
[2022-11-12 18:49:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][300/1251]	eta 0:11:56 lr 0.000447	time 0.7387 (0.7535)	loss 3.4889 (3.0860)	grad_norm 1.5676 (1.7195)	mem 23876MB
[2022-11-12 18:49:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][350/1251]	eta 0:11:18 lr 0.000447	time 0.7407 (0.7526)	loss 3.6998 (3.0976)	grad_norm 1.4806 (1.7214)	mem 23876MB
[2022-11-12 18:50:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][400/1251]	eta 0:10:40 lr 0.000446	time 0.7398 (0.7521)	loss 3.9801 (3.1118)	grad_norm 1.6630 (1.7205)	mem 23876MB
[2022-11-12 18:51:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][450/1251]	eta 0:10:01 lr 0.000446	time 0.7347 (0.7513)	loss 3.3670 (3.1278)	grad_norm 1.4999 (1.7213)	mem 23876MB
[2022-11-12 18:51:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][500/1251]	eta 0:09:23 lr 0.000446	time 0.7339 (0.7506)	loss 3.0034 (3.1207)	grad_norm 1.5348 (1.7284)	mem 23876MB
[2022-11-12 18:52:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][550/1251]	eta 0:08:45 lr 0.000446	time 0.7382 (0.7500)	loss 2.8209 (3.1229)	grad_norm 1.6114 (1.7252)	mem 23876MB
[2022-11-12 18:52:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][600/1251]	eta 0:08:08 lr 0.000446	time 0.7380 (0.7498)	loss 3.7648 (3.1222)	grad_norm 1.6676 (1.7246)	mem 23876MB
[2022-11-12 18:53:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][650/1251]	eta 0:07:30 lr 0.000445	time 0.7405 (0.7497)	loss 3.3842 (3.1246)	grad_norm 1.9888 (1.7241)	mem 23876MB
[2022-11-12 18:54:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][700/1251]	eta 0:06:52 lr 0.000445	time 0.7349 (0.7495)	loss 3.7665 (3.1218)	grad_norm 1.5741 (1.7229)	mem 23876MB
[2022-11-12 18:54:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][750/1251]	eta 0:06:15 lr 0.000445	time 0.7425 (0.7492)	loss 2.8161 (3.1278)	grad_norm 1.7113 (1.7275)	mem 23876MB
[2022-11-12 18:55:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][800/1251]	eta 0:05:37 lr 0.000445	time 0.7363 (0.7490)	loss 3.2949 (3.1287)	grad_norm 2.0184 (1.7278)	mem 23876MB
[2022-11-12 18:56:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][850/1251]	eta 0:05:00 lr 0.000445	time 0.7369 (0.7490)	loss 3.6468 (3.1296)	grad_norm 1.7426 (1.7262)	mem 23876MB
[2022-11-12 18:56:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][900/1251]	eta 0:04:22 lr 0.000444	time 0.7358 (0.7488)	loss 3.4862 (3.1323)	grad_norm 1.7010 (1.7267)	mem 23876MB
[2022-11-12 18:57:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][950/1251]	eta 0:03:45 lr 0.000444	time 0.7392 (0.7488)	loss 3.2536 (3.1361)	grad_norm 1.8476 (1.7300)	mem 23876MB
[2022-11-12 18:57:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][1000/1251]	eta 0:03:07 lr 0.000444	time 0.8201 (0.7487)	loss 3.5297 (3.1388)	grad_norm 1.7012 (1.7318)	mem 23876MB
[2022-11-12 18:58:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][1050/1251]	eta 0:02:30 lr 0.000444	time 0.7406 (0.7487)	loss 3.6370 (3.1400)	grad_norm 1.7777 (1.7302)	mem 23876MB
[2022-11-12 18:59:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][1100/1251]	eta 0:01:53 lr 0.000444	time 0.7413 (0.7485)	loss 3.5283 (3.1374)	grad_norm 1.5918 (1.7289)	mem 23876MB
[2022-11-12 18:59:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][1150/1251]	eta 0:01:15 lr 0.000443	time 0.7429 (0.7484)	loss 3.3895 (3.1396)	grad_norm 1.5331 (1.7285)	mem 23876MB
[2022-11-12 19:00:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][1200/1251]	eta 0:00:38 lr 0.000443	time 0.7270 (0.7484)	loss 3.2718 (3.1385)	grad_norm 1.5552 (1.7280)	mem 23876MB
[2022-11-12 19:01:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [161/300][1250/1251]	eta 0:00:00 lr 0.000443	time 0.7285 (0.7482)	loss 3.0144 (3.1406)	grad_norm 1.4776 (1.7269)	mem 23876MB
[2022-11-12 19:01:01 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 161 training takes 0:15:36
[2022-11-12 19:01:01 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_161.pth saving......
[2022-11-12 19:01:02 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_161.pth saved !!!
[2022-11-12 19:01:04 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.576 (1.576)	Loss 0.8542 (0.8542)	Acc@1 80.176 (80.176)	Acc@5 95.898 (95.898)	Mem 23876MB
[2022-11-12 19:01:14 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.014 Acc@5 95.332
[2022-11-12 19:01:14 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.0%
[2022-11-12 19:01:16 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.757 (1.757)	Loss 0.7456 (0.7456)	Acc@1 82.617 (82.617)	Acc@5 96.680 (96.680)	Mem 23876MB
[2022-11-12 19:01:27 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.986 Acc@5 96.166
[2022-11-12 19:01:27 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.0%
[2022-11-12 19:01:27 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.99% at 160 epoch
[2022-11-12 19:01:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][0/1251]	eta 0:49:09 lr 0.000443	time 2.3576 (2.3576)	loss 2.5542 (2.5542)	grad_norm 1.5016 (1.5016)	mem 23876MB
[2022-11-12 19:02:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][50/1251]	eta 0:15:39 lr 0.000443	time 0.8316 (0.7820)	loss 2.6935 (3.0839)	grad_norm 1.9963 (1.6825)	mem 23876MB
[2022-11-12 19:02:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][100/1251]	eta 0:14:37 lr 0.000443	time 0.7358 (0.7628)	loss 2.9411 (3.0589)	grad_norm 1.9094 (1.6752)	mem 23876MB
[2022-11-12 19:03:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][150/1251]	eta 0:13:53 lr 0.000442	time 0.7389 (0.7573)	loss 2.8031 (3.0885)	grad_norm 1.6696 (1.6799)	mem 23876MB
[2022-11-12 19:03:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][200/1251]	eta 0:13:13 lr 0.000442	time 0.7417 (0.7551)	loss 2.9080 (3.0834)	grad_norm 1.7480 (1.6735)	mem 23876MB
[2022-11-12 19:04:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][250/1251]	eta 0:12:33 lr 0.000442	time 0.7365 (0.7523)	loss 3.1544 (3.0829)	grad_norm 2.7358 (1.6795)	mem 23876MB
[2022-11-12 19:05:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][300/1251]	eta 0:11:54 lr 0.000442	time 0.7451 (0.7518)	loss 3.3277 (3.0871)	grad_norm 1.6357 (1.6759)	mem 23876MB
[2022-11-12 19:05:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][350/1251]	eta 0:11:16 lr 0.000442	time 0.7353 (0.7509)	loss 2.9331 (3.0963)	grad_norm 1.4700 (1.6852)	mem 23876MB
[2022-11-12 19:06:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][400/1251]	eta 0:10:38 lr 0.000441	time 0.8014 (0.7503)	loss 3.1864 (3.1062)	grad_norm 1.4157 (1.6925)	mem 23876MB
[2022-11-12 19:07:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][450/1251]	eta 0:10:00 lr 0.000441	time 0.7374 (0.7496)	loss 3.5481 (3.1165)	grad_norm 1.7019 (1.6949)	mem 23876MB
[2022-11-12 19:07:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][500/1251]	eta 0:09:22 lr 0.000441	time 0.8204 (0.7491)	loss 3.4776 (3.1171)	grad_norm 1.7632 (1.7055)	mem 23876MB
[2022-11-12 19:08:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][550/1251]	eta 0:08:44 lr 0.000441	time 0.8203 (0.7487)	loss 3.6216 (3.1313)	grad_norm 1.8315 (1.7035)	mem 23876MB
[2022-11-12 19:08:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][600/1251]	eta 0:08:07 lr 0.000440	time 0.7392 (0.7487)	loss 2.8020 (3.1261)	grad_norm 1.6311 (1.7051)	mem 23876MB
[2022-11-12 19:09:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][650/1251]	eta 0:07:29 lr 0.000440	time 0.7545 (0.7483)	loss 3.7715 (3.1259)	grad_norm 1.5577 (1.7079)	mem 23876MB
[2022-11-12 19:10:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][700/1251]	eta 0:06:52 lr 0.000440	time 0.7400 (0.7484)	loss 3.3180 (3.1280)	grad_norm 1.4886 (1.7133)	mem 23876MB
[2022-11-12 19:10:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][750/1251]	eta 0:06:14 lr 0.000440	time 0.7361 (0.7481)	loss 2.8354 (3.1306)	grad_norm 1.6744 (1.7134)	mem 23876MB
[2022-11-12 19:11:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][800/1251]	eta 0:05:37 lr 0.000440	time 0.7396 (0.7480)	loss 3.2983 (3.1335)	grad_norm 1.5945 (1.7116)	mem 23876MB
[2022-11-12 19:12:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][850/1251]	eta 0:04:59 lr 0.000439	time 0.7399 (0.7478)	loss 3.4828 (3.1341)	grad_norm 1.6169 (1.7125)	mem 23876MB
[2022-11-12 19:12:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][900/1251]	eta 0:04:22 lr 0.000439	time 0.8133 (0.7479)	loss 2.9311 (3.1335)	grad_norm 1.4939 (1.7168)	mem 23876MB
[2022-11-12 19:13:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][950/1251]	eta 0:03:45 lr 0.000439	time 0.7372 (0.7476)	loss 2.9435 (3.1345)	grad_norm 1.5953 (1.7164)	mem 23876MB
[2022-11-12 19:13:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][1000/1251]	eta 0:03:07 lr 0.000439	time 0.7471 (0.7477)	loss 2.4055 (3.1303)	grad_norm 1.8598 (inf)	mem 23876MB
[2022-11-12 19:14:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][1050/1251]	eta 0:02:30 lr 0.000439	time 0.7384 (0.7475)	loss 2.4492 (3.1358)	grad_norm 1.7185 (inf)	mem 23876MB
[2022-11-12 19:15:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][1100/1251]	eta 0:01:52 lr 0.000438	time 0.7398 (0.7476)	loss 3.4715 (3.1363)	grad_norm 1.8036 (inf)	mem 23876MB
[2022-11-12 19:15:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][1150/1251]	eta 0:01:15 lr 0.000438	time 0.7361 (0.7475)	loss 3.6392 (3.1381)	grad_norm 1.5304 (inf)	mem 23876MB
[2022-11-12 19:16:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][1200/1251]	eta 0:00:38 lr 0.000438	time 0.7398 (0.7475)	loss 3.5343 (3.1469)	grad_norm 1.8583 (inf)	mem 23876MB
[2022-11-12 19:17:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [162/300][1250/1251]	eta 0:00:00 lr 0.000438	time 0.7288 (0.7473)	loss 3.5734 (3.1458)	grad_norm 1.5215 (inf)	mem 23876MB
[2022-11-12 19:17:02 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 162 training takes 0:15:35
[2022-11-12 19:17:02 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_162.pth saving......
[2022-11-12 19:17:03 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_162.pth saved !!!
[2022-11-12 19:17:05 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.631 (1.631)	Loss 0.8554 (0.8554)	Acc@1 78.418 (78.418)	Acc@5 95.410 (95.410)	Mem 23876MB
[2022-11-12 19:17:15 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.016 Acc@5 95.372
[2022-11-12 19:17:15 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.0%
[2022-11-12 19:17:17 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.774 (1.774)	Loss 0.7996 (0.7996)	Acc@1 80.273 (80.273)	Acc@5 95.215 (95.215)	Mem 23876MB
[2022-11-12 19:17:28 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.970 Acc@5 96.172
[2022-11-12 19:17:28 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.0%
[2022-11-12 19:17:28 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 81.99% at 160 epoch
[2022-11-12 19:17:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][0/1251]	eta 0:49:07 lr 0.000438	time 2.3558 (2.3558)	loss 3.6529 (3.6529)	grad_norm 1.8229 (1.8229)	mem 23876MB
[2022-11-12 19:18:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][50/1251]	eta 0:15:36 lr 0.000438	time 0.7409 (0.7799)	loss 3.6054 (3.2058)	grad_norm 1.7364 (1.7239)	mem 23876MB
[2022-11-12 19:18:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][100/1251]	eta 0:14:39 lr 0.000437	time 0.7450 (0.7639)	loss 2.2184 (3.1672)	grad_norm 1.7633 (1.7199)	mem 23876MB
[2022-11-12 19:19:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][150/1251]	eta 0:13:55 lr 0.000437	time 0.7401 (0.7593)	loss 3.3808 (3.1646)	grad_norm 1.7698 (1.7310)	mem 23876MB
[2022-11-12 19:20:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][200/1251]	eta 0:13:14 lr 0.000437	time 0.7266 (0.7559)	loss 3.1974 (3.1370)	grad_norm 1.9052 (1.7331)	mem 23876MB
[2022-11-12 19:20:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][250/1251]	eta 0:12:34 lr 0.000437	time 0.7471 (0.7542)	loss 4.0055 (3.1476)	grad_norm 2.0648 (1.7488)	mem 23876MB
[2022-11-12 19:21:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][300/1251]	eta 0:11:55 lr 0.000437	time 0.7360 (0.7525)	loss 1.9816 (3.1263)	grad_norm 1.4910 (1.7403)	mem 23876MB
[2022-11-12 19:21:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][350/1251]	eta 0:11:17 lr 0.000436	time 0.7385 (0.7520)	loss 3.4374 (3.1385)	grad_norm 1.8120 (1.7427)	mem 23876MB
[2022-11-12 19:22:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][400/1251]	eta 0:10:39 lr 0.000436	time 0.7354 (0.7511)	loss 2.7955 (3.1417)	grad_norm 1.6177 (1.7410)	mem 23876MB
[2022-11-12 19:23:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][450/1251]	eta 0:10:01 lr 0.000436	time 0.7376 (0.7508)	loss 2.6873 (3.1378)	grad_norm 1.9237 (1.7365)	mem 23876MB
[2022-11-12 19:23:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][500/1251]	eta 0:09:23 lr 0.000436	time 0.7381 (0.7502)	loss 2.0741 (3.1365)	grad_norm 1.5502 (1.7385)	mem 23876MB
[2022-11-12 19:24:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][550/1251]	eta 0:08:45 lr 0.000436	time 0.8382 (0.7503)	loss 2.9326 (3.1392)	grad_norm 1.5431 (1.7390)	mem 23876MB
[2022-11-12 19:24:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][600/1251]	eta 0:08:08 lr 0.000435	time 0.7426 (0.7496)	loss 3.0797 (3.1350)	grad_norm 1.6120 (1.7415)	mem 23876MB
[2022-11-12 19:25:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][650/1251]	eta 0:07:30 lr 0.000435	time 0.7339 (0.7498)	loss 2.7644 (3.1395)	grad_norm 1.6712 (1.7442)	mem 23876MB
[2022-11-12 19:26:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][700/1251]	eta 0:06:52 lr 0.000435	time 0.7377 (0.7493)	loss 3.4793 (3.1392)	grad_norm 2.0575 (1.7448)	mem 23876MB
[2022-11-12 19:26:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][750/1251]	eta 0:06:15 lr 0.000435	time 0.7325 (0.7494)	loss 3.0024 (3.1341)	grad_norm 1.3913 (1.7451)	mem 23876MB
[2022-11-12 19:27:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][800/1251]	eta 0:05:37 lr 0.000435	time 0.7047 (0.7490)	loss 3.2256 (3.1324)	grad_norm inf (inf)	mem 23876MB
[2022-11-12 19:28:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][850/1251]	eta 0:05:00 lr 0.000434	time 0.7388 (0.7489)	loss 3.1283 (3.1372)	grad_norm 1.7081 (inf)	mem 23876MB
[2022-11-12 19:28:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][900/1251]	eta 0:04:22 lr 0.000434	time 0.7413 (0.7488)	loss 3.6101 (3.1368)	grad_norm 1.6339 (inf)	mem 23876MB
[2022-11-12 19:29:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][950/1251]	eta 0:03:45 lr 0.000434	time 0.7366 (0.7488)	loss 3.7812 (3.1369)	grad_norm 1.9876 (inf)	mem 23876MB
[2022-11-12 19:29:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][1000/1251]	eta 0:03:07 lr 0.000434	time 0.7392 (0.7487)	loss 1.9998 (3.1361)	grad_norm 1.7573 (inf)	mem 23876MB
[2022-11-12 19:30:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][1050/1251]	eta 0:02:30 lr 0.000434	time 0.7419 (0.7488)	loss 3.4209 (3.1346)	grad_norm 1.8958 (inf)	mem 23876MB
[2022-11-12 19:31:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][1100/1251]	eta 0:01:53 lr 0.000433	time 0.7339 (0.7485)	loss 3.4085 (3.1344)	grad_norm 1.7147 (inf)	mem 23876MB
[2022-11-12 19:31:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][1150/1251]	eta 0:01:15 lr 0.000433	time 0.7393 (0.7486)	loss 3.3805 (3.1338)	grad_norm 1.5742 (inf)	mem 23876MB
[2022-11-12 19:32:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][1200/1251]	eta 0:00:38 lr 0.000433	time 0.7384 (0.7486)	loss 3.3789 (3.1345)	grad_norm 1.7279 (inf)	mem 23876MB
[2022-11-12 19:33:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [163/300][1250/1251]	eta 0:00:00 lr 0.000433	time 0.7245 (0.7484)	loss 2.6186 (3.1325)	grad_norm 1.6353 (inf)	mem 23876MB
[2022-11-12 19:33:04 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 163 training takes 0:15:36
[2022-11-12 19:33:04 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_163.pth saving......
[2022-11-12 19:33:06 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_163.pth saved !!!
[2022-11-12 19:33:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.729 (1.729)	Loss 0.8799 (0.8799)	Acc@1 79.004 (79.004)	Acc@5 94.531 (94.531)	Mem 23876MB
[2022-11-12 19:33:18 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 79.872 Acc@5 95.296
[2022-11-12 19:33:18 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 79.9%
[2022-11-12 19:33:20 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.020 (2.020)	Loss 0.7946 (0.7946)	Acc@1 80.469 (80.469)	Acc@5 96.582 (96.582)	Mem 23876MB
[2022-11-12 19:33:31 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.998 Acc@5 96.184
[2022-11-12 19:33:31 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.0%
[2022-11-12 19:33:31 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.00% at 163 epoch
[2022-11-12 19:33:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][0/1251]	eta 0:50:38 lr 0.000433	time 2.4291 (2.4291)	loss 3.5100 (3.5100)	grad_norm 1.8005 (1.8005)	mem 23876MB
[2022-11-12 19:34:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][50/1251]	eta 0:15:39 lr 0.000432	time 0.7391 (0.7826)	loss 3.9054 (3.2332)	grad_norm 1.5582 (1.7528)	mem 23876MB
[2022-11-12 19:34:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][100/1251]	eta 0:14:39 lr 0.000432	time 0.7505 (0.7644)	loss 2.5245 (3.2119)	grad_norm 1.9369 (1.7523)	mem 23876MB
[2022-11-12 19:35:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][150/1251]	eta 0:13:54 lr 0.000432	time 0.7391 (0.7583)	loss 3.8507 (3.1935)	grad_norm 1.8659 (1.7631)	mem 23876MB
[2022-11-12 19:36:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][200/1251]	eta 0:13:13 lr 0.000432	time 0.7390 (0.7549)	loss 3.3567 (3.1976)	grad_norm 1.8359 (1.7632)	mem 23876MB
[2022-11-12 19:36:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][250/1251]	eta 0:12:34 lr 0.000432	time 0.7371 (0.7533)	loss 3.5211 (3.1857)	grad_norm 1.6120 (1.7583)	mem 23876MB
[2022-11-12 19:37:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][300/1251]	eta 0:11:55 lr 0.000431	time 0.7379 (0.7519)	loss 2.5539 (3.1581)	grad_norm 1.5363 (1.7469)	mem 23876MB
[2022-11-12 19:37:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][350/1251]	eta 0:11:16 lr 0.000431	time 0.7361 (0.7506)	loss 3.7192 (3.1583)	grad_norm 2.0740 (1.7375)	mem 23876MB
[2022-11-12 19:38:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][400/1251]	eta 0:10:38 lr 0.000431	time 0.7351 (0.7502)	loss 3.5750 (3.1527)	grad_norm 1.7256 (1.7314)	mem 23876MB
[2022-11-12 19:39:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][450/1251]	eta 0:10:00 lr 0.000431	time 0.7460 (0.7499)	loss 2.4771 (3.1561)	grad_norm 1.6002 (1.7275)	mem 23876MB
[2022-11-12 19:39:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][500/1251]	eta 0:09:22 lr 0.000431	time 0.7462 (0.7494)	loss 3.3713 (3.1527)	grad_norm 1.6120 (1.7361)	mem 23876MB
[2022-11-12 19:40:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][550/1251]	eta 0:08:45 lr 0.000430	time 0.7373 (0.7492)	loss 3.8373 (3.1481)	grad_norm 1.7107 (1.7338)	mem 23876MB
[2022-11-12 19:41:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][600/1251]	eta 0:08:07 lr 0.000430	time 0.7385 (0.7487)	loss 3.7808 (3.1469)	grad_norm 2.1022 (1.7325)	mem 23876MB
[2022-11-12 19:41:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][650/1251]	eta 0:07:29 lr 0.000430	time 0.7377 (0.7486)	loss 3.1982 (3.1503)	grad_norm 1.7503 (1.7332)	mem 23876MB
[2022-11-12 19:42:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][700/1251]	eta 0:06:52 lr 0.000430	time 0.7367 (0.7486)	loss 3.4690 (3.1427)	grad_norm 1.7820 (1.7284)	mem 23876MB
[2022-11-12 19:42:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][750/1251]	eta 0:06:14 lr 0.000430	time 0.7418 (0.7483)	loss 3.1497 (3.1509)	grad_norm 1.7185 (1.7270)	mem 23876MB
[2022-11-12 19:43:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][800/1251]	eta 0:05:37 lr 0.000429	time 0.7366 (0.7483)	loss 3.2742 (3.1478)	grad_norm 1.8674 (1.7273)	mem 23876MB
[2022-11-12 19:44:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][850/1251]	eta 0:04:59 lr 0.000429	time 0.7379 (0.7481)	loss 3.4991 (3.1468)	grad_norm 1.8462 (1.7303)	mem 23876MB
[2022-11-12 19:44:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][900/1251]	eta 0:04:22 lr 0.000429	time 0.7371 (0.7478)	loss 3.3036 (3.1522)	grad_norm 1.3963 (1.7306)	mem 23876MB
[2022-11-12 19:45:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][950/1251]	eta 0:03:45 lr 0.000429	time 0.7403 (0.7478)	loss 2.0968 (3.1523)	grad_norm 1.5884 (1.7353)	mem 23876MB
[2022-11-12 19:45:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][1000/1251]	eta 0:03:07 lr 0.000429	time 0.7384 (0.7477)	loss 2.0236 (3.1458)	grad_norm 1.6967 (1.7347)	mem 23876MB
[2022-11-12 19:46:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][1050/1251]	eta 0:02:30 lr 0.000428	time 0.7422 (0.7478)	loss 2.1558 (3.1455)	grad_norm 1.4952 (1.7378)	mem 23876MB
[2022-11-12 19:47:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][1100/1251]	eta 0:01:52 lr 0.000428	time 0.7394 (0.7478)	loss 3.7422 (3.1440)	grad_norm 1.7445 (1.7372)	mem 23876MB
[2022-11-12 19:47:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][1150/1251]	eta 0:01:15 lr 0.000428	time 0.7429 (0.7477)	loss 2.6973 (3.1474)	grad_norm 1.7575 (1.7361)	mem 23876MB
[2022-11-12 19:48:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][1200/1251]	eta 0:00:38 lr 0.000428	time 0.7431 (0.7477)	loss 2.7630 (3.1446)	grad_norm 2.1140 (1.7387)	mem 23876MB
[2022-11-12 19:49:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [164/300][1250/1251]	eta 0:00:00 lr 0.000428	time 0.7256 (0.7475)	loss 2.4566 (3.1512)	grad_norm 1.7426 (1.7413)	mem 23876MB
[2022-11-12 19:49:06 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 164 training takes 0:15:35
[2022-11-12 19:49:06 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_164.pth saving......
[2022-11-12 19:49:08 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_164.pth saved !!!
[2022-11-12 19:49:09 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.590 (1.590)	Loss 0.8846 (0.8846)	Acc@1 79.785 (79.785)	Acc@5 94.727 (94.727)	Mem 23876MB
[2022-11-12 19:49:20 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.104 Acc@5 95.426
[2022-11-12 19:49:20 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.1%
[2022-11-12 19:49:22 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.828 (1.828)	Loss 0.7268 (0.7268)	Acc@1 82.324 (82.324)	Acc@5 96.582 (96.582)	Mem 23876MB
[2022-11-12 19:49:32 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.996 Acc@5 96.204
[2022-11-12 19:49:32 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.0%
[2022-11-12 19:49:32 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.00% at 163 epoch
[2022-11-12 19:49:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][0/1251]	eta 0:50:38 lr 0.000428	time 2.4292 (2.4292)	loss 2.9309 (2.9309)	grad_norm 1.7995 (1.7995)	mem 23876MB
[2022-11-12 19:50:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][50/1251]	eta 0:15:40 lr 0.000427	time 0.7398 (0.7828)	loss 2.7996 (3.0744)	grad_norm 1.6293 (1.6590)	mem 23876MB
[2022-11-12 19:50:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][100/1251]	eta 0:14:41 lr 0.000427	time 0.7382 (0.7660)	loss 3.3342 (3.1598)	grad_norm 1.9326 (1.7428)	mem 23876MB
[2022-11-12 19:51:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][150/1251]	eta 0:13:55 lr 0.000427	time 0.7436 (0.7590)	loss 2.5787 (3.1108)	grad_norm 1.5123 (1.7397)	mem 23876MB
[2022-11-12 19:52:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][200/1251]	eta 0:13:15 lr 0.000427	time 0.8139 (0.7568)	loss 3.0071 (3.1279)	grad_norm 1.8237 (1.7393)	mem 23876MB
[2022-11-12 19:52:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][250/1251]	eta 0:12:35 lr 0.000427	time 0.7422 (0.7549)	loss 3.2464 (3.1421)	grad_norm 1.7370 (1.7424)	mem 23876MB
[2022-11-12 19:53:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][300/1251]	eta 0:11:56 lr 0.000426	time 0.7387 (0.7536)	loss 2.1986 (3.1478)	grad_norm 2.0241 (1.7404)	mem 23876MB
[2022-11-12 19:53:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][350/1251]	eta 0:11:18 lr 0.000426	time 0.7409 (0.7529)	loss 3.2940 (3.1380)	grad_norm 1.6296 (1.7436)	mem 23876MB
[2022-11-12 19:54:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][400/1251]	eta 0:10:40 lr 0.000426	time 0.8244 (0.7522)	loss 2.6760 (3.1305)	grad_norm 1.8639 (inf)	mem 23876MB
[2022-11-12 19:55:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][450/1251]	eta 0:10:01 lr 0.000426	time 0.7334 (0.7516)	loss 2.3950 (3.1340)	grad_norm 1.7768 (inf)	mem 23876MB
[2022-11-12 19:55:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][500/1251]	eta 0:09:24 lr 0.000426	time 0.7211 (0.7511)	loss 3.6063 (3.1352)	grad_norm 1.8871 (inf)	mem 23876MB
[2022-11-12 19:56:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][550/1251]	eta 0:08:46 lr 0.000425	time 0.7472 (0.7506)	loss 3.8881 (3.1363)	grad_norm 1.7273 (inf)	mem 23876MB
[2022-11-12 19:57:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][600/1251]	eta 0:08:08 lr 0.000425	time 0.8252 (0.7504)	loss 3.3287 (3.1400)	grad_norm 1.7304 (inf)	mem 23876MB
[2022-11-12 19:57:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][650/1251]	eta 0:07:30 lr 0.000425	time 0.7357 (0.7501)	loss 2.1419 (3.1404)	grad_norm 1.6109 (inf)	mem 23876MB
[2022-11-12 19:58:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][700/1251]	eta 0:06:53 lr 0.000425	time 0.7430 (0.7497)	loss 3.5335 (3.1518)	grad_norm 1.9250 (inf)	mem 23876MB
[2022-11-12 19:58:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][750/1251]	eta 0:06:15 lr 0.000424	time 0.7377 (0.7496)	loss 3.1016 (3.1511)	grad_norm 1.7704 (inf)	mem 23876MB
[2022-11-12 19:59:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][800/1251]	eta 0:05:37 lr 0.000424	time 0.7374 (0.7494)	loss 2.9751 (3.1509)	grad_norm 1.6284 (inf)	mem 23876MB
[2022-11-12 20:00:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][850/1251]	eta 0:05:00 lr 0.000424	time 0.7358 (0.7493)	loss 3.0767 (3.1477)	grad_norm 1.7981 (inf)	mem 23876MB
[2022-11-12 20:00:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][900/1251]	eta 0:04:22 lr 0.000424	time 0.7377 (0.7492)	loss 2.9051 (3.1448)	grad_norm 1.4725 (inf)	mem 23876MB
[2022-11-12 20:01:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][950/1251]	eta 0:03:45 lr 0.000424	time 0.8052 (0.7491)	loss 3.4157 (3.1400)	grad_norm 1.7782 (inf)	mem 23876MB
[2022-11-12 20:02:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][1000/1251]	eta 0:03:07 lr 0.000423	time 0.8147 (0.7489)	loss 3.2307 (3.1380)	grad_norm 1.8567 (inf)	mem 23876MB
[2022-11-12 20:02:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][1050/1251]	eta 0:02:30 lr 0.000423	time 0.7327 (0.7488)	loss 3.3752 (3.1411)	grad_norm 1.5086 (inf)	mem 23876MB
[2022-11-12 20:03:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][1100/1251]	eta 0:01:53 lr 0.000423	time 0.7403 (0.7486)	loss 3.5376 (3.1347)	grad_norm 1.4861 (inf)	mem 23876MB
[2022-11-12 20:03:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][1150/1251]	eta 0:01:15 lr 0.000423	time 0.7487 (0.7485)	loss 3.3612 (3.1349)	grad_norm 1.8931 (inf)	mem 23876MB
[2022-11-12 20:04:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][1200/1251]	eta 0:00:38 lr 0.000423	time 0.7353 (0.7484)	loss 3.4776 (3.1387)	grad_norm 1.7335 (inf)	mem 23876MB
[2022-11-12 20:05:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [165/300][1250/1251]	eta 0:00:00 lr 0.000422	time 0.7242 (0.7482)	loss 3.7977 (3.1347)	grad_norm 2.0784 (inf)	mem 23876MB
[2022-11-12 20:05:09 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 165 training takes 0:15:36
[2022-11-12 20:05:09 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_165.pth saving......
[2022-11-12 20:05:10 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_165.pth saved !!!
[2022-11-12 20:05:11 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.595 (1.595)	Loss 0.8381 (0.8381)	Acc@1 80.176 (80.176)	Acc@5 95.215 (95.215)	Mem 23876MB
[2022-11-12 20:05:22 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.082 Acc@5 95.290
[2022-11-12 20:05:22 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.1%
[2022-11-12 20:05:24 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.783 (1.783)	Loss 0.7257 (0.7257)	Acc@1 82.227 (82.227)	Acc@5 96.582 (96.582)	Mem 23876MB
[2022-11-12 20:05:35 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.044 Acc@5 96.216
[2022-11-12 20:05:35 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.0%
[2022-11-12 20:05:35 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.04% at 165 epoch
[2022-11-12 20:05:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][0/1251]	eta 0:48:24 lr 0.000422	time 2.3220 (2.3220)	loss 3.9041 (3.9041)	grad_norm 2.1031 (2.1031)	mem 23876MB
[2022-11-12 20:06:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][50/1251]	eta 0:15:36 lr 0.000422	time 0.7372 (0.7795)	loss 3.5960 (3.0950)	grad_norm 1.5115 (1.7592)	mem 23876MB
[2022-11-12 20:06:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][100/1251]	eta 0:14:39 lr 0.000422	time 0.7396 (0.7643)	loss 3.4655 (3.1601)	grad_norm 1.8665 (1.7456)	mem 23876MB
[2022-11-12 20:07:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][150/1251]	eta 0:13:54 lr 0.000422	time 0.7519 (0.7582)	loss 2.4742 (3.1220)	grad_norm 1.9513 (1.7450)	mem 23876MB
[2022-11-12 20:08:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][200/1251]	eta 0:13:14 lr 0.000422	time 0.7449 (0.7556)	loss 3.0387 (3.1039)	grad_norm 1.7112 (1.7333)	mem 23876MB
[2022-11-12 20:08:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][250/1251]	eta 0:12:34 lr 0.000421	time 0.7428 (0.7541)	loss 3.1447 (3.0834)	grad_norm 1.9564 (1.7360)	mem 23876MB
[2022-11-12 20:09:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][300/1251]	eta 0:11:55 lr 0.000421	time 0.7399 (0.7523)	loss 3.4717 (3.1012)	grad_norm 1.8816 (1.7356)	mem 23876MB
[2022-11-12 20:09:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][350/1251]	eta 0:11:17 lr 0.000421	time 0.7443 (0.7517)	loss 3.4399 (3.1103)	grad_norm 1.7472 (1.7377)	mem 23876MB
[2022-11-12 20:10:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][400/1251]	eta 0:10:39 lr 0.000421	time 0.7429 (0.7511)	loss 2.8705 (3.1030)	grad_norm 1.9276 (1.7485)	mem 23876MB
[2022-11-12 20:11:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][450/1251]	eta 0:10:01 lr 0.000421	time 0.7352 (0.7504)	loss 3.4206 (3.1027)	grad_norm 1.7258 (1.7507)	mem 23876MB
[2022-11-12 20:11:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][500/1251]	eta 0:09:23 lr 0.000420	time 0.7379 (0.7499)	loss 3.1954 (3.1029)	grad_norm 1.5486 (1.7415)	mem 23876MB
[2022-11-12 20:12:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][550/1251]	eta 0:08:45 lr 0.000420	time 0.7272 (0.7497)	loss 2.4280 (3.0987)	grad_norm 1.5566 (1.7435)	mem 23876MB
[2022-11-12 20:13:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][600/1251]	eta 0:08:07 lr 0.000420	time 0.8140 (0.7493)	loss 3.1443 (3.1057)	grad_norm 1.7497 (1.7464)	mem 23876MB
[2022-11-12 20:13:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][650/1251]	eta 0:07:30 lr 0.000420	time 0.7356 (0.7490)	loss 3.8416 (3.1064)	grad_norm 1.8678 (1.7444)	mem 23876MB
[2022-11-12 20:14:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][700/1251]	eta 0:06:52 lr 0.000420	time 0.7375 (0.7487)	loss 3.4453 (3.1109)	grad_norm 1.6330 (1.7466)	mem 23876MB
[2022-11-12 20:14:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][750/1251]	eta 0:06:15 lr 0.000419	time 0.7464 (0.7486)	loss 3.5458 (3.1111)	grad_norm 1.8322 (1.7469)	mem 23876MB
[2022-11-12 20:15:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][800/1251]	eta 0:05:37 lr 0.000419	time 0.7330 (0.7485)	loss 2.3285 (3.1048)	grad_norm 1.7593 (1.7474)	mem 23876MB
[2022-11-12 20:16:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][850/1251]	eta 0:05:00 lr 0.000419	time 0.8148 (0.7483)	loss 3.5647 (3.1003)	grad_norm 1.5129 (1.7478)	mem 23876MB
[2022-11-12 20:16:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][900/1251]	eta 0:04:22 lr 0.000419	time 0.7415 (0.7481)	loss 3.5014 (3.1010)	grad_norm 2.0749 (1.7499)	mem 23876MB
[2022-11-12 20:17:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][950/1251]	eta 0:03:45 lr 0.000419	time 0.7368 (0.7481)	loss 3.4778 (3.1058)	grad_norm 1.6708 (1.7505)	mem 23876MB
[2022-11-12 20:18:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][1000/1251]	eta 0:03:07 lr 0.000418	time 0.7374 (0.7480)	loss 3.6263 (3.1055)	grad_norm 2.0272 (1.7559)	mem 23876MB
[2022-11-12 20:18:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][1050/1251]	eta 0:02:30 lr 0.000418	time 0.7419 (0.7480)	loss 3.4873 (3.1006)	grad_norm 1.6666 (1.7519)	mem 23876MB
[2022-11-12 20:19:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][1100/1251]	eta 0:01:52 lr 0.000418	time 0.7371 (0.7479)	loss 3.2610 (3.1034)	grad_norm 1.9649 (1.7496)	mem 23876MB
[2022-11-12 20:19:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][1150/1251]	eta 0:01:15 lr 0.000418	time 0.7385 (0.7479)	loss 3.2079 (3.1071)	grad_norm 1.9690 (1.7530)	mem 23876MB
[2022-11-12 20:20:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][1200/1251]	eta 0:00:38 lr 0.000418	time 0.7359 (0.7479)	loss 3.2885 (3.1139)	grad_norm 2.0230 (1.7562)	mem 23876MB
[2022-11-12 20:21:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [166/300][1250/1251]	eta 0:00:00 lr 0.000417	time 0.7242 (0.7476)	loss 3.2866 (3.1161)	grad_norm 1.6212 (1.7566)	mem 23876MB
[2022-11-12 20:21:10 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 166 training takes 0:15:35
[2022-11-12 20:21:10 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_166.pth saving......
[2022-11-12 20:21:11 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_166.pth saved !!!
[2022-11-12 20:21:13 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.655 (1.655)	Loss 0.8385 (0.8385)	Acc@1 79.688 (79.688)	Acc@5 95.410 (95.410)	Mem 23876MB
[2022-11-12 20:21:24 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.038 Acc@5 95.310
[2022-11-12 20:21:24 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.0%
[2022-11-12 20:21:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.831 (1.831)	Loss 0.7107 (0.7107)	Acc@1 83.203 (83.203)	Acc@5 96.484 (96.484)	Mem 23876MB
[2022-11-12 20:21:36 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.106 Acc@5 96.216
[2022-11-12 20:21:36 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.1%
[2022-11-12 20:21:36 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.11% at 166 epoch
[2022-11-12 20:21:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][0/1251]	eta 0:52:59 lr 0.000417	time 2.5414 (2.5414)	loss 3.6097 (3.6097)	grad_norm 1.5827 (1.5827)	mem 23876MB
[2022-11-12 20:22:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][50/1251]	eta 0:15:40 lr 0.000417	time 0.7401 (0.7834)	loss 3.3156 (3.1252)	grad_norm 1.7652 (1.7337)	mem 23876MB
[2022-11-12 20:22:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][100/1251]	eta 0:14:41 lr 0.000417	time 0.7373 (0.7659)	loss 3.1765 (3.1188)	grad_norm 1.8959 (1.7689)	mem 23876MB
[2022-11-12 20:23:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][150/1251]	eta 0:13:56 lr 0.000417	time 0.7433 (0.7597)	loss 2.3436 (3.1416)	grad_norm 1.6556 (1.7480)	mem 23876MB
[2022-11-12 20:24:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][200/1251]	eta 0:13:15 lr 0.000417	time 0.7585 (0.7570)	loss 3.5523 (3.1775)	grad_norm 1.8612 (1.7682)	mem 23876MB
[2022-11-12 20:24:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][250/1251]	eta 0:12:35 lr 0.000416	time 0.7414 (0.7550)	loss 2.5490 (3.1748)	grad_norm 1.9086 (1.7611)	mem 23876MB
[2022-11-12 20:25:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][300/1251]	eta 0:11:56 lr 0.000416	time 0.7384 (0.7536)	loss 3.7694 (3.1710)	grad_norm 1.7557 (1.7753)	mem 23876MB
[2022-11-12 20:26:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][350/1251]	eta 0:11:18 lr 0.000416	time 0.7367 (0.7526)	loss 3.1921 (3.1427)	grad_norm 1.9205 (1.7678)	mem 23876MB
[2022-11-12 20:26:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][400/1251]	eta 0:10:39 lr 0.000416	time 0.7409 (0.7519)	loss 3.4918 (3.1422)	grad_norm 2.3611 (1.7631)	mem 23876MB
[2022-11-12 20:27:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][450/1251]	eta 0:10:01 lr 0.000416	time 0.7394 (0.7512)	loss 3.1495 (3.1411)	grad_norm 1.9605 (1.7679)	mem 23876MB
[2022-11-12 20:27:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][500/1251]	eta 0:09:23 lr 0.000415	time 0.7373 (0.7508)	loss 3.5444 (3.1406)	grad_norm 1.5754 (1.7672)	mem 23876MB
[2022-11-12 20:28:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][550/1251]	eta 0:08:46 lr 0.000415	time 0.7408 (0.7506)	loss 3.5363 (3.1428)	grad_norm 1.6996 (1.7641)	mem 23876MB
[2022-11-12 20:29:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][600/1251]	eta 0:08:08 lr 0.000415	time 0.7374 (0.7503)	loss 3.8669 (3.1426)	grad_norm 1.6995 (1.7608)	mem 23876MB
[2022-11-12 20:29:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][650/1251]	eta 0:07:30 lr 0.000415	time 0.7589 (0.7502)	loss 3.4910 (3.1447)	grad_norm 2.1620 (1.7630)	mem 23876MB
[2022-11-12 20:30:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][700/1251]	eta 0:06:53 lr 0.000414	time 0.7403 (0.7500)	loss 3.6330 (3.1447)	grad_norm 2.5198 (1.7633)	mem 23876MB
[2022-11-12 20:31:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][750/1251]	eta 0:06:15 lr 0.000414	time 0.7370 (0.7499)	loss 3.1137 (3.1385)	grad_norm 1.8051 (inf)	mem 23876MB
[2022-11-12 20:31:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][800/1251]	eta 0:05:38 lr 0.000414	time 0.7400 (0.7496)	loss 3.0792 (3.1438)	grad_norm 1.4788 (inf)	mem 23876MB
[2022-11-12 20:32:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][850/1251]	eta 0:05:00 lr 0.000414	time 0.7364 (0.7497)	loss 3.2623 (3.1408)	grad_norm 1.6407 (inf)	mem 23876MB
[2022-11-12 20:32:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][900/1251]	eta 0:04:22 lr 0.000414	time 0.7373 (0.7493)	loss 2.3532 (3.1368)	grad_norm 1.5754 (inf)	mem 23876MB
[2022-11-12 20:33:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][950/1251]	eta 0:03:45 lr 0.000413	time 0.8098 (0.7492)	loss 2.4908 (3.1367)	grad_norm 1.6583 (inf)	mem 23876MB
[2022-11-12 20:34:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][1000/1251]	eta 0:03:07 lr 0.000413	time 0.8366 (0.7490)	loss 2.2211 (3.1378)	grad_norm 1.8626 (inf)	mem 23876MB
[2022-11-12 20:34:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][1050/1251]	eta 0:02:30 lr 0.000413	time 0.7358 (0.7489)	loss 2.5648 (3.1350)	grad_norm 2.0901 (inf)	mem 23876MB
[2022-11-12 20:35:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][1100/1251]	eta 0:01:53 lr 0.000413	time 0.7374 (0.7488)	loss 2.9915 (3.1349)	grad_norm 1.8996 (inf)	mem 23876MB
[2022-11-12 20:35:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][1150/1251]	eta 0:01:15 lr 0.000413	time 0.7368 (0.7488)	loss 2.7244 (3.1347)	grad_norm 1.8742 (inf)	mem 23876MB
[2022-11-12 20:36:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][1200/1251]	eta 0:00:38 lr 0.000412	time 0.7345 (0.7487)	loss 3.7589 (3.1332)	grad_norm 1.7973 (inf)	mem 23876MB
[2022-11-12 20:37:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [167/300][1250/1251]	eta 0:00:00 lr 0.000412	time 0.8136 (0.7486)	loss 2.6819 (3.1290)	grad_norm 1.4300 (inf)	mem 23876MB
[2022-11-12 20:37:13 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 167 training takes 0:15:36
[2022-11-12 20:37:13 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_167.pth saving......
[2022-11-12 20:37:14 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_167.pth saved !!!
[2022-11-12 20:37:16 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.608 (1.608)	Loss 0.8878 (0.8878)	Acc@1 80.469 (80.469)	Acc@5 94.629 (94.629)	Mem 23876MB
[2022-11-12 20:37:26 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.470 Acc@5 95.452
[2022-11-12 20:37:26 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.5%
[2022-11-12 20:37:28 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.889 (1.889)	Loss 0.7788 (0.7788)	Acc@1 81.934 (81.934)	Acc@5 95.801 (95.801)	Mem 23876MB
[2022-11-12 20:37:39 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.132 Acc@5 96.236
[2022-11-12 20:37:39 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.1%
[2022-11-12 20:37:39 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.13% at 167 epoch
[2022-11-12 20:37:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][0/1251]	eta 0:51:02 lr 0.000412	time 2.4482 (2.4482)	loss 3.3002 (3.3002)	grad_norm 1.6964 (1.6964)	mem 23876MB
[2022-11-12 20:38:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][50/1251]	eta 0:15:40 lr 0.000412	time 0.8434 (0.7833)	loss 3.2868 (3.1285)	grad_norm 1.8904 (1.7695)	mem 23876MB
[2022-11-12 20:38:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][100/1251]	eta 0:14:40 lr 0.000412	time 0.7340 (0.7647)	loss 3.7042 (3.1565)	grad_norm 1.7744 (1.8289)	mem 23876MB
[2022-11-12 20:39:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][150/1251]	eta 0:13:55 lr 0.000412	time 0.7426 (0.7588)	loss 3.1340 (3.0971)	grad_norm 1.7315 (1.8114)	mem 23876MB
[2022-11-12 20:40:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][200/1251]	eta 0:13:15 lr 0.000411	time 0.8136 (0.7566)	loss 2.8127 (3.0976)	grad_norm 1.6989 (1.8046)	mem 23876MB
[2022-11-12 20:40:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][250/1251]	eta 0:12:35 lr 0.000411	time 0.7397 (0.7546)	loss 3.1891 (3.1063)	grad_norm 1.5517 (1.7806)	mem 23876MB
[2022-11-12 20:41:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][300/1251]	eta 0:11:56 lr 0.000411	time 0.7390 (0.7535)	loss 2.8843 (3.0966)	grad_norm 1.7219 (1.7838)	mem 23876MB
[2022-11-12 20:42:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][350/1251]	eta 0:11:17 lr 0.000411	time 0.7452 (0.7522)	loss 2.8827 (3.1144)	grad_norm 1.6473 (1.7781)	mem 23876MB
[2022-11-12 20:42:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][400/1251]	eta 0:10:39 lr 0.000411	time 0.7369 (0.7520)	loss 3.3347 (3.1117)	grad_norm 1.8690 (1.7696)	mem 23876MB
[2022-11-12 20:43:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][450/1251]	eta 0:10:01 lr 0.000410	time 0.7424 (0.7514)	loss 3.5144 (3.1037)	grad_norm 1.9294 (1.7620)	mem 23876MB
[2022-11-12 20:43:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][500/1251]	eta 0:09:23 lr 0.000410	time 0.7377 (0.7509)	loss 2.8853 (3.1029)	grad_norm 1.6336 (1.7609)	mem 23876MB
[2022-11-12 20:44:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][550/1251]	eta 0:08:46 lr 0.000410	time 0.7378 (0.7504)	loss 3.5443 (3.0892)	grad_norm 1.5822 (1.7630)	mem 23876MB
[2022-11-12 20:45:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][600/1251]	eta 0:08:08 lr 0.000410	time 0.8236 (0.7502)	loss 3.3791 (3.0979)	grad_norm 1.8192 (1.7643)	mem 23876MB
[2022-11-12 20:45:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][650/1251]	eta 0:07:30 lr 0.000410	time 0.7397 (0.7499)	loss 3.4278 (3.0932)	grad_norm 1.5844 (1.7608)	mem 23876MB
[2022-11-12 20:46:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][700/1251]	eta 0:06:53 lr 0.000409	time 0.7344 (0.7497)	loss 2.5393 (3.0950)	grad_norm 1.6527 (1.7649)	mem 23876MB
[2022-11-12 20:47:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][750/1251]	eta 0:06:15 lr 0.000409	time 0.7354 (0.7495)	loss 2.6527 (3.0982)	grad_norm 1.8248 (1.7653)	mem 23876MB
[2022-11-12 20:47:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][800/1251]	eta 0:05:37 lr 0.000409	time 0.7365 (0.7493)	loss 2.6605 (3.1016)	grad_norm 1.6085 (1.7627)	mem 23876MB
[2022-11-12 20:48:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][850/1251]	eta 0:05:00 lr 0.000409	time 0.7291 (0.7492)	loss 3.6396 (3.1135)	grad_norm 1.5695 (1.7608)	mem 23876MB
[2022-11-12 20:48:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][900/1251]	eta 0:04:22 lr 0.000409	time 0.7345 (0.7489)	loss 3.5624 (3.1157)	grad_norm 1.5630 (1.7625)	mem 23876MB
[2022-11-12 20:49:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][950/1251]	eta 0:03:45 lr 0.000408	time 0.7518 (0.7488)	loss 3.5279 (3.1112)	grad_norm 1.4938 (1.7638)	mem 23876MB
[2022-11-12 20:50:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][1000/1251]	eta 0:03:07 lr 0.000408	time 0.8188 (0.7488)	loss 2.3024 (3.1150)	grad_norm 1.6183 (1.7630)	mem 23876MB
[2022-11-12 20:50:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][1050/1251]	eta 0:02:30 lr 0.000408	time 0.7403 (0.7486)	loss 3.6041 (3.1174)	grad_norm 1.5312 (1.7609)	mem 23876MB
[2022-11-12 20:51:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][1100/1251]	eta 0:01:53 lr 0.000408	time 0.7530 (0.7486)	loss 3.0080 (3.1169)	grad_norm 1.6867 (1.7624)	mem 23876MB
[2022-11-12 20:52:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][1150/1251]	eta 0:01:15 lr 0.000408	time 0.7402 (0.7484)	loss 2.8946 (3.1093)	grad_norm 1.4032 (1.7644)	mem 23876MB
[2022-11-12 20:52:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][1200/1251]	eta 0:00:38 lr 0.000407	time 0.7353 (0.7484)	loss 3.0502 (3.1006)	grad_norm 1.6228 (1.7634)	mem 23876MB
[2022-11-12 20:53:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [168/300][1250/1251]	eta 0:00:00 lr 0.000407	time 0.7268 (0.7483)	loss 3.4939 (3.1040)	grad_norm 1.7040 (1.7659)	mem 23876MB
[2022-11-12 20:53:15 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 168 training takes 0:15:36
[2022-11-12 20:53:16 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_168.pth saving......
[2022-11-12 20:53:17 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_168.pth saved !!!
[2022-11-12 20:53:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.711 (1.711)	Loss 0.7501 (0.7501)	Acc@1 81.250 (81.250)	Acc@5 96.094 (96.094)	Mem 23876MB
[2022-11-12 20:53:29 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.438 Acc@5 95.452
[2022-11-12 20:53:29 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.4%
[2022-11-12 20:53:31 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.879 (1.879)	Loss 0.7347 (0.7347)	Acc@1 82.812 (82.812)	Acc@5 95.410 (95.410)	Mem 23876MB
[2022-11-12 20:53:42 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.182 Acc@5 96.272
[2022-11-12 20:53:42 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-12 20:53:42 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.18% at 168 epoch
[2022-11-12 20:53:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][0/1251]	eta 0:49:04 lr 0.000407	time 2.3538 (2.3538)	loss 3.3578 (3.3578)	grad_norm 2.0420 (2.0420)	mem 23876MB
[2022-11-12 20:54:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][50/1251]	eta 0:15:38 lr 0.000407	time 0.8160 (0.7813)	loss 3.2354 (2.9765)	grad_norm 1.8335 (1.8299)	mem 23876MB
[2022-11-12 20:54:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][100/1251]	eta 0:14:38 lr 0.000407	time 0.7413 (0.7635)	loss 3.1823 (3.0689)	grad_norm 1.7237 (1.8172)	mem 23876MB
[2022-11-12 20:55:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][150/1251]	eta 0:13:54 lr 0.000407	time 0.7415 (0.7577)	loss 2.7327 (3.0794)	grad_norm 1.8696 (1.7874)	mem 23876MB
[2022-11-12 20:56:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][200/1251]	eta 0:13:14 lr 0.000406	time 0.7408 (0.7557)	loss 3.6186 (3.0697)	grad_norm 1.5779 (1.8048)	mem 23876MB
[2022-11-12 20:56:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][250/1251]	eta 0:12:34 lr 0.000406	time 0.7382 (0.7540)	loss 3.1564 (3.0970)	grad_norm 1.6647 (1.7856)	mem 23876MB
[2022-11-12 20:57:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][300/1251]	eta 0:11:55 lr 0.000406	time 0.7418 (0.7528)	loss 3.0402 (3.1035)	grad_norm 1.5013 (1.7792)	mem 23876MB
[2022-11-12 20:58:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][350/1251]	eta 0:11:17 lr 0.000406	time 0.8279 (0.7525)	loss 2.5960 (3.0986)	grad_norm 1.6834 (1.7735)	mem 23876MB
[2022-11-12 20:58:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][400/1251]	eta 0:10:39 lr 0.000406	time 0.7409 (0.7514)	loss 3.3884 (3.1040)	grad_norm 1.7467 (1.7805)	mem 23876MB
[2022-11-12 20:59:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][450/1251]	eta 0:10:01 lr 0.000405	time 0.7417 (0.7510)	loss 2.5100 (3.1113)	grad_norm 1.7263 (1.7849)	mem 23876MB
[2022-11-12 20:59:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][500/1251]	eta 0:09:23 lr 0.000405	time 0.7393 (0.7504)	loss 3.3317 (3.1118)	grad_norm 1.7677 (1.7872)	mem 23876MB
[2022-11-12 21:00:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][550/1251]	eta 0:08:45 lr 0.000405	time 0.7401 (0.7502)	loss 3.0686 (3.1164)	grad_norm 1.7113 (1.7836)	mem 23876MB
[2022-11-12 21:01:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][600/1251]	eta 0:08:08 lr 0.000405	time 0.7357 (0.7500)	loss 2.5889 (3.1260)	grad_norm 1.6478 (1.7849)	mem 23876MB
[2022-11-12 21:01:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][650/1251]	eta 0:07:30 lr 0.000405	time 0.7371 (0.7498)	loss 3.3099 (3.1231)	grad_norm 1.5938 (1.7812)	mem 23876MB
[2022-11-12 21:02:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][700/1251]	eta 0:06:52 lr 0.000404	time 0.7371 (0.7495)	loss 3.0680 (3.1215)	grad_norm 1.6842 (inf)	mem 23876MB
[2022-11-12 21:03:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][750/1251]	eta 0:06:15 lr 0.000404	time 0.8132 (0.7494)	loss 3.6672 (3.1224)	grad_norm 1.6403 (inf)	mem 23876MB
[2022-11-12 21:03:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][800/1251]	eta 0:05:37 lr 0.000404	time 0.7368 (0.7491)	loss 3.5430 (3.1202)	grad_norm 1.7596 (inf)	mem 23876MB
[2022-11-12 21:04:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][850/1251]	eta 0:05:00 lr 0.000404	time 0.7322 (0.7490)	loss 3.2618 (3.1181)	grad_norm 1.7577 (inf)	mem 23876MB
[2022-11-12 21:04:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][900/1251]	eta 0:04:22 lr 0.000404	time 0.7535 (0.7490)	loss 3.6670 (3.1154)	grad_norm 1.6527 (inf)	mem 23876MB
[2022-11-12 21:05:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][950/1251]	eta 0:03:45 lr 0.000403	time 0.7348 (0.7489)	loss 3.4417 (3.1151)	grad_norm 1.5497 (inf)	mem 23876MB
[2022-11-12 21:06:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][1000/1251]	eta 0:03:07 lr 0.000403	time 0.7394 (0.7488)	loss 3.4032 (3.1224)	grad_norm 1.7092 (inf)	mem 23876MB
[2022-11-12 21:06:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][1050/1251]	eta 0:02:30 lr 0.000403	time 0.7387 (0.7488)	loss 2.9485 (3.1234)	grad_norm 1.6238 (inf)	mem 23876MB
[2022-11-12 21:07:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][1100/1251]	eta 0:01:53 lr 0.000403	time 0.7346 (0.7486)	loss 3.5519 (3.1213)	grad_norm 1.9068 (inf)	mem 23876MB
[2022-11-12 21:08:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][1150/1251]	eta 0:01:15 lr 0.000402	time 0.7395 (0.7487)	loss 3.7577 (3.1237)	grad_norm 1.7078 (inf)	mem 23876MB
[2022-11-12 21:08:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][1200/1251]	eta 0:00:38 lr 0.000402	time 0.7356 (0.7485)	loss 2.1211 (3.1215)	grad_norm 1.6657 (inf)	mem 23876MB
[2022-11-12 21:09:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [169/300][1250/1251]	eta 0:00:00 lr 0.000402	time 0.7335 (0.7484)	loss 2.8529 (3.1212)	grad_norm 1.8565 (inf)	mem 23876MB
[2022-11-12 21:09:18 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 169 training takes 0:15:36
[2022-11-12 21:09:18 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_169.pth saving......
[2022-11-12 21:09:20 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_169.pth saved !!!
[2022-11-12 21:09:21 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.763 (1.763)	Loss 0.9327 (0.9327)	Acc@1 79.492 (79.492)	Acc@5 93.945 (93.945)	Mem 23876MB
[2022-11-12 21:09:32 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.118 Acc@5 95.534
[2022-11-12 21:09:32 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.1%
[2022-11-12 21:09:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.774 (1.774)	Loss 0.7568 (0.7568)	Acc@1 81.445 (81.445)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-12 21:09:44 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.208 Acc@5 96.282
[2022-11-12 21:09:44 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.2%
[2022-11-12 21:09:44 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.21% at 169 epoch
[2022-11-12 21:09:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][0/1251]	eta 0:50:24 lr 0.000402	time 2.4178 (2.4178)	loss 2.5506 (2.5506)	grad_norm 1.6616 (1.6616)	mem 23876MB
[2022-11-12 21:10:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][50/1251]	eta 0:15:39 lr 0.000402	time 0.7475 (0.7824)	loss 3.4766 (3.0900)	grad_norm 1.7338 (1.7312)	mem 23876MB
[2022-11-12 21:11:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][100/1251]	eta 0:14:40 lr 0.000402	time 0.7415 (0.7652)	loss 3.2920 (3.0875)	grad_norm 1.7088 (nan)	mem 23876MB
[2022-11-12 21:11:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][150/1251]	eta 0:13:55 lr 0.000401	time 0.7444 (0.7592)	loss 3.1753 (3.1085)	grad_norm 2.0364 (nan)	mem 23876MB
[2022-11-12 21:12:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][200/1251]	eta 0:13:14 lr 0.000401	time 0.8146 (0.7559)	loss 2.6010 (3.1021)	grad_norm 1.9034 (nan)	mem 23876MB
[2022-11-12 21:12:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][250/1251]	eta 0:12:34 lr 0.000401	time 0.7370 (0.7541)	loss 3.7291 (3.0916)	grad_norm 2.1199 (nan)	mem 23876MB
[2022-11-12 21:13:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][300/1251]	eta 0:11:56 lr 0.000401	time 0.7411 (0.7534)	loss 3.0589 (3.0970)	grad_norm 1.8261 (nan)	mem 23876MB
[2022-11-12 21:14:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][350/1251]	eta 0:11:17 lr 0.000401	time 0.7332 (0.7523)	loss 3.0504 (3.0964)	grad_norm 1.7604 (nan)	mem 23876MB
[2022-11-12 21:14:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][400/1251]	eta 0:10:39 lr 0.000400	time 0.7388 (0.7517)	loss 3.6064 (3.0944)	grad_norm 3.3956 (nan)	mem 23876MB
[2022-11-12 21:15:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][450/1251]	eta 0:10:01 lr 0.000400	time 0.7349 (0.7513)	loss 2.7397 (3.0876)	grad_norm 1.7871 (nan)	mem 23876MB
[2022-11-12 21:16:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][500/1251]	eta 0:09:23 lr 0.000400	time 0.7351 (0.7505)	loss 3.2181 (3.0898)	grad_norm 1.6320 (nan)	mem 23876MB
[2022-11-12 21:16:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][550/1251]	eta 0:08:45 lr 0.000400	time 0.7416 (0.7502)	loss 2.7923 (3.0903)	grad_norm 1.5992 (nan)	mem 23876MB
[2022-11-12 21:17:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][600/1251]	eta 0:08:08 lr 0.000400	time 0.8170 (0.7499)	loss 3.4152 (3.0899)	grad_norm 1.9634 (nan)	mem 23876MB
[2022-11-12 21:17:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][650/1251]	eta 0:07:30 lr 0.000399	time 0.7334 (0.7497)	loss 3.6015 (3.0883)	grad_norm 2.2329 (nan)	mem 23876MB
[2022-11-12 21:18:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][700/1251]	eta 0:06:52 lr 0.000399	time 0.7298 (0.7495)	loss 3.2110 (3.0931)	grad_norm 1.4898 (nan)	mem 23876MB
[2022-11-12 21:19:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][750/1251]	eta 0:06:15 lr 0.000399	time 0.7383 (0.7490)	loss 2.1114 (3.0969)	grad_norm 1.4197 (nan)	mem 23876MB
[2022-11-12 21:19:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][800/1251]	eta 0:05:37 lr 0.000399	time 0.7353 (0.7489)	loss 3.3542 (3.0953)	grad_norm 1.6978 (nan)	mem 23876MB
[2022-11-12 21:20:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][850/1251]	eta 0:05:00 lr 0.000399	time 0.7364 (0.7488)	loss 3.0111 (3.1063)	grad_norm 1.7501 (nan)	mem 23876MB
[2022-11-12 21:20:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][900/1251]	eta 0:04:22 lr 0.000398	time 0.7393 (0.7487)	loss 2.3866 (3.1088)	grad_norm 1.7152 (nan)	mem 23876MB
[2022-11-12 21:21:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][950/1251]	eta 0:03:45 lr 0.000398	time 0.7404 (0.7485)	loss 3.6782 (3.1095)	grad_norm 1.7801 (nan)	mem 23876MB
[2022-11-12 21:22:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][1000/1251]	eta 0:03:07 lr 0.000398	time 0.8097 (0.7484)	loss 3.6391 (3.1053)	grad_norm 1.7009 (nan)	mem 23876MB
[2022-11-12 21:22:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][1050/1251]	eta 0:02:30 lr 0.000398	time 0.7439 (0.7483)	loss 2.2287 (3.1017)	grad_norm 1.4124 (nan)	mem 23876MB
[2022-11-12 21:23:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][1100/1251]	eta 0:01:52 lr 0.000398	time 0.7504 (0.7481)	loss 3.3510 (3.0982)	grad_norm 1.8579 (nan)	mem 23876MB
[2022-11-12 21:24:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][1150/1251]	eta 0:01:15 lr 0.000397	time 0.7447 (0.7481)	loss 3.4410 (3.0968)	grad_norm 1.7071 (nan)	mem 23876MB
[2022-11-12 21:24:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][1200/1251]	eta 0:00:38 lr 0.000397	time 0.7380 (0.7481)	loss 3.2458 (3.1030)	grad_norm 1.7068 (nan)	mem 23876MB
[2022-11-12 21:25:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [170/300][1250/1251]	eta 0:00:00 lr 0.000397	time 0.7262 (0.7480)	loss 2.9803 (3.1013)	grad_norm 1.5929 (nan)	mem 23876MB
[2022-11-12 21:25:20 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 170 training takes 0:15:35
[2022-11-12 21:25:21 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_170.pth saving......
[2022-11-12 21:25:22 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_170.pth saved !!!
[2022-11-12 21:25:23 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.680 (1.680)	Loss 0.8917 (0.8917)	Acc@1 78.711 (78.711)	Acc@5 94.824 (94.824)	Mem 23876MB
[2022-11-12 21:25:34 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.306 Acc@5 95.508
[2022-11-12 21:25:34 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.3%
[2022-11-12 21:25:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.847 (1.847)	Loss 0.6954 (0.6954)	Acc@1 82.910 (82.910)	Acc@5 96.777 (96.777)	Mem 23876MB
[2022-11-12 21:25:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.284 Acc@5 96.288
[2022-11-12 21:25:47 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-12 21:25:47 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.28% at 170 epoch
[2022-11-12 21:25:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][0/1251]	eta 0:49:07 lr 0.000397	time 2.3560 (2.3560)	loss 4.0249 (4.0249)	grad_norm 1.9288 (1.9288)	mem 23876MB
[2022-11-12 21:26:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][50/1251]	eta 0:15:41 lr 0.000397	time 0.8191 (0.7836)	loss 2.4361 (3.0427)	grad_norm 1.9175 (1.8193)	mem 23876MB
[2022-11-12 21:27:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][100/1251]	eta 0:14:40 lr 0.000397	time 0.7283 (0.7652)	loss 2.1106 (3.0318)	grad_norm 1.6401 (1.7780)	mem 23876MB
[2022-11-12 21:27:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][150/1251]	eta 0:13:54 lr 0.000396	time 0.7388 (0.7577)	loss 3.3103 (3.0727)	grad_norm 1.5951 (1.7872)	mem 23876MB
[2022-11-12 21:28:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][200/1251]	eta 0:13:14 lr 0.000396	time 0.8156 (0.7556)	loss 2.8093 (3.0608)	grad_norm 1.6144 (1.7830)	mem 23876MB
[2022-11-12 21:28:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][250/1251]	eta 0:12:34 lr 0.000396	time 0.7366 (0.7540)	loss 3.7082 (3.0588)	grad_norm 1.5389 (1.7991)	mem 23876MB
[2022-11-12 21:29:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][300/1251]	eta 0:11:55 lr 0.000396	time 0.7341 (0.7527)	loss 3.0762 (3.0458)	grad_norm 1.7267 (1.8231)	mem 23876MB
[2022-11-12 21:30:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][350/1251]	eta 0:11:17 lr 0.000396	time 0.7368 (0.7519)	loss 3.0856 (3.0545)	grad_norm 1.6869 (1.8197)	mem 23876MB
[2022-11-12 21:30:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][400/1251]	eta 0:10:39 lr 0.000395	time 0.8214 (0.7513)	loss 3.5020 (3.0663)	grad_norm 1.5520 (1.8207)	mem 23876MB
[2022-11-12 21:31:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][450/1251]	eta 0:10:01 lr 0.000395	time 0.7436 (0.7509)	loss 3.0419 (3.0658)	grad_norm 1.9369 (1.8345)	mem 23876MB
[2022-11-12 21:32:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][500/1251]	eta 0:09:23 lr 0.000395	time 0.7434 (0.7504)	loss 3.1618 (3.0703)	grad_norm 1.8455 (1.8341)	mem 23876MB
[2022-11-12 21:32:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][550/1251]	eta 0:08:45 lr 0.000395	time 0.7452 (0.7503)	loss 2.8707 (3.0805)	grad_norm 1.9484 (1.8234)	mem 23876MB
[2022-11-12 21:33:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][600/1251]	eta 0:08:08 lr 0.000395	time 0.8250 (0.7499)	loss 2.2630 (3.0807)	grad_norm 1.5029 (1.8206)	mem 23876MB
[2022-11-12 21:33:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][650/1251]	eta 0:07:30 lr 0.000394	time 0.8218 (0.7498)	loss 3.4579 (3.0817)	grad_norm 1.6252 (1.8134)	mem 23876MB
[2022-11-12 21:34:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][700/1251]	eta 0:06:52 lr 0.000394	time 0.7392 (0.7494)	loss 3.5509 (3.0882)	grad_norm 1.6735 (1.8175)	mem 23876MB
[2022-11-12 21:35:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][750/1251]	eta 0:06:15 lr 0.000394	time 0.7443 (0.7492)	loss 2.8844 (3.0926)	grad_norm 3.2839 (1.8220)	mem 23876MB
[2022-11-12 21:35:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][800/1251]	eta 0:05:37 lr 0.000394	time 0.7366 (0.7492)	loss 3.2956 (3.0911)	grad_norm 1.7222 (1.8202)	mem 23876MB
[2022-11-12 21:36:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][850/1251]	eta 0:05:00 lr 0.000394	time 0.7444 (0.7490)	loss 2.4679 (3.0979)	grad_norm 1.7797 (1.8217)	mem 23876MB
[2022-11-12 21:37:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][900/1251]	eta 0:04:22 lr 0.000393	time 0.7506 (0.7490)	loss 3.3542 (3.0981)	grad_norm 1.6375 (1.8219)	mem 23876MB
[2022-11-12 21:37:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][950/1251]	eta 0:03:45 lr 0.000393	time 0.7319 (0.7487)	loss 3.8211 (3.0993)	grad_norm 2.1569 (1.8258)	mem 23876MB
[2022-11-12 21:38:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][1000/1251]	eta 0:03:07 lr 0.000393	time 0.8184 (0.7486)	loss 2.5487 (3.0970)	grad_norm 1.6040 (1.8211)	mem 23876MB
[2022-11-12 21:38:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][1050/1251]	eta 0:02:30 lr 0.000393	time 0.7395 (0.7485)	loss 3.0457 (3.0976)	grad_norm 1.8985 (1.8204)	mem 23876MB
[2022-11-12 21:39:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][1100/1251]	eta 0:01:53 lr 0.000393	time 0.7419 (0.7484)	loss 2.8535 (3.0974)	grad_norm 1.9636 (1.8205)	mem 23876MB
[2022-11-12 21:40:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][1150/1251]	eta 0:01:15 lr 0.000392	time 0.7329 (0.7484)	loss 3.2946 (3.1024)	grad_norm 1.5981 (1.8204)	mem 23876MB
[2022-11-12 21:40:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][1200/1251]	eta 0:00:38 lr 0.000392	time 0.7381 (0.7484)	loss 3.7089 (3.1014)	grad_norm 1.7478 (1.8174)	mem 23876MB
[2022-11-12 21:41:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [171/300][1250/1251]	eta 0:00:00 lr 0.000392	time 0.7274 (0.7481)	loss 3.2749 (3.1024)	grad_norm 1.9343 (1.8155)	mem 23876MB
[2022-11-12 21:41:23 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 171 training takes 0:15:36
[2022-11-12 21:41:23 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_171.pth saving......
[2022-11-12 21:41:24 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_171.pth saved !!!
[2022-11-12 21:41:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.665 (1.665)	Loss 0.8482 (0.8482)	Acc@1 79.199 (79.199)	Acc@5 95.312 (95.312)	Mem 23876MB
[2022-11-12 21:41:36 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.252 Acc@5 95.528
[2022-11-12 21:41:36 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.3%
[2022-11-12 21:41:38 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.864 (1.864)	Loss 0.6680 (0.6680)	Acc@1 84.180 (84.180)	Acc@5 96.973 (96.973)	Mem 23876MB
[2022-11-12 21:41:49 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.292 Acc@5 96.298
[2022-11-12 21:41:49 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-12 21:41:49 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.29% at 171 epoch
[2022-11-12 21:41:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][0/1251]	eta 0:49:21 lr 0.000392	time 2.3674 (2.3674)	loss 2.3820 (2.3820)	grad_norm 1.9170 (1.9170)	mem 23876MB
[2022-11-12 21:42:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][50/1251]	eta 0:15:37 lr 0.000392	time 0.7417 (0.7803)	loss 2.2972 (3.0535)	grad_norm 1.4771 (1.7647)	mem 23876MB
[2022-11-12 21:43:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][100/1251]	eta 0:14:41 lr 0.000392	time 0.7366 (0.7656)	loss 3.1794 (3.0167)	grad_norm 1.9004 (1.8206)	mem 23876MB
[2022-11-12 21:43:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][150/1251]	eta 0:13:56 lr 0.000391	time 0.7347 (0.7595)	loss 3.0598 (3.0288)	grad_norm 1.9593 (1.7911)	mem 23876MB
[2022-11-12 21:44:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][200/1251]	eta 0:13:15 lr 0.000391	time 0.8130 (0.7565)	loss 2.7548 (3.0195)	grad_norm 1.9988 (1.7850)	mem 23876MB
[2022-11-12 21:44:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][250/1251]	eta 0:12:35 lr 0.000391	time 0.7356 (0.7552)	loss 2.1085 (3.0216)	grad_norm 1.8934 (1.7927)	mem 23876MB
[2022-11-12 21:45:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][300/1251]	eta 0:11:56 lr 0.000391	time 0.7341 (0.7532)	loss 3.4070 (3.0373)	grad_norm 1.9825 (1.7877)	mem 23876MB
[2022-11-12 21:46:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][350/1251]	eta 0:11:17 lr 0.000391	time 0.7384 (0.7524)	loss 3.7325 (3.0376)	grad_norm 1.6159 (1.7931)	mem 23876MB
[2022-11-12 21:46:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][400/1251]	eta 0:10:39 lr 0.000390	time 0.7420 (0.7517)	loss 3.6164 (3.0411)	grad_norm 1.6169 (1.7978)	mem 23876MB
[2022-11-12 21:47:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][450/1251]	eta 0:10:01 lr 0.000390	time 0.7351 (0.7509)	loss 1.9427 (3.0483)	grad_norm 1.7367 (1.7917)	mem 23876MB
[2022-11-12 21:48:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][500/1251]	eta 0:09:23 lr 0.000390	time 0.7373 (0.7505)	loss 3.1208 (3.0495)	grad_norm 1.8997 (1.7953)	mem 23876MB
[2022-11-12 21:48:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][550/1251]	eta 0:08:45 lr 0.000390	time 0.7318 (0.7503)	loss 3.6072 (3.0504)	grad_norm 1.6426 (1.7915)	mem 23876MB
[2022-11-12 21:49:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][600/1251]	eta 0:08:08 lr 0.000390	time 0.8146 (0.7497)	loss 3.2209 (3.0569)	grad_norm 1.7422 (1.7887)	mem 23876MB
[2022-11-12 21:49:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][650/1251]	eta 0:07:30 lr 0.000389	time 0.7377 (0.7495)	loss 2.8451 (3.0622)	grad_norm 1.6875 (1.7871)	mem 23876MB
[2022-11-12 21:50:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][700/1251]	eta 0:06:52 lr 0.000389	time 0.7383 (0.7490)	loss 3.7527 (3.0627)	grad_norm 2.0490 (1.7913)	mem 23876MB
[2022-11-12 21:51:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][750/1251]	eta 0:06:15 lr 0.000389	time 0.7549 (0.7490)	loss 3.6325 (3.0687)	grad_norm 1.8614 (1.7913)	mem 23876MB
[2022-11-12 21:51:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][800/1251]	eta 0:05:37 lr 0.000389	time 0.7401 (0.7490)	loss 3.2525 (3.0685)	grad_norm 1.9704 (1.7939)	mem 23876MB
[2022-11-12 21:52:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][850/1251]	eta 0:05:00 lr 0.000389	time 0.7388 (0.7487)	loss 3.6985 (3.0655)	grad_norm 1.8683 (1.7928)	mem 23876MB
[2022-11-12 21:53:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][900/1251]	eta 0:04:22 lr 0.000388	time 0.7379 (0.7485)	loss 3.4301 (3.0697)	grad_norm 1.7830 (1.7910)	mem 23876MB
[2022-11-12 21:53:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][950/1251]	eta 0:03:45 lr 0.000388	time 0.8220 (0.7486)	loss 2.0639 (3.0695)	grad_norm 1.7311 (1.7971)	mem 23876MB
[2022-11-12 21:54:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][1000/1251]	eta 0:03:07 lr 0.000388	time 0.8127 (0.7483)	loss 3.3216 (3.0728)	grad_norm 1.5858 (1.7952)	mem 23876MB
[2022-11-12 21:54:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][1050/1251]	eta 0:02:30 lr 0.000388	time 0.7328 (0.7484)	loss 3.4372 (3.0757)	grad_norm 1.8269 (1.7994)	mem 23876MB
[2022-11-12 21:55:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][1100/1251]	eta 0:01:52 lr 0.000388	time 0.7512 (0.7481)	loss 3.2834 (3.0752)	grad_norm 1.7062 (1.7967)	mem 23876MB
[2022-11-12 21:56:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][1150/1251]	eta 0:01:15 lr 0.000387	time 0.7405 (0.7482)	loss 2.5975 (3.0737)	grad_norm 1.6587 (1.7949)	mem 23876MB
[2022-11-12 21:56:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][1200/1251]	eta 0:00:38 lr 0.000387	time 0.7372 (0.7481)	loss 4.0710 (3.0754)	grad_norm 1.9054 (1.7940)	mem 23876MB
[2022-11-12 21:57:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [172/300][1250/1251]	eta 0:00:00 lr 0.000387	time 0.7268 (0.7478)	loss 2.8081 (3.0748)	grad_norm 1.8001 (1.7935)	mem 23876MB
[2022-11-12 21:57:25 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 172 training takes 0:15:35
[2022-11-12 21:57:25 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_172.pth saving......
[2022-11-12 21:57:26 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_172.pth saved !!!
[2022-11-12 21:57:27 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.570 (1.570)	Loss 0.8688 (0.8688)	Acc@1 79.297 (79.297)	Acc@5 94.629 (94.629)	Mem 23876MB
[2022-11-12 21:57:38 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.390 Acc@5 95.502
[2022-11-12 21:57:38 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.4%
[2022-11-12 21:57:40 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.841 (1.841)	Loss 0.7725 (0.7725)	Acc@1 81.445 (81.445)	Acc@5 95.996 (95.996)	Mem 23876MB
[2022-11-12 21:57:51 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.278 Acc@5 96.346
[2022-11-12 21:57:51 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-12 21:57:51 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.29% at 171 epoch
[2022-11-12 21:57:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][0/1251]	eta 0:50:59 lr 0.000387	time 2.4460 (2.4460)	loss 3.3788 (3.3788)	grad_norm 1.8254 (1.8254)	mem 23876MB
[2022-11-12 21:58:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][50/1251]	eta 0:15:39 lr 0.000387	time 0.7391 (0.7824)	loss 2.5433 (3.0542)	grad_norm 2.0301 (1.7921)	mem 23876MB
[2022-11-12 21:59:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][100/1251]	eta 0:14:40 lr 0.000387	time 0.7401 (0.7647)	loss 2.6040 (3.0444)	grad_norm 1.4686 (1.7879)	mem 23876MB
[2022-11-12 21:59:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][150/1251]	eta 0:13:54 lr 0.000386	time 0.7425 (0.7582)	loss 3.3411 (3.0685)	grad_norm 1.6002 (1.7862)	mem 23876MB
[2022-11-12 22:00:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][200/1251]	eta 0:13:14 lr 0.000386	time 0.7440 (0.7563)	loss 2.2960 (3.0920)	grad_norm 1.6791 (1.7844)	mem 23876MB
[2022-11-12 22:01:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][250/1251]	eta 0:12:34 lr 0.000386	time 0.7383 (0.7541)	loss 3.3612 (3.1046)	grad_norm 1.5642 (1.7811)	mem 23876MB
[2022-11-12 22:01:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][300/1251]	eta 0:11:56 lr 0.000386	time 0.7403 (0.7536)	loss 3.1409 (3.0967)	grad_norm 1.5101 (1.7914)	mem 23876MB
[2022-11-12 22:02:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][350/1251]	eta 0:11:18 lr 0.000386	time 0.7339 (0.7528)	loss 2.6851 (3.1017)	grad_norm 1.3855 (1.7954)	mem 23876MB
[2022-11-12 22:02:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][400/1251]	eta 0:10:40 lr 0.000385	time 0.7372 (0.7522)	loss 3.0931 (3.0852)	grad_norm 1.9750 (1.7966)	mem 23876MB
[2022-11-12 22:03:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][450/1251]	eta 0:10:01 lr 0.000385	time 0.7370 (0.7515)	loss 2.4495 (3.0872)	grad_norm 1.5292 (1.7969)	mem 23876MB
[2022-11-12 22:04:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][500/1251]	eta 0:09:24 lr 0.000385	time 0.7359 (0.7510)	loss 2.8853 (3.0840)	grad_norm 1.5359 (1.7971)	mem 23876MB
[2022-11-12 22:04:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][550/1251]	eta 0:08:46 lr 0.000385	time 0.7467 (0.7509)	loss 3.3160 (3.0834)	grad_norm 1.6480 (1.8001)	mem 23876MB
[2022-11-12 22:05:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][600/1251]	eta 0:08:08 lr 0.000385	time 0.7348 (0.7503)	loss 3.1514 (3.0848)	grad_norm 1.5899 (1.8010)	mem 23876MB
[2022-11-12 22:05:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][650/1251]	eta 0:07:30 lr 0.000384	time 0.7390 (0.7501)	loss 3.3123 (3.0942)	grad_norm 1.8094 (1.8042)	mem 23876MB
[2022-11-12 22:06:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][700/1251]	eta 0:06:53 lr 0.000384	time 0.7347 (0.7496)	loss 2.3478 (3.0994)	grad_norm 1.7905 (1.8058)	mem 23876MB
[2022-11-12 22:07:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][750/1251]	eta 0:06:15 lr 0.000384	time 0.7421 (0.7496)	loss 2.2116 (3.0933)	grad_norm 1.8631 (inf)	mem 23876MB
[2022-11-12 22:07:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][800/1251]	eta 0:05:37 lr 0.000384	time 0.7375 (0.7494)	loss 3.5376 (3.0938)	grad_norm 1.7044 (inf)	mem 23876MB
[2022-11-12 22:08:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][850/1251]	eta 0:05:00 lr 0.000384	time 0.7360 (0.7493)	loss 3.5715 (3.1013)	grad_norm 1.8192 (inf)	mem 23876MB
[2022-11-12 22:09:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][900/1251]	eta 0:04:22 lr 0.000383	time 0.7343 (0.7489)	loss 2.0938 (3.0993)	grad_norm 1.6941 (inf)	mem 23876MB
[2022-11-12 22:09:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][950/1251]	eta 0:03:45 lr 0.000383	time 0.7315 (0.7490)	loss 3.3142 (3.1058)	grad_norm 1.7928 (inf)	mem 23876MB
[2022-11-12 22:10:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][1000/1251]	eta 0:03:07 lr 0.000383	time 0.7377 (0.7488)	loss 3.4266 (3.1090)	grad_norm 1.8486 (inf)	mem 23876MB
[2022-11-12 22:10:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][1050/1251]	eta 0:02:30 lr 0.000383	time 0.7400 (0.7489)	loss 3.3906 (3.1092)	grad_norm 1.9413 (inf)	mem 23876MB
[2022-11-12 22:11:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][1100/1251]	eta 0:01:53 lr 0.000383	time 0.7442 (0.7487)	loss 2.8028 (3.1053)	grad_norm 1.6780 (inf)	mem 23876MB
[2022-11-12 22:12:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][1150/1251]	eta 0:01:15 lr 0.000382	time 0.7457 (0.7487)	loss 3.4252 (3.1021)	grad_norm 2.1903 (inf)	mem 23876MB
[2022-11-12 22:12:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][1200/1251]	eta 0:00:38 lr 0.000382	time 0.7400 (0.7486)	loss 3.0859 (3.1016)	grad_norm 2.2812 (inf)	mem 23876MB
[2022-11-12 22:13:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [173/300][1250/1251]	eta 0:00:00 lr 0.000382	time 0.7258 (0.7485)	loss 3.6008 (3.1043)	grad_norm 1.7427 (inf)	mem 23876MB
[2022-11-12 22:13:27 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 173 training takes 0:15:36
[2022-11-12 22:13:28 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_173.pth saving......
[2022-11-12 22:13:29 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_173.pth saved !!!
[2022-11-12 22:13:30 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.584 (1.584)	Loss 0.8169 (0.8169)	Acc@1 79.980 (79.980)	Acc@5 95.410 (95.410)	Mem 23876MB
[2022-11-12 22:13:41 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.360 Acc@5 95.554
[2022-11-12 22:13:41 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.4%
[2022-11-12 22:13:43 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.832 (1.832)	Loss 0.7576 (0.7576)	Acc@1 82.227 (82.227)	Acc@5 96.289 (96.289)	Mem 23876MB
[2022-11-12 22:13:54 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.288 Acc@5 96.340
[2022-11-12 22:13:54 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-12 22:13:54 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.29% at 171 epoch
[2022-11-12 22:13:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][0/1251]	eta 0:50:02 lr 0.000382	time 2.3999 (2.3999)	loss 2.9822 (2.9822)	grad_norm 1.7208 (1.7208)	mem 23876MB
[2022-11-12 22:14:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][50/1251]	eta 0:15:43 lr 0.000382	time 0.7446 (0.7859)	loss 2.4408 (3.1328)	grad_norm 1.9947 (1.7974)	mem 23876MB
[2022-11-12 22:15:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][100/1251]	eta 0:14:42 lr 0.000381	time 0.7398 (0.7670)	loss 2.9129 (3.0852)	grad_norm 1.7163 (1.7645)	mem 23876MB
[2022-11-12 22:15:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][150/1251]	eta 0:13:57 lr 0.000381	time 0.7367 (0.7610)	loss 3.7569 (3.0791)	grad_norm 2.0792 (1.7774)	mem 23876MB
[2022-11-12 22:16:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][200/1251]	eta 0:13:17 lr 0.000381	time 0.8035 (0.7587)	loss 3.2911 (3.1010)	grad_norm 1.5948 (1.7830)	mem 23876MB
[2022-11-12 22:17:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][250/1251]	eta 0:12:37 lr 0.000381	time 0.7413 (0.7563)	loss 3.1499 (3.1073)	grad_norm 1.7551 (1.8217)	mem 23876MB
[2022-11-12 22:17:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][300/1251]	eta 0:11:57 lr 0.000381	time 0.7418 (0.7547)	loss 2.1369 (3.0992)	grad_norm 1.7514 (1.8217)	mem 23876MB
[2022-11-12 22:18:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][350/1251]	eta 0:11:19 lr 0.000380	time 0.7404 (0.7542)	loss 3.0968 (3.1025)	grad_norm 1.5217 (1.8222)	mem 23876MB
[2022-11-12 22:18:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][400/1251]	eta 0:10:41 lr 0.000380	time 0.7364 (0.7534)	loss 3.7106 (3.1087)	grad_norm 1.6898 (1.8273)	mem 23876MB
[2022-11-12 22:19:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][450/1251]	eta 0:10:03 lr 0.000380	time 0.7386 (0.7528)	loss 3.5889 (3.1132)	grad_norm 1.5329 (1.8311)	mem 23876MB
[2022-11-12 22:20:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][500/1251]	eta 0:09:24 lr 0.000380	time 0.7381 (0.7520)	loss 2.9807 (3.1211)	grad_norm 1.7380 (1.8271)	mem 23876MB
[2022-11-12 22:20:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][550/1251]	eta 0:08:46 lr 0.000380	time 0.7414 (0.7517)	loss 2.9602 (3.1108)	grad_norm 1.5271 (1.8237)	mem 23876MB
[2022-11-12 22:21:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][600/1251]	eta 0:08:09 lr 0.000379	time 0.8058 (0.7515)	loss 3.1354 (3.1133)	grad_norm 1.8243 (1.8243)	mem 23876MB
[2022-11-12 22:22:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][650/1251]	eta 0:07:31 lr 0.000379	time 0.7364 (0.7512)	loss 3.0868 (3.1034)	grad_norm 2.8323 (1.8219)	mem 23876MB
[2022-11-12 22:22:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][700/1251]	eta 0:06:53 lr 0.000379	time 0.7426 (0.7509)	loss 1.7565 (3.0994)	grad_norm 1.8200 (1.8226)	mem 23876MB
[2022-11-12 22:23:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][750/1251]	eta 0:06:16 lr 0.000379	time 0.7337 (0.7507)	loss 3.2682 (3.0937)	grad_norm 2.2595 (1.8230)	mem 23876MB
[2022-11-12 22:23:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][800/1251]	eta 0:05:38 lr 0.000379	time 0.7414 (0.7504)	loss 2.6580 (3.0889)	grad_norm 1.9120 (1.8224)	mem 23876MB
[2022-11-12 22:24:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][850/1251]	eta 0:05:00 lr 0.000378	time 0.7346 (0.7503)	loss 2.0780 (3.0932)	grad_norm 1.7036 (1.8234)	mem 23876MB
[2022-11-12 22:25:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][900/1251]	eta 0:04:23 lr 0.000378	time 0.7346 (0.7500)	loss 3.2994 (3.0949)	grad_norm 1.4282 (1.8226)	mem 23876MB
[2022-11-12 22:25:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][950/1251]	eta 0:03:45 lr 0.000378	time 0.7418 (0.7501)	loss 3.4335 (3.0950)	grad_norm 1.7574 (1.8204)	mem 23876MB
[2022-11-12 22:26:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][1000/1251]	eta 0:03:08 lr 0.000378	time 0.8086 (0.7499)	loss 3.1349 (3.0904)	grad_norm 1.6350 (1.8170)	mem 23876MB
[2022-11-12 22:27:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][1050/1251]	eta 0:02:30 lr 0.000378	time 0.7436 (0.7498)	loss 2.7188 (3.0897)	grad_norm 1.5438 (1.8242)	mem 23876MB
[2022-11-12 22:27:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][1100/1251]	eta 0:01:53 lr 0.000377	time 0.7361 (0.7497)	loss 2.2500 (3.0901)	grad_norm 1.8385 (1.8278)	mem 23876MB
[2022-11-12 22:28:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][1150/1251]	eta 0:01:15 lr 0.000377	time 0.7443 (0.7496)	loss 2.9415 (3.0940)	grad_norm 1.8517 (1.8267)	mem 23876MB
[2022-11-12 22:28:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][1200/1251]	eta 0:00:38 lr 0.000377	time 0.7370 (0.7495)	loss 3.1652 (3.0939)	grad_norm 1.6682 (1.8283)	mem 23876MB
[2022-11-12 22:29:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [174/300][1250/1251]	eta 0:00:00 lr 0.000377	time 0.7244 (0.7493)	loss 3.0928 (3.0948)	grad_norm 1.8571 (1.8287)	mem 23876MB
[2022-11-12 22:29:31 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 174 training takes 0:15:37
[2022-11-12 22:29:31 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_174.pth saving......
[2022-11-12 22:29:32 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_174.pth saved !!!
[2022-11-12 22:29:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.759 (1.759)	Loss 0.8496 (0.8496)	Acc@1 79.980 (79.980)	Acc@5 95.312 (95.312)	Mem 23876MB
[2022-11-12 22:29:45 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.314 Acc@5 95.456
[2022-11-12 22:29:45 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.3%
[2022-11-12 22:29:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.891 (1.891)	Loss 0.7330 (0.7330)	Acc@1 81.738 (81.738)	Acc@5 96.777 (96.777)	Mem 23876MB
[2022-11-12 22:29:57 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.292 Acc@5 96.356
[2022-11-12 22:29:57 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-12 22:29:57 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.29% at 171 epoch
[2022-11-12 22:30:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][0/1251]	eta 0:51:45 lr 0.000377	time 2.4823 (2.4823)	loss 3.4676 (3.4676)	grad_norm 1.6746 (1.6746)	mem 23876MB
[2022-11-12 22:30:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][50/1251]	eta 0:15:43 lr 0.000377	time 0.7342 (0.7856)	loss 2.8979 (3.0871)	grad_norm 1.6345 (1.8129)	mem 23876MB
[2022-11-12 22:31:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][100/1251]	eta 0:14:42 lr 0.000376	time 0.7355 (0.7671)	loss 3.3744 (3.1340)	grad_norm 1.8635 (1.7919)	mem 23876MB
[2022-11-12 22:31:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][150/1251]	eta 0:13:57 lr 0.000376	time 0.7396 (0.7610)	loss 3.0946 (3.0895)	grad_norm 1.7247 (1.8000)	mem 23876MB
[2022-11-12 22:32:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][200/1251]	eta 0:13:16 lr 0.000376	time 0.8059 (0.7580)	loss 3.4147 (3.0813)	grad_norm 1.6556 (1.7964)	mem 23876MB
[2022-11-12 22:33:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][250/1251]	eta 0:12:36 lr 0.000376	time 0.7481 (0.7558)	loss 1.9734 (3.0717)	grad_norm 2.3317 (1.8090)	mem 23876MB
[2022-11-12 22:33:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][300/1251]	eta 0:11:57 lr 0.000376	time 0.7344 (0.7544)	loss 2.9860 (3.0844)	grad_norm 1.8320 (1.8134)	mem 23876MB
[2022-11-12 22:34:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][350/1251]	eta 0:11:19 lr 0.000375	time 0.7381 (0.7541)	loss 3.6608 (3.0869)	grad_norm 1.9557 (1.8163)	mem 23876MB
[2022-11-12 22:34:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][400/1251]	eta 0:10:40 lr 0.000375	time 0.7362 (0.7528)	loss 2.1730 (3.0765)	grad_norm 2.1592 (1.8242)	mem 23876MB
[2022-11-12 22:35:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][450/1251]	eta 0:10:02 lr 0.000375	time 0.7441 (0.7524)	loss 2.2579 (3.0804)	grad_norm 2.3686 (1.8260)	mem 23876MB
[2022-11-12 22:36:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][500/1251]	eta 0:09:24 lr 0.000375	time 0.7403 (0.7516)	loss 3.2799 (3.0840)	grad_norm 1.6489 (1.8212)	mem 23876MB
[2022-11-12 22:36:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][550/1251]	eta 0:08:46 lr 0.000375	time 0.7343 (0.7516)	loss 3.2006 (3.0868)	grad_norm 1.6783 (nan)	mem 23876MB
[2022-11-12 22:37:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][600/1251]	eta 0:08:09 lr 0.000374	time 0.7449 (0.7512)	loss 2.8031 (3.0947)	grad_norm 1.5382 (nan)	mem 23876MB
[2022-11-12 22:38:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][650/1251]	eta 0:07:31 lr 0.000374	time 0.7390 (0.7510)	loss 3.5026 (3.0886)	grad_norm 2.0637 (nan)	mem 23876MB
[2022-11-12 22:38:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][700/1251]	eta 0:06:53 lr 0.000374	time 0.7347 (0.7505)	loss 3.4508 (3.0841)	grad_norm 1.8648 (nan)	mem 23876MB
[2022-11-12 22:39:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][750/1251]	eta 0:06:15 lr 0.000374	time 0.8282 (0.7504)	loss 3.3719 (3.0814)	grad_norm 2.1198 (nan)	mem 23876MB
[2022-11-12 22:39:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][800/1251]	eta 0:05:38 lr 0.000374	time 0.7368 (0.7502)	loss 3.3569 (3.0815)	grad_norm 1.8923 (nan)	mem 23876MB
[2022-11-12 22:40:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][850/1251]	eta 0:05:00 lr 0.000373	time 0.7416 (0.7501)	loss 3.4791 (3.0807)	grad_norm 1.9465 (nan)	mem 23876MB
[2022-11-12 22:41:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][900/1251]	eta 0:04:23 lr 0.000373	time 0.7401 (0.7500)	loss 3.6792 (3.0839)	grad_norm 1.6594 (nan)	mem 23876MB
[2022-11-12 22:41:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][950/1251]	eta 0:03:45 lr 0.000373	time 0.8218 (0.7500)	loss 3.0226 (3.0798)	grad_norm 1.4553 (nan)	mem 23876MB
[2022-11-12 22:42:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][1000/1251]	eta 0:03:08 lr 0.000373	time 0.7358 (0.7499)	loss 3.3874 (3.0746)	grad_norm 1.6596 (nan)	mem 23876MB
[2022-11-12 22:43:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][1050/1251]	eta 0:02:30 lr 0.000373	time 0.7372 (0.7497)	loss 2.2124 (3.0750)	grad_norm 2.2361 (nan)	mem 23876MB
[2022-11-12 22:43:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][1100/1251]	eta 0:01:53 lr 0.000372	time 0.7439 (0.7496)	loss 3.2439 (3.0762)	grad_norm 1.7560 (nan)	mem 23876MB
[2022-11-12 22:44:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][1150/1251]	eta 0:01:15 lr 0.000372	time 0.7350 (0.7497)	loss 3.3793 (3.0781)	grad_norm 2.0145 (nan)	mem 23876MB
[2022-11-12 22:44:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][1200/1251]	eta 0:00:38 lr 0.000372	time 0.7394 (0.7497)	loss 3.1696 (3.0752)	grad_norm 1.7723 (nan)	mem 23876MB
[2022-11-12 22:45:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [175/300][1250/1251]	eta 0:00:00 lr 0.000372	time 0.7285 (0.7494)	loss 2.6395 (3.0735)	grad_norm 1.9192 (nan)	mem 23876MB
[2022-11-12 22:45:35 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 175 training takes 0:15:37
[2022-11-12 22:45:35 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_175.pth saving......
[2022-11-12 22:45:36 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_175.pth saved !!!
[2022-11-12 22:45:38 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.868 (1.868)	Loss 0.8500 (0.8500)	Acc@1 80.664 (80.664)	Acc@5 95.605 (95.605)	Mem 23876MB
[2022-11-12 22:45:49 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.426 Acc@5 95.566
[2022-11-12 22:45:49 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.4%
[2022-11-12 22:45:51 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.938 (1.938)	Loss 0.7372 (0.7372)	Acc@1 81.152 (81.152)	Acc@5 97.168 (97.168)	Mem 23876MB
[2022-11-12 22:46:02 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.298 Acc@5 96.352
[2022-11-12 22:46:02 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-12 22:46:02 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.30% at 175 epoch
[2022-11-12 22:46:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][0/1251]	eta 0:50:28 lr 0.000372	time 2.4211 (2.4211)	loss 3.4521 (3.4521)	grad_norm 1.8348 (1.8348)	mem 23876MB
[2022-11-12 22:46:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][50/1251]	eta 0:15:37 lr 0.000372	time 0.7411 (0.7806)	loss 3.1373 (3.0734)	grad_norm 1.6531 (1.8114)	mem 23876MB
[2022-11-12 22:47:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][100/1251]	eta 0:14:38 lr 0.000371	time 0.7367 (0.7636)	loss 3.6237 (3.0961)	grad_norm 2.0238 (1.8592)	mem 23876MB
[2022-11-12 22:47:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][150/1251]	eta 0:13:55 lr 0.000371	time 0.7403 (0.7592)	loss 3.2404 (3.0751)	grad_norm 1.7290 (1.8477)	mem 23876MB
[2022-11-12 22:48:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][200/1251]	eta 0:13:14 lr 0.000371	time 0.7322 (0.7564)	loss 3.4689 (3.0587)	grad_norm 1.6005 (1.8308)	mem 23876MB
[2022-11-12 22:49:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][250/1251]	eta 0:12:34 lr 0.000371	time 0.7363 (0.7538)	loss 2.9619 (3.0447)	grad_norm 1.9782 (1.8373)	mem 23876MB
[2022-11-12 22:49:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][300/1251]	eta 0:11:56 lr 0.000371	time 0.7381 (0.7532)	loss 2.8933 (3.0580)	grad_norm 1.8467 (1.8367)	mem 23876MB
[2022-11-12 22:50:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][350/1251]	eta 0:11:17 lr 0.000370	time 0.7426 (0.7517)	loss 3.1974 (3.0725)	grad_norm 2.0749 (1.8526)	mem 23876MB
[2022-11-12 22:51:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][400/1251]	eta 0:10:39 lr 0.000370	time 0.7330 (0.7516)	loss 2.2521 (3.0809)	grad_norm 1.5056 (1.8588)	mem 23876MB
[2022-11-12 22:51:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][450/1251]	eta 0:10:01 lr 0.000370	time 0.7354 (0.7504)	loss 3.1946 (3.0903)	grad_norm 1.6683 (1.8523)	mem 23876MB
[2022-11-12 22:52:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][500/1251]	eta 0:09:23 lr 0.000370	time 0.7461 (0.7501)	loss 2.9274 (3.0948)	grad_norm 1.7448 (1.8475)	mem 23876MB
[2022-11-12 22:52:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][550/1251]	eta 0:08:45 lr 0.000370	time 0.7391 (0.7497)	loss 3.4939 (3.0868)	grad_norm 1.6921 (1.8417)	mem 23876MB
[2022-11-12 22:53:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][600/1251]	eta 0:08:07 lr 0.000369	time 0.8343 (0.7495)	loss 3.0292 (3.0841)	grad_norm 2.0666 (1.8413)	mem 23876MB
[2022-11-12 22:54:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][650/1251]	eta 0:07:30 lr 0.000369	time 0.7416 (0.7492)	loss 2.9672 (3.0800)	grad_norm 1.9479 (1.8390)	mem 23876MB
[2022-11-12 22:54:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][700/1251]	eta 0:06:52 lr 0.000369	time 0.7405 (0.7488)	loss 2.3977 (3.0802)	grad_norm 2.0209 (1.8353)	mem 23876MB
[2022-11-12 22:55:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][750/1251]	eta 0:06:15 lr 0.000369	time 0.8068 (0.7487)	loss 3.3519 (3.0805)	grad_norm 1.6409 (1.8361)	mem 23876MB
[2022-11-12 22:56:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][800/1251]	eta 0:05:37 lr 0.000369	time 0.7353 (0.7486)	loss 2.2717 (3.0818)	grad_norm 1.9411 (1.8332)	mem 23876MB
[2022-11-12 22:56:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][850/1251]	eta 0:05:00 lr 0.000368	time 0.7396 (0.7484)	loss 3.3134 (3.0737)	grad_norm 1.9733 (1.8329)	mem 23876MB
[2022-11-12 22:57:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][900/1251]	eta 0:04:22 lr 0.000368	time 0.7404 (0.7483)	loss 2.7867 (3.0742)	grad_norm 1.5615 (1.8318)	mem 23876MB
[2022-11-12 22:57:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][950/1251]	eta 0:03:45 lr 0.000368	time 0.7445 (0.7483)	loss 3.1289 (3.0763)	grad_norm 1.8278 (1.8324)	mem 23876MB
[2022-11-12 22:58:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][1000/1251]	eta 0:03:07 lr 0.000368	time 0.7460 (0.7481)	loss 3.3297 (3.0736)	grad_norm 1.6364 (1.8355)	mem 23876MB
[2022-11-12 22:59:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][1050/1251]	eta 0:02:30 lr 0.000368	time 0.7467 (0.7482)	loss 3.3806 (3.0749)	grad_norm 1.7106 (1.8377)	mem 23876MB
[2022-11-12 22:59:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][1100/1251]	eta 0:01:52 lr 0.000368	time 0.7465 (0.7480)	loss 3.2366 (3.0731)	grad_norm 2.0483 (1.8373)	mem 23876MB
[2022-11-12 23:00:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][1150/1251]	eta 0:01:15 lr 0.000367	time 0.7455 (0.7481)	loss 2.8886 (3.0716)	grad_norm 1.6719 (1.8427)	mem 23876MB
[2022-11-12 23:01:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][1200/1251]	eta 0:00:38 lr 0.000367	time 0.7390 (0.7480)	loss 2.3310 (3.0674)	grad_norm 1.4895 (1.8417)	mem 23876MB
[2022-11-12 23:01:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [176/300][1250/1251]	eta 0:00:00 lr 0.000367	time 0.7258 (0.7478)	loss 2.5201 (3.0687)	grad_norm 1.7248 (1.8427)	mem 23876MB
[2022-11-12 23:01:37 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 176 training takes 0:15:35
[2022-11-12 23:01:38 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_176.pth saving......
[2022-11-12 23:01:39 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_176.pth saved !!!
[2022-11-12 23:01:40 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.747 (1.747)	Loss 0.8420 (0.8420)	Acc@1 79.883 (79.883)	Acc@5 95.508 (95.508)	Mem 23876MB
[2022-11-12 23:01:51 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.560 Acc@5 95.544
[2022-11-12 23:01:51 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.6%
[2022-11-12 23:01:53 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.842 (1.842)	Loss 0.7323 (0.7323)	Acc@1 82.324 (82.324)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-12 23:02:04 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.332 Acc@5 96.346
[2022-11-12 23:02:04 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-12 23:02:04 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.33% at 176 epoch
[2022-11-12 23:02:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][0/1251]	eta 0:48:12 lr 0.000367	time 2.3124 (2.3124)	loss 3.6180 (3.6180)	grad_norm 2.0241 (2.0241)	mem 23876MB
[2022-11-12 23:02:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][50/1251]	eta 0:15:37 lr 0.000367	time 0.7370 (0.7808)	loss 1.9166 (3.0123)	grad_norm 1.8905 (inf)	mem 23876MB
[2022-11-12 23:03:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][100/1251]	eta 0:14:36 lr 0.000367	time 0.7359 (0.7612)	loss 2.8679 (3.0553)	grad_norm 1.7359 (inf)	mem 23876MB
[2022-11-12 23:03:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][150/1251]	eta 0:13:52 lr 0.000366	time 0.7347 (0.7564)	loss 3.6131 (3.0516)	grad_norm 1.8311 (inf)	mem 23876MB
[2022-11-12 23:04:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][200/1251]	eta 0:13:13 lr 0.000366	time 0.7424 (0.7546)	loss 3.2548 (3.0619)	grad_norm 1.6432 (inf)	mem 23876MB
[2022-11-12 23:05:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][250/1251]	eta 0:12:33 lr 0.000366	time 0.7415 (0.7527)	loss 2.0793 (3.0867)	grad_norm 1.8383 (inf)	mem 23876MB
[2022-11-12 23:05:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][300/1251]	eta 0:11:54 lr 0.000366	time 0.7340 (0.7516)	loss 3.5526 (3.0877)	grad_norm 1.7648 (inf)	mem 23876MB
[2022-11-12 23:06:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][350/1251]	eta 0:11:16 lr 0.000366	time 0.7395 (0.7509)	loss 3.3818 (3.0598)	grad_norm 1.7031 (inf)	mem 23876MB
[2022-11-12 23:07:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][400/1251]	eta 0:10:38 lr 0.000365	time 0.7396 (0.7502)	loss 3.5943 (3.0508)	grad_norm 2.1600 (inf)	mem 23876MB
[2022-11-12 23:07:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][450/1251]	eta 0:10:00 lr 0.000365	time 0.7386 (0.7498)	loss 3.1157 (3.0550)	grad_norm 2.1085 (inf)	mem 23876MB
[2022-11-12 23:08:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][500/1251]	eta 0:09:22 lr 0.000365	time 0.7342 (0.7493)	loss 3.4868 (3.0464)	grad_norm 1.9133 (inf)	mem 23876MB
[2022-11-12 23:08:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][550/1251]	eta 0:08:45 lr 0.000365	time 0.7449 (0.7490)	loss 2.1058 (3.0601)	grad_norm 1.8309 (inf)	mem 23876MB
[2022-11-12 23:09:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][600/1251]	eta 0:08:07 lr 0.000365	time 0.7500 (0.7488)	loss 3.3983 (3.0708)	grad_norm 1.8048 (inf)	mem 23876MB
[2022-11-12 23:10:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][650/1251]	eta 0:07:29 lr 0.000364	time 0.7364 (0.7486)	loss 3.5690 (3.0799)	grad_norm 1.9655 (inf)	mem 23876MB
[2022-11-12 23:10:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][700/1251]	eta 0:06:52 lr 0.000364	time 0.7397 (0.7484)	loss 2.3841 (3.0790)	grad_norm 1.6995 (inf)	mem 23876MB
[2022-11-12 23:11:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][750/1251]	eta 0:06:14 lr 0.000364	time 0.7361 (0.7484)	loss 3.3313 (3.0798)	grad_norm 1.7283 (inf)	mem 23876MB
[2022-11-12 23:12:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][800/1251]	eta 0:05:37 lr 0.000364	time 0.7381 (0.7481)	loss 3.5478 (3.0771)	grad_norm 1.8355 (inf)	mem 23876MB
[2022-11-12 23:12:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][850/1251]	eta 0:05:00 lr 0.000364	time 0.7426 (0.7482)	loss 3.2192 (3.0722)	grad_norm 2.4836 (inf)	mem 23876MB
[2022-11-12 23:13:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][900/1251]	eta 0:04:22 lr 0.000363	time 0.7378 (0.7479)	loss 3.2835 (3.0744)	grad_norm 1.9081 (inf)	mem 23876MB
[2022-11-12 23:13:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][950/1251]	eta 0:03:45 lr 0.000363	time 0.7496 (0.7479)	loss 2.5701 (3.0768)	grad_norm 1.8767 (inf)	mem 23876MB
[2022-11-12 23:14:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][1000/1251]	eta 0:03:07 lr 0.000363	time 0.7385 (0.7479)	loss 3.6338 (3.0781)	grad_norm 2.1322 (inf)	mem 23876MB
[2022-11-12 23:15:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][1050/1251]	eta 0:02:30 lr 0.000363	time 0.7415 (0.7478)	loss 3.0958 (3.0752)	grad_norm 1.7627 (inf)	mem 23876MB
[2022-11-12 23:15:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][1100/1251]	eta 0:01:52 lr 0.000363	time 0.7349 (0.7477)	loss 2.7012 (3.0747)	grad_norm 1.8506 (inf)	mem 23876MB
[2022-11-12 23:16:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][1150/1251]	eta 0:01:15 lr 0.000362	time 0.7343 (0.7477)	loss 3.0519 (3.0732)	grad_norm 1.7806 (inf)	mem 23876MB
[2022-11-12 23:17:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][1200/1251]	eta 0:00:38 lr 0.000362	time 0.7448 (0.7476)	loss 2.8069 (3.0708)	grad_norm 1.8168 (inf)	mem 23876MB
[2022-11-12 23:17:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [177/300][1250/1251]	eta 0:00:00 lr 0.000362	time 0.7275 (0.7475)	loss 2.7564 (3.0719)	grad_norm 1.8913 (inf)	mem 23876MB
[2022-11-12 23:17:39 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 177 training takes 0:15:35
[2022-11-12 23:17:39 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_177.pth saving......
[2022-11-12 23:17:40 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_177.pth saved !!!
[2022-11-12 23:17:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.599 (1.599)	Loss 0.7589 (0.7589)	Acc@1 82.324 (82.324)	Acc@5 96.191 (96.191)	Mem 23876MB
[2022-11-12 23:17:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.580 Acc@5 95.604
[2022-11-12 23:17:53 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.6%
[2022-11-12 23:17:54 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.731 (1.731)	Loss 0.8269 (0.8269)	Acc@1 80.957 (80.957)	Acc@5 95.020 (95.020)	Mem 23876MB
[2022-11-12 23:18:05 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.336 Acc@5 96.356
[2022-11-12 23:18:05 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-12 23:18:05 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.34% at 177 epoch
[2022-11-12 23:18:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][0/1251]	eta 0:51:56 lr 0.000362	time 2.4908 (2.4908)	loss 2.1408 (2.1408)	grad_norm 1.7269 (1.7269)	mem 23876MB
[2022-11-12 23:18:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][50/1251]	eta 0:15:41 lr 0.000362	time 0.7411 (0.7837)	loss 3.0136 (3.0514)	grad_norm 1.8196 (1.8941)	mem 23876MB
[2022-11-12 23:19:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][100/1251]	eta 0:14:41 lr 0.000362	time 0.7374 (0.7662)	loss 3.6103 (3.0690)	grad_norm 1.8931 (1.8591)	mem 23876MB
[2022-11-12 23:20:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][150/1251]	eta 0:13:57 lr 0.000361	time 0.7365 (0.7604)	loss 3.3060 (3.0856)	grad_norm 1.8759 (1.8520)	mem 23876MB
[2022-11-12 23:20:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][200/1251]	eta 0:13:14 lr 0.000361	time 0.8208 (0.7563)	loss 3.2222 (3.0908)	grad_norm 1.7541 (nan)	mem 23876MB
[2022-11-12 23:21:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][250/1251]	eta 0:12:35 lr 0.000361	time 0.7420 (0.7547)	loss 2.1945 (3.0609)	grad_norm 1.8434 (nan)	mem 23876MB
[2022-11-12 23:21:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][300/1251]	eta 0:11:56 lr 0.000361	time 0.7361 (0.7533)	loss 3.5308 (3.0746)	grad_norm 1.7747 (nan)	mem 23876MB
[2022-11-12 23:22:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][350/1251]	eta 0:11:18 lr 0.000361	time 0.7361 (0.7526)	loss 2.9760 (3.0829)	grad_norm 1.9118 (nan)	mem 23876MB
[2022-11-12 23:23:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][400/1251]	eta 0:10:39 lr 0.000360	time 0.7343 (0.7519)	loss 2.7734 (3.0657)	grad_norm 2.3731 (nan)	mem 23876MB
[2022-11-12 23:23:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][450/1251]	eta 0:10:02 lr 0.000360	time 0.7392 (0.7518)	loss 3.2699 (3.0734)	grad_norm 1.7353 (nan)	mem 23876MB
[2022-11-12 23:24:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][500/1251]	eta 0:09:23 lr 0.000360	time 0.7407 (0.7508)	loss 2.2256 (3.0692)	grad_norm 1.8590 (nan)	mem 23876MB
[2022-11-12 23:24:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][550/1251]	eta 0:08:46 lr 0.000360	time 0.7391 (0.7507)	loss 2.2619 (3.0647)	grad_norm 1.7376 (nan)	mem 23876MB
[2022-11-12 23:25:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][600/1251]	eta 0:08:08 lr 0.000360	time 0.7283 (0.7505)	loss 3.3306 (3.0603)	grad_norm 1.7789 (nan)	mem 23876MB
[2022-11-12 23:26:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][650/1251]	eta 0:07:31 lr 0.000359	time 0.7411 (0.7505)	loss 3.1288 (3.0600)	grad_norm 1.6226 (nan)	mem 23876MB
[2022-11-12 23:26:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][700/1251]	eta 0:06:53 lr 0.000359	time 0.7393 (0.7502)	loss 2.7401 (3.0607)	grad_norm 1.8627 (nan)	mem 23876MB
[2022-11-12 23:27:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][750/1251]	eta 0:06:15 lr 0.000359	time 0.8260 (0.7501)	loss 3.5934 (3.0657)	grad_norm 2.2255 (nan)	mem 23876MB
[2022-11-12 23:28:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][800/1251]	eta 0:05:38 lr 0.000359	time 0.7438 (0.7499)	loss 2.8468 (3.0720)	grad_norm 1.8712 (nan)	mem 23876MB
[2022-11-12 23:28:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][850/1251]	eta 0:05:00 lr 0.000359	time 0.7411 (0.7498)	loss 3.3177 (3.0722)	grad_norm 1.8483 (nan)	mem 23876MB
[2022-11-12 23:29:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][900/1251]	eta 0:04:23 lr 0.000358	time 0.7403 (0.7497)	loss 3.4130 (3.0760)	grad_norm 1.7248 (nan)	mem 23876MB
[2022-11-12 23:29:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][950/1251]	eta 0:03:45 lr 0.000358	time 0.7378 (0.7497)	loss 3.4664 (3.0715)	grad_norm 1.8967 (nan)	mem 23876MB
[2022-11-12 23:30:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][1000/1251]	eta 0:03:08 lr 0.000358	time 0.8163 (0.7494)	loss 2.3594 (3.0689)	grad_norm 1.6509 (nan)	mem 23876MB
[2022-11-12 23:31:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][1050/1251]	eta 0:02:30 lr 0.000358	time 0.7356 (0.7493)	loss 3.5564 (3.0764)	grad_norm 2.0757 (nan)	mem 23876MB
[2022-11-12 23:31:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][1100/1251]	eta 0:01:53 lr 0.000358	time 0.7462 (0.7490)	loss 3.1767 (3.0765)	grad_norm 1.7557 (nan)	mem 23876MB
[2022-11-12 23:32:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][1150/1251]	eta 0:01:15 lr 0.000357	time 0.7373 (0.7491)	loss 3.0901 (3.0747)	grad_norm 1.7301 (nan)	mem 23876MB
[2022-11-12 23:33:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][1200/1251]	eta 0:00:38 lr 0.000357	time 0.7370 (0.7490)	loss 3.6159 (3.0738)	grad_norm 1.9314 (nan)	mem 23876MB
[2022-11-12 23:33:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [178/300][1250/1251]	eta 0:00:00 lr 0.000357	time 0.7258 (0.7488)	loss 3.0275 (3.0731)	grad_norm 2.0996 (nan)	mem 23876MB
[2022-11-12 23:33:42 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 178 training takes 0:15:36
[2022-11-12 23:33:42 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_178.pth saving......
[2022-11-12 23:33:43 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_178.pth saved !!!
[2022-11-12 23:33:45 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.734 (1.734)	Loss 0.7998 (0.7998)	Acc@1 80.762 (80.762)	Acc@5 96.191 (96.191)	Mem 23876MB
[2022-11-12 23:33:56 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.668 Acc@5 95.570
[2022-11-12 23:33:56 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.7%
[2022-11-12 23:33:58 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.866 (1.866)	Loss 0.7517 (0.7517)	Acc@1 82.129 (82.129)	Acc@5 95.508 (95.508)	Mem 23876MB
[2022-11-12 23:34:08 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.336 Acc@5 96.362
[2022-11-12 23:34:08 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.3%
[2022-11-12 23:34:08 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.34% at 177 epoch
[2022-11-12 23:34:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][0/1251]	eta 0:49:45 lr 0.000357	time 2.3864 (2.3864)	loss 3.3138 (3.3138)	grad_norm 1.9802 (1.9802)	mem 23876MB
[2022-11-12 23:34:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][50/1251]	eta 0:15:41 lr 0.000357	time 0.8219 (0.7838)	loss 2.3354 (3.1188)	grad_norm 2.0287 (2.0403)	mem 23876MB
[2022-11-12 23:35:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][100/1251]	eta 0:14:39 lr 0.000357	time 0.7355 (0.7643)	loss 2.8612 (3.0734)	grad_norm 1.5041 (1.9785)	mem 23876MB
[2022-11-12 23:36:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][150/1251]	eta 0:13:55 lr 0.000356	time 0.7465 (0.7586)	loss 3.4871 (3.1019)	grad_norm 1.6024 (1.9493)	mem 23876MB
[2022-11-12 23:36:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][200/1251]	eta 0:13:13 lr 0.000356	time 0.7425 (0.7552)	loss 2.2201 (3.0985)	grad_norm 1.7453 (1.9117)	mem 23876MB
[2022-11-12 23:37:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][250/1251]	eta 0:12:34 lr 0.000356	time 0.7533 (0.7541)	loss 3.6720 (3.0929)	grad_norm 2.1087 (1.8947)	mem 23876MB
[2022-11-12 23:37:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][300/1251]	eta 0:11:56 lr 0.000356	time 0.7381 (0.7530)	loss 2.6065 (3.0755)	grad_norm 3.1730 (1.8828)	mem 23876MB
[2022-11-12 23:38:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][350/1251]	eta 0:11:17 lr 0.000356	time 0.7376 (0.7518)	loss 3.5836 (3.0707)	grad_norm 2.1839 (1.8814)	mem 23876MB
[2022-11-12 23:39:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][400/1251]	eta 0:10:39 lr 0.000355	time 0.7329 (0.7513)	loss 2.7802 (3.0675)	grad_norm 1.8682 (1.8775)	mem 23876MB
[2022-11-12 23:39:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][450/1251]	eta 0:10:01 lr 0.000355	time 0.7358 (0.7506)	loss 3.4742 (3.0669)	grad_norm 1.7734 (1.8822)	mem 23876MB
[2022-11-12 23:40:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][500/1251]	eta 0:09:23 lr 0.000355	time 0.7406 (0.7501)	loss 3.3244 (3.0632)	grad_norm 1.8148 (1.8735)	mem 23876MB
[2022-11-12 23:41:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][550/1251]	eta 0:08:45 lr 0.000355	time 0.7364 (0.7499)	loss 3.0861 (3.0689)	grad_norm 1.7933 (1.8693)	mem 23876MB
[2022-11-12 23:41:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][600/1251]	eta 0:08:07 lr 0.000355	time 0.7374 (0.7496)	loss 3.1195 (3.0723)	grad_norm 1.8018 (1.8637)	mem 23876MB
[2022-11-12 23:42:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][650/1251]	eta 0:07:30 lr 0.000354	time 0.7464 (0.7494)	loss 3.6902 (3.0769)	grad_norm 2.0863 (1.8634)	mem 23876MB
[2022-11-12 23:42:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][700/1251]	eta 0:06:52 lr 0.000354	time 0.7415 (0.7492)	loss 3.1486 (3.0747)	grad_norm 1.7392 (1.8583)	mem 23876MB
[2022-11-12 23:43:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][750/1251]	eta 0:06:15 lr 0.000354	time 0.7375 (0.7488)	loss 3.5275 (3.0698)	grad_norm 1.9108 (1.8506)	mem 23876MB
[2022-11-12 23:44:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][800/1251]	eta 0:05:37 lr 0.000354	time 0.7369 (0.7487)	loss 3.4890 (3.0704)	grad_norm 1.7195 (1.8490)	mem 23876MB
[2022-11-12 23:44:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][850/1251]	eta 0:05:00 lr 0.000354	time 0.7381 (0.7486)	loss 2.8097 (3.0656)	grad_norm 1.6749 (1.8463)	mem 23876MB
[2022-11-12 23:45:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][900/1251]	eta 0:04:22 lr 0.000353	time 0.7438 (0.7484)	loss 3.3480 (3.0605)	grad_norm 2.2583 (1.8489)	mem 23876MB
[2022-11-12 23:46:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][950/1251]	eta 0:03:45 lr 0.000353	time 0.8081 (0.7484)	loss 2.7799 (3.0569)	grad_norm 2.0310 (1.8493)	mem 23876MB
[2022-11-12 23:46:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][1000/1251]	eta 0:03:07 lr 0.000353	time 0.7521 (0.7481)	loss 3.3707 (3.0616)	grad_norm 1.9822 (1.8498)	mem 23876MB
[2022-11-12 23:47:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][1050/1251]	eta 0:02:30 lr 0.000353	time 0.7400 (0.7481)	loss 3.6711 (3.0555)	grad_norm 3.7493 (1.8529)	mem 23876MB
[2022-11-12 23:47:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][1100/1251]	eta 0:01:52 lr 0.000353	time 0.7372 (0.7481)	loss 3.4567 (3.0552)	grad_norm 3.2469 (1.8546)	mem 23876MB
[2022-11-12 23:48:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][1150/1251]	eta 0:01:15 lr 0.000352	time 0.7381 (0.7478)	loss 3.4109 (3.0561)	grad_norm 1.7141 (1.8523)	mem 23876MB
[2022-11-12 23:49:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][1200/1251]	eta 0:00:38 lr 0.000352	time 0.7348 (0.7480)	loss 3.4355 (3.0570)	grad_norm 2.1583 (1.8531)	mem 23876MB
[2022-11-12 23:49:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [179/300][1250/1251]	eta 0:00:00 lr 0.000352	time 0.7246 (0.7477)	loss 3.2144 (3.0562)	grad_norm 2.5308 (1.8526)	mem 23876MB
[2022-11-12 23:49:44 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 179 training takes 0:15:35
[2022-11-12 23:49:44 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_179.pth saving......
[2022-11-12 23:49:45 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_179.pth saved !!!
[2022-11-12 23:49:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.640 (1.640)	Loss 0.7759 (0.7759)	Acc@1 81.641 (81.641)	Acc@5 95.312 (95.312)	Mem 23876MB
[2022-11-12 23:49:57 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.690 Acc@5 95.688
[2022-11-12 23:49:57 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.7%
[2022-11-12 23:49:59 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.877 (1.877)	Loss 0.6542 (0.6542)	Acc@1 82.910 (82.910)	Acc@5 97.461 (97.461)	Mem 23876MB
[2022-11-12 23:50:10 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.360 Acc@5 96.374
[2022-11-12 23:50:10 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-12 23:50:10 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.36% at 179 epoch
[2022-11-12 23:50:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][0/1251]	eta 0:50:23 lr 0.000352	time 2.4166 (2.4166)	loss 3.2012 (3.2012)	grad_norm 2.1133 (2.1133)	mem 23876MB
[2022-11-12 23:50:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][50/1251]	eta 0:15:41 lr 0.000352	time 0.7421 (0.7836)	loss 2.4973 (3.0735)	grad_norm 1.8176 (1.8758)	mem 23876MB
[2022-11-12 23:51:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][100/1251]	eta 0:14:41 lr 0.000352	time 0.7432 (0.7663)	loss 2.5078 (3.0596)	grad_norm 1.5770 (1.8243)	mem 23876MB
[2022-11-12 23:52:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][150/1251]	eta 0:13:55 lr 0.000351	time 0.7457 (0.7589)	loss 2.1567 (3.0255)	grad_norm 1.7474 (1.8061)	mem 23876MB
[2022-11-12 23:52:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][200/1251]	eta 0:13:15 lr 0.000351	time 0.7359 (0.7569)	loss 2.7858 (3.0420)	grad_norm 2.1014 (1.8138)	mem 23876MB
[2022-11-12 23:53:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][250/1251]	eta 0:12:35 lr 0.000351	time 0.7339 (0.7550)	loss 3.3575 (3.0331)	grad_norm 2.3471 (1.8299)	mem 23876MB
[2022-11-12 23:53:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][300/1251]	eta 0:11:56 lr 0.000351	time 0.7388 (0.7536)	loss 2.9986 (3.0349)	grad_norm 1.8998 (1.8447)	mem 23876MB
[2022-11-12 23:54:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][350/1251]	eta 0:11:18 lr 0.000351	time 0.7388 (0.7529)	loss 2.7968 (3.0358)	grad_norm 2.1439 (1.8491)	mem 23876MB
[2022-11-12 23:55:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][400/1251]	eta 0:10:40 lr 0.000350	time 0.7388 (0.7521)	loss 3.3457 (3.0393)	grad_norm 1.5213 (1.8507)	mem 23876MB
[2022-11-12 23:55:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][450/1251]	eta 0:10:01 lr 0.000350	time 0.7390 (0.7513)	loss 2.6045 (3.0437)	grad_norm 1.9325 (1.8522)	mem 23876MB
[2022-11-12 23:56:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][500/1251]	eta 0:09:24 lr 0.000350	time 0.7383 (0.7511)	loss 1.8533 (3.0338)	grad_norm 1.6446 (1.8431)	mem 23876MB
[2022-11-12 23:57:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][550/1251]	eta 0:08:46 lr 0.000350	time 0.7363 (0.7507)	loss 3.3866 (3.0390)	grad_norm 1.7656 (1.8452)	mem 23876MB
[2022-11-12 23:57:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][600/1251]	eta 0:08:08 lr 0.000350	time 0.8064 (0.7505)	loss 2.6499 (3.0466)	grad_norm 1.7566 (1.8470)	mem 23876MB
[2022-11-12 23:58:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][650/1251]	eta 0:07:30 lr 0.000349	time 0.7378 (0.7501)	loss 2.7458 (3.0454)	grad_norm 2.0613 (1.8442)	mem 23876MB
[2022-11-12 23:58:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][700/1251]	eta 0:06:53 lr 0.000349	time 0.7422 (0.7497)	loss 3.1963 (3.0501)	grad_norm 1.6480 (1.8438)	mem 23876MB
[2022-11-12 23:59:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][750/1251]	eta 0:06:15 lr 0.000349	time 0.8196 (0.7495)	loss 2.9233 (3.0486)	grad_norm 1.7616 (1.8427)	mem 23876MB
[2022-11-13 00:00:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][800/1251]	eta 0:05:37 lr 0.000349	time 0.7282 (0.7493)	loss 2.6778 (3.0527)	grad_norm 1.7731 (1.8443)	mem 23876MB
[2022-11-13 00:00:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][850/1251]	eta 0:05:00 lr 0.000349	time 0.7546 (0.7493)	loss 2.7439 (3.0561)	grad_norm 2.1663 (1.8451)	mem 23876MB
[2022-11-13 00:01:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][900/1251]	eta 0:04:22 lr 0.000348	time 0.7378 (0.7490)	loss 3.3982 (3.0586)	grad_norm 1.7733 (1.8431)	mem 23876MB
[2022-11-13 00:02:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][950/1251]	eta 0:03:45 lr 0.000348	time 0.8071 (0.7489)	loss 3.0677 (3.0535)	grad_norm 1.8697 (1.8445)	mem 23876MB
[2022-11-13 00:02:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][1000/1251]	eta 0:03:07 lr 0.000348	time 0.7393 (0.7486)	loss 3.2140 (3.0524)	grad_norm 1.7669 (1.8478)	mem 23876MB
[2022-11-13 00:03:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][1050/1251]	eta 0:02:30 lr 0.000348	time 0.7432 (0.7487)	loss 3.8661 (3.0535)	grad_norm 1.9782 (1.8497)	mem 23876MB
[2022-11-13 00:03:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][1100/1251]	eta 0:01:53 lr 0.000348	time 0.7339 (0.7485)	loss 3.7558 (3.0561)	grad_norm 2.0154 (1.8491)	mem 23876MB
[2022-11-13 00:04:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][1150/1251]	eta 0:01:15 lr 0.000348	time 0.7355 (0.7485)	loss 2.9628 (3.0564)	grad_norm 1.7958 (1.8494)	mem 23876MB
[2022-11-13 00:05:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][1200/1251]	eta 0:00:38 lr 0.000347	time 0.7468 (0.7484)	loss 3.3981 (3.0549)	grad_norm 2.2286 (1.8496)	mem 23876MB
[2022-11-13 00:05:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [180/300][1250/1251]	eta 0:00:00 lr 0.000347	time 0.7252 (0.7481)	loss 3.0007 (3.0603)	grad_norm 1.7301 (1.8491)	mem 23876MB
[2022-11-13 00:05:46 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 180 training takes 0:15:35
[2022-11-13 00:05:46 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_180.pth saving......
[2022-11-13 00:05:47 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_180.pth saved !!!
[2022-11-13 00:05:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.756 (1.756)	Loss 0.7349 (0.7349)	Acc@1 81.641 (81.641)	Acc@5 96.094 (96.094)	Mem 23876MB
[2022-11-13 00:06:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.784 Acc@5 95.538
[2022-11-13 00:06:00 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.8%
[2022-11-13 00:06:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.969 (1.969)	Loss 0.7678 (0.7678)	Acc@1 80.176 (80.176)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-13 00:06:12 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.388 Acc@5 96.386
[2022-11-13 00:06:12 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-13 00:06:12 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.39% at 180 epoch
[2022-11-13 00:06:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][0/1251]	eta 0:51:37 lr 0.000347	time 2.4762 (2.4762)	loss 3.6074 (3.6074)	grad_norm 1.8111 (1.8111)	mem 23876MB
[2022-11-13 00:06:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][50/1251]	eta 0:15:36 lr 0.000347	time 0.7388 (0.7796)	loss 3.3329 (3.1745)	grad_norm 1.7686 (1.8893)	mem 23876MB
[2022-11-13 00:07:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][100/1251]	eta 0:14:39 lr 0.000347	time 0.7401 (0.7641)	loss 2.1167 (3.1387)	grad_norm 1.9625 (1.9015)	mem 23876MB
[2022-11-13 00:08:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][150/1251]	eta 0:13:53 lr 0.000347	time 0.7345 (0.7575)	loss 2.8960 (3.1180)	grad_norm 1.7308 (1.8769)	mem 23876MB
[2022-11-13 00:08:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][200/1251]	eta 0:13:13 lr 0.000346	time 0.7419 (0.7553)	loss 3.2369 (3.1271)	grad_norm 2.2540 (1.8864)	mem 23876MB
[2022-11-13 00:09:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][250/1251]	eta 0:12:33 lr 0.000346	time 0.7420 (0.7528)	loss 2.0490 (3.1042)	grad_norm 1.8359 (1.8745)	mem 23876MB
[2022-11-13 00:09:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][300/1251]	eta 0:11:55 lr 0.000346	time 0.7428 (0.7519)	loss 2.2921 (3.0975)	grad_norm 1.9832 (1.8803)	mem 23876MB
[2022-11-13 00:10:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][350/1251]	eta 0:11:17 lr 0.000346	time 0.7410 (0.7516)	loss 2.5013 (3.0796)	grad_norm 1.8891 (1.8998)	mem 23876MB
[2022-11-13 00:11:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][400/1251]	eta 0:10:38 lr 0.000346	time 0.7286 (0.7508)	loss 2.8170 (3.0685)	grad_norm 1.6618 (1.8995)	mem 23876MB
[2022-11-13 00:11:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][450/1251]	eta 0:10:01 lr 0.000345	time 0.7405 (0.7504)	loss 2.2403 (3.0730)	grad_norm 1.8354 (1.8912)	mem 23876MB
[2022-11-13 00:12:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][500/1251]	eta 0:09:23 lr 0.000345	time 0.7390 (0.7500)	loss 3.2583 (3.0790)	grad_norm 1.7858 (1.8974)	mem 23876MB
[2022-11-13 00:13:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][550/1251]	eta 0:08:45 lr 0.000345	time 0.7444 (0.7496)	loss 2.3343 (3.0784)	grad_norm 2.0683 (1.8984)	mem 23876MB
[2022-11-13 00:13:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][600/1251]	eta 0:08:07 lr 0.000345	time 0.8196 (0.7495)	loss 3.2952 (3.0758)	grad_norm 1.9762 (1.9011)	mem 23876MB
[2022-11-13 00:14:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][650/1251]	eta 0:07:30 lr 0.000345	time 0.7375 (0.7491)	loss 3.2288 (3.0746)	grad_norm 1.7317 (1.9012)	mem 23876MB
[2022-11-13 00:14:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][700/1251]	eta 0:06:52 lr 0.000344	time 0.7395 (0.7488)	loss 3.3066 (3.0764)	grad_norm 1.9555 (1.9034)	mem 23876MB
[2022-11-13 00:15:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][750/1251]	eta 0:06:15 lr 0.000344	time 0.8209 (0.7490)	loss 3.4585 (3.0687)	grad_norm 1.9266 (1.9060)	mem 23876MB
[2022-11-13 00:16:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][800/1251]	eta 0:05:37 lr 0.000344	time 0.7366 (0.7483)	loss 3.2111 (3.0667)	grad_norm 2.0003 (1.9056)	mem 23876MB
[2022-11-13 00:16:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][850/1251]	eta 0:05:00 lr 0.000344	time 0.7419 (0.7485)	loss 3.2792 (3.0651)	grad_norm 1.6538 (1.9045)	mem 23876MB
[2022-11-13 00:17:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][900/1251]	eta 0:04:22 lr 0.000344	time 0.7396 (0.7482)	loss 3.2252 (3.0597)	grad_norm 1.6285 (1.8999)	mem 23876MB
[2022-11-13 00:18:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][950/1251]	eta 0:03:45 lr 0.000343	time 0.7362 (0.7483)	loss 3.3979 (3.0590)	grad_norm 1.5427 (1.9044)	mem 23876MB
[2022-11-13 00:18:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][1000/1251]	eta 0:03:07 lr 0.000343	time 0.7408 (0.7482)	loss 2.6455 (3.0576)	grad_norm 1.6053 (1.9034)	mem 23876MB
[2022-11-13 00:19:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][1050/1251]	eta 0:02:30 lr 0.000343	time 0.7393 (0.7482)	loss 3.8201 (3.0583)	grad_norm 1.8565 (1.9005)	mem 23876MB
[2022-11-13 00:19:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][1100/1251]	eta 0:01:52 lr 0.000343	time 0.7406 (0.7479)	loss 2.3986 (3.0596)	grad_norm 1.7103 (1.8968)	mem 23876MB
[2022-11-13 00:20:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][1150/1251]	eta 0:01:15 lr 0.000343	time 0.7407 (0.7481)	loss 3.1467 (3.0579)	grad_norm 1.5028 (1.8967)	mem 23876MB
[2022-11-13 00:21:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][1200/1251]	eta 0:00:38 lr 0.000342	time 0.7402 (0.7480)	loss 2.6836 (3.0578)	grad_norm 2.0412 (1.8955)	mem 23876MB
[2022-11-13 00:21:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [181/300][1250/1251]	eta 0:00:00 lr 0.000342	time 0.7249 (0.7479)	loss 3.3331 (3.0564)	grad_norm 1.8263 (1.8962)	mem 23876MB
[2022-11-13 00:21:48 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 181 training takes 0:15:35
[2022-11-13 00:21:48 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_181.pth saving......
[2022-11-13 00:21:49 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_181.pth saved !!!
[2022-11-13 00:21:51 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.606 (1.606)	Loss 0.7910 (0.7910)	Acc@1 81.934 (81.934)	Acc@5 95.117 (95.117)	Mem 23876MB
[2022-11-13 00:22:02 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.732 Acc@5 95.620
[2022-11-13 00:22:02 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.7%
[2022-11-13 00:22:03 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.831 (1.831)	Loss 0.7524 (0.7524)	Acc@1 82.129 (82.129)	Acc@5 96.484 (96.484)	Mem 23876MB
[2022-11-13 00:22:14 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.404 Acc@5 96.384
[2022-11-13 00:22:14 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-13 00:22:14 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.40% at 181 epoch
[2022-11-13 00:22:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][0/1251]	eta 0:51:40 lr 0.000342	time 2.4783 (2.4783)	loss 1.9566 (1.9566)	grad_norm 2.0139 (2.0139)	mem 23876MB
[2022-11-13 00:22:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][50/1251]	eta 0:15:45 lr 0.000342	time 0.7386 (0.7873)	loss 3.6074 (3.0770)	grad_norm 2.0055 (1.8921)	mem 23876MB
[2022-11-13 00:23:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][100/1251]	eta 0:14:42 lr 0.000342	time 0.7420 (0.7668)	loss 3.4835 (3.0619)	grad_norm 2.0020 (1.8922)	mem 23876MB
[2022-11-13 00:24:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][150/1251]	eta 0:13:58 lr 0.000342	time 0.7498 (0.7616)	loss 2.0348 (3.0694)	grad_norm 1.6878 (1.8794)	mem 23876MB
[2022-11-13 00:24:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][200/1251]	eta 0:13:17 lr 0.000341	time 0.7383 (0.7584)	loss 3.1547 (3.0494)	grad_norm 1.8692 (1.8743)	mem 23876MB
[2022-11-13 00:25:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][250/1251]	eta 0:12:36 lr 0.000341	time 0.7387 (0.7555)	loss 2.8381 (3.0622)	grad_norm 1.7358 (1.8744)	mem 23876MB
[2022-11-13 00:26:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][300/1251]	eta 0:11:57 lr 0.000341	time 0.7423 (0.7543)	loss 3.4241 (3.0589)	grad_norm 1.8744 (1.8750)	mem 23876MB
[2022-11-13 00:26:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][350/1251]	eta 0:11:18 lr 0.000341	time 0.7341 (0.7532)	loss 3.9195 (3.0786)	grad_norm 2.2772 (1.8832)	mem 23876MB
[2022-11-13 00:27:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][400/1251]	eta 0:10:40 lr 0.000341	time 0.7428 (0.7523)	loss 3.4974 (3.0796)	grad_norm 1.7076 (nan)	mem 23876MB
[2022-11-13 00:27:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][450/1251]	eta 0:10:02 lr 0.000340	time 0.7364 (0.7520)	loss 3.3113 (3.0726)	grad_norm 1.8437 (nan)	mem 23876MB
[2022-11-13 00:28:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][500/1251]	eta 0:09:24 lr 0.000340	time 0.7344 (0.7514)	loss 3.4017 (3.0720)	grad_norm 2.0161 (nan)	mem 23876MB
[2022-11-13 00:29:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][550/1251]	eta 0:08:46 lr 0.000340	time 0.7378 (0.7511)	loss 2.6745 (3.0772)	grad_norm 1.4877 (nan)	mem 23876MB
[2022-11-13 00:29:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][600/1251]	eta 0:08:08 lr 0.000340	time 0.7384 (0.7510)	loss 3.3280 (3.0774)	grad_norm 1.8268 (nan)	mem 23876MB
[2022-11-13 00:30:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][650/1251]	eta 0:07:30 lr 0.000340	time 0.7393 (0.7504)	loss 2.7316 (3.0786)	grad_norm 1.7969 (nan)	mem 23876MB
[2022-11-13 00:31:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][700/1251]	eta 0:06:53 lr 0.000339	time 0.7399 (0.7504)	loss 2.9074 (3.0782)	grad_norm 1.8237 (nan)	mem 23876MB
[2022-11-13 00:31:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][750/1251]	eta 0:06:15 lr 0.000339	time 0.7425 (0.7499)	loss 2.6892 (3.0708)	grad_norm 2.1102 (nan)	mem 23876MB
[2022-11-13 00:32:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][800/1251]	eta 0:05:38 lr 0.000339	time 0.7374 (0.7499)	loss 2.3232 (3.0600)	grad_norm 1.6864 (nan)	mem 23876MB
[2022-11-13 00:32:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][850/1251]	eta 0:05:00 lr 0.000339	time 0.7418 (0.7497)	loss 3.1758 (3.0552)	grad_norm 1.8902 (nan)	mem 23876MB
[2022-11-13 00:33:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][900/1251]	eta 0:04:23 lr 0.000339	time 0.7425 (0.7495)	loss 3.2257 (3.0523)	grad_norm 1.7914 (nan)	mem 23876MB
[2022-11-13 00:34:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][950/1251]	eta 0:03:45 lr 0.000338	time 0.7388 (0.7494)	loss 2.6253 (3.0542)	grad_norm 1.9509 (nan)	mem 23876MB
[2022-11-13 00:34:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][1000/1251]	eta 0:03:08 lr 0.000338	time 0.8096 (0.7494)	loss 3.4288 (3.0549)	grad_norm 2.0391 (nan)	mem 23876MB
[2022-11-13 00:35:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][1050/1251]	eta 0:02:30 lr 0.000338	time 0.7418 (0.7491)	loss 3.2250 (3.0555)	grad_norm 1.7094 (nan)	mem 23876MB
[2022-11-13 00:35:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][1100/1251]	eta 0:01:53 lr 0.000338	time 0.7342 (0.7491)	loss 3.4634 (3.0610)	grad_norm 1.7569 (nan)	mem 23876MB
[2022-11-13 00:36:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][1150/1251]	eta 0:01:15 lr 0.000338	time 0.7438 (0.7489)	loss 3.4331 (3.0619)	grad_norm 1.8945 (nan)	mem 23876MB
[2022-11-13 00:37:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][1200/1251]	eta 0:00:38 lr 0.000338	time 0.7355 (0.7490)	loss 3.1886 (3.0596)	grad_norm 2.2088 (nan)	mem 23876MB
[2022-11-13 00:37:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [182/300][1250/1251]	eta 0:00:00 lr 0.000337	time 0.7269 (0.7488)	loss 1.7848 (3.0571)	grad_norm 1.7271 (nan)	mem 23876MB
[2022-11-13 00:37:51 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 182 training takes 0:15:36
[2022-11-13 00:37:51 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_182.pth saving......
[2022-11-13 00:37:52 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_182.pth saved !!!
[2022-11-13 00:37:54 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.770 (1.770)	Loss 0.7666 (0.7666)	Acc@1 81.445 (81.445)	Acc@5 95.996 (95.996)	Mem 23876MB
[2022-11-13 00:38:05 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.748 Acc@5 95.576
[2022-11-13 00:38:05 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.7%
[2022-11-13 00:38:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.842 (1.842)	Loss 0.7016 (0.7016)	Acc@1 82.617 (82.617)	Acc@5 96.289 (96.289)	Mem 23876MB
[2022-11-13 00:38:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.426 Acc@5 96.402
[2022-11-13 00:38:17 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.4%
[2022-11-13 00:38:17 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.43% at 182 epoch
[2022-11-13 00:38:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][0/1251]	eta 0:51:25 lr 0.000337	time 2.4663 (2.4663)	loss 2.9057 (2.9057)	grad_norm 1.8338 (1.8338)	mem 23876MB
[2022-11-13 00:38:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][50/1251]	eta 0:15:43 lr 0.000337	time 0.8219 (0.7854)	loss 3.0153 (3.0398)	grad_norm 1.6368 (1.8640)	mem 23876MB
[2022-11-13 00:39:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][100/1251]	eta 0:14:42 lr 0.000337	time 0.7384 (0.7668)	loss 2.5752 (2.9826)	grad_norm 1.7665 (1.8914)	mem 23876MB
[2022-11-13 00:40:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][150/1251]	eta 0:13:56 lr 0.000337	time 0.7257 (0.7596)	loss 3.5431 (3.0512)	grad_norm 1.8967 (1.8646)	mem 23876MB
[2022-11-13 00:40:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][200/1251]	eta 0:13:15 lr 0.000337	time 0.8218 (0.7569)	loss 3.1307 (3.0734)	grad_norm 1.9632 (1.8544)	mem 23876MB
[2022-11-13 00:41:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][250/1251]	eta 0:12:35 lr 0.000336	time 0.7419 (0.7548)	loss 3.4568 (3.0801)	grad_norm 1.6095 (1.8578)	mem 23876MB
[2022-11-13 00:42:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][300/1251]	eta 0:11:56 lr 0.000336	time 0.7461 (0.7538)	loss 2.7585 (3.0674)	grad_norm 1.6829 (1.8538)	mem 23876MB
[2022-11-13 00:42:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][350/1251]	eta 0:11:18 lr 0.000336	time 0.7427 (0.7533)	loss 2.5337 (3.0575)	grad_norm 1.8939 (1.8579)	mem 23876MB
[2022-11-13 00:43:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][400/1251]	eta 0:10:40 lr 0.000336	time 0.7417 (0.7526)	loss 3.6420 (3.0559)	grad_norm 2.1577 (1.8564)	mem 23876MB
[2022-11-13 00:43:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][450/1251]	eta 0:10:02 lr 0.000336	time 0.7438 (0.7519)	loss 2.7423 (3.0394)	grad_norm 2.0297 (1.8663)	mem 23876MB
[2022-11-13 00:44:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][500/1251]	eta 0:09:24 lr 0.000335	time 0.7346 (0.7514)	loss 3.1174 (3.0344)	grad_norm 1.8504 (1.8635)	mem 23876MB
[2022-11-13 00:45:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][550/1251]	eta 0:08:46 lr 0.000335	time 0.7430 (0.7510)	loss 3.0437 (3.0385)	grad_norm 1.9172 (1.8683)	mem 23876MB
[2022-11-13 00:45:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][600/1251]	eta 0:08:08 lr 0.000335	time 0.7391 (0.7507)	loss 3.2057 (3.0372)	grad_norm 1.6784 (1.8715)	mem 23876MB
[2022-11-13 00:46:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][650/1251]	eta 0:07:31 lr 0.000335	time 0.7427 (0.7507)	loss 2.6548 (3.0388)	grad_norm 1.8789 (1.8768)	mem 23876MB
[2022-11-13 00:47:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][700/1251]	eta 0:06:53 lr 0.000335	time 0.7399 (0.7502)	loss 3.3304 (3.0469)	grad_norm 2.0117 (1.8753)	mem 23876MB
[2022-11-13 00:47:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][750/1251]	eta 0:06:15 lr 0.000334	time 0.7356 (0.7501)	loss 3.2383 (3.0490)	grad_norm 1.7269 (1.8740)	mem 23876MB
[2022-11-13 00:48:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][800/1251]	eta 0:05:38 lr 0.000334	time 0.7359 (0.7500)	loss 3.6632 (3.0477)	grad_norm 1.8994 (1.8745)	mem 23876MB
[2022-11-13 00:48:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][850/1251]	eta 0:05:00 lr 0.000334	time 0.7371 (0.7498)	loss 2.8461 (3.0519)	grad_norm 1.8039 (1.8759)	mem 23876MB
[2022-11-13 00:49:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][900/1251]	eta 0:04:23 lr 0.000334	time 0.7337 (0.7498)	loss 3.0952 (3.0475)	grad_norm 1.7944 (1.8791)	mem 23876MB
[2022-11-13 00:50:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][950/1251]	eta 0:03:45 lr 0.000334	time 0.7363 (0.7497)	loss 3.2663 (3.0454)	grad_norm 1.6159 (1.8799)	mem 23876MB
[2022-11-13 00:50:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][1000/1251]	eta 0:03:08 lr 0.000333	time 0.8133 (0.7496)	loss 1.9125 (3.0494)	grad_norm 1.8398 (1.8799)	mem 23876MB
[2022-11-13 00:51:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][1050/1251]	eta 0:02:30 lr 0.000333	time 0.7350 (0.7495)	loss 3.2610 (3.0504)	grad_norm 2.0880 (1.8801)	mem 23876MB
[2022-11-13 00:52:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][1100/1251]	eta 0:01:53 lr 0.000333	time 0.7385 (0.7494)	loss 2.9598 (3.0475)	grad_norm 1.7089 (1.8808)	mem 23876MB
[2022-11-13 00:52:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][1150/1251]	eta 0:01:15 lr 0.000333	time 0.7366 (0.7493)	loss 3.2372 (3.0403)	grad_norm 1.8262 (1.8816)	mem 23876MB
[2022-11-13 00:53:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][1200/1251]	eta 0:00:38 lr 0.000333	time 0.7332 (0.7492)	loss 3.2263 (3.0389)	grad_norm 1.9814 (1.8805)	mem 23876MB
[2022-11-13 00:53:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [183/300][1250/1251]	eta 0:00:00 lr 0.000332	time 0.6962 (0.7488)	loss 1.8810 (3.0418)	grad_norm inf (inf)	mem 23876MB
[2022-11-13 00:53:54 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 183 training takes 0:15:36
[2022-11-13 00:53:55 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_183.pth saving......
[2022-11-13 00:53:56 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_183.pth saved !!!
[2022-11-13 00:53:57 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.805 (1.805)	Loss 0.7705 (0.7705)	Acc@1 81.348 (81.348)	Acc@5 96.875 (96.875)	Mem 23876MB
[2022-11-13 00:54:08 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.766 Acc@5 95.654
[2022-11-13 00:54:08 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.8%
[2022-11-13 00:54:10 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.780 (1.780)	Loss 0.7384 (0.7384)	Acc@1 80.762 (80.762)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-13 00:54:21 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.492 Acc@5 96.404
[2022-11-13 00:54:21 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-13 00:54:21 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.49% at 183 epoch
[2022-11-13 00:54:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][0/1251]	eta 0:48:22 lr 0.000332	time 2.3200 (2.3200)	loss 2.1701 (2.1701)	grad_norm 1.8536 (1.8536)	mem 23876MB
[2022-11-13 00:55:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][50/1251]	eta 0:15:33 lr 0.000332	time 0.7385 (0.7769)	loss 2.7665 (3.0390)	grad_norm 2.2272 (1.8573)	mem 23876MB
[2022-11-13 00:55:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][100/1251]	eta 0:14:38 lr 0.000332	time 0.7384 (0.7629)	loss 1.8686 (3.0349)	grad_norm 1.7362 (1.9003)	mem 23876MB
[2022-11-13 00:56:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][150/1251]	eta 0:13:54 lr 0.000332	time 0.7455 (0.7577)	loss 3.0415 (3.0201)	grad_norm 1.7683 (1.9249)	mem 23876MB
[2022-11-13 00:56:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][200/1251]	eta 0:13:13 lr 0.000332	time 0.7428 (0.7550)	loss 2.6608 (3.0387)	grad_norm 1.6224 (1.9076)	mem 23876MB
[2022-11-13 00:57:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][250/1251]	eta 0:12:34 lr 0.000331	time 0.7392 (0.7535)	loss 3.0204 (3.0641)	grad_norm 1.6452 (1.9158)	mem 23876MB
[2022-11-13 00:58:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][300/1251]	eta 0:11:55 lr 0.000331	time 0.7303 (0.7521)	loss 3.1000 (3.0591)	grad_norm 1.9964 (1.9260)	mem 23876MB
[2022-11-13 00:58:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][350/1251]	eta 0:11:17 lr 0.000331	time 0.7390 (0.7514)	loss 2.7848 (3.0460)	grad_norm 1.8703 (inf)	mem 23876MB
[2022-11-13 00:59:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][400/1251]	eta 0:10:38 lr 0.000331	time 0.7318 (0.7507)	loss 3.2931 (3.0573)	grad_norm 1.8694 (inf)	mem 23876MB
[2022-11-13 00:59:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][450/1251]	eta 0:10:00 lr 0.000331	time 0.7400 (0.7501)	loss 3.3272 (3.0567)	grad_norm 2.3047 (inf)	mem 23876MB
[2022-11-13 01:00:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][500/1251]	eta 0:09:23 lr 0.000331	time 0.7390 (0.7500)	loss 2.9431 (3.0636)	grad_norm 1.6771 (inf)	mem 23876MB
[2022-11-13 01:01:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][550/1251]	eta 0:08:45 lr 0.000330	time 0.7368 (0.7496)	loss 2.6583 (3.0711)	grad_norm 2.4430 (inf)	mem 23876MB
[2022-11-13 01:01:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][600/1251]	eta 0:08:07 lr 0.000330	time 0.7421 (0.7495)	loss 3.2987 (3.0740)	grad_norm 2.3184 (inf)	mem 23876MB
[2022-11-13 01:02:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][650/1251]	eta 0:07:30 lr 0.000330	time 0.7321 (0.7492)	loss 3.4160 (3.0769)	grad_norm 2.2740 (inf)	mem 23876MB
[2022-11-13 01:03:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][700/1251]	eta 0:06:52 lr 0.000330	time 0.7353 (0.7491)	loss 3.2896 (3.0796)	grad_norm 1.7980 (inf)	mem 23876MB
[2022-11-13 01:03:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][750/1251]	eta 0:06:15 lr 0.000330	time 0.7418 (0.7489)	loss 3.2151 (3.0755)	grad_norm 1.6974 (inf)	mem 23876MB
[2022-11-13 01:04:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][800/1251]	eta 0:05:37 lr 0.000329	time 0.7363 (0.7488)	loss 3.3775 (3.0758)	grad_norm 1.6620 (inf)	mem 23876MB
[2022-11-13 01:04:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][850/1251]	eta 0:05:00 lr 0.000329	time 0.7352 (0.7487)	loss 2.6251 (3.0691)	grad_norm 1.5301 (inf)	mem 23876MB
[2022-11-13 01:05:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][900/1251]	eta 0:04:22 lr 0.000329	time 0.7377 (0.7486)	loss 3.5351 (3.0709)	grad_norm 1.5447 (inf)	mem 23876MB
[2022-11-13 01:06:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][950/1251]	eta 0:03:45 lr 0.000329	time 0.7339 (0.7484)	loss 2.6471 (3.0663)	grad_norm 1.9080 (inf)	mem 23876MB
[2022-11-13 01:06:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][1000/1251]	eta 0:03:07 lr 0.000329	time 0.7412 (0.7484)	loss 1.8788 (3.0602)	grad_norm 1.7913 (inf)	mem 23876MB
[2022-11-13 01:07:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][1050/1251]	eta 0:02:30 lr 0.000328	time 0.7356 (0.7484)	loss 3.3170 (3.0577)	grad_norm 1.8546 (inf)	mem 23876MB
[2022-11-13 01:08:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][1100/1251]	eta 0:01:52 lr 0.000328	time 0.7365 (0.7481)	loss 2.7597 (3.0566)	grad_norm 1.6620 (inf)	mem 23876MB
[2022-11-13 01:08:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][1150/1251]	eta 0:01:15 lr 0.000328	time 0.7361 (0.7482)	loss 3.3374 (3.0546)	grad_norm 2.2709 (inf)	mem 23876MB
[2022-11-13 01:09:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][1200/1251]	eta 0:00:38 lr 0.000328	time 0.7395 (0.7481)	loss 2.9805 (3.0613)	grad_norm 1.7107 (inf)	mem 23876MB
[2022-11-13 01:09:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [184/300][1250/1251]	eta 0:00:00 lr 0.000328	time 0.7265 (0.7479)	loss 3.0563 (3.0624)	grad_norm 1.8307 (inf)	mem 23876MB
[2022-11-13 01:09:57 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 184 training takes 0:15:35
[2022-11-13 01:09:57 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_184.pth saving......
[2022-11-13 01:09:58 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_184.pth saved !!!
[2022-11-13 01:09:59 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.600 (1.600)	Loss 0.7511 (0.7511)	Acc@1 83.105 (83.105)	Acc@5 95.801 (95.801)	Mem 23876MB
[2022-11-13 01:10:10 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.832 Acc@5 95.750
[2022-11-13 01:10:10 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.8%
[2022-11-13 01:10:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.810 (1.810)	Loss 0.6409 (0.6409)	Acc@1 83.691 (83.691)	Acc@5 96.680 (96.680)	Mem 23876MB
[2022-11-13 01:10:23 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.474 Acc@5 96.430
[2022-11-13 01:10:23 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-13 01:10:23 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.49% at 183 epoch
[2022-11-13 01:10:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][0/1251]	eta 0:51:02 lr 0.000328	time 2.4482 (2.4482)	loss 2.9747 (2.9747)	grad_norm 1.7270 (1.7270)	mem 23876MB
[2022-11-13 01:11:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][50/1251]	eta 0:15:37 lr 0.000327	time 0.7434 (0.7805)	loss 2.2145 (2.8945)	grad_norm 1.7358 (1.8168)	mem 23876MB
[2022-11-13 01:11:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][100/1251]	eta 0:14:38 lr 0.000327	time 0.7371 (0.7629)	loss 2.2444 (2.9637)	grad_norm 1.6040 (1.8061)	mem 23876MB
[2022-11-13 01:12:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][150/1251]	eta 0:13:53 lr 0.000327	time 0.7344 (0.7574)	loss 3.3477 (2.9974)	grad_norm 1.9379 (1.8436)	mem 23876MB
[2022-11-13 01:12:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][200/1251]	eta 0:13:12 lr 0.000327	time 0.7403 (0.7543)	loss 2.9701 (2.9872)	grad_norm 1.7244 (1.9090)	mem 23876MB
[2022-11-13 01:13:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][250/1251]	eta 0:12:33 lr 0.000327	time 0.7405 (0.7529)	loss 3.3321 (3.0120)	grad_norm 1.7035 (1.9014)	mem 23876MB
[2022-11-13 01:14:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][300/1251]	eta 0:11:54 lr 0.000326	time 0.7381 (0.7516)	loss 2.6781 (3.0056)	grad_norm 1.9954 (1.8930)	mem 23876MB
[2022-11-13 01:14:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][350/1251]	eta 0:11:16 lr 0.000326	time 0.7236 (0.7508)	loss 3.3969 (3.0101)	grad_norm 1.9868 (1.8965)	mem 23876MB
[2022-11-13 01:15:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][400/1251]	eta 0:10:38 lr 0.000326	time 0.7445 (0.7499)	loss 3.3471 (3.0200)	grad_norm 2.0866 (1.8946)	mem 23876MB
[2022-11-13 01:16:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][450/1251]	eta 0:10:00 lr 0.000326	time 0.7395 (0.7496)	loss 2.8066 (3.0158)	grad_norm 2.1897 (1.9035)	mem 23876MB
[2022-11-13 01:16:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][500/1251]	eta 0:09:22 lr 0.000326	time 0.7407 (0.7488)	loss 3.0967 (3.0194)	grad_norm 1.8204 (1.8973)	mem 23876MB
[2022-11-13 01:17:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][550/1251]	eta 0:08:44 lr 0.000325	time 0.7396 (0.7487)	loss 2.5011 (3.0182)	grad_norm 1.8309 (1.8969)	mem 23876MB
[2022-11-13 01:17:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][600/1251]	eta 0:08:06 lr 0.000325	time 0.7405 (0.7480)	loss 2.9319 (3.0224)	grad_norm 2.0003 (1.9032)	mem 23876MB
[2022-11-13 01:18:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][650/1251]	eta 0:07:29 lr 0.000325	time 0.8168 (0.7481)	loss 3.3196 (3.0226)	grad_norm 2.5697 (1.9075)	mem 23876MB
[2022-11-13 01:19:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][700/1251]	eta 0:06:51 lr 0.000325	time 0.7375 (0.7477)	loss 3.1778 (3.0315)	grad_norm 1.9204 (1.9105)	mem 23876MB
[2022-11-13 01:19:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][750/1251]	eta 0:06:14 lr 0.000325	time 0.7356 (0.7475)	loss 3.7399 (3.0319)	grad_norm 2.5891 (1.9082)	mem 23876MB
[2022-11-13 01:20:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][800/1251]	eta 0:05:37 lr 0.000325	time 0.7394 (0.7472)	loss 3.1147 (3.0361)	grad_norm 1.7907 (1.9113)	mem 23876MB
[2022-11-13 01:20:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][850/1251]	eta 0:04:59 lr 0.000324	time 0.7395 (0.7472)	loss 3.5343 (3.0396)	grad_norm 1.9080 (1.9097)	mem 23876MB
[2022-11-13 01:21:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][900/1251]	eta 0:04:22 lr 0.000324	time 0.7381 (0.7469)	loss 3.3245 (3.0331)	grad_norm 1.6732 (1.9131)	mem 23876MB
[2022-11-13 01:22:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][950/1251]	eta 0:03:44 lr 0.000324	time 0.7436 (0.7468)	loss 2.8345 (3.0331)	grad_norm 1.6996 (1.9123)	mem 23876MB
[2022-11-13 01:22:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][1000/1251]	eta 0:03:07 lr 0.000324	time 0.7391 (0.7465)	loss 2.8356 (3.0361)	grad_norm 2.2559 (1.9099)	mem 23876MB
[2022-11-13 01:23:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][1050/1251]	eta 0:02:30 lr 0.000324	time 0.7362 (0.7466)	loss 3.2788 (3.0383)	grad_norm 1.9616 (1.9083)	mem 23876MB
[2022-11-13 01:24:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][1100/1251]	eta 0:01:52 lr 0.000323	time 0.7370 (0.7464)	loss 2.1146 (3.0358)	grad_norm 1.8969 (1.9116)	mem 23876MB
[2022-11-13 01:24:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][1150/1251]	eta 0:01:15 lr 0.000323	time 0.7409 (0.7464)	loss 2.7247 (3.0346)	grad_norm 1.7967 (1.9128)	mem 23876MB
[2022-11-13 01:25:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][1200/1251]	eta 0:00:38 lr 0.000323	time 0.7418 (0.7463)	loss 3.5579 (3.0364)	grad_norm 1.7641 (1.9120)	mem 23876MB
[2022-11-13 01:25:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [185/300][1250/1251]	eta 0:00:00 lr 0.000323	time 0.7245 (0.7462)	loss 3.1552 (3.0404)	grad_norm 1.8547 (1.9163)	mem 23876MB
[2022-11-13 01:25:57 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 185 training takes 0:15:33
[2022-11-13 01:25:57 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_185.pth saving......
[2022-11-13 01:25:58 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_185.pth saved !!!
[2022-11-13 01:26:00 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.676 (1.676)	Loss 0.7634 (0.7634)	Acc@1 80.566 (80.566)	Acc@5 95.996 (95.996)	Mem 23876MB
[2022-11-13 01:26:10 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.036 Acc@5 95.770
[2022-11-13 01:26:10 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.0%
[2022-11-13 01:26:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.877 (1.877)	Loss 0.6894 (0.6894)	Acc@1 82.617 (82.617)	Acc@5 97.070 (97.070)	Mem 23876MB
[2022-11-13 01:26:23 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.496 Acc@5 96.438
[2022-11-13 01:26:23 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-13 01:26:23 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.50% at 185 epoch
[2022-11-13 01:26:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][0/1251]	eta 0:51:36 lr 0.000323	time 2.4752 (2.4752)	loss 2.3287 (2.3287)	grad_norm 1.5556 (1.5556)	mem 23876MB
[2022-11-13 01:27:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][50/1251]	eta 0:15:42 lr 0.000323	time 0.7384 (0.7849)	loss 2.1624 (2.9696)	grad_norm 1.9239 (1.8965)	mem 23876MB
[2022-11-13 01:27:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][100/1251]	eta 0:14:41 lr 0.000322	time 0.7472 (0.7656)	loss 2.0563 (3.0538)	grad_norm 2.1486 (1.9056)	mem 23876MB
[2022-11-13 01:28:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][150/1251]	eta 0:13:56 lr 0.000322	time 0.7367 (0.7597)	loss 3.2372 (3.0530)	grad_norm 1.9612 (1.9126)	mem 23876MB
[2022-11-13 01:28:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][200/1251]	eta 0:13:15 lr 0.000322	time 0.8177 (0.7565)	loss 2.6593 (3.0179)	grad_norm 1.7008 (1.9046)	mem 23876MB
[2022-11-13 01:29:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][250/1251]	eta 0:12:34 lr 0.000322	time 0.7496 (0.7541)	loss 3.0035 (3.0321)	grad_norm 1.7769 (1.9195)	mem 23876MB
[2022-11-13 01:30:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][300/1251]	eta 0:11:55 lr 0.000322	time 0.7362 (0.7528)	loss 3.5838 (3.0304)	grad_norm 1.6507 (1.9191)	mem 23876MB
[2022-11-13 01:30:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][350/1251]	eta 0:11:17 lr 0.000321	time 0.7375 (0.7516)	loss 3.1951 (3.0436)	grad_norm 1.7342 (1.9126)	mem 23876MB
[2022-11-13 01:31:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][400/1251]	eta 0:10:39 lr 0.000321	time 0.7462 (0.7513)	loss 2.9953 (3.0419)	grad_norm 1.8862 (1.9122)	mem 23876MB
[2022-11-13 01:32:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][450/1251]	eta 0:10:01 lr 0.000321	time 0.7215 (0.7509)	loss 3.5617 (3.0433)	grad_norm 1.8712 (1.9138)	mem 23876MB
[2022-11-13 01:32:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][500/1251]	eta 0:09:23 lr 0.000321	time 0.7384 (0.7503)	loss 2.4299 (3.0362)	grad_norm 1.5347 (1.9100)	mem 23876MB
[2022-11-13 01:33:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][550/1251]	eta 0:08:45 lr 0.000321	time 0.7381 (0.7503)	loss 2.3303 (3.0364)	grad_norm 2.2968 (1.9142)	mem 23876MB
[2022-11-13 01:33:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][600/1251]	eta 0:08:08 lr 0.000320	time 0.8240 (0.7499)	loss 3.1669 (3.0346)	grad_norm 1.9088 (1.9146)	mem 23876MB
[2022-11-13 01:34:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][650/1251]	eta 0:07:30 lr 0.000320	time 0.7388 (0.7497)	loss 2.0486 (3.0239)	grad_norm 1.7764 (1.9164)	mem 23876MB
[2022-11-13 01:35:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][700/1251]	eta 0:06:52 lr 0.000320	time 0.7403 (0.7495)	loss 3.6439 (3.0343)	grad_norm 1.9483 (1.9166)	mem 23876MB
[2022-11-13 01:35:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][750/1251]	eta 0:06:15 lr 0.000320	time 0.7370 (0.7491)	loss 3.4842 (3.0382)	grad_norm 1.6455 (1.9144)	mem 23876MB
[2022-11-13 01:36:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][800/1251]	eta 0:05:37 lr 0.000320	time 0.7362 (0.7491)	loss 3.2632 (3.0394)	grad_norm 1.8097 (1.9114)	mem 23876MB
[2022-11-13 01:37:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][850/1251]	eta 0:05:00 lr 0.000320	time 0.7416 (0.7490)	loss 2.4045 (3.0402)	grad_norm 1.6597 (1.9144)	mem 23876MB
[2022-11-13 01:37:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][900/1251]	eta 0:04:22 lr 0.000319	time 0.7348 (0.7487)	loss 3.0314 (3.0417)	grad_norm 1.9021 (1.9130)	mem 23876MB
[2022-11-13 01:38:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][950/1251]	eta 0:03:45 lr 0.000319	time 0.7329 (0.7487)	loss 2.7793 (3.0345)	grad_norm 1.5602 (1.9113)	mem 23876MB
[2022-11-13 01:38:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][1000/1251]	eta 0:03:07 lr 0.000319	time 0.8110 (0.7485)	loss 2.0641 (3.0324)	grad_norm 1.7332 (1.9134)	mem 23876MB
[2022-11-13 01:39:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][1050/1251]	eta 0:02:30 lr 0.000319	time 0.7381 (0.7484)	loss 3.2040 (3.0297)	grad_norm 1.8728 (1.9083)	mem 23876MB
[2022-11-13 01:40:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][1100/1251]	eta 0:01:52 lr 0.000319	time 0.7345 (0.7482)	loss 3.3963 (3.0248)	grad_norm 1.8325 (1.9070)	mem 23876MB
[2022-11-13 01:40:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][1150/1251]	eta 0:01:15 lr 0.000318	time 0.7378 (0.7481)	loss 2.9982 (3.0218)	grad_norm 1.9531 (1.9068)	mem 23876MB
[2022-11-13 01:41:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][1200/1251]	eta 0:00:38 lr 0.000318	time 0.7364 (0.7480)	loss 3.3602 (3.0197)	grad_norm 2.6182 (1.9083)	mem 23876MB
[2022-11-13 01:41:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [186/300][1250/1251]	eta 0:00:00 lr 0.000318	time 0.7256 (0.7478)	loss 3.1106 (3.0247)	grad_norm 2.0458 (1.9101)	mem 23876MB
[2022-11-13 01:41:58 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 186 training takes 0:15:35
[2022-11-13 01:41:59 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_186.pth saving......
[2022-11-13 01:42:00 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_186.pth saved !!!
[2022-11-13 01:42:01 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.579 (1.579)	Loss 0.8436 (0.8436)	Acc@1 79.395 (79.395)	Acc@5 95.898 (95.898)	Mem 23876MB
[2022-11-13 01:42:12 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.904 Acc@5 95.740
[2022-11-13 01:42:12 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 80.9%
[2022-11-13 01:42:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.836 (1.836)	Loss 0.7461 (0.7461)	Acc@1 81.543 (81.543)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-13 01:42:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.536 Acc@5 96.438
[2022-11-13 01:42:25 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.5%
[2022-11-13 01:42:25 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.54% at 186 epoch
[2022-11-13 01:42:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][0/1251]	eta 0:50:35 lr 0.000318	time 2.4265 (2.4265)	loss 2.7631 (2.7631)	grad_norm 1.6802 (1.6802)	mem 23876MB
[2022-11-13 01:43:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][50/1251]	eta 0:15:39 lr 0.000318	time 0.7394 (0.7822)	loss 3.5133 (3.0655)	grad_norm 1.7903 (1.8638)	mem 23876MB
[2022-11-13 01:43:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][100/1251]	eta 0:14:42 lr 0.000318	time 0.7533 (0.7664)	loss 3.3445 (3.0155)	grad_norm 1.8491 (1.8813)	mem 23876MB
[2022-11-13 01:44:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][150/1251]	eta 0:13:55 lr 0.000317	time 0.7369 (0.7593)	loss 3.1291 (3.0214)	grad_norm 1.8014 (1.9169)	mem 23876MB
[2022-11-13 01:44:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][200/1251]	eta 0:13:14 lr 0.000317	time 0.8086 (0.7564)	loss 3.3939 (3.0373)	grad_norm 1.9173 (1.9174)	mem 23876MB
[2022-11-13 01:45:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][250/1251]	eta 0:12:35 lr 0.000317	time 0.7487 (0.7546)	loss 3.1769 (3.0586)	grad_norm 1.8610 (1.9145)	mem 23876MB
[2022-11-13 01:46:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][300/1251]	eta 0:11:56 lr 0.000317	time 0.7404 (0.7534)	loss 3.1828 (3.0543)	grad_norm 2.4305 (1.9294)	mem 23876MB
[2022-11-13 01:46:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][350/1251]	eta 0:11:18 lr 0.000317	time 0.7329 (0.7530)	loss 2.0094 (3.0374)	grad_norm 1.7733 (1.9236)	mem 23876MB
[2022-11-13 01:47:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][400/1251]	eta 0:10:40 lr 0.000316	time 0.7395 (0.7526)	loss 3.5221 (3.0345)	grad_norm 1.9128 (1.9277)	mem 23876MB
[2022-11-13 01:48:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][450/1251]	eta 0:10:01 lr 0.000316	time 0.7393 (0.7514)	loss 3.6141 (3.0466)	grad_norm 1.9200 (1.9223)	mem 23876MB
[2022-11-13 01:48:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][500/1251]	eta 0:09:24 lr 0.000316	time 0.7389 (0.7513)	loss 2.4158 (3.0355)	grad_norm 1.8429 (1.9259)	mem 23876MB
[2022-11-13 01:49:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][550/1251]	eta 0:08:46 lr 0.000316	time 0.7338 (0.7510)	loss 1.9126 (3.0367)	grad_norm 2.4826 (1.9273)	mem 23876MB
[2022-11-13 01:49:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][600/1251]	eta 0:08:08 lr 0.000316	time 0.7409 (0.7509)	loss 3.5764 (3.0444)	grad_norm 2.0544 (1.9263)	mem 23876MB
[2022-11-13 01:50:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][650/1251]	eta 0:07:31 lr 0.000315	time 0.7502 (0.7506)	loss 3.8406 (3.0328)	grad_norm 2.1471 (1.9244)	mem 23876MB
[2022-11-13 01:51:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][700/1251]	eta 0:06:53 lr 0.000315	time 0.7401 (0.7500)	loss 2.5463 (3.0256)	grad_norm 1.6137 (1.9185)	mem 23876MB
[2022-11-13 01:51:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][750/1251]	eta 0:06:15 lr 0.000315	time 0.7465 (0.7497)	loss 3.2618 (3.0238)	grad_norm 1.8246 (inf)	mem 23876MB
[2022-11-13 01:52:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][800/1251]	eta 0:05:38 lr 0.000315	time 0.7368 (0.7496)	loss 2.6056 (3.0192)	grad_norm 1.8157 (inf)	mem 23876MB
[2022-11-13 01:53:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][850/1251]	eta 0:05:00 lr 0.000315	time 0.7360 (0.7496)	loss 2.8079 (3.0168)	grad_norm 1.8476 (inf)	mem 23876MB
[2022-11-13 01:53:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][900/1251]	eta 0:04:23 lr 0.000315	time 0.8236 (0.7496)	loss 2.3110 (3.0075)	grad_norm 2.2984 (inf)	mem 23876MB
[2022-11-13 01:54:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][950/1251]	eta 0:03:45 lr 0.000314	time 0.8414 (0.7495)	loss 3.1940 (3.0080)	grad_norm 1.8532 (inf)	mem 23876MB
[2022-11-13 01:54:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][1000/1251]	eta 0:03:08 lr 0.000314	time 0.7359 (0.7492)	loss 3.2266 (3.0170)	grad_norm 1.7454 (inf)	mem 23876MB
[2022-11-13 01:55:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][1050/1251]	eta 0:02:30 lr 0.000314	time 0.7383 (0.7491)	loss 3.4241 (3.0148)	grad_norm 2.1093 (inf)	mem 23876MB
[2022-11-13 01:56:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][1100/1251]	eta 0:01:53 lr 0.000314	time 0.7336 (0.7490)	loss 3.6211 (3.0137)	grad_norm 2.9362 (inf)	mem 23876MB
[2022-11-13 01:56:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][1150/1251]	eta 0:01:15 lr 0.000314	time 0.7380 (0.7490)	loss 2.9403 (3.0138)	grad_norm 2.0023 (inf)	mem 23876MB
[2022-11-13 01:57:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][1200/1251]	eta 0:00:38 lr 0.000313	time 0.7377 (0.7491)	loss 3.1824 (3.0167)	grad_norm 1.8470 (inf)	mem 23876MB
[2022-11-13 01:58:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [187/300][1250/1251]	eta 0:00:00 lr 0.000313	time 0.7278 (0.7487)	loss 3.4381 (3.0137)	grad_norm 1.9138 (inf)	mem 23876MB
[2022-11-13 01:58:01 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 187 training takes 0:15:36
[2022-11-13 01:58:01 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_187.pth saving......
[2022-11-13 01:58:03 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_187.pth saved !!!
[2022-11-13 01:58:04 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.561 (1.561)	Loss 0.7915 (0.7915)	Acc@1 80.762 (80.762)	Acc@5 95.996 (95.996)	Mem 23876MB
[2022-11-13 01:58:15 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.964 Acc@5 95.692
[2022-11-13 01:58:15 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.0%
[2022-11-13 01:58:17 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.819 (1.819)	Loss 0.7473 (0.7473)	Acc@1 82.031 (82.031)	Acc@5 96.094 (96.094)	Mem 23876MB
[2022-11-13 01:58:27 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.568 Acc@5 96.426
[2022-11-13 01:58:27 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.6%
[2022-11-13 01:58:27 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.57% at 187 epoch
[2022-11-13 01:58:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][0/1251]	eta 0:51:17 lr 0.000313	time 2.4596 (2.4596)	loss 2.4742 (2.4742)	grad_norm 1.7876 (1.7876)	mem 23876MB
[2022-11-13 01:59:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][50/1251]	eta 0:15:34 lr 0.000313	time 0.7373 (0.7780)	loss 3.4824 (3.0502)	grad_norm 2.3622 (1.8723)	mem 23876MB
[2022-11-13 01:59:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][100/1251]	eta 0:14:39 lr 0.000313	time 0.7385 (0.7642)	loss 3.2345 (3.0112)	grad_norm 1.9395 (1.8992)	mem 23876MB
[2022-11-13 02:00:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][150/1251]	eta 0:13:55 lr 0.000313	time 0.7431 (0.7584)	loss 3.2725 (3.0398)	grad_norm 1.9200 (1.9062)	mem 23876MB
[2022-11-13 02:00:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][200/1251]	eta 0:13:14 lr 0.000312	time 0.7452 (0.7557)	loss 2.9655 (3.0444)	grad_norm 1.9968 (1.9213)	mem 23876MB
[2022-11-13 02:01:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][250/1251]	eta 0:12:33 lr 0.000312	time 0.7401 (0.7530)	loss 2.9421 (3.0448)	grad_norm 2.1223 (1.9404)	mem 23876MB
[2022-11-13 02:02:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][300/1251]	eta 0:11:55 lr 0.000312	time 0.7452 (0.7524)	loss 2.7637 (3.0317)	grad_norm 1.7744 (1.9335)	mem 23876MB
[2022-11-13 02:02:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][350/1251]	eta 0:11:16 lr 0.000312	time 0.7425 (0.7513)	loss 2.2530 (3.0228)	grad_norm 2.0168 (1.9292)	mem 23876MB
[2022-11-13 02:03:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][400/1251]	eta 0:10:39 lr 0.000312	time 0.7380 (0.7512)	loss 3.4449 (3.0435)	grad_norm 1.8444 (1.9322)	mem 23876MB
[2022-11-13 02:04:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][450/1251]	eta 0:10:00 lr 0.000311	time 0.7392 (0.7501)	loss 3.2333 (3.0490)	grad_norm 1.9475 (1.9303)	mem 23876MB
[2022-11-13 02:04:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][500/1251]	eta 0:09:23 lr 0.000311	time 0.7383 (0.7498)	loss 2.5249 (3.0418)	grad_norm 1.7449 (1.9288)	mem 23876MB
[2022-11-13 02:05:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][550/1251]	eta 0:08:45 lr 0.000311	time 0.7373 (0.7491)	loss 2.8358 (3.0338)	grad_norm 1.7481 (1.9337)	mem 23876MB
[2022-11-13 02:05:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][600/1251]	eta 0:08:07 lr 0.000311	time 0.8167 (0.7492)	loss 3.2129 (3.0281)	grad_norm 1.8699 (1.9323)	mem 23876MB
[2022-11-13 02:06:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][650/1251]	eta 0:07:29 lr 0.000311	time 0.7393 (0.7485)	loss 3.2639 (3.0271)	grad_norm 1.8326 (1.9476)	mem 23876MB
[2022-11-13 02:07:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][700/1251]	eta 0:06:52 lr 0.000311	time 0.7362 (0.7484)	loss 2.4928 (3.0323)	grad_norm 1.8427 (1.9511)	mem 23876MB
[2022-11-13 02:07:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][750/1251]	eta 0:06:14 lr 0.000310	time 0.7377 (0.7483)	loss 2.6668 (3.0317)	grad_norm 3.1989 (1.9557)	mem 23876MB
[2022-11-13 02:08:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][800/1251]	eta 0:05:37 lr 0.000310	time 0.7365 (0.7479)	loss 3.4258 (3.0340)	grad_norm 1.9641 (1.9478)	mem 23876MB
[2022-11-13 02:09:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][850/1251]	eta 0:04:59 lr 0.000310	time 0.7371 (0.7479)	loss 3.6414 (3.0317)	grad_norm 1.9993 (1.9468)	mem 23876MB
[2022-11-13 02:09:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][900/1251]	eta 0:04:22 lr 0.000310	time 0.7393 (0.7477)	loss 3.3250 (3.0305)	grad_norm 1.7789 (1.9450)	mem 23876MB
[2022-11-13 02:10:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][950/1251]	eta 0:03:45 lr 0.000310	time 0.7404 (0.7477)	loss 3.1491 (3.0278)	grad_norm 1.8460 (nan)	mem 23876MB
[2022-11-13 02:10:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][1000/1251]	eta 0:03:07 lr 0.000309	time 0.7430 (0.7476)	loss 2.3643 (3.0256)	grad_norm 1.6091 (nan)	mem 23876MB
[2022-11-13 02:11:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][1050/1251]	eta 0:02:30 lr 0.000309	time 0.7344 (0.7476)	loss 3.2734 (3.0208)	grad_norm 1.8077 (nan)	mem 23876MB
[2022-11-13 02:12:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][1100/1251]	eta 0:01:52 lr 0.000309	time 0.8111 (0.7475)	loss 2.2081 (3.0197)	grad_norm 1.7966 (nan)	mem 23876MB
[2022-11-13 02:12:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][1150/1251]	eta 0:01:15 lr 0.000309	time 0.7427 (0.7475)	loss 3.3229 (3.0209)	grad_norm 1.9961 (nan)	mem 23876MB
[2022-11-13 02:13:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][1200/1251]	eta 0:00:38 lr 0.000309	time 0.7365 (0.7474)	loss 3.4490 (3.0205)	grad_norm 2.5259 (nan)	mem 23876MB
[2022-11-13 02:14:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [188/300][1250/1251]	eta 0:00:00 lr 0.000308	time 0.7260 (0.7472)	loss 3.1117 (3.0226)	grad_norm 1.9950 (nan)	mem 23876MB
[2022-11-13 02:14:02 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 188 training takes 0:15:34
[2022-11-13 02:14:02 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_188.pth saving......
[2022-11-13 02:14:04 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_188.pth saved !!!
[2022-11-13 02:14:05 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.605 (1.605)	Loss 0.7739 (0.7739)	Acc@1 80.469 (80.469)	Acc@5 96.191 (96.191)	Mem 23876MB
[2022-11-13 02:14:16 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 80.956 Acc@5 95.726
[2022-11-13 02:14:16 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.0%
[2022-11-13 02:14:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.782 (1.782)	Loss 0.7253 (0.7253)	Acc@1 82.324 (82.324)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-13 02:14:28 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.600 Acc@5 96.436
[2022-11-13 02:14:28 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.6%
[2022-11-13 02:14:28 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.60% at 188 epoch
[2022-11-13 02:14:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][0/1251]	eta 0:48:08 lr 0.000308	time 2.3086 (2.3086)	loss 2.8468 (2.8468)	grad_norm 1.5496 (1.5496)	mem 23876MB
[2022-11-13 02:15:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][50/1251]	eta 0:15:39 lr 0.000308	time 0.7401 (0.7825)	loss 2.9281 (2.9201)	grad_norm 1.6798 (1.8755)	mem 23876MB
[2022-11-13 02:15:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][100/1251]	eta 0:14:39 lr 0.000308	time 0.7388 (0.7645)	loss 3.5880 (2.9738)	grad_norm 1.7486 (1.8819)	mem 23876MB
[2022-11-13 02:16:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][150/1251]	eta 0:13:56 lr 0.000308	time 0.7420 (0.7596)	loss 3.2063 (2.9959)	grad_norm 1.9438 (1.9397)	mem 23876MB
[2022-11-13 02:17:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][200/1251]	eta 0:13:14 lr 0.000308	time 0.8221 (0.7563)	loss 2.6163 (3.0123)	grad_norm 1.8596 (1.9447)	mem 23876MB
[2022-11-13 02:17:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][250/1251]	eta 0:12:34 lr 0.000307	time 0.7370 (0.7537)	loss 3.0598 (3.0203)	grad_norm 1.9428 (1.9404)	mem 23876MB
[2022-11-13 02:18:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][300/1251]	eta 0:11:55 lr 0.000307	time 0.7366 (0.7526)	loss 3.3592 (3.0045)	grad_norm 1.9184 (1.9275)	mem 23876MB
[2022-11-13 02:18:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][350/1251]	eta 0:11:17 lr 0.000307	time 0.7424 (0.7524)	loss 2.6635 (2.9994)	grad_norm 1.6505 (1.9212)	mem 23876MB
[2022-11-13 02:19:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][400/1251]	eta 0:10:39 lr 0.000307	time 0.7353 (0.7510)	loss 3.5142 (2.9976)	grad_norm 1.8957 (1.9117)	mem 23876MB
[2022-11-13 02:20:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][450/1251]	eta 0:10:01 lr 0.000307	time 0.7383 (0.7511)	loss 2.3310 (2.9901)	grad_norm 2.1210 (1.9129)	mem 23876MB
[2022-11-13 02:20:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][500/1251]	eta 0:09:23 lr 0.000307	time 0.7410 (0.7502)	loss 3.1390 (2.9913)	grad_norm 1.7151 (1.9117)	mem 23876MB
[2022-11-13 02:21:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][550/1251]	eta 0:08:45 lr 0.000306	time 0.7356 (0.7500)	loss 3.3988 (2.9903)	grad_norm 2.0984 (1.9105)	mem 23876MB
[2022-11-13 02:21:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][600/1251]	eta 0:08:08 lr 0.000306	time 0.7393 (0.7499)	loss 3.4273 (2.9846)	grad_norm 2.2080 (1.9179)	mem 23876MB
[2022-11-13 02:22:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][650/1251]	eta 0:07:30 lr 0.000306	time 0.8146 (0.7494)	loss 3.0966 (2.9794)	grad_norm 1.8595 (1.9195)	mem 23876MB
[2022-11-13 02:23:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][700/1251]	eta 0:06:52 lr 0.000306	time 0.7411 (0.7493)	loss 3.3159 (2.9799)	grad_norm 2.0279 (1.9289)	mem 23876MB
[2022-11-13 02:23:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][750/1251]	eta 0:06:15 lr 0.000306	time 0.7343 (0.7491)	loss 3.3763 (2.9777)	grad_norm 1.7924 (1.9265)	mem 23876MB
[2022-11-13 02:24:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][800/1251]	eta 0:05:37 lr 0.000305	time 0.7428 (0.7486)	loss 3.2101 (2.9865)	grad_norm 1.8849 (1.9301)	mem 23876MB
[2022-11-13 02:25:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][850/1251]	eta 0:05:00 lr 0.000305	time 0.7442 (0.7486)	loss 2.0731 (2.9844)	grad_norm 2.1231 (1.9321)	mem 23876MB
[2022-11-13 02:25:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][900/1251]	eta 0:04:22 lr 0.000305	time 0.7373 (0.7484)	loss 3.1969 (2.9837)	grad_norm 1.8971 (1.9270)	mem 23876MB
[2022-11-13 02:26:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][950/1251]	eta 0:03:45 lr 0.000305	time 0.7352 (0.7483)	loss 2.2713 (2.9859)	grad_norm 1.8225 (1.9247)	mem 23876MB
[2022-11-13 02:26:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][1000/1251]	eta 0:03:07 lr 0.000305	time 0.7360 (0.7483)	loss 2.5684 (2.9857)	grad_norm 1.9141 (1.9281)	mem 23876MB
[2022-11-13 02:27:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][1050/1251]	eta 0:02:30 lr 0.000304	time 0.7421 (0.7480)	loss 2.5872 (2.9818)	grad_norm 2.4231 (1.9290)	mem 23876MB
[2022-11-13 02:28:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][1100/1251]	eta 0:01:52 lr 0.000304	time 0.7371 (0.7480)	loss 3.6287 (2.9825)	grad_norm 1.8631 (1.9290)	mem 23876MB
[2022-11-13 02:28:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][1150/1251]	eta 0:01:15 lr 0.000304	time 0.7358 (0.7480)	loss 2.3042 (2.9859)	grad_norm 1.7002 (1.9277)	mem 23876MB
[2022-11-13 02:29:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][1200/1251]	eta 0:00:38 lr 0.000304	time 0.7347 (0.7478)	loss 3.4319 (2.9872)	grad_norm 1.6901 (1.9271)	mem 23876MB
[2022-11-13 02:30:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [189/300][1250/1251]	eta 0:00:00 lr 0.000304	time 0.7245 (0.7477)	loss 2.4796 (2.9904)	grad_norm 1.8633 (1.9266)	mem 23876MB
[2022-11-13 02:30:04 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 189 training takes 0:15:35
[2022-11-13 02:30:04 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_189.pth saving......
[2022-11-13 02:30:05 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_189.pth saved !!!
[2022-11-13 02:30:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.876 (1.876)	Loss 0.7391 (0.7391)	Acc@1 82.227 (82.227)	Acc@5 96.582 (96.582)	Mem 23876MB
[2022-11-13 02:30:18 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.162 Acc@5 95.892
[2022-11-13 02:30:18 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.2%
[2022-11-13 02:30:20 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.744 (1.744)	Loss 0.8148 (0.8148)	Acc@1 81.055 (81.055)	Acc@5 95.508 (95.508)	Mem 23876MB
[2022-11-13 02:30:30 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.628 Acc@5 96.434
[2022-11-13 02:30:30 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.6%
[2022-11-13 02:30:30 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.63% at 189 epoch
[2022-11-13 02:30:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][0/1251]	eta 0:50:44 lr 0.000304	time 2.4333 (2.4333)	loss 2.6591 (2.6591)	grad_norm 1.8514 (1.8514)	mem 23876MB
[2022-11-13 02:31:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][50/1251]	eta 0:15:45 lr 0.000303	time 0.7479 (0.7870)	loss 3.1148 (2.9621)	grad_norm 1.4858 (1.9278)	mem 23876MB
[2022-11-13 02:31:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][100/1251]	eta 0:14:43 lr 0.000303	time 0.7396 (0.7679)	loss 3.2362 (2.9839)	grad_norm 2.5590 (1.9539)	mem 23876MB
[2022-11-13 02:32:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][150/1251]	eta 0:13:59 lr 0.000303	time 0.7409 (0.7628)	loss 2.2083 (3.0006)	grad_norm 1.8596 (1.9480)	mem 23876MB
[2022-11-13 02:33:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][200/1251]	eta 0:13:17 lr 0.000303	time 0.8139 (0.7584)	loss 2.7729 (3.0148)	grad_norm 2.1703 (1.9441)	mem 23876MB
[2022-11-13 02:33:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][250/1251]	eta 0:12:37 lr 0.000303	time 0.7399 (0.7563)	loss 3.1097 (2.9985)	grad_norm 2.0031 (1.9406)	mem 23876MB
[2022-11-13 02:34:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][300/1251]	eta 0:11:57 lr 0.000303	time 0.7284 (0.7549)	loss 3.2250 (2.9874)	grad_norm 1.7706 (1.9360)	mem 23876MB
[2022-11-13 02:34:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][350/1251]	eta 0:11:19 lr 0.000302	time 0.7404 (0.7538)	loss 2.3009 (2.9879)	grad_norm 1.9256 (1.9324)	mem 23876MB
[2022-11-13 02:35:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][400/1251]	eta 0:10:40 lr 0.000302	time 0.7336 (0.7532)	loss 2.9853 (2.9878)	grad_norm 1.6719 (1.9366)	mem 23876MB
[2022-11-13 02:36:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][450/1251]	eta 0:10:02 lr 0.000302	time 0.7393 (0.7524)	loss 2.6845 (2.9902)	grad_norm 1.8878 (1.9417)	mem 23876MB
[2022-11-13 02:36:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][500/1251]	eta 0:09:24 lr 0.000302	time 0.7359 (0.7517)	loss 3.6021 (2.9998)	grad_norm 1.8642 (1.9463)	mem 23876MB
[2022-11-13 02:37:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][550/1251]	eta 0:08:46 lr 0.000302	time 0.7337 (0.7512)	loss 2.7583 (2.9945)	grad_norm 1.9006 (1.9442)	mem 23876MB
[2022-11-13 02:38:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][600/1251]	eta 0:08:08 lr 0.000301	time 0.7368 (0.7510)	loss 2.8093 (2.9891)	grad_norm 1.9338 (1.9459)	mem 23876MB
[2022-11-13 02:38:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][650/1251]	eta 0:07:31 lr 0.000301	time 0.7393 (0.7509)	loss 3.1750 (2.9925)	grad_norm 1.9172 (1.9510)	mem 23876MB
[2022-11-13 02:39:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][700/1251]	eta 0:06:53 lr 0.000301	time 0.7409 (0.7508)	loss 3.1399 (2.9952)	grad_norm 1.9172 (1.9505)	mem 23876MB
[2022-11-13 02:39:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][750/1251]	eta 0:06:15 lr 0.000301	time 0.8122 (0.7505)	loss 3.5444 (2.9889)	grad_norm 2.0416 (1.9457)	mem 23876MB
[2022-11-13 02:40:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][800/1251]	eta 0:05:38 lr 0.000301	time 0.7395 (0.7503)	loss 2.8875 (2.9912)	grad_norm 1.7835 (1.9464)	mem 23876MB
[2022-11-13 02:41:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][850/1251]	eta 0:05:00 lr 0.000300	time 0.7375 (0.7502)	loss 3.3524 (2.9908)	grad_norm 1.9161 (1.9512)	mem 23876MB
[2022-11-13 02:41:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][900/1251]	eta 0:04:23 lr 0.000300	time 0.7394 (0.7502)	loss 2.0490 (2.9921)	grad_norm 1.9804 (1.9556)	mem 23876MB
[2022-11-13 02:42:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][950/1251]	eta 0:03:45 lr 0.000300	time 0.7408 (0.7503)	loss 2.6604 (2.9894)	grad_norm 1.9606 (1.9561)	mem 23876MB
[2022-11-13 02:43:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][1000/1251]	eta 0:03:08 lr 0.000300	time 0.8065 (0.7502)	loss 3.0795 (2.9921)	grad_norm 3.3968 (1.9544)	mem 23876MB
[2022-11-13 02:43:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][1050/1251]	eta 0:02:30 lr 0.000300	time 0.7384 (0.7499)	loss 3.5428 (2.9907)	grad_norm 1.9560 (1.9562)	mem 23876MB
[2022-11-13 02:44:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][1100/1251]	eta 0:01:53 lr 0.000300	time 0.7389 (0.7499)	loss 3.5200 (2.9966)	grad_norm 2.0036 (1.9597)	mem 23876MB
[2022-11-13 02:44:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][1150/1251]	eta 0:01:15 lr 0.000299	time 0.7316 (0.7497)	loss 3.1663 (2.9948)	grad_norm 1.8204 (1.9593)	mem 23876MB
[2022-11-13 02:45:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][1200/1251]	eta 0:00:38 lr 0.000299	time 0.7347 (0.7499)	loss 1.9711 (2.9957)	grad_norm 1.7097 (1.9621)	mem 23876MB
[2022-11-13 02:46:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [190/300][1250/1251]	eta 0:00:00 lr 0.000299	time 0.7268 (0.7496)	loss 2.0444 (2.9952)	grad_norm 1.6254 (1.9639)	mem 23876MB
[2022-11-13 02:46:08 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 190 training takes 0:15:37
[2022-11-13 02:46:08 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_190.pth saving......
[2022-11-13 02:46:10 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_190.pth saved !!!
[2022-11-13 02:46:11 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.834 (1.834)	Loss 0.8039 (0.8039)	Acc@1 80.273 (80.273)	Acc@5 95.605 (95.605)	Mem 23876MB
[2022-11-13 02:46:22 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.052 Acc@5 95.724
[2022-11-13 02:46:22 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.1%
[2022-11-13 02:46:24 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.812 (1.812)	Loss 0.7016 (0.7016)	Acc@1 82.520 (82.520)	Acc@5 96.875 (96.875)	Mem 23876MB
[2022-11-13 02:46:35 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.674 Acc@5 96.462
[2022-11-13 02:46:35 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.7%
[2022-11-13 02:46:35 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.67% at 190 epoch
[2022-11-13 02:46:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][0/1251]	eta 0:49:41 lr 0.000299	time 2.3835 (2.3835)	loss 3.4064 (3.4064)	grad_norm 2.8356 (2.8356)	mem 23876MB
[2022-11-13 02:47:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][50/1251]	eta 0:15:39 lr 0.000299	time 0.8255 (0.7820)	loss 3.1785 (2.9202)	grad_norm 1.8493 (1.9981)	mem 23876MB
[2022-11-13 02:47:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][100/1251]	eta 0:14:39 lr 0.000299	time 0.7452 (0.7639)	loss 3.3805 (2.9754)	grad_norm 2.6668 (2.0865)	mem 23876MB
[2022-11-13 02:48:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][150/1251]	eta 0:13:55 lr 0.000298	time 0.7424 (0.7592)	loss 3.8409 (2.9974)	grad_norm 2.4187 (2.0573)	mem 23876MB
[2022-11-13 02:49:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][200/1251]	eta 0:13:14 lr 0.000298	time 0.7422 (0.7563)	loss 3.1550 (3.0058)	grad_norm 1.7424 (2.0311)	mem 23876MB
[2022-11-13 02:49:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][250/1251]	eta 0:12:34 lr 0.000298	time 0.7347 (0.7540)	loss 3.1839 (2.9964)	grad_norm 2.1120 (2.0075)	mem 23876MB
[2022-11-13 02:50:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][300/1251]	eta 0:11:56 lr 0.000298	time 0.7324 (0.7531)	loss 2.8031 (3.0118)	grad_norm 1.8635 (1.9939)	mem 23876MB
[2022-11-13 02:50:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][350/1251]	eta 0:11:17 lr 0.000298	time 0.8236 (0.7520)	loss 3.4974 (3.0336)	grad_norm 1.8487 (1.9932)	mem 23876MB
[2022-11-13 02:51:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][400/1251]	eta 0:10:39 lr 0.000297	time 0.7347 (0.7516)	loss 3.4796 (3.0370)	grad_norm 1.7102 (2.0057)	mem 23876MB
[2022-11-13 02:52:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][450/1251]	eta 0:10:01 lr 0.000297	time 0.7398 (0.7509)	loss 3.3438 (3.0231)	grad_norm 2.1021 (1.9975)	mem 23876MB
[2022-11-13 02:52:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][500/1251]	eta 0:09:23 lr 0.000297	time 0.7477 (0.7507)	loss 3.2575 (3.0210)	grad_norm 2.2273 (1.9969)	mem 23876MB
[2022-11-13 02:53:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][550/1251]	eta 0:08:45 lr 0.000297	time 0.8068 (0.7502)	loss 3.3965 (3.0265)	grad_norm 1.9269 (1.9877)	mem 23876MB
[2022-11-13 02:54:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][600/1251]	eta 0:08:08 lr 0.000297	time 0.7339 (0.7499)	loss 3.4162 (3.0274)	grad_norm 2.0460 (1.9859)	mem 23876MB
[2022-11-13 02:54:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][650/1251]	eta 0:07:30 lr 0.000296	time 0.7352 (0.7496)	loss 3.2276 (3.0232)	grad_norm 2.0724 (1.9791)	mem 23876MB
[2022-11-13 02:55:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][700/1251]	eta 0:06:52 lr 0.000296	time 0.7320 (0.7495)	loss 2.2565 (3.0229)	grad_norm 2.2368 (1.9743)	mem 23876MB
[2022-11-13 02:55:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][750/1251]	eta 0:06:15 lr 0.000296	time 0.7343 (0.7492)	loss 3.3170 (3.0237)	grad_norm 1.9342 (1.9738)	mem 23876MB
[2022-11-13 02:56:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][800/1251]	eta 0:05:37 lr 0.000296	time 0.7370 (0.7492)	loss 3.0416 (3.0232)	grad_norm 1.8023 (1.9710)	mem 23876MB
[2022-11-13 02:57:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][850/1251]	eta 0:05:00 lr 0.000296	time 0.7443 (0.7489)	loss 2.7881 (3.0220)	grad_norm 1.5527 (1.9673)	mem 23876MB
[2022-11-13 02:57:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][900/1251]	eta 0:04:22 lr 0.000296	time 0.7478 (0.7489)	loss 3.1847 (3.0158)	grad_norm 1.7416 (1.9687)	mem 23876MB
[2022-11-13 02:58:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][950/1251]	eta 0:03:45 lr 0.000295	time 0.8028 (0.7488)	loss 3.0249 (3.0076)	grad_norm 1.8945 (1.9701)	mem 23876MB
[2022-11-13 02:59:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][1000/1251]	eta 0:03:07 lr 0.000295	time 0.7361 (0.7486)	loss 3.1516 (3.0040)	grad_norm 2.1278 (inf)	mem 23876MB
[2022-11-13 02:59:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][1050/1251]	eta 0:02:30 lr 0.000295	time 0.7403 (0.7484)	loss 3.2751 (3.0032)	grad_norm 1.7905 (inf)	mem 23876MB
[2022-11-13 03:00:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][1100/1251]	eta 0:01:53 lr 0.000295	time 0.7343 (0.7484)	loss 2.3902 (3.0065)	grad_norm 2.1506 (inf)	mem 23876MB
[2022-11-13 03:00:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][1150/1251]	eta 0:01:15 lr 0.000295	time 0.7326 (0.7483)	loss 2.8965 (3.0065)	grad_norm 2.2128 (inf)	mem 23876MB
[2022-11-13 03:01:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][1200/1251]	eta 0:00:38 lr 0.000294	time 0.7403 (0.7482)	loss 1.9251 (3.0067)	grad_norm 1.8882 (inf)	mem 23876MB
[2022-11-13 03:02:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [191/300][1250/1251]	eta 0:00:00 lr 0.000294	time 0.7235 (0.7480)	loss 3.0966 (3.0067)	grad_norm 1.9746 (inf)	mem 23876MB
[2022-11-13 03:02:11 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 191 training takes 0:15:35
[2022-11-13 03:02:11 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_191.pth saving......
[2022-11-13 03:02:12 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_191.pth saved !!!
[2022-11-13 03:02:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.782 (1.782)	Loss 0.8254 (0.8254)	Acc@1 79.785 (79.785)	Acc@5 96.582 (96.582)	Mem 23876MB
[2022-11-13 03:02:24 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.098 Acc@5 95.806
[2022-11-13 03:02:24 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.1%
[2022-11-13 03:02:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.846 (1.846)	Loss 0.7515 (0.7515)	Acc@1 83.301 (83.301)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-13 03:02:37 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.652 Acc@5 96.470
[2022-11-13 03:02:37 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.7%
[2022-11-13 03:02:37 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.67% at 190 epoch
[2022-11-13 03:02:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][0/1251]	eta 0:48:51 lr 0.000294	time 2.3434 (2.3434)	loss 1.9374 (1.9374)	grad_norm 2.3279 (2.3279)	mem 23876MB
[2022-11-13 03:03:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][50/1251]	eta 0:15:34 lr 0.000294	time 0.7375 (0.7782)	loss 3.2025 (2.9639)	grad_norm 1.9400 (2.0056)	mem 23876MB
[2022-11-13 03:03:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][100/1251]	eta 0:14:36 lr 0.000294	time 0.7306 (0.7618)	loss 3.3440 (3.0430)	grad_norm 2.3598 (1.9873)	mem 23876MB
[2022-11-13 03:04:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][150/1251]	eta 0:13:53 lr 0.000294	time 0.7396 (0.7569)	loss 1.8890 (2.9968)	grad_norm 1.7519 (1.9866)	mem 23876MB
[2022-11-13 03:05:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][200/1251]	eta 0:13:12 lr 0.000293	time 0.7369 (0.7538)	loss 3.2610 (2.9995)	grad_norm 1.9992 (1.9711)	mem 23876MB
[2022-11-13 03:05:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][250/1251]	eta 0:12:33 lr 0.000293	time 0.7457 (0.7523)	loss 3.0845 (3.0064)	grad_norm 2.8020 (1.9745)	mem 23876MB
[2022-11-13 03:06:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][300/1251]	eta 0:11:54 lr 0.000293	time 0.7387 (0.7508)	loss 2.0973 (2.9906)	grad_norm 1.8711 (1.9729)	mem 23876MB
[2022-11-13 03:07:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][350/1251]	eta 0:11:15 lr 0.000293	time 0.7407 (0.7503)	loss 2.8739 (2.9869)	grad_norm 1.7968 (1.9681)	mem 23876MB
[2022-11-13 03:07:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][400/1251]	eta 0:10:37 lr 0.000293	time 0.7354 (0.7494)	loss 3.3098 (2.9918)	grad_norm 1.9045 (1.9562)	mem 23876MB
[2022-11-13 03:08:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][450/1251]	eta 0:10:00 lr 0.000293	time 0.7413 (0.7495)	loss 3.4539 (2.9857)	grad_norm 2.2937 (1.9615)	mem 23876MB
[2022-11-13 03:08:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][500/1251]	eta 0:09:22 lr 0.000292	time 0.7359 (0.7488)	loss 3.3252 (2.9892)	grad_norm 1.9106 (1.9625)	mem 23876MB
[2022-11-13 03:09:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][550/1251]	eta 0:08:44 lr 0.000292	time 0.8281 (0.7488)	loss 3.1364 (2.9928)	grad_norm 2.0524 (1.9723)	mem 23876MB
[2022-11-13 03:10:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][600/1251]	eta 0:08:07 lr 0.000292	time 0.8125 (0.7484)	loss 3.2715 (2.9910)	grad_norm 1.7846 (1.9670)	mem 23876MB
[2022-11-13 03:10:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][650/1251]	eta 0:07:29 lr 0.000292	time 0.7350 (0.7485)	loss 1.7104 (2.9852)	grad_norm 1.8046 (1.9741)	mem 23876MB
[2022-11-13 03:11:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][700/1251]	eta 0:06:52 lr 0.000292	time 0.7361 (0.7480)	loss 2.6325 (2.9878)	grad_norm 1.9190 (1.9746)	mem 23876MB
[2022-11-13 03:11:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][750/1251]	eta 0:06:14 lr 0.000291	time 0.7357 (0.7480)	loss 3.1745 (2.9893)	grad_norm 1.9404 (1.9848)	mem 23876MB
[2022-11-13 03:12:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][800/1251]	eta 0:05:37 lr 0.000291	time 0.7385 (0.7475)	loss 2.7175 (2.9936)	grad_norm 1.6883 (1.9840)	mem 23876MB
[2022-11-13 03:13:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][850/1251]	eta 0:04:59 lr 0.000291	time 0.8230 (0.7477)	loss 2.9200 (3.0067)	grad_norm 1.7719 (1.9782)	mem 23876MB
[2022-11-13 03:13:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][900/1251]	eta 0:04:22 lr 0.000291	time 0.7371 (0.7473)	loss 3.1560 (3.0083)	grad_norm 1.6554 (1.9735)	mem 23876MB
[2022-11-13 03:14:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][950/1251]	eta 0:03:44 lr 0.000291	time 0.7341 (0.7474)	loss 2.6572 (3.0093)	grad_norm 1.9289 (1.9705)	mem 23876MB
[2022-11-13 03:15:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][1000/1251]	eta 0:03:07 lr 0.000290	time 0.7482 (0.7473)	loss 2.9056 (3.0049)	grad_norm 1.8035 (1.9678)	mem 23876MB
[2022-11-13 03:15:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][1050/1251]	eta 0:02:30 lr 0.000290	time 0.7429 (0.7473)	loss 3.3306 (3.0056)	grad_norm 1.8612 (1.9684)	mem 23876MB
[2022-11-13 03:16:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][1100/1251]	eta 0:01:52 lr 0.000290	time 0.7343 (0.7472)	loss 3.1761 (3.0004)	grad_norm 1.7791 (1.9704)	mem 23876MB
[2022-11-13 03:16:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][1150/1251]	eta 0:01:15 lr 0.000290	time 0.7338 (0.7472)	loss 3.6473 (3.0027)	grad_norm 2.2634 (1.9684)	mem 23876MB
[2022-11-13 03:17:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][1200/1251]	eta 0:00:38 lr 0.000290	time 0.7365 (0.7469)	loss 2.4174 (3.0053)	grad_norm 1.8864 (1.9663)	mem 23876MB
[2022-11-13 03:18:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [192/300][1250/1251]	eta 0:00:00 lr 0.000290	time 0.7930 (0.7469)	loss 3.2440 (3.0067)	grad_norm 1.7374 (1.9661)	mem 23876MB
[2022-11-13 03:18:11 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 192 training takes 0:15:34
[2022-11-13 03:18:12 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_192.pth saving......
[2022-11-13 03:18:13 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_192.pth saved !!!
[2022-11-13 03:18:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.674 (1.674)	Loss 0.7022 (0.7022)	Acc@1 81.836 (81.836)	Acc@5 97.461 (97.461)	Mem 23876MB
[2022-11-13 03:18:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.364 Acc@5 95.902
[2022-11-13 03:18:25 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.4%
[2022-11-13 03:18:27 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.803 (1.803)	Loss 0.6446 (0.6446)	Acc@1 84.180 (84.180)	Acc@5 96.582 (96.582)	Mem 23876MB
[2022-11-13 03:18:38 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.682 Acc@5 96.476
[2022-11-13 03:18:38 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.7%
[2022-11-13 03:18:38 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.68% at 192 epoch
[2022-11-13 03:18:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][0/1251]	eta 0:53:27 lr 0.000290	time 2.5637 (2.5637)	loss 3.0559 (3.0559)	grad_norm 1.7105 (1.7105)	mem 23876MB
[2022-11-13 03:19:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][50/1251]	eta 0:15:48 lr 0.000289	time 0.7412 (0.7900)	loss 3.4006 (3.0197)	grad_norm 1.8449 (2.0422)	mem 23876MB
[2022-11-13 03:19:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][100/1251]	eta 0:14:42 lr 0.000289	time 0.7434 (0.7668)	loss 2.5731 (3.0186)	grad_norm 1.9101 (2.0084)	mem 23876MB
[2022-11-13 03:20:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][150/1251]	eta 0:13:59 lr 0.000289	time 0.7396 (0.7623)	loss 3.5901 (3.0097)	grad_norm 3.3898 (2.0005)	mem 23876MB
[2022-11-13 03:21:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][200/1251]	eta 0:13:17 lr 0.000289	time 0.8064 (0.7584)	loss 2.1716 (3.0024)	grad_norm 1.5940 (1.9894)	mem 23876MB
[2022-11-13 03:21:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][250/1251]	eta 0:12:37 lr 0.000289	time 0.7374 (0.7566)	loss 2.1551 (2.9920)	grad_norm 1.8011 (1.9779)	mem 23876MB
[2022-11-13 03:22:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][300/1251]	eta 0:11:58 lr 0.000288	time 0.7349 (0.7554)	loss 1.9249 (2.9920)	grad_norm 1.7135 (1.9603)	mem 23876MB
[2022-11-13 03:23:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][350/1251]	eta 0:11:19 lr 0.000288	time 0.7362 (0.7539)	loss 3.1038 (2.9726)	grad_norm 1.9032 (1.9615)	mem 23876MB
[2022-11-13 03:23:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][400/1251]	eta 0:10:40 lr 0.000288	time 0.7386 (0.7530)	loss 3.2312 (2.9750)	grad_norm 2.0396 (1.9691)	mem 23876MB
[2022-11-13 03:24:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][450/1251]	eta 0:10:02 lr 0.000288	time 0.7406 (0.7528)	loss 3.0052 (2.9719)	grad_norm 1.6927 (1.9716)	mem 23876MB
[2022-11-13 03:24:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][500/1251]	eta 0:09:24 lr 0.000288	time 0.7432 (0.7519)	loss 2.8500 (2.9731)	grad_norm 1.8332 (1.9728)	mem 23876MB
[2022-11-13 03:25:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][550/1251]	eta 0:08:47 lr 0.000288	time 0.7400 (0.7519)	loss 2.8816 (2.9749)	grad_norm 1.8248 (1.9725)	mem 23876MB
[2022-11-13 03:26:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][600/1251]	eta 0:08:09 lr 0.000287	time 0.7330 (0.7513)	loss 3.4592 (2.9783)	grad_norm 1.8242 (1.9672)	mem 23876MB
[2022-11-13 03:26:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][650/1251]	eta 0:07:31 lr 0.000287	time 0.7471 (0.7512)	loss 2.0732 (2.9744)	grad_norm 1.6051 (1.9700)	mem 23876MB
[2022-11-13 03:27:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][700/1251]	eta 0:06:53 lr 0.000287	time 0.7357 (0.7509)	loss 3.2329 (2.9842)	grad_norm 1.7363 (1.9673)	mem 23876MB
[2022-11-13 03:28:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][750/1251]	eta 0:06:16 lr 0.000287	time 0.7353 (0.7506)	loss 3.3169 (2.9840)	grad_norm 2.6316 (1.9668)	mem 23876MB
[2022-11-13 03:28:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][800/1251]	eta 0:05:38 lr 0.000287	time 0.7350 (0.7503)	loss 3.2393 (2.9861)	grad_norm 1.6934 (1.9677)	mem 23876MB
[2022-11-13 03:29:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][850/1251]	eta 0:05:00 lr 0.000286	time 0.7367 (0.7504)	loss 3.2592 (2.9857)	grad_norm 1.9610 (1.9689)	mem 23876MB
[2022-11-13 03:29:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][900/1251]	eta 0:04:23 lr 0.000286	time 0.7406 (0.7500)	loss 3.3294 (2.9833)	grad_norm 1.8725 (1.9723)	mem 23876MB
[2022-11-13 03:30:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][950/1251]	eta 0:03:45 lr 0.000286	time 0.7365 (0.7500)	loss 3.4022 (2.9859)	grad_norm 2.0999 (1.9750)	mem 23876MB
[2022-11-13 03:31:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][1000/1251]	eta 0:03:08 lr 0.000286	time 0.7364 (0.7498)	loss 3.2787 (2.9852)	grad_norm 1.9266 (1.9723)	mem 23876MB
[2022-11-13 03:31:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][1050/1251]	eta 0:02:30 lr 0.000286	time 0.7351 (0.7497)	loss 2.7497 (2.9886)	grad_norm 2.0917 (1.9707)	mem 23876MB
[2022-11-13 03:32:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][1100/1251]	eta 0:01:53 lr 0.000285	time 0.7422 (0.7496)	loss 2.1727 (2.9818)	grad_norm 1.7831 (1.9692)	mem 23876MB
[2022-11-13 03:33:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][1150/1251]	eta 0:01:15 lr 0.000285	time 0.7423 (0.7494)	loss 2.9657 (2.9815)	grad_norm 2.1192 (1.9691)	mem 23876MB
[2022-11-13 03:33:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][1200/1251]	eta 0:00:38 lr 0.000285	time 0.8126 (0.7494)	loss 2.6293 (2.9747)	grad_norm 1.7670 (1.9689)	mem 23876MB
[2022-11-13 03:34:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [193/300][1250/1251]	eta 0:00:00 lr 0.000285	time 0.7280 (0.7492)	loss 2.9680 (2.9812)	grad_norm 1.8563 (1.9681)	mem 23876MB
[2022-11-13 03:34:15 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 193 training takes 0:15:37
[2022-11-13 03:34:15 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_193.pth saving......
[2022-11-13 03:34:16 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_193.pth saved !!!
[2022-11-13 03:34:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.634 (1.634)	Loss 0.7540 (0.7540)	Acc@1 82.129 (82.129)	Acc@5 95.996 (95.996)	Mem 23876MB
[2022-11-13 03:34:29 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.366 Acc@5 95.866
[2022-11-13 03:34:29 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.4%
[2022-11-13 03:34:31 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.799 (1.799)	Loss 0.7817 (0.7817)	Acc@1 81.641 (81.641)	Acc@5 95.508 (95.508)	Mem 23876MB
[2022-11-13 03:34:41 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.706 Acc@5 96.492
[2022-11-13 03:34:41 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.7%
[2022-11-13 03:34:41 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.71% at 193 epoch
[2022-11-13 03:34:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][0/1251]	eta 0:49:02 lr 0.000285	time 2.3518 (2.3518)	loss 2.8316 (2.8316)	grad_norm 1.9537 (1.9537)	mem 23876MB
[2022-11-13 03:35:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][50/1251]	eta 0:15:39 lr 0.000285	time 0.7420 (0.7821)	loss 3.0898 (2.9985)	grad_norm 1.7182 (1.9668)	mem 23876MB
[2022-11-13 03:35:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][100/1251]	eta 0:14:40 lr 0.000285	time 0.7378 (0.7648)	loss 3.2593 (3.0181)	grad_norm 1.7864 (1.9331)	mem 23876MB
[2022-11-13 03:36:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][150/1251]	eta 0:13:55 lr 0.000284	time 0.7384 (0.7589)	loss 3.4081 (2.9989)	grad_norm 2.0840 (1.9789)	mem 23876MB
[2022-11-13 03:37:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][200/1251]	eta 0:13:13 lr 0.000284	time 0.7392 (0.7552)	loss 3.2524 (2.9831)	grad_norm 1.9469 (1.9959)	mem 23876MB
[2022-11-13 03:37:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][250/1251]	eta 0:12:34 lr 0.000284	time 0.7350 (0.7540)	loss 3.5474 (2.9650)	grad_norm 2.0109 (1.9791)	mem 23876MB
[2022-11-13 03:38:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][300/1251]	eta 0:11:56 lr 0.000284	time 0.7376 (0.7529)	loss 2.7655 (2.9693)	grad_norm 1.9887 (1.9697)	mem 23876MB
[2022-11-13 03:39:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][350/1251]	eta 0:11:17 lr 0.000284	time 0.7349 (0.7523)	loss 3.0142 (2.9754)	grad_norm 1.8475 (1.9623)	mem 23876MB
[2022-11-13 03:39:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][400/1251]	eta 0:10:39 lr 0.000283	time 0.8161 (0.7518)	loss 3.1228 (2.9728)	grad_norm 1.8700 (1.9617)	mem 23876MB
[2022-11-13 03:40:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][450/1251]	eta 0:10:01 lr 0.000283	time 0.7382 (0.7509)	loss 2.9267 (2.9776)	grad_norm 1.6047 (1.9625)	mem 23876MB
[2022-11-13 03:40:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][500/1251]	eta 0:09:23 lr 0.000283	time 0.7394 (0.7504)	loss 2.6542 (2.9738)	grad_norm 1.6845 (1.9632)	mem 23876MB
[2022-11-13 03:41:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][550/1251]	eta 0:08:45 lr 0.000283	time 0.7389 (0.7501)	loss 3.1425 (2.9772)	grad_norm 1.8290 (1.9590)	mem 23876MB
[2022-11-13 03:42:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][600/1251]	eta 0:08:08 lr 0.000283	time 0.7340 (0.7500)	loss 3.0619 (2.9871)	grad_norm 1.8354 (1.9568)	mem 23876MB
[2022-11-13 03:42:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][650/1251]	eta 0:07:30 lr 0.000282	time 0.7402 (0.7499)	loss 3.3374 (2.9833)	grad_norm 1.7861 (1.9601)	mem 23876MB
[2022-11-13 03:43:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][700/1251]	eta 0:06:53 lr 0.000282	time 0.7387 (0.7497)	loss 3.2819 (2.9759)	grad_norm 1.9324 (1.9544)	mem 23876MB
[2022-11-13 03:44:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][750/1251]	eta 0:06:15 lr 0.000282	time 0.7390 (0.7493)	loss 3.4349 (2.9827)	grad_norm 1.9386 (1.9502)	mem 23876MB
[2022-11-13 03:44:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][800/1251]	eta 0:05:37 lr 0.000282	time 0.7398 (0.7494)	loss 3.2279 (2.9847)	grad_norm 1.8527 (1.9545)	mem 23876MB
[2022-11-13 03:45:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][850/1251]	eta 0:05:00 lr 0.000282	time 0.7375 (0.7494)	loss 2.3660 (2.9773)	grad_norm 1.8740 (1.9594)	mem 23876MB
[2022-11-13 03:45:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][900/1251]	eta 0:04:22 lr 0.000282	time 0.7404 (0.7492)	loss 3.2997 (2.9686)	grad_norm 2.3510 (1.9563)	mem 23876MB
[2022-11-13 03:46:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][950/1251]	eta 0:03:45 lr 0.000281	time 0.8147 (0.7491)	loss 1.9143 (2.9721)	grad_norm 1.4740 (1.9555)	mem 23876MB
[2022-11-13 03:47:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][1000/1251]	eta 0:03:07 lr 0.000281	time 0.8026 (0.7489)	loss 1.8227 (2.9753)	grad_norm 2.2889 (1.9604)	mem 23876MB
[2022-11-13 03:47:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][1050/1251]	eta 0:02:30 lr 0.000281	time 0.7421 (0.7488)	loss 3.1670 (2.9772)	grad_norm 2.1031 (1.9606)	mem 23876MB
[2022-11-13 03:48:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][1100/1251]	eta 0:01:53 lr 0.000281	time 0.7427 (0.7488)	loss 3.2828 (2.9708)	grad_norm 1.8970 (1.9616)	mem 23876MB
[2022-11-13 03:49:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][1150/1251]	eta 0:01:15 lr 0.000281	time 0.7383 (0.7488)	loss 2.3492 (2.9691)	grad_norm 1.6979 (1.9605)	mem 23876MB
[2022-11-13 03:49:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][1200/1251]	eta 0:00:38 lr 0.000280	time 0.7449 (0.7488)	loss 2.1304 (2.9710)	grad_norm 2.3755 (1.9664)	mem 23876MB
[2022-11-13 03:50:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [194/300][1250/1251]	eta 0:00:00 lr 0.000280	time 0.7265 (0.7485)	loss 3.2431 (2.9676)	grad_norm 1.9104 (1.9663)	mem 23876MB
[2022-11-13 03:50:18 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 194 training takes 0:15:36
[2022-11-13 03:50:18 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_194.pth saving......
[2022-11-13 03:50:19 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_194.pth saved !!!
[2022-11-13 03:50:21 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.638 (1.638)	Loss 0.7424 (0.7424)	Acc@1 83.301 (83.301)	Acc@5 96.484 (96.484)	Mem 23876MB
[2022-11-13 03:50:32 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.284 Acc@5 95.822
[2022-11-13 03:50:32 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.3%
[2022-11-13 03:50:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.939 (1.939)	Loss 0.6610 (0.6610)	Acc@1 84.375 (84.375)	Acc@5 96.973 (96.973)	Mem 23876MB
[2022-11-13 03:50:44 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.734 Acc@5 96.504
[2022-11-13 03:50:44 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.7%
[2022-11-13 03:50:44 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.73% at 194 epoch
[2022-11-13 03:50:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][0/1251]	eta 0:50:16 lr 0.000280	time 2.4117 (2.4117)	loss 3.5107 (3.5107)	grad_norm 2.0880 (2.0880)	mem 23876MB
[2022-11-13 03:51:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][50/1251]	eta 0:15:35 lr 0.000280	time 0.8135 (0.7787)	loss 2.3445 (2.9425)	grad_norm 1.7936 (nan)	mem 23876MB
[2022-11-13 03:52:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][100/1251]	eta 0:14:38 lr 0.000280	time 0.7437 (0.7630)	loss 3.4989 (2.9381)	grad_norm 2.5071 (nan)	mem 23876MB
[2022-11-13 03:52:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][150/1251]	eta 0:13:53 lr 0.000280	time 0.7393 (0.7574)	loss 3.5023 (2.9809)	grad_norm 2.6584 (nan)	mem 23876MB
[2022-11-13 03:53:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][200/1251]	eta 0:13:12 lr 0.000280	time 0.7420 (0.7544)	loss 3.3808 (2.9710)	grad_norm 1.5943 (nan)	mem 23876MB
[2022-11-13 03:53:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][250/1251]	eta 0:12:33 lr 0.000279	time 0.7385 (0.7527)	loss 3.4736 (2.9749)	grad_norm 1.8054 (nan)	mem 23876MB
[2022-11-13 03:54:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][300/1251]	eta 0:11:54 lr 0.000279	time 0.7395 (0.7517)	loss 2.2096 (2.9756)	grad_norm 1.8982 (nan)	mem 23876MB
[2022-11-13 03:55:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][350/1251]	eta 0:11:16 lr 0.000279	time 0.7403 (0.7510)	loss 2.1233 (2.9739)	grad_norm 1.9041 (nan)	mem 23876MB
[2022-11-13 03:55:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][400/1251]	eta 0:10:38 lr 0.000279	time 0.7390 (0.7504)	loss 3.4057 (2.9730)	grad_norm 2.0381 (nan)	mem 23876MB
[2022-11-13 03:56:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][450/1251]	eta 0:10:00 lr 0.000279	time 0.7351 (0.7499)	loss 3.0192 (2.9646)	grad_norm 1.9010 (nan)	mem 23876MB
[2022-11-13 03:57:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][500/1251]	eta 0:09:22 lr 0.000278	time 0.7377 (0.7495)	loss 2.8825 (2.9704)	grad_norm 1.6238 (nan)	mem 23876MB
[2022-11-13 03:57:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][550/1251]	eta 0:08:45 lr 0.000278	time 0.7378 (0.7493)	loss 1.8300 (2.9732)	grad_norm 2.1296 (nan)	mem 23876MB
[2022-11-13 03:58:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][600/1251]	eta 0:08:07 lr 0.000278	time 0.8090 (0.7489)	loss 3.2006 (2.9740)	grad_norm 1.6518 (nan)	mem 23876MB
[2022-11-13 03:58:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][650/1251]	eta 0:07:30 lr 0.000278	time 0.7368 (0.7489)	loss 3.2338 (2.9799)	grad_norm 2.5766 (nan)	mem 23876MB
[2022-11-13 03:59:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][700/1251]	eta 0:06:52 lr 0.000278	time 0.7388 (0.7485)	loss 2.5755 (2.9846)	grad_norm 2.8705 (nan)	mem 23876MB
[2022-11-13 04:00:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][750/1251]	eta 0:06:14 lr 0.000278	time 0.7373 (0.7485)	loss 3.6648 (2.9916)	grad_norm 1.6764 (nan)	mem 23876MB
[2022-11-13 04:00:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][800/1251]	eta 0:05:37 lr 0.000277	time 0.7333 (0.7484)	loss 2.5193 (2.9875)	grad_norm 1.9594 (nan)	mem 23876MB
[2022-11-13 04:01:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][850/1251]	eta 0:05:00 lr 0.000277	time 0.7411 (0.7484)	loss 1.7096 (2.9830)	grad_norm 2.0288 (nan)	mem 23876MB
[2022-11-13 04:01:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][900/1251]	eta 0:04:22 lr 0.000277	time 0.7397 (0.7482)	loss 2.5526 (2.9785)	grad_norm 1.6651 (nan)	mem 23876MB
[2022-11-13 04:02:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][950/1251]	eta 0:03:45 lr 0.000277	time 0.7382 (0.7483)	loss 3.0039 (2.9850)	grad_norm 1.7857 (nan)	mem 23876MB
[2022-11-13 04:03:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][1000/1251]	eta 0:03:07 lr 0.000277	time 0.7377 (0.7479)	loss 3.5536 (2.9887)	grad_norm 1.9291 (nan)	mem 23876MB
[2022-11-13 04:03:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][1050/1251]	eta 0:02:30 lr 0.000276	time 0.7418 (0.7480)	loss 2.8248 (2.9936)	grad_norm 1.8671 (nan)	mem 23876MB
[2022-11-13 04:04:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][1100/1251]	eta 0:01:52 lr 0.000276	time 0.7487 (0.7478)	loss 2.3329 (2.9896)	grad_norm 5.7692 (nan)	mem 23876MB
[2022-11-13 04:05:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][1150/1251]	eta 0:01:15 lr 0.000276	time 0.7427 (0.7479)	loss 3.7009 (2.9879)	grad_norm 2.2335 (nan)	mem 23876MB
[2022-11-13 04:05:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][1200/1251]	eta 0:00:38 lr 0.000276	time 0.7352 (0.7478)	loss 2.7974 (2.9863)	grad_norm 1.7527 (nan)	mem 23876MB
[2022-11-13 04:06:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [195/300][1250/1251]	eta 0:00:00 lr 0.000276	time 0.7254 (0.7476)	loss 3.4848 (2.9860)	grad_norm 2.2795 (nan)	mem 23876MB
[2022-11-13 04:06:20 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 195 training takes 0:15:35
[2022-11-13 04:06:20 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_195.pth saving......
[2022-11-13 04:06:21 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_195.pth saved !!!
[2022-11-13 04:06:23 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.812 (1.812)	Loss 0.7770 (0.7770)	Acc@1 81.641 (81.641)	Acc@5 96.582 (96.582)	Mem 23876MB
[2022-11-13 04:06:34 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.264 Acc@5 95.902
[2022-11-13 04:06:34 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.3%
[2022-11-13 04:06:35 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.835 (1.835)	Loss 0.7655 (0.7655)	Acc@1 80.371 (80.371)	Acc@5 95.996 (95.996)	Mem 23876MB
[2022-11-13 04:06:46 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.772 Acc@5 96.480
[2022-11-13 04:06:46 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.8%
[2022-11-13 04:06:46 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.77% at 195 epoch
[2022-11-13 04:06:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][0/1251]	eta 0:48:55 lr 0.000276	time 2.3465 (2.3465)	loss 2.9788 (2.9788)	grad_norm 1.8893 (1.8893)	mem 23876MB
[2022-11-13 04:07:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][50/1251]	eta 0:15:37 lr 0.000275	time 0.7397 (0.7806)	loss 2.7164 (2.9522)	grad_norm 1.9446 (2.0173)	mem 23876MB
[2022-11-13 04:08:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][100/1251]	eta 0:14:40 lr 0.000275	time 0.7395 (0.7647)	loss 2.1090 (2.9760)	grad_norm 2.6825 (2.0087)	mem 23876MB
[2022-11-13 04:08:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][150/1251]	eta 0:13:55 lr 0.000275	time 0.7376 (0.7585)	loss 3.4357 (2.9755)	grad_norm 1.8793 (2.0036)	mem 23876MB
[2022-11-13 04:09:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][200/1251]	eta 0:13:14 lr 0.000275	time 0.8237 (0.7561)	loss 2.0361 (2.9404)	grad_norm 1.7193 (2.0017)	mem 23876MB
[2022-11-13 04:09:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][250/1251]	eta 0:12:35 lr 0.000275	time 0.7354 (0.7544)	loss 3.1908 (2.9200)	grad_norm 1.8771 (1.9949)	mem 23876MB
[2022-11-13 04:10:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][300/1251]	eta 0:11:56 lr 0.000275	time 0.7507 (0.7533)	loss 3.2018 (2.9151)	grad_norm 1.8606 (1.9895)	mem 23876MB
[2022-11-13 04:11:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][350/1251]	eta 0:11:18 lr 0.000274	time 0.7390 (0.7525)	loss 3.0985 (2.9241)	grad_norm 2.2638 (1.9867)	mem 23876MB
[2022-11-13 04:11:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][400/1251]	eta 0:10:39 lr 0.000274	time 0.7376 (0.7518)	loss 3.0982 (2.9258)	grad_norm 2.4605 (1.9856)	mem 23876MB
[2022-11-13 04:12:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][450/1251]	eta 0:10:01 lr 0.000274	time 0.7384 (0.7513)	loss 3.4144 (2.9327)	grad_norm 1.9534 (inf)	mem 23876MB
[2022-11-13 04:13:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][500/1251]	eta 0:09:23 lr 0.000274	time 0.7336 (0.7509)	loss 3.1232 (2.9424)	grad_norm 1.9681 (inf)	mem 23876MB
[2022-11-13 04:13:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][550/1251]	eta 0:08:46 lr 0.000274	time 0.7397 (0.7505)	loss 2.2944 (2.9492)	grad_norm 1.7881 (inf)	mem 23876MB
[2022-11-13 04:14:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][600/1251]	eta 0:08:08 lr 0.000273	time 0.8271 (0.7503)	loss 3.2209 (2.9527)	grad_norm 1.9290 (inf)	mem 23876MB
[2022-11-13 04:14:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][650/1251]	eta 0:07:30 lr 0.000273	time 0.7421 (0.7501)	loss 2.0546 (2.9604)	grad_norm 1.7806 (inf)	mem 23876MB
[2022-11-13 04:15:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][700/1251]	eta 0:06:53 lr 0.000273	time 0.7395 (0.7498)	loss 3.0424 (2.9683)	grad_norm 1.8331 (inf)	mem 23876MB
[2022-11-13 04:16:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][750/1251]	eta 0:06:15 lr 0.000273	time 0.7408 (0.7498)	loss 3.2439 (2.9677)	grad_norm 1.8792 (inf)	mem 23876MB
[2022-11-13 04:16:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][800/1251]	eta 0:05:38 lr 0.000273	time 0.7381 (0.7496)	loss 3.3163 (2.9711)	grad_norm 1.7489 (inf)	mem 23876MB
[2022-11-13 04:17:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][850/1251]	eta 0:05:00 lr 0.000273	time 0.7405 (0.7497)	loss 3.2014 (2.9671)	grad_norm 1.9339 (inf)	mem 23876MB
[2022-11-13 04:18:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][900/1251]	eta 0:04:23 lr 0.000272	time 0.7382 (0.7493)	loss 2.3806 (2.9650)	grad_norm 1.8277 (inf)	mem 23876MB
[2022-11-13 04:18:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][950/1251]	eta 0:03:45 lr 0.000272	time 0.7356 (0.7494)	loss 3.5242 (2.9651)	grad_norm 1.8156 (inf)	mem 23876MB
[2022-11-13 04:19:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][1000/1251]	eta 0:03:08 lr 0.000272	time 0.8291 (0.7492)	loss 2.6103 (2.9674)	grad_norm 1.9754 (inf)	mem 23876MB
[2022-11-13 04:19:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][1050/1251]	eta 0:02:30 lr 0.000272	time 0.7430 (0.7490)	loss 3.1331 (2.9723)	grad_norm 1.8044 (inf)	mem 23876MB
[2022-11-13 04:20:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][1100/1251]	eta 0:01:53 lr 0.000272	time 0.7397 (0.7489)	loss 2.4661 (2.9712)	grad_norm 2.0149 (inf)	mem 23876MB
[2022-11-13 04:21:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][1150/1251]	eta 0:01:15 lr 0.000271	time 0.7404 (0.7488)	loss 2.1053 (2.9749)	grad_norm 2.1381 (inf)	mem 23876MB
[2022-11-13 04:21:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][1200/1251]	eta 0:00:38 lr 0.000271	time 0.7510 (0.7488)	loss 3.5016 (2.9771)	grad_norm 3.1378 (inf)	mem 23876MB
[2022-11-13 04:22:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [196/300][1250/1251]	eta 0:00:00 lr 0.000271	time 0.7257 (0.7486)	loss 2.0392 (2.9756)	grad_norm 2.2863 (inf)	mem 23876MB
[2022-11-13 04:22:23 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 196 training takes 0:15:36
[2022-11-13 04:22:23 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_196.pth saving......
[2022-11-13 04:22:24 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_196.pth saved !!!
[2022-11-13 04:22:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.688 (1.688)	Loss 0.7204 (0.7204)	Acc@1 82.031 (82.031)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-13 04:22:37 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.354 Acc@5 95.890
[2022-11-13 04:22:37 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.4%
[2022-11-13 04:22:38 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.888 (1.888)	Loss 0.6628 (0.6628)	Acc@1 84.473 (84.473)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-13 04:22:49 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.780 Acc@5 96.484
[2022-11-13 04:22:49 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.8%
[2022-11-13 04:22:49 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.78% at 196 epoch
[2022-11-13 04:22:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][0/1251]	eta 0:51:39 lr 0.000271	time 2.4778 (2.4778)	loss 3.1046 (3.1046)	grad_norm 1.8503 (1.8503)	mem 23876MB
[2022-11-13 04:23:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][50/1251]	eta 0:15:38 lr 0.000271	time 0.7429 (0.7813)	loss 2.9826 (2.9135)	grad_norm 1.6493 (1.9648)	mem 23876MB
[2022-11-13 04:24:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][100/1251]	eta 0:14:42 lr 0.000271	time 0.7355 (0.7671)	loss 3.3093 (2.9579)	grad_norm 1.8225 (2.0224)	mem 23876MB
[2022-11-13 04:24:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][150/1251]	eta 0:13:55 lr 0.000271	time 0.7330 (0.7589)	loss 2.7844 (2.9679)	grad_norm 1.8817 (2.0087)	mem 23876MB
[2022-11-13 04:25:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][200/1251]	eta 0:13:14 lr 0.000270	time 0.7451 (0.7562)	loss 2.9026 (2.9716)	grad_norm 2.0074 (2.0129)	mem 23876MB
[2022-11-13 04:25:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][250/1251]	eta 0:12:35 lr 0.000270	time 0.7387 (0.7552)	loss 3.3182 (2.9743)	grad_norm 1.8737 (2.0156)	mem 23876MB
[2022-11-13 04:26:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][300/1251]	eta 0:11:56 lr 0.000270	time 0.7461 (0.7539)	loss 3.3807 (2.9734)	grad_norm 2.1844 (2.0163)	mem 23876MB
[2022-11-13 04:27:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][350/1251]	eta 0:11:18 lr 0.000270	time 0.7419 (0.7533)	loss 2.9075 (2.9716)	grad_norm 1.8711 (2.0194)	mem 23876MB
[2022-11-13 04:27:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][400/1251]	eta 0:10:40 lr 0.000270	time 0.7609 (0.7525)	loss 2.4342 (2.9645)	grad_norm 1.8602 (2.0206)	mem 23876MB
[2022-11-13 04:28:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][450/1251]	eta 0:10:02 lr 0.000269	time 0.7366 (0.7517)	loss 2.6275 (2.9639)	grad_norm 2.6227 (2.0221)	mem 23876MB
[2022-11-13 04:29:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][500/1251]	eta 0:09:24 lr 0.000269	time 0.7339 (0.7513)	loss 3.0177 (2.9678)	grad_norm 1.8578 (2.0311)	mem 23876MB
[2022-11-13 04:29:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][550/1251]	eta 0:08:46 lr 0.000269	time 0.7413 (0.7507)	loss 2.3796 (2.9585)	grad_norm 2.2074 (2.0244)	mem 23876MB
[2022-11-13 04:30:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][600/1251]	eta 0:08:08 lr 0.000269	time 0.7369 (0.7505)	loss 3.4435 (2.9580)	grad_norm 1.8325 (2.0362)	mem 23876MB
[2022-11-13 04:30:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][650/1251]	eta 0:07:30 lr 0.000269	time 0.7366 (0.7502)	loss 3.2304 (2.9634)	grad_norm 1.7173 (2.0317)	mem 23876MB
[2022-11-13 04:31:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][700/1251]	eta 0:06:53 lr 0.000269	time 0.7363 (0.7496)	loss 3.1498 (2.9657)	grad_norm 1.8954 (2.0242)	mem 23876MB
[2022-11-13 04:32:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][750/1251]	eta 0:06:15 lr 0.000268	time 0.7388 (0.7493)	loss 3.3322 (2.9722)	grad_norm 1.9878 (2.0182)	mem 23876MB
[2022-11-13 04:32:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][800/1251]	eta 0:05:37 lr 0.000268	time 0.7472 (0.7492)	loss 2.8086 (2.9724)	grad_norm 1.8192 (2.0197)	mem 23876MB
[2022-11-13 04:33:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][850/1251]	eta 0:05:00 lr 0.000268	time 0.7350 (0.7491)	loss 2.1547 (2.9717)	grad_norm 1.8404 (2.0224)	mem 23876MB
[2022-11-13 04:34:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][900/1251]	eta 0:04:22 lr 0.000268	time 0.7352 (0.7490)	loss 3.1994 (2.9657)	grad_norm 2.2124 (2.0171)	mem 23876MB
[2022-11-13 04:34:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][950/1251]	eta 0:03:45 lr 0.000268	time 0.7346 (0.7489)	loss 1.9231 (2.9641)	grad_norm 2.2694 (2.0164)	mem 23876MB
[2022-11-13 04:35:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][1000/1251]	eta 0:03:07 lr 0.000267	time 0.8072 (0.7486)	loss 3.2552 (2.9703)	grad_norm 1.8432 (2.0123)	mem 23876MB
[2022-11-13 04:35:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][1050/1251]	eta 0:02:30 lr 0.000267	time 0.7360 (0.7486)	loss 3.2547 (2.9700)	grad_norm 3.2513 (2.0175)	mem 23876MB
[2022-11-13 04:36:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][1100/1251]	eta 0:01:52 lr 0.000267	time 0.7374 (0.7483)	loss 3.3690 (2.9709)	grad_norm 2.0872 (2.0162)	mem 23876MB
[2022-11-13 04:37:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][1150/1251]	eta 0:01:15 lr 0.000267	time 0.7367 (0.7483)	loss 3.2345 (2.9709)	grad_norm 1.9721 (2.0163)	mem 23876MB
[2022-11-13 04:37:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][1200/1251]	eta 0:00:38 lr 0.000267	time 0.7357 (0.7482)	loss 3.2357 (2.9726)	grad_norm 1.8558 (2.0178)	mem 23876MB
[2022-11-13 04:38:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [197/300][1250/1251]	eta 0:00:00 lr 0.000267	time 0.7272 (0.7480)	loss 2.7614 (2.9713)	grad_norm 2.1967 (2.0180)	mem 23876MB
[2022-11-13 04:38:25 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 197 training takes 0:15:35
[2022-11-13 04:38:25 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_197.pth saving......
[2022-11-13 04:38:26 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_197.pth saved !!!
[2022-11-13 04:38:28 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.688 (1.688)	Loss 0.8083 (0.8083)	Acc@1 80.078 (80.078)	Acc@5 95.898 (95.898)	Mem 23876MB
[2022-11-13 04:38:39 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.424 Acc@5 95.954
[2022-11-13 04:38:39 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.4%
[2022-11-13 04:38:40 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.778 (1.778)	Loss 0.7927 (0.7927)	Acc@1 81.152 (81.152)	Acc@5 95.801 (95.801)	Mem 23876MB
[2022-11-13 04:38:51 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.856 Acc@5 96.504
[2022-11-13 04:38:51 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.9%
[2022-11-13 04:38:51 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.86% at 197 epoch
[2022-11-13 04:38:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][0/1251]	eta 0:49:48 lr 0.000267	time 2.3885 (2.3885)	loss 3.0739 (3.0739)	grad_norm 1.9333 (1.9333)	mem 23876MB
[2022-11-13 04:39:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][50/1251]	eta 0:15:35 lr 0.000266	time 0.8277 (0.7791)	loss 3.1601 (3.0774)	grad_norm 1.7426 (1.9906)	mem 23876MB
[2022-11-13 04:40:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][100/1251]	eta 0:14:36 lr 0.000266	time 0.7372 (0.7618)	loss 3.2773 (3.0205)	grad_norm 1.8682 (1.9852)	mem 23876MB
[2022-11-13 04:40:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][150/1251]	eta 0:13:52 lr 0.000266	time 0.7381 (0.7565)	loss 3.4490 (2.9963)	grad_norm 2.3071 (1.9940)	mem 23876MB
[2022-11-13 04:41:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][200/1251]	eta 0:13:13 lr 0.000266	time 0.8131 (0.7545)	loss 3.4785 (2.9769)	grad_norm 2.4790 (1.9983)	mem 23876MB
[2022-11-13 04:42:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][250/1251]	eta 0:12:33 lr 0.000266	time 0.7394 (0.7524)	loss 3.1231 (2.9744)	grad_norm 1.6680 (1.9867)	mem 23876MB
[2022-11-13 04:42:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][300/1251]	eta 0:11:54 lr 0.000265	time 0.7505 (0.7514)	loss 2.9374 (2.9641)	grad_norm 2.9548 (1.9906)	mem 23876MB
[2022-11-13 04:43:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][350/1251]	eta 0:11:16 lr 0.000265	time 0.7393 (0.7506)	loss 2.9683 (2.9552)	grad_norm 1.8133 (1.9914)	mem 23876MB
[2022-11-13 04:43:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][400/1251]	eta 0:10:38 lr 0.000265	time 0.7382 (0.7500)	loss 3.0115 (2.9657)	grad_norm 1.7694 (1.9931)	mem 23876MB
[2022-11-13 04:44:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][450/1251]	eta 0:10:00 lr 0.000265	time 0.7389 (0.7498)	loss 3.5128 (2.9618)	grad_norm 2.0672 (1.9957)	mem 23876MB
[2022-11-13 04:45:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][500/1251]	eta 0:09:22 lr 0.000265	time 0.7337 (0.7493)	loss 3.2107 (2.9603)	grad_norm 2.5828 (2.0082)	mem 23876MB
[2022-11-13 04:45:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][550/1251]	eta 0:08:44 lr 0.000265	time 0.7340 (0.7488)	loss 2.0483 (2.9715)	grad_norm 1.8364 (inf)	mem 23876MB
[2022-11-13 04:46:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][600/1251]	eta 0:08:07 lr 0.000264	time 0.8041 (0.7489)	loss 2.6837 (2.9726)	grad_norm 1.7485 (inf)	mem 23876MB
[2022-11-13 04:46:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][650/1251]	eta 0:07:29 lr 0.000264	time 0.7408 (0.7484)	loss 3.3123 (2.9658)	grad_norm 1.7864 (inf)	mem 23876MB
[2022-11-13 04:47:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][700/1251]	eta 0:06:52 lr 0.000264	time 0.7440 (0.7482)	loss 2.7392 (2.9646)	grad_norm 1.8953 (inf)	mem 23876MB
[2022-11-13 04:48:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][750/1251]	eta 0:06:14 lr 0.000264	time 0.7484 (0.7481)	loss 2.6552 (2.9683)	grad_norm 2.2643 (inf)	mem 23876MB
[2022-11-13 04:48:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][800/1251]	eta 0:05:37 lr 0.000264	time 0.7363 (0.7479)	loss 2.2379 (2.9648)	grad_norm 2.0344 (inf)	mem 23876MB
[2022-11-13 04:49:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][850/1251]	eta 0:04:59 lr 0.000263	time 0.7357 (0.7479)	loss 2.7236 (2.9630)	grad_norm 2.1705 (inf)	mem 23876MB
[2022-11-13 04:50:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][900/1251]	eta 0:04:22 lr 0.000263	time 0.7341 (0.7477)	loss 2.5007 (2.9582)	grad_norm 1.6994 (inf)	mem 23876MB
[2022-11-13 04:50:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][950/1251]	eta 0:03:45 lr 0.000263	time 0.7330 (0.7479)	loss 2.4468 (2.9578)	grad_norm 1.7750 (inf)	mem 23876MB
[2022-11-13 04:51:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][1000/1251]	eta 0:03:07 lr 0.000263	time 0.8390 (0.7478)	loss 3.0589 (2.9560)	grad_norm 2.0090 (inf)	mem 23876MB
[2022-11-13 04:51:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][1050/1251]	eta 0:02:30 lr 0.000263	time 0.7380 (0.7477)	loss 3.2188 (2.9529)	grad_norm 2.0436 (inf)	mem 23876MB
[2022-11-13 04:52:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][1100/1251]	eta 0:01:52 lr 0.000263	time 0.7375 (0.7476)	loss 3.1107 (2.9563)	grad_norm 2.0304 (inf)	mem 23876MB
[2022-11-13 04:53:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][1150/1251]	eta 0:01:15 lr 0.000262	time 0.7347 (0.7476)	loss 2.6162 (2.9582)	grad_norm 1.6898 (inf)	mem 23876MB
[2022-11-13 04:53:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][1200/1251]	eta 0:00:38 lr 0.000262	time 0.7411 (0.7475)	loss 2.7502 (2.9608)	grad_norm 1.7641 (inf)	mem 23876MB
[2022-11-13 04:54:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [198/300][1250/1251]	eta 0:00:00 lr 0.000262	time 0.7261 (0.7474)	loss 2.7519 (2.9604)	grad_norm 1.7524 (inf)	mem 23876MB
[2022-11-13 04:54:26 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 198 training takes 0:15:35
[2022-11-13 04:54:27 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_198.pth saving......
[2022-11-13 04:54:28 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_198.pth saved !!!
[2022-11-13 04:54:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.602 (1.602)	Loss 0.9116 (0.9116)	Acc@1 79.590 (79.590)	Acc@5 94.336 (94.336)	Mem 23876MB
[2022-11-13 04:54:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.490 Acc@5 95.878
[2022-11-13 04:54:40 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.5%
[2022-11-13 04:54:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.825 (1.825)	Loss 0.6477 (0.6477)	Acc@1 83.496 (83.496)	Acc@5 97.168 (97.168)	Mem 23876MB
[2022-11-13 04:54:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.818 Acc@5 96.504
[2022-11-13 04:54:53 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.8%
[2022-11-13 04:54:53 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.86% at 197 epoch
[2022-11-13 04:54:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][0/1251]	eta 0:50:58 lr 0.000262	time 2.4446 (2.4446)	loss 1.9923 (1.9923)	grad_norm 1.8121 (1.8121)	mem 23876MB
[2022-11-13 04:55:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][50/1251]	eta 0:15:42 lr 0.000262	time 0.7384 (0.7847)	loss 2.0701 (2.9719)	grad_norm 2.0030 (2.0390)	mem 23876MB
[2022-11-13 04:56:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][100/1251]	eta 0:14:42 lr 0.000262	time 0.7373 (0.7665)	loss 3.3078 (2.9199)	grad_norm 1.7985 (2.0624)	mem 23876MB
[2022-11-13 04:56:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][150/1251]	eta 0:13:57 lr 0.000261	time 0.7413 (0.7607)	loss 2.8647 (2.9506)	grad_norm 1.7890 (2.0585)	mem 23876MB
[2022-11-13 04:57:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][200/1251]	eta 0:13:16 lr 0.000261	time 0.8203 (0.7575)	loss 1.7519 (2.9406)	grad_norm 2.1613 (2.0553)	mem 23876MB
[2022-11-13 04:58:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][250/1251]	eta 0:12:36 lr 0.000261	time 0.7351 (0.7555)	loss 3.1675 (2.9454)	grad_norm 2.0082 (2.0496)	mem 23876MB
[2022-11-13 04:58:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][300/1251]	eta 0:11:57 lr 0.000261	time 0.7386 (0.7540)	loss 2.5105 (2.9427)	grad_norm 1.8462 (2.0633)	mem 23876MB
[2022-11-13 04:59:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][350/1251]	eta 0:11:18 lr 0.000261	time 0.7370 (0.7532)	loss 2.4446 (2.9383)	grad_norm 2.0211 (2.0600)	mem 23876MB
[2022-11-13 04:59:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][400/1251]	eta 0:10:40 lr 0.000261	time 0.7414 (0.7521)	loss 2.6350 (2.9456)	grad_norm 1.8434 (2.0635)	mem 23876MB
[2022-11-13 05:00:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][450/1251]	eta 0:10:02 lr 0.000260	time 0.7435 (0.7519)	loss 2.9876 (2.9541)	grad_norm 1.8048 (2.0572)	mem 23876MB
[2022-11-13 05:01:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][500/1251]	eta 0:09:24 lr 0.000260	time 0.7384 (0.7513)	loss 2.6813 (2.9419)	grad_norm 1.9493 (2.0452)	mem 23876MB
[2022-11-13 05:01:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][550/1251]	eta 0:08:46 lr 0.000260	time 0.7378 (0.7511)	loss 2.0344 (2.9380)	grad_norm 2.1813 (2.0435)	mem 23876MB
[2022-11-13 05:02:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][600/1251]	eta 0:08:08 lr 0.000260	time 0.8287 (0.7508)	loss 3.2710 (2.9415)	grad_norm 2.2227 (2.0440)	mem 23876MB
[2022-11-13 05:03:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][650/1251]	eta 0:07:30 lr 0.000260	time 0.8314 (0.7503)	loss 3.3498 (2.9407)	grad_norm 1.9849 (2.0471)	mem 23876MB
[2022-11-13 05:03:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][700/1251]	eta 0:06:53 lr 0.000259	time 0.7344 (0.7500)	loss 2.9144 (2.9406)	grad_norm 2.1872 (2.0528)	mem 23876MB
[2022-11-13 05:04:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][750/1251]	eta 0:06:15 lr 0.000259	time 0.7360 (0.7498)	loss 2.9492 (2.9477)	grad_norm 2.0455 (2.0525)	mem 23876MB
[2022-11-13 05:04:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][800/1251]	eta 0:05:38 lr 0.000259	time 0.7383 (0.7495)	loss 3.6824 (2.9478)	grad_norm 2.0498 (2.0633)	mem 23876MB
[2022-11-13 05:05:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][850/1251]	eta 0:05:00 lr 0.000259	time 0.7414 (0.7495)	loss 3.3882 (2.9563)	grad_norm 1.6878 (2.0611)	mem 23876MB
[2022-11-13 05:06:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][900/1251]	eta 0:04:22 lr 0.000259	time 0.7376 (0.7493)	loss 2.3588 (2.9592)	grad_norm 1.9467 (2.0602)	mem 23876MB
[2022-11-13 05:06:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][950/1251]	eta 0:03:45 lr 0.000259	time 0.7359 (0.7492)	loss 2.8103 (2.9595)	grad_norm 2.0533 (2.0606)	mem 23876MB
[2022-11-13 05:07:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][1000/1251]	eta 0:03:08 lr 0.000258	time 0.8204 (0.7491)	loss 3.2829 (2.9570)	grad_norm 2.1312 (2.0581)	mem 23876MB
[2022-11-13 05:08:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][1050/1251]	eta 0:02:30 lr 0.000258	time 0.7371 (0.7488)	loss 3.4548 (2.9536)	grad_norm 1.9539 (2.0544)	mem 23876MB
[2022-11-13 05:08:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][1100/1251]	eta 0:01:53 lr 0.000258	time 0.7362 (0.7487)	loss 3.0972 (2.9519)	grad_norm 2.9503 (2.0548)	mem 23876MB
[2022-11-13 05:09:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][1150/1251]	eta 0:01:15 lr 0.000258	time 0.7372 (0.7486)	loss 2.0398 (2.9453)	grad_norm 2.2218 (2.0575)	mem 23876MB
[2022-11-13 05:09:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][1200/1251]	eta 0:00:38 lr 0.000258	time 0.7380 (0.7486)	loss 3.2561 (2.9434)	grad_norm 1.8981 (2.0562)	mem 23876MB
[2022-11-13 05:10:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [199/300][1250/1251]	eta 0:00:00 lr 0.000258	time 0.7268 (0.7484)	loss 3.1010 (2.9449)	grad_norm 1.9807 (2.0552)	mem 23876MB
[2022-11-13 05:10:29 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 199 training takes 0:15:36
[2022-11-13 05:10:29 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_199.pth saving......
[2022-11-13 05:10:30 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_199.pth saved !!!
[2022-11-13 05:10:32 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.599 (1.599)	Loss 0.7599 (0.7599)	Acc@1 80.859 (80.859)	Acc@5 96.191 (96.191)	Mem 23876MB
[2022-11-13 05:10:43 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.492 Acc@5 95.906
[2022-11-13 05:10:43 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.5%
[2022-11-13 05:10:44 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.794 (1.794)	Loss 0.7554 (0.7554)	Acc@1 81.738 (81.738)	Acc@5 96.484 (96.484)	Mem 23876MB
[2022-11-13 05:10:55 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.852 Acc@5 96.514
[2022-11-13 05:10:55 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.9%
[2022-11-13 05:10:55 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.86% at 197 epoch
[2022-11-13 05:10:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][0/1251]	eta 0:51:51 lr 0.000258	time 2.4870 (2.4870)	loss 2.8274 (2.8274)	grad_norm 1.8765 (1.8765)	mem 23876MB
[2022-11-13 05:11:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][50/1251]	eta 0:15:37 lr 0.000257	time 0.7373 (0.7808)	loss 1.8840 (2.9046)	grad_norm 2.0976 (1.9263)	mem 23876MB
[2022-11-13 05:12:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][100/1251]	eta 0:14:41 lr 0.000257	time 0.7445 (0.7655)	loss 3.2605 (2.9227)	grad_norm 2.2515 (2.0011)	mem 23876MB
[2022-11-13 05:12:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][150/1251]	eta 0:13:57 lr 0.000257	time 0.7374 (0.7603)	loss 2.0235 (2.9259)	grad_norm 1.9909 (2.0024)	mem 23876MB
[2022-11-13 05:13:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][200/1251]	eta 0:13:15 lr 0.000257	time 0.7366 (0.7569)	loss 2.6705 (2.9294)	grad_norm 2.2001 (1.9911)	mem 23876MB
[2022-11-13 05:14:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][250/1251]	eta 0:12:35 lr 0.000257	time 0.7418 (0.7549)	loss 3.1786 (2.9122)	grad_norm 2.0603 (1.9867)	mem 23876MB
[2022-11-13 05:14:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][300/1251]	eta 0:11:56 lr 0.000256	time 0.7360 (0.7533)	loss 2.8329 (2.9288)	grad_norm 1.8764 (1.9886)	mem 23876MB
[2022-11-13 05:15:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][350/1251]	eta 0:11:18 lr 0.000256	time 0.7441 (0.7528)	loss 3.0093 (2.9171)	grad_norm 1.7557 (1.9929)	mem 23876MB
[2022-11-13 05:15:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][400/1251]	eta 0:10:40 lr 0.000256	time 0.7350 (0.7522)	loss 2.8738 (2.9281)	grad_norm 2.2638 (2.0024)	mem 23876MB
[2022-11-13 05:16:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][450/1251]	eta 0:10:01 lr 0.000256	time 0.7406 (0.7515)	loss 3.5420 (2.9346)	grad_norm 2.5836 (2.0076)	mem 23876MB
[2022-11-13 05:17:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][500/1251]	eta 0:09:24 lr 0.000256	time 0.7396 (0.7510)	loss 3.3528 (2.9328)	grad_norm 1.8812 (2.0133)	mem 23876MB
[2022-11-13 05:17:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][550/1251]	eta 0:08:46 lr 0.000256	time 0.7395 (0.7506)	loss 2.2467 (2.9331)	grad_norm 1.8156 (2.0121)	mem 23876MB
[2022-11-13 05:18:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][600/1251]	eta 0:08:08 lr 0.000255	time 0.7351 (0.7504)	loss 3.1065 (2.9383)	grad_norm 2.1725 (2.0170)	mem 23876MB
[2022-11-13 05:19:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][650/1251]	eta 0:07:30 lr 0.000255	time 0.7404 (0.7502)	loss 2.1359 (2.9415)	grad_norm 2.0762 (2.0156)	mem 23876MB
[2022-11-13 05:19:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][700/1251]	eta 0:06:53 lr 0.000255	time 0.7385 (0.7498)	loss 2.8815 (2.9463)	grad_norm 1.9499 (2.0168)	mem 23876MB
[2022-11-13 05:20:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][750/1251]	eta 0:06:15 lr 0.000255	time 0.7416 (0.7499)	loss 3.4814 (2.9517)	grad_norm 2.0849 (2.0179)	mem 23876MB
[2022-11-13 05:20:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][800/1251]	eta 0:05:38 lr 0.000255	time 0.7400 (0.7498)	loss 3.0200 (2.9526)	grad_norm 2.2830 (2.0218)	mem 23876MB
[2022-11-13 05:21:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][850/1251]	eta 0:05:00 lr 0.000254	time 0.7410 (0.7496)	loss 2.0390 (2.9555)	grad_norm 2.0085 (2.0211)	mem 23876MB
[2022-11-13 05:22:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][900/1251]	eta 0:04:23 lr 0.000254	time 0.7401 (0.7495)	loss 2.9847 (2.9553)	grad_norm 1.8742 (2.0255)	mem 23876MB
[2022-11-13 05:22:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][950/1251]	eta 0:03:45 lr 0.000254	time 0.7399 (0.7494)	loss 2.8798 (2.9563)	grad_norm 1.9374 (2.0300)	mem 23876MB
[2022-11-13 05:23:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][1000/1251]	eta 0:03:08 lr 0.000254	time 0.7342 (0.7492)	loss 2.3470 (2.9587)	grad_norm 2.0818 (2.0300)	mem 23876MB
[2022-11-13 05:24:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][1050/1251]	eta 0:02:30 lr 0.000254	time 0.7467 (0.7491)	loss 3.1712 (2.9562)	grad_norm 1.7694 (2.0285)	mem 23876MB
[2022-11-13 05:24:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][1100/1251]	eta 0:01:53 lr 0.000254	time 0.7394 (0.7489)	loss 3.1596 (2.9541)	grad_norm 2.3513 (2.0269)	mem 23876MB
[2022-11-13 05:25:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][1150/1251]	eta 0:01:15 lr 0.000253	time 0.7378 (0.7489)	loss 2.4569 (2.9536)	grad_norm 2.1518 (2.0289)	mem 23876MB
[2022-11-13 05:25:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][1200/1251]	eta 0:00:38 lr 0.000253	time 0.7372 (0.7489)	loss 3.3294 (2.9581)	grad_norm 2.5577 (2.0359)	mem 23876MB
[2022-11-13 05:26:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [200/300][1250/1251]	eta 0:00:00 lr 0.000253	time 0.7258 (0.7485)	loss 3.8103 (2.9639)	grad_norm 2.4509 (2.0341)	mem 23876MB
[2022-11-13 05:26:32 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 200 training takes 0:15:36
[2022-11-13 05:26:32 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_200.pth saving......
[2022-11-13 05:26:33 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_200.pth saved !!!
[2022-11-13 05:26:35 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.776 (1.776)	Loss 0.8484 (0.8484)	Acc@1 80.762 (80.762)	Acc@5 95.703 (95.703)	Mem 23876MB
[2022-11-13 05:26:45 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.606 Acc@5 95.950
[2022-11-13 05:26:45 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.6%
[2022-11-13 05:26:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.785 (1.785)	Loss 0.6939 (0.6939)	Acc@1 82.227 (82.227)	Acc@5 96.582 (96.582)	Mem 23876MB
[2022-11-13 05:26:58 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.880 Acc@5 96.482
[2022-11-13 05:26:58 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.9%
[2022-11-13 05:26:58 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.88% at 200 epoch
[2022-11-13 05:27:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][0/1251]	eta 0:48:54 lr 0.000253	time 2.3461 (2.3461)	loss 2.2515 (2.2515)	grad_norm 2.0204 (2.0204)	mem 23876MB
[2022-11-13 05:27:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][50/1251]	eta 0:15:33 lr 0.000253	time 0.7389 (0.7773)	loss 3.1468 (2.9341)	grad_norm 1.8603 (2.0438)	mem 23876MB
[2022-11-13 05:28:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][100/1251]	eta 0:14:39 lr 0.000253	time 0.7412 (0.7638)	loss 2.9697 (2.9459)	grad_norm 1.8658 (1.9997)	mem 23876MB
[2022-11-13 05:28:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][150/1251]	eta 0:13:53 lr 0.000252	time 0.7343 (0.7573)	loss 3.3091 (2.9963)	grad_norm 2.0023 (2.0044)	mem 23876MB
[2022-11-13 05:29:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][200/1251]	eta 0:13:14 lr 0.000252	time 0.7380 (0.7555)	loss 3.3857 (3.0045)	grad_norm 2.0661 (2.0005)	mem 23876MB
[2022-11-13 05:30:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][250/1251]	eta 0:12:33 lr 0.000252	time 0.7394 (0.7531)	loss 3.2677 (2.9961)	grad_norm 2.1632 (2.0000)	mem 23876MB
[2022-11-13 05:30:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][300/1251]	eta 0:11:55 lr 0.000252	time 0.7351 (0.7526)	loss 3.3611 (2.9645)	grad_norm 1.9622 (2.0014)	mem 23876MB
[2022-11-13 05:31:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][350/1251]	eta 0:11:17 lr 0.000252	time 0.7387 (0.7516)	loss 3.5004 (2.9631)	grad_norm 1.8519 (2.0034)	mem 23876MB
[2022-11-13 05:31:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][400/1251]	eta 0:10:39 lr 0.000252	time 0.7398 (0.7511)	loss 3.1471 (2.9661)	grad_norm 2.5787 (2.0080)	mem 23876MB
[2022-11-13 05:32:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][450/1251]	eta 0:10:01 lr 0.000251	time 0.7406 (0.7505)	loss 3.6128 (2.9744)	grad_norm 2.0022 (2.0104)	mem 23876MB
[2022-11-13 05:33:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][500/1251]	eta 0:09:23 lr 0.000251	time 0.7421 (0.7501)	loss 3.4490 (2.9801)	grad_norm 2.5083 (2.0135)	mem 23876MB
[2022-11-13 05:33:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][550/1251]	eta 0:08:45 lr 0.000251	time 0.7410 (0.7498)	loss 2.7736 (2.9790)	grad_norm 2.5378 (2.0271)	mem 23876MB
[2022-11-13 05:34:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][600/1251]	eta 0:08:07 lr 0.000251	time 0.8021 (0.7496)	loss 3.2972 (2.9730)	grad_norm 2.7829 (2.0249)	mem 23876MB
[2022-11-13 05:35:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][650/1251]	eta 0:07:30 lr 0.000251	time 0.7434 (0.7492)	loss 3.5683 (2.9702)	grad_norm 2.5839 (2.0270)	mem 23876MB
[2022-11-13 05:35:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][700/1251]	eta 0:06:52 lr 0.000251	time 0.7397 (0.7489)	loss 3.0436 (2.9669)	grad_norm 1.6725 (2.0227)	mem 23876MB
[2022-11-13 05:36:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][750/1251]	eta 0:06:15 lr 0.000250	time 0.8026 (0.7488)	loss 3.0094 (2.9669)	grad_norm 1.9441 (2.0265)	mem 23876MB
[2022-11-13 05:36:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][800/1251]	eta 0:05:37 lr 0.000250	time 0.7413 (0.7484)	loss 3.3920 (2.9716)	grad_norm 2.1711 (2.0313)	mem 23876MB
[2022-11-13 05:37:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][850/1251]	eta 0:05:00 lr 0.000250	time 0.7364 (0.7484)	loss 2.8998 (2.9693)	grad_norm 2.0206 (2.0297)	mem 23876MB
[2022-11-13 05:38:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][900/1251]	eta 0:04:22 lr 0.000250	time 0.7371 (0.7482)	loss 3.0058 (2.9697)	grad_norm 2.4147 (2.0334)	mem 23876MB
[2022-11-13 05:38:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][950/1251]	eta 0:03:45 lr 0.000250	time 0.7351 (0.7482)	loss 3.1265 (2.9735)	grad_norm 1.9887 (2.0356)	mem 23876MB
[2022-11-13 05:39:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][1000/1251]	eta 0:03:07 lr 0.000249	time 0.7330 (0.7481)	loss 2.7737 (2.9708)	grad_norm 1.9002 (2.0442)	mem 23876MB
[2022-11-13 05:40:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][1050/1251]	eta 0:02:30 lr 0.000249	time 0.7416 (0.7481)	loss 2.9227 (2.9661)	grad_norm 1.8063 (2.0437)	mem 23876MB
[2022-11-13 05:40:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][1100/1251]	eta 0:01:52 lr 0.000249	time 0.7334 (0.7479)	loss 3.2991 (2.9632)	grad_norm 1.8360 (2.0435)	mem 23876MB
[2022-11-13 05:41:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][1150/1251]	eta 0:01:15 lr 0.000249	time 0.7430 (0.7478)	loss 2.1611 (2.9626)	grad_norm 1.7574 (2.0464)	mem 23876MB
[2022-11-13 05:41:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][1200/1251]	eta 0:00:38 lr 0.000249	time 0.8518 (0.7477)	loss 3.2151 (2.9621)	grad_norm 1.9083 (inf)	mem 23876MB
[2022-11-13 05:42:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [201/300][1250/1251]	eta 0:00:00 lr 0.000249	time 0.7279 (0.7476)	loss 3.1531 (2.9626)	grad_norm 1.9276 (inf)	mem 23876MB
[2022-11-13 05:42:33 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 201 training takes 0:15:35
[2022-11-13 05:42:34 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_201.pth saving......
[2022-11-13 05:42:35 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_201.pth saved !!!
[2022-11-13 05:42:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.596 (1.596)	Loss 0.8351 (0.8351)	Acc@1 79.102 (79.102)	Acc@5 95.703 (95.703)	Mem 23876MB
[2022-11-13 05:42:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.500 Acc@5 95.964
[2022-11-13 05:42:47 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.5%
[2022-11-13 05:42:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.839 (1.839)	Loss 0.6991 (0.6991)	Acc@1 83.203 (83.203)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-13 05:43:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.922 Acc@5 96.514
[2022-11-13 05:43:00 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 82.9%
[2022-11-13 05:43:00 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.92% at 201 epoch
[2022-11-13 05:43:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][0/1251]	eta 0:52:47 lr 0.000249	time 2.5320 (2.5320)	loss 3.3962 (3.3962)	grad_norm 2.2149 (2.2149)	mem 23876MB
[2022-11-13 05:43:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][50/1251]	eta 0:15:45 lr 0.000248	time 0.7340 (0.7876)	loss 2.8855 (2.9932)	grad_norm 2.0250 (2.1429)	mem 23876MB
[2022-11-13 05:44:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][100/1251]	eta 0:14:40 lr 0.000248	time 0.7382 (0.7649)	loss 3.5167 (3.0475)	grad_norm 2.1625 (2.1002)	mem 23876MB
[2022-11-13 05:44:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][150/1251]	eta 0:13:56 lr 0.000248	time 0.7359 (0.7595)	loss 2.9884 (2.9659)	grad_norm 2.1908 (2.0694)	mem 23876MB
[2022-11-13 05:45:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][200/1251]	eta 0:13:14 lr 0.000248	time 0.7421 (0.7561)	loss 3.2135 (2.9624)	grad_norm 1.9746 (2.0801)	mem 23876MB
[2022-11-13 05:46:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][250/1251]	eta 0:12:34 lr 0.000248	time 0.7400 (0.7540)	loss 3.0151 (2.9592)	grad_norm 1.8600 (2.1094)	mem 23876MB
[2022-11-13 05:46:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][300/1251]	eta 0:11:55 lr 0.000248	time 0.7388 (0.7529)	loss 2.9673 (2.9627)	grad_norm 1.9855 (2.0906)	mem 23876MB
[2022-11-13 05:47:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][350/1251]	eta 0:11:17 lr 0.000247	time 0.7392 (0.7521)	loss 2.5292 (2.9442)	grad_norm 1.8465 (2.0994)	mem 23876MB
[2022-11-13 05:48:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][400/1251]	eta 0:10:39 lr 0.000247	time 0.7427 (0.7510)	loss 3.4224 (2.9463)	grad_norm 2.0961 (2.1137)	mem 23876MB
[2022-11-13 05:48:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][450/1251]	eta 0:10:01 lr 0.000247	time 0.7379 (0.7506)	loss 2.9588 (2.9581)	grad_norm 2.0642 (2.1025)	mem 23876MB
[2022-11-13 05:49:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][500/1251]	eta 0:09:23 lr 0.000247	time 0.7358 (0.7499)	loss 3.3256 (2.9602)	grad_norm 2.2082 (2.0964)	mem 23876MB
[2022-11-13 05:49:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][550/1251]	eta 0:08:45 lr 0.000247	time 0.7399 (0.7495)	loss 2.9806 (2.9610)	grad_norm 2.2197 (2.0966)	mem 23876MB
[2022-11-13 05:50:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][600/1251]	eta 0:08:07 lr 0.000246	time 0.7344 (0.7493)	loss 3.1383 (2.9651)	grad_norm 2.0326 (2.0897)	mem 23876MB
[2022-11-13 05:51:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][650/1251]	eta 0:07:30 lr 0.000246	time 0.8285 (0.7489)	loss 2.0914 (2.9606)	grad_norm 1.9451 (2.0853)	mem 23876MB
[2022-11-13 05:51:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][700/1251]	eta 0:06:52 lr 0.000246	time 0.7435 (0.7488)	loss 3.1709 (2.9587)	grad_norm 1.7699 (2.0760)	mem 23876MB
[2022-11-13 05:52:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][750/1251]	eta 0:06:15 lr 0.000246	time 0.8005 (0.7486)	loss 3.1739 (2.9525)	grad_norm 1.9308 (2.0738)	mem 23876MB
[2022-11-13 05:52:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][800/1251]	eta 0:05:37 lr 0.000246	time 0.7337 (0.7484)	loss 3.4526 (2.9488)	grad_norm 2.4010 (2.0711)	mem 23876MB
[2022-11-13 05:53:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][850/1251]	eta 0:05:00 lr 0.000246	time 0.7361 (0.7483)	loss 3.0128 (2.9465)	grad_norm 1.8522 (2.0660)	mem 23876MB
[2022-11-13 05:54:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][900/1251]	eta 0:04:22 lr 0.000245	time 0.7388 (0.7480)	loss 2.8908 (2.9424)	grad_norm 2.0200 (2.0692)	mem 23876MB
[2022-11-13 05:54:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][950/1251]	eta 0:03:45 lr 0.000245	time 0.7409 (0.7480)	loss 3.0633 (2.9393)	grad_norm 2.0810 (2.0653)	mem 23876MB
[2022-11-13 05:55:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][1000/1251]	eta 0:03:07 lr 0.000245	time 0.7404 (0.7480)	loss 3.2652 (2.9387)	grad_norm 2.2591 (2.0692)	mem 23876MB
[2022-11-13 05:56:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][1050/1251]	eta 0:02:30 lr 0.000245	time 0.7349 (0.7478)	loss 3.3715 (2.9386)	grad_norm 3.9831 (2.0733)	mem 23876MB
[2022-11-13 05:56:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][1100/1251]	eta 0:01:52 lr 0.000245	time 0.7482 (0.7479)	loss 2.8933 (2.9416)	grad_norm 1.9960 (2.0726)	mem 23876MB
[2022-11-13 05:57:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][1150/1251]	eta 0:01:15 lr 0.000245	time 0.7362 (0.7477)	loss 2.2460 (2.9427)	grad_norm 1.9746 (2.0743)	mem 23876MB
[2022-11-13 05:57:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][1200/1251]	eta 0:00:38 lr 0.000244	time 0.7329 (0.7476)	loss 2.8810 (2.9442)	grad_norm 2.3715 (2.0728)	mem 23876MB
[2022-11-13 05:58:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [202/300][1250/1251]	eta 0:00:00 lr 0.000244	time 0.7241 (0.7474)	loss 3.3581 (2.9435)	grad_norm 1.9724 (2.0692)	mem 23876MB
[2022-11-13 05:58:35 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 202 training takes 0:15:35
[2022-11-13 05:58:35 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_202.pth saving......
[2022-11-13 05:58:36 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_202.pth saved !!!
[2022-11-13 05:58:38 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.606 (1.606)	Loss 0.8207 (0.8207)	Acc@1 80.176 (80.176)	Acc@5 95.410 (95.410)	Mem 23876MB
[2022-11-13 05:58:48 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.732 Acc@5 95.968
[2022-11-13 05:58:48 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.7%
[2022-11-13 05:58:50 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.833 (1.833)	Loss 0.7183 (0.7183)	Acc@1 82.422 (82.422)	Acc@5 97.070 (97.070)	Mem 23876MB
[2022-11-13 05:59:01 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.974 Acc@5 96.540
[2022-11-13 05:59:01 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.0%
[2022-11-13 05:59:01 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 82.97% at 202 epoch
[2022-11-13 05:59:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][0/1251]	eta 0:50:34 lr 0.000244	time 2.4259 (2.4259)	loss 2.5623 (2.5623)	grad_norm 1.8820 (1.8820)	mem 23876MB
[2022-11-13 05:59:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][50/1251]	eta 0:15:41 lr 0.000244	time 0.7343 (0.7840)	loss 3.2759 (3.0146)	grad_norm 1.8996 (2.0386)	mem 23876MB
[2022-11-13 06:00:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][100/1251]	eta 0:14:42 lr 0.000244	time 0.7348 (0.7667)	loss 3.2911 (2.9933)	grad_norm 1.9486 (2.0403)	mem 23876MB
[2022-11-13 06:00:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][150/1251]	eta 0:13:57 lr 0.000244	time 0.7371 (0.7603)	loss 3.4961 (2.9991)	grad_norm 2.1956 (2.0487)	mem 23876MB
[2022-11-13 06:01:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][200/1251]	eta 0:13:15 lr 0.000243	time 0.8231 (0.7567)	loss 3.3795 (3.0043)	grad_norm 2.1036 (2.0634)	mem 23876MB
[2022-11-13 06:02:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][250/1251]	eta 0:12:35 lr 0.000243	time 0.7376 (0.7546)	loss 2.1057 (2.9984)	grad_norm 2.2446 (2.0535)	mem 23876MB
[2022-11-13 06:02:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][300/1251]	eta 0:11:56 lr 0.000243	time 0.7371 (0.7532)	loss 3.0903 (2.9706)	grad_norm 1.8090 (2.0559)	mem 23876MB
[2022-11-13 06:03:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][350/1251]	eta 0:11:18 lr 0.000243	time 0.7414 (0.7527)	loss 2.8947 (2.9595)	grad_norm 1.8410 (2.0598)	mem 23876MB
[2022-11-13 06:04:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][400/1251]	eta 0:10:40 lr 0.000243	time 0.7376 (0.7522)	loss 2.4260 (2.9536)	grad_norm 1.6267 (2.0613)	mem 23876MB
[2022-11-13 06:04:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][450/1251]	eta 0:10:01 lr 0.000243	time 0.7352 (0.7513)	loss 3.3568 (2.9554)	grad_norm 1.9962 (2.0617)	mem 23876MB
[2022-11-13 06:05:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][500/1251]	eta 0:09:23 lr 0.000242	time 0.7374 (0.7509)	loss 1.9841 (2.9442)	grad_norm 1.9927 (2.0795)	mem 23876MB
[2022-11-13 06:05:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][550/1251]	eta 0:08:46 lr 0.000242	time 0.7319 (0.7505)	loss 3.1616 (2.9509)	grad_norm 2.1812 (2.0851)	mem 23876MB
[2022-11-13 06:06:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][600/1251]	eta 0:08:08 lr 0.000242	time 0.7417 (0.7504)	loss 3.5996 (2.9598)	grad_norm 2.2338 (2.0920)	mem 23876MB
[2022-11-13 06:07:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][650/1251]	eta 0:07:30 lr 0.000242	time 0.7392 (0.7501)	loss 3.2989 (2.9611)	grad_norm 2.1222 (2.0894)	mem 23876MB
[2022-11-13 06:07:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][700/1251]	eta 0:06:53 lr 0.000242	time 0.7479 (0.7499)	loss 2.6502 (2.9606)	grad_norm 1.8656 (2.0881)	mem 23876MB
[2022-11-13 06:08:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][750/1251]	eta 0:06:15 lr 0.000242	time 0.7489 (0.7497)	loss 2.5042 (2.9632)	grad_norm 2.0636 (2.0831)	mem 23876MB
[2022-11-13 06:09:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][800/1251]	eta 0:05:38 lr 0.000241	time 0.7339 (0.7495)	loss 2.4173 (2.9681)	grad_norm 1.8806 (2.0782)	mem 23876MB
[2022-11-13 06:09:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][850/1251]	eta 0:05:00 lr 0.000241	time 0.7429 (0.7493)	loss 2.8951 (2.9605)	grad_norm 1.8290 (2.0816)	mem 23876MB
[2022-11-13 06:10:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][900/1251]	eta 0:04:23 lr 0.000241	time 0.7319 (0.7494)	loss 2.7089 (2.9662)	grad_norm 1.9651 (inf)	mem 23876MB
[2022-11-13 06:10:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][950/1251]	eta 0:03:45 lr 0.000241	time 0.8046 (0.7493)	loss 2.3464 (2.9683)	grad_norm 2.1318 (inf)	mem 23876MB
[2022-11-13 06:11:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][1000/1251]	eta 0:03:08 lr 0.000241	time 0.8109 (0.7492)	loss 2.2102 (2.9687)	grad_norm 1.8339 (inf)	mem 23876MB
[2022-11-13 06:12:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][1050/1251]	eta 0:02:30 lr 0.000240	time 0.7362 (0.7490)	loss 3.2937 (2.9633)	grad_norm 2.1156 (inf)	mem 23876MB
[2022-11-13 06:12:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][1100/1251]	eta 0:01:53 lr 0.000240	time 0.7339 (0.7490)	loss 2.1736 (2.9662)	grad_norm 1.9079 (inf)	mem 23876MB
[2022-11-13 06:13:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][1150/1251]	eta 0:01:15 lr 0.000240	time 0.7509 (0.7489)	loss 2.3536 (2.9646)	grad_norm 1.9286 (nan)	mem 23876MB
[2022-11-13 06:14:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][1200/1251]	eta 0:00:38 lr 0.000240	time 0.7382 (0.7490)	loss 3.3865 (2.9672)	grad_norm 2.0788 (nan)	mem 23876MB
[2022-11-13 06:14:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [203/300][1250/1251]	eta 0:00:00 lr 0.000240	time 0.7260 (0.7486)	loss 2.6331 (2.9654)	grad_norm 1.8102 (nan)	mem 23876MB
[2022-11-13 06:14:38 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 203 training takes 0:15:36
[2022-11-13 06:14:38 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_203.pth saving......
[2022-11-13 06:14:39 QFormer_transformer_small_patch4_window7_224] (utils.py 117): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_203.pth saved !!!
[2022-11-13 06:14:41 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.603 (1.603)	Loss 0.7929 (0.7929)	Acc@1 80.469 (80.469)	Acc@5 96.387 (96.387)	Mem 23876MB
[2022-11-13 06:14:51 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.512 Acc@5 95.968
[2022-11-13 06:14:51 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.5%
[2022-11-13 06:14:53 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.784 (1.784)	Loss 0.6760 (0.6760)	Acc@1 83.594 (83.594)	Acc@5 97.754 (97.754)	Mem 23876MB
[2022-11-13 06:15:04 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.006 Acc@5 96.554
[2022-11-13 06:15:04 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.0%
[2022-11-13 06:15:04 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.01% at 203 epoch
[2022-11-13 06:15:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][0/1251]	eta 0:50:40 lr 0.000240	time 2.4306 (2.4306)	loss 2.2052 (2.2052)	grad_norm 2.0101 (2.0101)	mem 23876MB
[2022-11-13 06:15:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][50/1251]	eta 0:15:37 lr 0.000240	time 0.8125 (0.7809)	loss 3.4156 (2.9482)	grad_norm 1.9358 (2.0228)	mem 23876MB
[2022-11-13 06:16:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][100/1251]	eta 0:14:39 lr 0.000239	time 0.7404 (0.7642)	loss 3.1148 (2.9911)	grad_norm 2.1791 (2.0362)	mem 23876MB
[2022-11-13 06:16:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][150/1251]	eta 0:13:54 lr 0.000239	time 0.7425 (0.7582)	loss 2.9918 (2.9598)	grad_norm 2.0699 (2.0513)	mem 23876MB
[2022-11-13 06:17:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][200/1251]	eta 0:13:14 lr 0.000239	time 0.7384 (0.7555)	loss 3.4841 (2.9584)	grad_norm 1.8760 (2.0654)	mem 23876MB
[2022-11-13 06:18:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][250/1251]	eta 0:12:34 lr 0.000239	time 0.7406 (0.7534)	loss 3.1838 (2.9732)	grad_norm 1.8708 (2.0573)	mem 23876MB
[2022-11-13 06:18:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][300/1251]	eta 0:11:55 lr 0.000239	time 0.7392 (0.7526)	loss 1.9619 (2.9657)	grad_norm 1.7311 (2.0851)	mem 23876MB
[2022-11-13 06:19:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][350/1251]	eta 0:11:16 lr 0.000239	time 0.7406 (0.7510)	loss 2.3294 (2.9694)	grad_norm 1.8803 (2.0824)	mem 23876MB
[2022-11-13 06:20:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][400/1251]	eta 0:10:38 lr 0.000238	time 0.7347 (0.7506)	loss 2.7694 (2.9779)	grad_norm 2.1353 (2.0879)	mem 23876MB
[2022-11-13 06:20:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][450/1251]	eta 0:10:00 lr 0.000238	time 0.7383 (0.7498)	loss 3.3648 (2.9613)	grad_norm 3.0872 (2.0847)	mem 23876MB
[2022-11-13 06:21:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][500/1251]	eta 0:09:23 lr 0.000238	time 0.7418 (0.7497)	loss 3.1804 (2.9689)	grad_norm 1.9749 (2.0821)	mem 23876MB
[2022-11-13 06:21:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][550/1251]	eta 0:08:45 lr 0.000238	time 0.7491 (0.7492)	loss 3.0571 (2.9726)	grad_norm 6.9204 (2.0901)	mem 23876MB
[2022-11-13 06:22:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][600/1251]	eta 0:08:07 lr 0.000238	time 0.8160 (0.7492)	loss 2.3140 (2.9809)	grad_norm 2.3248 (2.0958)	mem 23876MB
[2022-11-13 06:23:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][650/1251]	eta 0:07:29 lr 0.000237	time 0.7381 (0.7486)	loss 3.3086 (2.9691)	grad_norm 1.6760 (2.0944)	mem 23876MB
[2022-11-13 06:23:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][700/1251]	eta 0:06:52 lr 0.000237	time 0.7348 (0.7483)	loss 3.2074 (2.9727)	grad_norm 2.1945 (2.0905)	mem 23876MB
[2022-11-13 06:24:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][750/1251]	eta 0:06:14 lr 0.000237	time 0.7346 (0.7483)	loss 2.8359 (2.9744)	grad_norm 1.9651 (2.0882)	mem 23876MB
[2022-11-13 06:25:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][800/1251]	eta 0:05:37 lr 0.000237	time 0.7368 (0.7480)	loss 2.8249 (2.9734)	grad_norm 2.3451 (2.0872)	mem 23876MB
[2022-11-13 06:25:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][850/1251]	eta 0:04:59 lr 0.000237	time 0.7511 (0.7479)	loss 2.2524 (2.9743)	grad_norm 2.0140 (2.0894)	mem 23876MB
[2022-11-13 06:26:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][900/1251]	eta 0:04:22 lr 0.000237	time 0.7387 (0.7478)	loss 2.2428 (2.9669)	grad_norm 1.9606 (2.0870)	mem 23876MB
[2022-11-13 06:26:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][950/1251]	eta 0:03:45 lr 0.000236	time 0.7344 (0.7476)	loss 2.8290 (2.9661)	grad_norm 2.1032 (2.0897)	mem 23876MB
[2022-11-13 06:27:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][1000/1251]	eta 0:03:07 lr 0.000236	time 0.7382 (0.7475)	loss 2.4663 (2.9627)	grad_norm 2.0060 (2.0901)	mem 23876MB
[2022-11-13 06:28:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][1050/1251]	eta 0:02:30 lr 0.000236	time 0.7342 (0.7474)	loss 3.1962 (2.9679)	grad_norm 1.9716 (2.0929)	mem 23876MB
[2022-11-13 06:28:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][1100/1251]	eta 0:01:52 lr 0.000236	time 0.7377 (0.7474)	loss 2.7672 (2.9681)	grad_norm 2.6248 (2.0973)	mem 23876MB
[2022-11-13 06:29:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][1150/1251]	eta 0:01:15 lr 0.000236	time 0.7406 (0.7473)	loss 3.3028 (2.9655)	grad_norm 2.5965 (2.1017)	mem 23876MB
[2022-11-13 06:30:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][1200/1251]	eta 0:00:38 lr 0.000236	time 0.7464 (0.7472)	loss 2.5249 (2.9664)	grad_norm 1.8765 (2.1029)	mem 23876MB
[2022-11-13 06:30:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][1250/1251]	eta 0:00:00 lr 0.000235	time 0.7234 (0.7471)	loss 2.7868 (2.9677)	grad_norm 2.0212 (2.1023)	mem 23876MB
[2022-11-13 06:30:39 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 204 training takes 0:15:34
[2022-11-13 06:30:39 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_204.pth saving......
[2022-11-13 12:01:36 QFormer_transformer_small_patch4_window7_224] (main.py 442): INFO Full config saved to output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/config.json
[2022-11-13 12:01:36 QFormer_transformer_small_patch4_window7_224] (main.py 445): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
  SCALE: null
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EMA:
  EMA_DECAY: 0.9999200015999999
  EMA_FORCE_CPU: false
  ENABLE_EMA: true
ENABLE_WANDB: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  EMSWIN:
    EM_FACTOR: 0.9
    EM_ITERS: 3
    INSTANCE_TOKENS:
    - 10
    - 10
    - 10
    - 0
  LABEL_SMOOTHING: 0.1
  NAME: QFormer_transformer_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  QuadrangleAttention:
    context_size: null
    pyramid_size:
    - 1
    rpe: v1
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    LAYER_RATIO:
    - 1
    - 2
    - 3
    - 4
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    RELATIVE_POS_EMBEDDING: true
    SHIFT: true
    WINDOW_SIZE: 7
  TYPE: qformer
OUTPUT: output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1
PRINT_FREQ: 50
RESUME_OPTIMIZER: true
SAVE_FREQ: 1
SEED: 0
TAG: 1024-dpr30-coords_lambda1e-1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 0.001
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05
  coords_lambda: 0.1

[2022-11-13 12:01:41 QFormer_transformer_small_patch4_window7_224] (main.py 99): INFO Creating model:qformer/QFormer_transformer_small_patch4_window7_224
[2022-11-13 12:01:42 QFormer_transformer_small_patch4_window7_224] (main.py 102): INFO QFormer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(96, 27, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(96, 27, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(192, 54, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(192, 54, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(768, 216, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(768, 216, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-11-13 12:01:42 QFormer_transformer_small_patch4_window7_224] (main.py 115): INFO enable EMA model
[2022-11-13 12:01:42 QFormer_transformer_small_patch4_window7_224] (main.py 129): INFO number of params: 51136498
[2022-11-13 12:01:42 QFormer_transformer_small_patch4_window7_224] (main.py 132): INFO number of GFLOPs: 0.015670272
[2022-11-13 12:01:49 QFormer_transformer_small_patch4_window7_224] (main.py 173): INFO auto resuming from output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_204.pth
[2022-11-13 12:01:49 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO ==============> Resuming form output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_204.pth....................
[2022-11-13 14:38:39 QFormer_transformer_small_patch4_window7_224] (main.py 442): INFO Full config saved to output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/config.json
[2022-11-13 14:38:39 QFormer_transformer_small_patch4_window7_224] (main.py 445): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
  SCALE: null
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EMA:
  EMA_DECAY: 0.9999200015999999
  EMA_FORCE_CPU: false
  ENABLE_EMA: true
ENABLE_WANDB: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  EMSWIN:
    EM_FACTOR: 0.9
    EM_ITERS: 3
    INSTANCE_TOKENS:
    - 10
    - 10
    - 10
    - 0
  LABEL_SMOOTHING: 0.1
  NAME: QFormer_transformer_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  QuadrangleAttention:
    context_size: null
    pyramid_size:
    - 1
    rpe: v1
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    LAYER_RATIO:
    - 1
    - 2
    - 3
    - 4
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    RELATIVE_POS_EMBEDDING: true
    SHIFT: true
    WINDOW_SIZE: 7
  TYPE: qformer
OUTPUT: output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1
PRINT_FREQ: 50
RESUME_OPTIMIZER: true
SAVE_FREQ: 1
SEED: 0
TAG: 1024-dpr30-coords_lambda1e-1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 0.001
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05
  coords_lambda: 0.1

[2022-11-13 14:38:45 QFormer_transformer_small_patch4_window7_224] (main.py 99): INFO Creating model:qformer/QFormer_transformer_small_patch4_window7_224
[2022-11-13 14:38:46 QFormer_transformer_small_patch4_window7_224] (main.py 102): INFO QFormer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(96, 27, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(96, 27, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(192, 54, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(192, 54, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(768, 216, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(768, 216, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-11-13 14:38:46 QFormer_transformer_small_patch4_window7_224] (main.py 115): INFO enable EMA model
[2022-11-13 14:38:46 QFormer_transformer_small_patch4_window7_224] (main.py 129): INFO number of params: 51136498
[2022-11-13 14:38:46 QFormer_transformer_small_patch4_window7_224] (main.py 132): INFO number of GFLOPs: 0.015670272
[2022-11-13 14:38:51 QFormer_transformer_small_patch4_window7_224] (main.py 173): INFO auto resuming from output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_203.pth
[2022-11-13 14:38:51 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO ==============> Resuming form output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_203.pth....................
[2022-11-13 14:38:53 QFormer_transformer_small_patch4_window7_224] (utils.py 122): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rel_pos_h', 'layers.0.blocks.0.attn.rel_pos_w', 'layers.0.blocks.1.attn.rel_pos_h', 'layers.0.blocks.1.attn.rel_pos_w', 'layers.1.blocks.0.attn.rel_pos_h', 'layers.1.blocks.0.attn.rel_pos_w', 'layers.1.blocks.1.attn.rel_pos_h', 'layers.1.blocks.1.attn.rel_pos_w', 'layers.2.blocks.0.attn.rel_pos_h', 'layers.2.blocks.0.attn.rel_pos_w', 'layers.2.blocks.1.attn.rel_pos_h', 'layers.2.blocks.1.attn.rel_pos_w', 'layers.2.blocks.2.attn.rel_pos_h', 'layers.2.blocks.2.attn.rel_pos_w', 'layers.2.blocks.3.attn.rel_pos_h', 'layers.2.blocks.3.attn.rel_pos_w', 'layers.2.blocks.4.attn.rel_pos_h', 'layers.2.blocks.4.attn.rel_pos_w', 'layers.2.blocks.5.attn.rel_pos_h', 'layers.2.blocks.5.attn.rel_pos_w', 'layers.2.blocks.6.attn.rel_pos_h', 'layers.2.blocks.6.attn.rel_pos_w', 'layers.2.blocks.7.attn.rel_pos_h', 'layers.2.blocks.7.attn.rel_pos_w', 'layers.2.blocks.8.attn.rel_pos_h', 'layers.2.blocks.8.attn.rel_pos_w', 'layers.2.blocks.9.attn.rel_pos_h', 'layers.2.blocks.9.attn.rel_pos_w', 'layers.2.blocks.10.attn.rel_pos_h', 'layers.2.blocks.10.attn.rel_pos_w', 'layers.2.blocks.11.attn.rel_pos_h', 'layers.2.blocks.11.attn.rel_pos_w', 'layers.2.blocks.12.attn.rel_pos_h', 'layers.2.blocks.12.attn.rel_pos_w', 'layers.2.blocks.13.attn.rel_pos_h', 'layers.2.blocks.13.attn.rel_pos_w', 'layers.2.blocks.14.attn.rel_pos_h', 'layers.2.blocks.14.attn.rel_pos_w', 'layers.2.blocks.15.attn.rel_pos_h', 'layers.2.blocks.15.attn.rel_pos_w', 'layers.2.blocks.16.attn.rel_pos_h', 'layers.2.blocks.16.attn.rel_pos_w', 'layers.2.blocks.17.attn.rel_pos_h', 'layers.2.blocks.17.attn.rel_pos_w', 'layers.3.blocks.0.attn.rel_pos_h', 'layers.3.blocks.0.attn.rel_pos_w', 'layers.3.blocks.1.attn.rel_pos_h', 'layers.3.blocks.1.attn.rel_pos_w'], unexpected_keys=['layers.0.blocks.0.attn.relative_position_bias_table', 'layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn.relative_position_bias_table', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_bias_table', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn.relative_position_bias_table', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_bias_table', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_bias_table', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_bias_table', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_bias_table', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_bias_table', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_bias_table', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_bias_table', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_bias_table', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_bias_table', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_bias_table', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_bias_table', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_bias_table', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_bias_table', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_bias_table', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_bias_table', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_bias_table', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_bias_table', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_bias_table', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_bias_table', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_bias_table', 'layers.3.blocks.1.attn.relative_position_index'])
[2022-11-13 14:38:53 QFormer_transformer_small_patch4_window7_224] (utils.py 128): INFO EMA: _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rel_pos_h', 'layers.0.blocks.0.attn.rel_pos_w', 'layers.0.blocks.1.attn.rel_pos_h', 'layers.0.blocks.1.attn.rel_pos_w', 'layers.1.blocks.0.attn.rel_pos_h', 'layers.1.blocks.0.attn.rel_pos_w', 'layers.1.blocks.1.attn.rel_pos_h', 'layers.1.blocks.1.attn.rel_pos_w', 'layers.2.blocks.0.attn.rel_pos_h', 'layers.2.blocks.0.attn.rel_pos_w', 'layers.2.blocks.1.attn.rel_pos_h', 'layers.2.blocks.1.attn.rel_pos_w', 'layers.2.blocks.2.attn.rel_pos_h', 'layers.2.blocks.2.attn.rel_pos_w', 'layers.2.blocks.3.attn.rel_pos_h', 'layers.2.blocks.3.attn.rel_pos_w', 'layers.2.blocks.4.attn.rel_pos_h', 'layers.2.blocks.4.attn.rel_pos_w', 'layers.2.blocks.5.attn.rel_pos_h', 'layers.2.blocks.5.attn.rel_pos_w', 'layers.2.blocks.6.attn.rel_pos_h', 'layers.2.blocks.6.attn.rel_pos_w', 'layers.2.blocks.7.attn.rel_pos_h', 'layers.2.blocks.7.attn.rel_pos_w', 'layers.2.blocks.8.attn.rel_pos_h', 'layers.2.blocks.8.attn.rel_pos_w', 'layers.2.blocks.9.attn.rel_pos_h', 'layers.2.blocks.9.attn.rel_pos_w', 'layers.2.blocks.10.attn.rel_pos_h', 'layers.2.blocks.10.attn.rel_pos_w', 'layers.2.blocks.11.attn.rel_pos_h', 'layers.2.blocks.11.attn.rel_pos_w', 'layers.2.blocks.12.attn.rel_pos_h', 'layers.2.blocks.12.attn.rel_pos_w', 'layers.2.blocks.13.attn.rel_pos_h', 'layers.2.blocks.13.attn.rel_pos_w', 'layers.2.blocks.14.attn.rel_pos_h', 'layers.2.blocks.14.attn.rel_pos_w', 'layers.2.blocks.15.attn.rel_pos_h', 'layers.2.blocks.15.attn.rel_pos_w', 'layers.2.blocks.16.attn.rel_pos_h', 'layers.2.blocks.16.attn.rel_pos_w', 'layers.2.blocks.17.attn.rel_pos_h', 'layers.2.blocks.17.attn.rel_pos_w', 'layers.3.blocks.0.attn.rel_pos_h', 'layers.3.blocks.0.attn.rel_pos_w', 'layers.3.blocks.1.attn.rel_pos_h', 'layers.3.blocks.1.attn.rel_pos_w'], unexpected_keys=['layers.0.blocks.0.attn.relative_position_bias_table', 'layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn.relative_position_bias_table', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_bias_table', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn.relative_position_bias_table', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_bias_table', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_bias_table', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_bias_table', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_bias_table', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_bias_table', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_bias_table', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_bias_table', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_bias_table', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_bias_table', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_bias_table', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_bias_table', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_bias_table', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_bias_table', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_bias_table', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_bias_table', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_bias_table', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_bias_table', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_bias_table', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_bias_table', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_bias_table', 'layers.3.blocks.1.attn.relative_position_index'])
[2022-11-13 14:49:10 QFormer_transformer_small_patch4_window7_224] (main.py 442): INFO Full config saved to output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/config.json
[2022-11-13 14:49:10 QFormer_transformer_small_patch4_window7_224] (main.py 445): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
  SCALE: null
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EMA:
  EMA_DECAY: 0.9999200015999999
  EMA_FORCE_CPU: false
  ENABLE_EMA: true
ENABLE_WANDB: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  EMSWIN:
    EM_FACTOR: 0.9
    EM_ITERS: 3
    INSTANCE_TOKENS:
    - 10
    - 10
    - 10
    - 0
  LABEL_SMOOTHING: 0.1
  NAME: QFormer_transformer_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  QuadrangleAttention:
    context_size: null
    pyramid_size:
    - 1
    rpe: v1
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    LAYER_RATIO:
    - 1
    - 2
    - 3
    - 4
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    RELATIVE_POS_EMBEDDING: true
    SHIFT: true
    WINDOW_SIZE: 7
  TYPE: qformer
OUTPUT: output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1
PRINT_FREQ: 50
RESUME_OPTIMIZER: true
SAVE_FREQ: 1
SEED: 0
TAG: 1024-dpr30-coords_lambda1e-1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 0.001
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05
  coords_lambda: 0.1

[2022-11-13 14:49:15 QFormer_transformer_small_patch4_window7_224] (main.py 99): INFO Creating model:qformer/QFormer_transformer_small_patch4_window7_224
[2022-11-13 14:49:16 QFormer_transformer_small_patch4_window7_224] (main.py 102): INFO QFormer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(96, 27, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(96, 27, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(192, 54, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(192, 54, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(384, 108, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(768, 216, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (pos): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
          (attn): QuadrangleAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (transform): Sequential(
              (0): AvgPool2d(kernel_size=7, stride=7, padding=0)
              (1): LeakyReLU(negative_slope=0.01)
              (2): Conv2d(768, 216, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-11-13 14:49:16 QFormer_transformer_small_patch4_window7_224] (main.py 115): INFO enable EMA model
[2022-11-13 14:49:16 QFormer_transformer_small_patch4_window7_224] (main.py 129): INFO number of params: 51164188
[2022-11-13 14:49:16 QFormer_transformer_small_patch4_window7_224] (main.py 132): INFO number of GFLOPs: 0.015670272
[2022-11-13 14:49:22 QFormer_transformer_small_patch4_window7_224] (main.py 173): INFO auto resuming from output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_203.pth
[2022-11-13 14:49:22 QFormer_transformer_small_patch4_window7_224] (utils.py 115): INFO ==============> Resuming form output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_203.pth....................
[2022-11-13 14:49:23 QFormer_transformer_small_patch4_window7_224] (utils.py 122): INFO <All keys matched successfully>
[2022-11-13 14:49:23 QFormer_transformer_small_patch4_window7_224] (utils.py 128): INFO EMA: <All keys matched successfully>
[2022-11-13 14:49:24 QFormer_transformer_small_patch4_window7_224] (utils.py 140): INFO => loaded successfully 'output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_203.pth' (epoch 203)
[2022-11-13 14:49:28 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 4.111 (4.111)	Loss 0.7790 (0.7790)	Acc@1 83.008 (83.008)	Acc@5 96.289 (96.289)	Mem 3424MB
[2022-11-13 14:49:39 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.516 Acc@5 95.968
[2022-11-13 14:49:39 QFormer_transformer_small_patch4_window7_224] (main.py 180): INFO Accuracy of the network on the 50000 test images: 81.5%
[2022-11-13 14:49:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.064 (2.064)	Loss 0.7360 (0.7360)	Acc@1 82.520 (82.520)	Acc@5 96.094 (96.094)	Mem 3510MB
[2022-11-13 14:49:52 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.008 Acc@5 96.554
[2022-11-13 14:49:52 QFormer_transformer_small_patch4_window7_224] (main.py 183): INFO Accuracy of the ema network on the 50000 test images: 83.0%
[2022-11-13 14:49:52 QFormer_transformer_small_patch4_window7_224] (main.py 201): INFO Start training
[2022-11-13 14:49:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][0/1251]	eta 2:21:47 lr 0.000240	time 6.8003 (6.8003)	loss 3.2196 (3.2196)	grad_norm 2.2436 (2.2436)	mem 23672MB
[2022-11-13 14:50:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][50/1251]	eta 0:18:07 lr 0.000240	time 0.7528 (0.9054)	loss 3.1646 (3.0650)	grad_norm 2.0584 (2.1031)	mem 23800MB
[2022-11-13 14:51:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][100/1251]	eta 0:15:55 lr 0.000239	time 0.7453 (0.8303)	loss 3.0932 (2.9952)	grad_norm 2.1658 (2.0931)	mem 23800MB
[2022-11-13 14:51:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][150/1251]	eta 0:14:45 lr 0.000239	time 0.7448 (0.8041)	loss 2.4698 (2.9744)	grad_norm 2.0212 (2.1103)	mem 23800MB
[2022-11-13 14:52:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][200/1251]	eta 0:13:50 lr 0.000239	time 0.7394 (0.7898)	loss 3.0003 (2.9590)	grad_norm 2.1372 (2.1454)	mem 23800MB
[2022-11-13 14:53:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][250/1251]	eta 0:13:02 lr 0.000239	time 0.7428 (0.7813)	loss 3.0979 (2.9364)	grad_norm 2.1973 (2.1357)	mem 23800MB
[2022-11-13 14:53:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][300/1251]	eta 0:12:17 lr 0.000239	time 0.7398 (0.7755)	loss 2.6652 (2.9180)	grad_norm 2.4541 (2.1288)	mem 23800MB
[2022-11-13 14:54:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][350/1251]	eta 0:11:34 lr 0.000239	time 0.7428 (0.7713)	loss 3.0597 (2.9264)	grad_norm 2.4154 (2.1257)	mem 23800MB
[2022-11-13 14:55:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][400/1251]	eta 0:10:53 lr 0.000238	time 0.7390 (0.7679)	loss 3.2591 (2.9225)	grad_norm 2.1091 (2.1276)	mem 23800MB
[2022-11-13 14:55:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][450/1251]	eta 0:10:13 lr 0.000238	time 0.7522 (0.7656)	loss 2.7408 (2.9333)	grad_norm 1.9941 (2.1241)	mem 23801MB
[2022-11-13 14:56:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][500/1251]	eta 0:09:33 lr 0.000238	time 0.7411 (0.7636)	loss 3.0802 (2.9258)	grad_norm 1.5050 (2.1106)	mem 23801MB
[2022-11-13 14:56:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][550/1251]	eta 0:08:54 lr 0.000238	time 0.7453 (0.7620)	loss 3.2415 (2.9257)	grad_norm 1.9626 (2.1073)	mem 23801MB
[2022-11-13 14:57:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][600/1251]	eta 0:08:15 lr 0.000238	time 0.7439 (0.7606)	loss 1.7614 (2.9355)	grad_norm 2.4496 (2.1059)	mem 23802MB
[2022-11-13 14:58:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][650/1251]	eta 0:07:36 lr 0.000237	time 0.7407 (0.7594)	loss 3.5945 (2.9395)	grad_norm 2.5182 (2.1000)	mem 23802MB
[2022-11-13 14:58:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][700/1251]	eta 0:06:57 lr 0.000237	time 0.7380 (0.7584)	loss 3.3657 (2.9356)	grad_norm 1.9253 (2.1117)	mem 23802MB
[2022-11-13 14:59:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][750/1251]	eta 0:06:19 lr 0.000237	time 0.7384 (0.7574)	loss 3.0883 (2.9350)	grad_norm 1.9696 (2.1089)	mem 23802MB
[2022-11-13 14:59:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][800/1251]	eta 0:05:41 lr 0.000237	time 0.7413 (0.7566)	loss 3.6342 (2.9355)	grad_norm 1.9929 (2.1079)	mem 23802MB
[2022-11-13 15:00:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][850/1251]	eta 0:05:03 lr 0.000237	time 0.7457 (0.7562)	loss 3.0212 (2.9258)	grad_norm 2.1845 (2.1220)	mem 23802MB
[2022-11-13 15:01:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][900/1251]	eta 0:04:25 lr 0.000237	time 0.7378 (0.7557)	loss 2.9262 (2.9230)	grad_norm 1.9746 (2.1237)	mem 23802MB
[2022-11-13 15:01:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][950/1251]	eta 0:03:47 lr 0.000236	time 0.7438 (0.7554)	loss 2.1810 (2.9201)	grad_norm 1.9285 (2.1201)	mem 23802MB
[2022-11-13 15:02:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][1000/1251]	eta 0:03:09 lr 0.000236	time 0.7456 (0.7549)	loss 2.6690 (2.9210)	grad_norm 2.2022 (2.1190)	mem 23802MB
[2022-11-13 15:03:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][1050/1251]	eta 0:02:31 lr 0.000236	time 0.7425 (0.7545)	loss 2.3283 (2.9219)	grad_norm 2.4996 (2.1233)	mem 23802MB
[2022-11-13 15:03:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][1100/1251]	eta 0:01:53 lr 0.000236	time 0.7448 (0.7541)	loss 3.2406 (2.9225)	grad_norm 2.7678 (2.1188)	mem 23802MB
[2022-11-13 15:04:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][1150/1251]	eta 0:01:16 lr 0.000236	time 0.7420 (0.7538)	loss 3.1892 (2.9246)	grad_norm 2.0404 (2.1202)	mem 23802MB
[2022-11-13 15:04:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][1200/1251]	eta 0:00:38 lr 0.000236	time 0.7397 (0.7536)	loss 3.0759 (2.9308)	grad_norm 2.0606 (2.1195)	mem 23802MB
[2022-11-13 15:05:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [204/300][1250/1251]	eta 0:00:00 lr 0.000235	time 0.7287 (0.7532)	loss 2.9287 (2.9297)	grad_norm 1.9884 (2.1202)	mem 23802MB
[2022-11-13 15:05:35 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 204 training takes 0:15:42
[2022-11-13 15:05:35 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_204.pth saving......
[2022-11-13 15:05:36 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_204.pth saved !!!
[2022-11-13 15:05:38 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.691 (1.691)	Loss 0.7745 (0.7745)	Acc@1 82.324 (82.324)	Acc@5 95.605 (95.605)	Mem 23802MB
[2022-11-13 15:05:49 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.860 Acc@5 95.938
[2022-11-13 15:05:49 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.9%
[2022-11-13 15:05:50 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.958 (1.958)	Loss 0.7114 (0.7114)	Acc@1 83.398 (83.398)	Acc@5 96.582 (96.582)	Mem 23802MB
[2022-11-13 15:06:01 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.034 Acc@5 96.560
[2022-11-13 15:06:01 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.0%
[2022-11-13 15:06:01 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.03% at 204 epoch
[2022-11-13 15:06:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][0/1251]	eta 0:51:50 lr 0.000235	time 2.4861 (2.4861)	loss 3.0544 (3.0544)	grad_norm 2.1808 (2.1808)	mem 23868MB
[2022-11-13 15:06:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][50/1251]	eta 0:15:42 lr 0.000235	time 0.8534 (0.7844)	loss 2.5362 (3.0335)	grad_norm 1.7887 (2.1069)	mem 23868MB
[2022-11-13 15:07:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][100/1251]	eta 0:14:41 lr 0.000235	time 0.7364 (0.7655)	loss 3.3083 (2.9673)	grad_norm 2.0183 (2.0543)	mem 23868MB
[2022-11-13 15:07:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][150/1251]	eta 0:13:54 lr 0.000235	time 0.7393 (0.7583)	loss 3.3055 (2.9777)	grad_norm 1.9473 (2.0488)	mem 23868MB
[2022-11-13 15:08:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][200/1251]	eta 0:13:13 lr 0.000235	time 0.7377 (0.7551)	loss 3.2660 (2.9703)	grad_norm 1.7687 (2.0565)	mem 23868MB
[2022-11-13 15:09:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][250/1251]	eta 0:12:33 lr 0.000235	time 0.7413 (0.7526)	loss 2.6861 (2.9386)	grad_norm 2.1652 (2.0685)	mem 23868MB
[2022-11-13 15:09:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][300/1251]	eta 0:11:54 lr 0.000234	time 0.7378 (0.7512)	loss 2.2262 (2.9404)	grad_norm 2.0862 (2.0669)	mem 23868MB
[2022-11-13 15:10:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][350/1251]	eta 0:11:15 lr 0.000234	time 0.7442 (0.7501)	loss 3.2156 (2.9419)	grad_norm 2.7473 (2.0715)	mem 23868MB
[2022-11-13 15:11:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][400/1251]	eta 0:10:37 lr 0.000234	time 0.7491 (0.7497)	loss 2.0841 (2.9305)	grad_norm 1.8487 (2.0690)	mem 23868MB
[2022-11-13 15:11:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][450/1251]	eta 0:09:59 lr 0.000234	time 0.7379 (0.7488)	loss 3.4413 (2.9306)	grad_norm 1.6130 (2.0684)	mem 23868MB
[2022-11-13 15:12:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][500/1251]	eta 0:09:21 lr 0.000234	time 0.7375 (0.7483)	loss 3.1181 (2.9257)	grad_norm 1.9002 (2.0925)	mem 23868MB
[2022-11-13 15:12:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][550/1251]	eta 0:08:44 lr 0.000233	time 0.7378 (0.7477)	loss 3.0805 (2.9321)	grad_norm 1.8734 (2.0950)	mem 23868MB
[2022-11-13 15:13:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][600/1251]	eta 0:08:06 lr 0.000233	time 0.7398 (0.7476)	loss 2.4124 (2.9323)	grad_norm 2.6775 (2.0937)	mem 23868MB
[2022-11-13 15:14:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][650/1251]	eta 0:07:29 lr 0.000233	time 0.7427 (0.7473)	loss 3.0730 (2.9293)	grad_norm 1.8103 (inf)	mem 23868MB
[2022-11-13 15:14:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][700/1251]	eta 0:06:51 lr 0.000233	time 0.7403 (0.7471)	loss 2.9334 (2.9227)	grad_norm 1.9738 (inf)	mem 23868MB
[2022-11-13 15:15:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][750/1251]	eta 0:06:14 lr 0.000233	time 0.7432 (0.7471)	loss 2.6293 (2.9149)	grad_norm 1.9047 (inf)	mem 23868MB
[2022-11-13 15:16:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][800/1251]	eta 0:05:36 lr 0.000233	time 0.7370 (0.7468)	loss 3.1230 (2.9222)	grad_norm 3.0800 (inf)	mem 23868MB
[2022-11-13 15:16:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][850/1251]	eta 0:04:59 lr 0.000232	time 0.7404 (0.7469)	loss 3.0190 (2.9231)	grad_norm 2.5135 (inf)	mem 23868MB
[2022-11-13 15:17:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][900/1251]	eta 0:04:22 lr 0.000232	time 0.7415 (0.7467)	loss 3.3570 (2.9284)	grad_norm 2.1454 (inf)	mem 23868MB
[2022-11-13 15:17:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][950/1251]	eta 0:03:44 lr 0.000232	time 0.7409 (0.7467)	loss 2.9786 (2.9303)	grad_norm 1.7989 (inf)	mem 23868MB
[2022-11-13 15:18:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][1000/1251]	eta 0:03:07 lr 0.000232	time 0.7362 (0.7467)	loss 3.5000 (2.9258)	grad_norm 2.1041 (inf)	mem 23868MB
[2022-11-13 15:19:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][1050/1251]	eta 0:02:30 lr 0.000232	time 0.7466 (0.7465)	loss 2.2459 (2.9257)	grad_norm 18.6881 (inf)	mem 23868MB
[2022-11-13 15:19:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][1100/1251]	eta 0:01:52 lr 0.000232	time 0.7400 (0.7465)	loss 3.1113 (2.9299)	grad_norm 1.8874 (inf)	mem 23868MB
[2022-11-13 15:20:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][1150/1251]	eta 0:01:15 lr 0.000231	time 0.7331 (0.7465)	loss 3.4248 (2.9267)	grad_norm 1.9084 (inf)	mem 23868MB
[2022-11-13 15:20:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][1200/1251]	eta 0:00:38 lr 0.000231	time 0.7403 (0.7464)	loss 3.5974 (2.9290)	grad_norm 1.9345 (inf)	mem 23868MB
[2022-11-13 15:21:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [205/300][1250/1251]	eta 0:00:00 lr 0.000231	time 0.7313 (0.7463)	loss 2.0097 (2.9291)	grad_norm 2.0289 (inf)	mem 23868MB
[2022-11-13 15:21:35 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 205 training takes 0:15:33
[2022-11-13 15:21:35 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_205.pth saving......
[2022-11-13 15:21:36 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_205.pth saved !!!
[2022-11-13 15:21:38 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.931 (1.931)	Loss 0.8018 (0.8018)	Acc@1 80.273 (80.273)	Acc@5 95.508 (95.508)	Mem 23868MB
[2022-11-13 15:21:49 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.852 Acc@5 96.098
[2022-11-13 15:21:49 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.9%
[2022-11-13 15:21:51 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.922 (1.922)	Loss 0.7383 (0.7383)	Acc@1 81.445 (81.445)	Acc@5 95.898 (95.898)	Mem 23868MB
[2022-11-13 15:22:02 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.040 Acc@5 96.556
[2022-11-13 15:22:02 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.0%
[2022-11-13 15:22:02 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.04% at 205 epoch
[2022-11-13 15:22:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][0/1251]	eta 0:52:07 lr 0.000231	time 2.5000 (2.5000)	loss 2.6613 (2.6613)	grad_norm 1.9747 (1.9747)	mem 23871MB
[2022-11-13 15:22:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][50/1251]	eta 0:15:41 lr 0.000231	time 0.7398 (0.7835)	loss 3.1966 (2.9761)	grad_norm 2.0283 (2.1756)	mem 23871MB
[2022-11-13 15:23:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][100/1251]	eta 0:14:38 lr 0.000231	time 0.7423 (0.7629)	loss 3.0341 (2.9970)	grad_norm 1.9281 (2.1423)	mem 23871MB
[2022-11-13 15:23:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][150/1251]	eta 0:13:54 lr 0.000231	time 0.7437 (0.7577)	loss 2.1461 (2.9714)	grad_norm 2.7576 (2.1832)	mem 23871MB
[2022-11-13 15:24:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][200/1251]	eta 0:13:12 lr 0.000230	time 0.7411 (0.7539)	loss 3.0075 (2.9941)	grad_norm 1.9102 (2.1607)	mem 23871MB
[2022-11-13 15:25:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][250/1251]	eta 0:12:33 lr 0.000230	time 0.7503 (0.7523)	loss 2.2611 (2.9827)	grad_norm 2.0396 (2.1561)	mem 23871MB
[2022-11-13 15:25:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][300/1251]	eta 0:11:54 lr 0.000230	time 0.7447 (0.7510)	loss 3.1585 (2.9726)	grad_norm 1.7635 (2.1360)	mem 23871MB
[2022-11-13 15:26:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][350/1251]	eta 0:11:15 lr 0.000230	time 0.7395 (0.7498)	loss 2.9716 (2.9674)	grad_norm 2.3339 (2.1260)	mem 23871MB
[2022-11-13 15:27:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][400/1251]	eta 0:10:37 lr 0.000230	time 0.7404 (0.7493)	loss 2.0469 (2.9384)	grad_norm 2.4841 (2.1160)	mem 23871MB
[2022-11-13 15:27:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][450/1251]	eta 0:09:59 lr 0.000230	time 0.7423 (0.7489)	loss 3.2194 (2.9340)	grad_norm 2.3886 (2.1356)	mem 23871MB
[2022-11-13 15:28:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][500/1251]	eta 0:09:22 lr 0.000229	time 0.7422 (0.7485)	loss 3.2931 (2.9361)	grad_norm 2.0037 (2.1338)	mem 23871MB
[2022-11-13 15:28:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][550/1251]	eta 0:08:44 lr 0.000229	time 0.7363 (0.7483)	loss 2.6002 (2.9359)	grad_norm 1.8909 (2.1305)	mem 23871MB
[2022-11-13 15:29:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][600/1251]	eta 0:08:07 lr 0.000229	time 0.7398 (0.7481)	loss 2.4646 (2.9307)	grad_norm 2.0811 (2.1244)	mem 23871MB
[2022-11-13 15:30:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][650/1251]	eta 0:07:29 lr 0.000229	time 0.8329 (0.7478)	loss 3.4105 (2.9291)	grad_norm 2.3404 (2.1195)	mem 23871MB
[2022-11-13 15:30:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][700/1251]	eta 0:06:51 lr 0.000229	time 0.7412 (0.7477)	loss 3.4839 (2.9280)	grad_norm 1.8422 (2.1175)	mem 23871MB
[2022-11-13 15:31:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][750/1251]	eta 0:06:14 lr 0.000228	time 0.7415 (0.7477)	loss 2.0421 (2.9352)	grad_norm 1.9673 (2.1104)	mem 23871MB
[2022-11-13 15:32:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][800/1251]	eta 0:05:37 lr 0.000228	time 0.7421 (0.7474)	loss 3.1706 (2.9362)	grad_norm 2.0381 (2.1139)	mem 23871MB
[2022-11-13 15:32:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][850/1251]	eta 0:04:59 lr 0.000228	time 0.7426 (0.7474)	loss 3.3187 (2.9372)	grad_norm 2.2019 (2.1071)	mem 23871MB
[2022-11-13 15:33:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][900/1251]	eta 0:04:22 lr 0.000228	time 0.7400 (0.7471)	loss 3.0002 (2.9357)	grad_norm 2.4445 (2.1047)	mem 23871MB
[2022-11-13 15:33:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][950/1251]	eta 0:03:44 lr 0.000228	time 0.7434 (0.7470)	loss 3.0154 (2.9356)	grad_norm 2.0850 (2.0985)	mem 23871MB
[2022-11-13 15:34:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][1000/1251]	eta 0:03:07 lr 0.000228	time 0.7376 (0.7470)	loss 3.3415 (2.9315)	grad_norm 1.9706 (2.1013)	mem 23871MB
[2022-11-13 15:35:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][1050/1251]	eta 0:02:30 lr 0.000227	time 0.7400 (0.7468)	loss 3.3621 (2.9281)	grad_norm 2.0718 (2.1015)	mem 23871MB
[2022-11-13 15:35:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][1100/1251]	eta 0:01:52 lr 0.000227	time 0.7423 (0.7468)	loss 1.8725 (2.9306)	grad_norm 2.0742 (2.1095)	mem 23871MB
[2022-11-13 15:36:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][1150/1251]	eta 0:01:15 lr 0.000227	time 0.8276 (0.7468)	loss 3.4685 (2.9375)	grad_norm 3.1128 (2.1095)	mem 23871MB
[2022-11-13 15:36:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][1200/1251]	eta 0:00:38 lr 0.000227	time 0.7432 (0.7465)	loss 3.1225 (2.9375)	grad_norm 1.8883 (2.1065)	mem 23871MB
[2022-11-13 15:37:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [206/300][1250/1251]	eta 0:00:00 lr 0.000227	time 0.7241 (0.7464)	loss 3.0979 (2.9377)	grad_norm 2.2696 (2.1086)	mem 23871MB
[2022-11-13 15:37:36 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 206 training takes 0:15:33
[2022-11-13 15:37:36 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_206.pth saving......
[2022-11-13 15:37:37 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_206.pth saved !!!
[2022-11-13 15:37:39 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.786 (1.786)	Loss 0.7716 (0.7716)	Acc@1 80.859 (80.859)	Acc@5 96.387 (96.387)	Mem 23871MB
[2022-11-13 15:37:49 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.738 Acc@5 96.042
[2022-11-13 15:37:49 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.7%
[2022-11-13 15:37:51 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.993 (1.993)	Loss 0.6548 (0.6548)	Acc@1 84.961 (84.961)	Acc@5 96.875 (96.875)	Mem 23871MB
[2022-11-13 15:38:02 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.026 Acc@5 96.570
[2022-11-13 15:38:02 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.0%
[2022-11-13 15:38:02 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.04% at 205 epoch
[2022-11-13 15:38:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][0/1251]	eta 0:54:09 lr 0.000227	time 2.5972 (2.5972)	loss 2.7321 (2.7321)	grad_norm 2.9983 (2.9983)	mem 23871MB
[2022-11-13 15:38:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][50/1251]	eta 0:15:40 lr 0.000227	time 0.7461 (0.7831)	loss 2.7536 (2.9912)	grad_norm 1.9836 (2.2239)	mem 23871MB
[2022-11-13 15:39:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][100/1251]	eta 0:14:42 lr 0.000226	time 0.8327 (0.7664)	loss 2.6941 (2.9305)	grad_norm 1.9185 (2.1455)	mem 23871MB
[2022-11-13 15:39:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][150/1251]	eta 0:13:56 lr 0.000226	time 0.7411 (0.7598)	loss 3.1594 (2.8866)	grad_norm 1.7621 (2.1056)	mem 23871MB
[2022-11-13 15:40:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][200/1251]	eta 0:13:15 lr 0.000226	time 0.8272 (0.7567)	loss 3.3557 (2.8812)	grad_norm 2.0027 (2.0861)	mem 23871MB
[2022-11-13 15:41:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][250/1251]	eta 0:12:35 lr 0.000226	time 0.7419 (0.7544)	loss 3.5518 (2.8958)	grad_norm 1.8258 (2.0954)	mem 23871MB
[2022-11-13 15:41:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][300/1251]	eta 0:11:56 lr 0.000226	time 0.7432 (0.7530)	loss 2.4018 (2.9106)	grad_norm 2.0906 (2.1162)	mem 23871MB
[2022-11-13 15:42:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][350/1251]	eta 0:11:17 lr 0.000226	time 0.7438 (0.7523)	loss 3.0498 (2.8915)	grad_norm 2.2636 (2.1364)	mem 23871MB
[2022-11-13 15:43:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][400/1251]	eta 0:10:39 lr 0.000225	time 0.7373 (0.7517)	loss 2.4561 (2.8943)	grad_norm 1.9857 (2.1344)	mem 23871MB
[2022-11-13 15:43:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][450/1251]	eta 0:10:01 lr 0.000225	time 0.7437 (0.7513)	loss 2.4407 (2.8969)	grad_norm 2.2397 (2.1264)	mem 23871MB
[2022-11-13 15:44:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][500/1251]	eta 0:09:23 lr 0.000225	time 0.7380 (0.7505)	loss 3.3072 (2.8997)	grad_norm 2.0713 (nan)	mem 23871MB
[2022-11-13 15:44:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][550/1251]	eta 0:08:45 lr 0.000225	time 0.7407 (0.7502)	loss 3.1896 (2.8945)	grad_norm 1.8364 (nan)	mem 23871MB
[2022-11-13 15:45:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][600/1251]	eta 0:08:08 lr 0.000225	time 0.7403 (0.7497)	loss 2.9396 (2.8947)	grad_norm 2.7528 (nan)	mem 23871MB
[2022-11-13 15:46:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][650/1251]	eta 0:07:30 lr 0.000225	time 0.7415 (0.7496)	loss 3.0571 (2.8998)	grad_norm 1.8648 (nan)	mem 23871MB
[2022-11-13 15:46:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][700/1251]	eta 0:06:52 lr 0.000224	time 0.7377 (0.7492)	loss 2.6631 (2.9011)	grad_norm 2.0263 (nan)	mem 23871MB
[2022-11-13 15:47:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][750/1251]	eta 0:06:15 lr 0.000224	time 0.7412 (0.7491)	loss 2.1252 (2.8986)	grad_norm 1.8010 (nan)	mem 23871MB
[2022-11-13 15:48:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][800/1251]	eta 0:05:37 lr 0.000224	time 0.7318 (0.7490)	loss 2.9488 (2.9003)	grad_norm 1.6589 (nan)	mem 23871MB
[2022-11-13 15:48:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][850/1251]	eta 0:05:00 lr 0.000224	time 0.7378 (0.7488)	loss 2.8138 (2.9055)	grad_norm 4.6872 (nan)	mem 23871MB
[2022-11-13 15:49:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][900/1251]	eta 0:04:22 lr 0.000224	time 0.7406 (0.7486)	loss 3.3976 (2.9084)	grad_norm 2.3476 (nan)	mem 23871MB
[2022-11-13 15:49:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][950/1251]	eta 0:03:45 lr 0.000224	time 0.7429 (0.7484)	loss 3.1434 (2.9165)	grad_norm 4.0245 (nan)	mem 23871MB
[2022-11-13 15:50:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][1000/1251]	eta 0:03:07 lr 0.000223	time 0.7441 (0.7484)	loss 3.0902 (2.9168)	grad_norm 2.1535 (nan)	mem 23871MB
[2022-11-13 15:51:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][1050/1251]	eta 0:02:30 lr 0.000223	time 0.7374 (0.7483)	loss 3.1452 (2.9154)	grad_norm 2.1343 (nan)	mem 23871MB
[2022-11-13 15:51:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][1100/1251]	eta 0:01:52 lr 0.000223	time 0.7372 (0.7481)	loss 3.6970 (2.9149)	grad_norm 2.4410 (nan)	mem 23871MB
[2022-11-13 15:52:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][1150/1251]	eta 0:01:15 lr 0.000223	time 0.7549 (0.7482)	loss 2.5964 (2.9086)	grad_norm 1.8975 (nan)	mem 23871MB
[2022-11-13 15:53:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][1200/1251]	eta 0:00:38 lr 0.000223	time 0.7379 (0.7481)	loss 3.3172 (2.9117)	grad_norm 2.0772 (nan)	mem 23871MB
[2022-11-13 15:53:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [207/300][1250/1251]	eta 0:00:00 lr 0.000223	time 0.7314 (0.7478)	loss 3.5097 (2.9137)	grad_norm 2.2669 (nan)	mem 23871MB
[2022-11-13 15:53:38 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 207 training takes 0:15:35
[2022-11-13 15:53:38 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_207.pth saving......
[2022-11-13 15:53:39 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_207.pth saved !!!
[2022-11-13 15:53:40 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.752 (1.752)	Loss 0.8109 (0.8109)	Acc@1 80.469 (80.469)	Acc@5 95.996 (95.996)	Mem 23871MB
[2022-11-13 15:53:51 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.990 Acc@5 95.978
[2022-11-13 15:53:51 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.0%
[2022-11-13 15:53:53 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.007 (2.007)	Loss 0.8210 (0.8210)	Acc@1 80.566 (80.566)	Acc@5 95.508 (95.508)	Mem 23871MB
[2022-11-13 15:54:04 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.018 Acc@5 96.566
[2022-11-13 15:54:04 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.0%
[2022-11-13 15:54:04 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.04% at 205 epoch
[2022-11-13 15:54:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][0/1251]	eta 0:52:48 lr 0.000222	time 2.5328 (2.5328)	loss 3.4549 (3.4549)	grad_norm 1.8687 (1.8687)	mem 23871MB
[2022-11-13 15:54:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][50/1251]	eta 0:15:40 lr 0.000222	time 0.7441 (0.7833)	loss 3.2406 (2.8712)	grad_norm 1.9821 (2.1615)	mem 23871MB
[2022-11-13 15:55:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][100/1251]	eta 0:14:40 lr 0.000222	time 0.7434 (0.7649)	loss 3.5316 (2.8820)	grad_norm 2.3682 (2.0905)	mem 23871MB
[2022-11-13 15:55:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][150/1251]	eta 0:13:56 lr 0.000222	time 0.7433 (0.7600)	loss 2.8556 (2.8904)	grad_norm 2.1203 (2.1043)	mem 23871MB
[2022-11-13 15:56:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][200/1251]	eta 0:13:15 lr 0.000222	time 0.7419 (0.7565)	loss 3.2360 (2.9140)	grad_norm 2.0018 (2.0974)	mem 23871MB
[2022-11-13 15:57:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][250/1251]	eta 0:12:35 lr 0.000222	time 0.7426 (0.7551)	loss 3.2903 (2.9257)	grad_norm 2.5049 (2.1163)	mem 23871MB
[2022-11-13 15:57:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][300/1251]	eta 0:11:57 lr 0.000221	time 0.8148 (0.7539)	loss 2.6643 (2.9292)	grad_norm 2.0073 (2.1076)	mem 23871MB
[2022-11-13 15:58:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][350/1251]	eta 0:11:18 lr 0.000221	time 0.7408 (0.7528)	loss 3.4739 (2.9207)	grad_norm 1.7634 (2.0942)	mem 23871MB
[2022-11-13 15:59:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][400/1251]	eta 0:10:40 lr 0.000221	time 0.7482 (0.7525)	loss 3.2003 (2.9177)	grad_norm 1.8584 (2.0935)	mem 23871MB
[2022-11-13 15:59:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][450/1251]	eta 0:10:02 lr 0.000221	time 0.7384 (0.7522)	loss 2.6099 (2.9175)	grad_norm 2.2326 (2.1013)	mem 23871MB
[2022-11-13 16:00:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][500/1251]	eta 0:09:24 lr 0.000221	time 0.7447 (0.7517)	loss 3.3247 (2.9192)	grad_norm 2.4392 (2.0959)	mem 23871MB
[2022-11-13 16:00:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][550/1251]	eta 0:08:46 lr 0.000221	time 0.7433 (0.7515)	loss 3.0793 (2.9105)	grad_norm 2.1111 (2.1044)	mem 23871MB
[2022-11-13 16:01:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][600/1251]	eta 0:08:08 lr 0.000220	time 0.7400 (0.7510)	loss 2.7091 (2.9035)	grad_norm 2.1354 (2.1148)	mem 23871MB
[2022-11-13 16:02:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][650/1251]	eta 0:07:31 lr 0.000220	time 0.7452 (0.7511)	loss 3.1622 (2.9072)	grad_norm 1.9176 (2.1201)	mem 23871MB
[2022-11-13 16:02:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][700/1251]	eta 0:06:53 lr 0.000220	time 0.8240 (0.7509)	loss 2.6202 (2.9104)	grad_norm 2.1976 (2.1232)	mem 23871MB
[2022-11-13 16:03:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][750/1251]	eta 0:06:16 lr 0.000220	time 0.7404 (0.7508)	loss 2.8365 (2.9082)	grad_norm 1.7346 (2.1183)	mem 23871MB
[2022-11-13 16:04:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][800/1251]	eta 0:05:38 lr 0.000220	time 0.7472 (0.7506)	loss 3.6998 (2.9087)	grad_norm 2.2246 (2.1161)	mem 23871MB
[2022-11-13 16:04:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][850/1251]	eta 0:05:00 lr 0.000220	time 0.7368 (0.7504)	loss 3.0389 (2.9103)	grad_norm 2.0291 (2.1150)	mem 23871MB
[2022-11-13 16:05:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][900/1251]	eta 0:04:23 lr 0.000219	time 0.7419 (0.7503)	loss 3.3297 (2.9141)	grad_norm 1.8115 (2.1224)	mem 23871MB
[2022-11-13 16:05:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][950/1251]	eta 0:03:45 lr 0.000219	time 0.7498 (0.7504)	loss 2.5946 (2.9189)	grad_norm 2.1792 (2.1209)	mem 23871MB
[2022-11-13 16:06:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][1000/1251]	eta 0:03:08 lr 0.000219	time 0.7379 (0.7501)	loss 3.2255 (2.9214)	grad_norm 1.9516 (2.1208)	mem 23871MB
[2022-11-13 16:07:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][1050/1251]	eta 0:02:30 lr 0.000219	time 0.7419 (0.7502)	loss 3.1207 (2.9199)	grad_norm 2.1846 (2.1234)	mem 23871MB
[2022-11-13 16:07:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][1100/1251]	eta 0:01:53 lr 0.000219	time 0.8131 (0.7500)	loss 2.6011 (2.9247)	grad_norm 1.9623 (2.1257)	mem 23871MB
[2022-11-13 16:08:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][1150/1251]	eta 0:01:15 lr 0.000219	time 0.7457 (0.7501)	loss 3.2669 (2.9203)	grad_norm 1.9769 (2.1240)	mem 23871MB
[2022-11-13 16:09:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][1200/1251]	eta 0:00:38 lr 0.000218	time 0.7424 (0.7501)	loss 2.3160 (2.9230)	grad_norm 2.0945 (2.1250)	mem 23871MB
[2022-11-13 16:09:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [208/300][1250/1251]	eta 0:00:00 lr 0.000218	time 0.7309 (0.7500)	loss 2.5727 (2.9234)	grad_norm 2.3930 (2.1257)	mem 23871MB
[2022-11-13 16:09:42 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 208 training takes 0:15:38
[2022-11-13 16:09:42 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_208.pth saving......
[2022-11-13 16:09:43 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_208.pth saved !!!
[2022-11-13 16:09:45 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.750 (1.750)	Loss 0.8063 (0.8063)	Acc@1 79.492 (79.492)	Acc@5 96.094 (96.094)	Mem 23871MB
[2022-11-13 16:09:56 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.688 Acc@5 95.988
[2022-11-13 16:09:56 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.7%
[2022-11-13 16:09:58 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.133 (2.133)	Loss 0.7908 (0.7908)	Acc@1 80.566 (80.566)	Acc@5 95.703 (95.703)	Mem 23871MB
[2022-11-13 16:10:09 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.010 Acc@5 96.564
[2022-11-13 16:10:09 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.0%
[2022-11-13 16:10:09 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.04% at 205 epoch
[2022-11-13 16:10:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][0/1251]	eta 0:52:22 lr 0.000218	time 2.5123 (2.5123)	loss 3.2225 (3.2225)	grad_norm 1.9733 (1.9733)	mem 23871MB
[2022-11-13 16:10:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][50/1251]	eta 0:15:42 lr 0.000218	time 0.7576 (0.7846)	loss 3.3074 (3.0586)	grad_norm 3.9419 (2.2365)	mem 23871MB
[2022-11-13 16:11:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][100/1251]	eta 0:14:44 lr 0.000218	time 0.7349 (0.7685)	loss 2.5912 (2.9893)	grad_norm 1.8592 (2.1803)	mem 23871MB
[2022-11-13 16:12:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][150/1251]	eta 0:13:58 lr 0.000218	time 0.7421 (0.7616)	loss 3.4395 (2.9847)	grad_norm 3.3508 (2.1485)	mem 23871MB
[2022-11-13 16:12:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][200/1251]	eta 0:13:17 lr 0.000218	time 0.7453 (0.7587)	loss 1.9718 (2.9492)	grad_norm 1.9537 (2.1230)	mem 23871MB
[2022-11-13 16:13:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][250/1251]	eta 0:12:37 lr 0.000217	time 0.7397 (0.7568)	loss 2.9643 (2.9256)	grad_norm 2.3025 (2.1406)	mem 23871MB
[2022-11-13 16:13:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][300/1251]	eta 0:11:58 lr 0.000217	time 0.7442 (0.7556)	loss 2.4441 (2.9328)	grad_norm 2.4188 (inf)	mem 23871MB
[2022-11-13 16:14:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][350/1251]	eta 0:11:20 lr 0.000217	time 0.7440 (0.7548)	loss 2.0269 (2.9261)	grad_norm 1.8379 (inf)	mem 23871MB
[2022-11-13 16:15:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][400/1251]	eta 0:10:41 lr 0.000217	time 0.7485 (0.7542)	loss 3.1044 (2.9226)	grad_norm 4.3585 (inf)	mem 23871MB
[2022-11-13 16:15:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][450/1251]	eta 0:10:03 lr 0.000217	time 0.7420 (0.7535)	loss 3.3488 (2.9172)	grad_norm 1.9924 (inf)	mem 23871MB
[2022-11-13 16:16:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][500/1251]	eta 0:09:25 lr 0.000217	time 0.7412 (0.7530)	loss 3.2390 (2.9171)	grad_norm 2.4252 (inf)	mem 23871MB
[2022-11-13 16:17:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][550/1251]	eta 0:08:47 lr 0.000216	time 0.7482 (0.7529)	loss 2.1435 (2.9155)	grad_norm 3.2015 (inf)	mem 23871MB
[2022-11-13 16:17:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][600/1251]	eta 0:08:09 lr 0.000216	time 0.7386 (0.7524)	loss 2.8585 (2.9092)	grad_norm 2.3652 (inf)	mem 23871MB
[2022-11-13 16:18:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][650/1251]	eta 0:07:32 lr 0.000216	time 0.7534 (0.7523)	loss 2.2162 (2.9043)	grad_norm 1.9083 (inf)	mem 23871MB
[2022-11-13 16:18:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][700/1251]	eta 0:06:54 lr 0.000216	time 0.7412 (0.7520)	loss 2.7356 (2.8994)	grad_norm 2.2134 (inf)	mem 23871MB
[2022-11-13 16:19:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][750/1251]	eta 0:06:16 lr 0.000216	time 0.7641 (0.7519)	loss 2.2525 (2.8936)	grad_norm 1.8690 (inf)	mem 23871MB
[2022-11-13 16:20:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][800/1251]	eta 0:05:39 lr 0.000216	time 0.7412 (0.7518)	loss 3.3789 (2.8938)	grad_norm 1.7716 (inf)	mem 23871MB
[2022-11-13 16:20:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][850/1251]	eta 0:05:01 lr 0.000215	time 0.7378 (0.7517)	loss 2.6852 (2.8943)	grad_norm 3.3027 (inf)	mem 23871MB
[2022-11-13 16:21:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][900/1251]	eta 0:04:23 lr 0.000215	time 0.7444 (0.7514)	loss 3.4204 (2.8990)	grad_norm 2.0313 (inf)	mem 23871MB
[2022-11-13 16:22:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][950/1251]	eta 0:03:46 lr 0.000215	time 0.7360 (0.7514)	loss 2.1026 (2.8982)	grad_norm 1.8427 (inf)	mem 23871MB
[2022-11-13 16:22:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][1000/1251]	eta 0:03:08 lr 0.000215	time 0.7426 (0.7512)	loss 3.2603 (2.8996)	grad_norm 2.3353 (inf)	mem 23871MB
[2022-11-13 16:23:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][1050/1251]	eta 0:02:30 lr 0.000215	time 0.7426 (0.7512)	loss 2.6020 (2.9009)	grad_norm 2.1538 (inf)	mem 23871MB
[2022-11-13 16:23:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][1100/1251]	eta 0:01:53 lr 0.000215	time 0.7385 (0.7510)	loss 3.2916 (2.9024)	grad_norm 1.9912 (inf)	mem 23871MB
[2022-11-13 16:24:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][1150/1251]	eta 0:01:15 lr 0.000214	time 0.7412 (0.7510)	loss 2.6107 (2.9009)	grad_norm 1.8360 (inf)	mem 23871MB
[2022-11-13 16:25:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][1200/1251]	eta 0:00:38 lr 0.000214	time 0.7411 (0.7509)	loss 3.4711 (2.8999)	grad_norm 2.0300 (inf)	mem 23871MB
[2022-11-13 16:25:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [209/300][1250/1251]	eta 0:00:00 lr 0.000214	time 0.7351 (0.7507)	loss 2.9097 (2.9005)	grad_norm 2.1991 (inf)	mem 23871MB
[2022-11-13 16:25:48 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 209 training takes 0:15:39
[2022-11-13 16:25:48 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_209.pth saving......
[2022-11-13 16:25:49 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_209.pth saved !!!
[2022-11-13 16:25:51 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.820 (1.820)	Loss 0.7403 (0.7403)	Acc@1 81.250 (81.250)	Acc@5 96.680 (96.680)	Mem 23871MB
[2022-11-13 16:26:02 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.826 Acc@5 96.072
[2022-11-13 16:26:02 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.8%
[2022-11-13 16:26:04 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.078 (2.078)	Loss 0.7947 (0.7947)	Acc@1 81.934 (81.934)	Acc@5 96.387 (96.387)	Mem 23871MB
[2022-11-13 16:26:15 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.046 Acc@5 96.566
[2022-11-13 16:26:15 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.0%
[2022-11-13 16:26:15 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.05% at 209 epoch
[2022-11-13 16:26:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][0/1251]	eta 0:51:47 lr 0.000214	time 2.4840 (2.4840)	loss 2.3483 (2.3483)	grad_norm 1.9068 (1.9068)	mem 23872MB
[2022-11-13 16:26:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][50/1251]	eta 0:15:45 lr 0.000214	time 0.7320 (0.7873)	loss 2.1636 (2.8487)	grad_norm 2.3596 (2.1252)	mem 23872MB
[2022-11-13 16:27:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][100/1251]	eta 0:14:44 lr 0.000214	time 0.7377 (0.7688)	loss 3.2952 (2.8637)	grad_norm 2.0201 (2.1177)	mem 23872MB
[2022-11-13 16:28:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][150/1251]	eta 0:13:58 lr 0.000214	time 0.7429 (0.7617)	loss 3.1289 (2.8696)	grad_norm 1.8752 (2.1431)	mem 23872MB
[2022-11-13 16:28:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][200/1251]	eta 0:13:17 lr 0.000213	time 0.7458 (0.7586)	loss 2.9942 (2.8878)	grad_norm 2.2720 (2.1587)	mem 23872MB
[2022-11-13 16:29:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][250/1251]	eta 0:12:37 lr 0.000213	time 0.7409 (0.7567)	loss 2.7721 (2.8675)	grad_norm 2.2072 (2.1626)	mem 23872MB
[2022-11-13 16:30:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][300/1251]	eta 0:11:59 lr 0.000213	time 0.7428 (0.7561)	loss 3.0183 (2.8701)	grad_norm 2.0011 (2.1583)	mem 23872MB
[2022-11-13 16:30:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][350/1251]	eta 0:11:20 lr 0.000213	time 0.7449 (0.7556)	loss 3.1199 (2.8740)	grad_norm 2.2508 (2.1460)	mem 23872MB
[2022-11-13 16:31:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][400/1251]	eta 0:10:42 lr 0.000213	time 0.7460 (0.7551)	loss 3.5257 (2.8891)	grad_norm 2.1008 (2.1390)	mem 23872MB
[2022-11-13 16:31:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][450/1251]	eta 0:10:04 lr 0.000213	time 0.7413 (0.7545)	loss 3.0825 (2.8851)	grad_norm 2.3984 (2.1497)	mem 23872MB
[2022-11-13 16:32:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][500/1251]	eta 0:09:26 lr 0.000212	time 0.7412 (0.7541)	loss 2.9659 (2.8834)	grad_norm 2.3652 (2.1494)	mem 23872MB
[2022-11-13 16:33:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][550/1251]	eta 0:08:48 lr 0.000212	time 0.7429 (0.7539)	loss 2.0024 (2.8764)	grad_norm 2.1222 (2.1406)	mem 23872MB
[2022-11-13 16:33:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][600/1251]	eta 0:08:10 lr 0.000212	time 0.7513 (0.7536)	loss 3.1400 (2.8824)	grad_norm 1.9117 (2.1481)	mem 23872MB
[2022-11-13 16:34:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][650/1251]	eta 0:07:32 lr 0.000212	time 0.7401 (0.7534)	loss 3.1532 (2.8844)	grad_norm 2.3753 (2.1558)	mem 23872MB
[2022-11-13 16:35:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][700/1251]	eta 0:06:54 lr 0.000212	time 0.7466 (0.7529)	loss 3.2028 (2.8859)	grad_norm 2.4818 (2.1601)	mem 23872MB
[2022-11-13 16:35:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][750/1251]	eta 0:06:17 lr 0.000212	time 0.7504 (0.7527)	loss 3.6439 (2.8817)	grad_norm 2.3281 (2.1615)	mem 23872MB
[2022-11-13 16:36:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][800/1251]	eta 0:05:39 lr 0.000211	time 0.7427 (0.7524)	loss 2.1739 (2.8867)	grad_norm 2.1340 (2.1608)	mem 23872MB
[2022-11-13 16:36:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][850/1251]	eta 0:05:01 lr 0.000211	time 0.7426 (0.7525)	loss 3.2827 (2.8886)	grad_norm 2.1198 (2.1555)	mem 23872MB
[2022-11-13 16:37:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][900/1251]	eta 0:04:24 lr 0.000211	time 0.8384 (0.7522)	loss 2.7425 (2.8916)	grad_norm 1.7991 (2.1584)	mem 23872MB
[2022-11-13 16:38:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][950/1251]	eta 0:03:46 lr 0.000211	time 0.7575 (0.7523)	loss 3.1973 (2.8935)	grad_norm 2.4950 (2.1614)	mem 23872MB
[2022-11-13 16:38:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][1000/1251]	eta 0:03:08 lr 0.000211	time 0.7439 (0.7520)	loss 3.2397 (2.8963)	grad_norm 2.0432 (2.1530)	mem 23872MB
[2022-11-13 16:39:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][1050/1251]	eta 0:02:31 lr 0.000211	time 0.7393 (0.7520)	loss 2.9555 (2.9016)	grad_norm 2.0653 (2.1619)	mem 23872MB
[2022-11-13 16:40:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][1100/1251]	eta 0:01:53 lr 0.000210	time 0.7421 (0.7518)	loss 2.8894 (2.9038)	grad_norm 2.0133 (2.1583)	mem 23872MB
[2022-11-13 16:40:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][1150/1251]	eta 0:01:15 lr 0.000210	time 0.7422 (0.7518)	loss 2.7444 (2.9048)	grad_norm 1.8140 (2.1644)	mem 23872MB
[2022-11-13 16:41:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][1200/1251]	eta 0:00:38 lr 0.000210	time 0.7407 (0.7517)	loss 2.6835 (2.9032)	grad_norm 3.2064 (2.1642)	mem 23872MB
[2022-11-13 16:41:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [210/300][1250/1251]	eta 0:00:00 lr 0.000210	time 0.7308 (0.7514)	loss 2.9036 (2.9061)	grad_norm 2.0291 (2.1629)	mem 23872MB
[2022-11-13 16:41:55 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 210 training takes 0:15:40
[2022-11-13 16:41:55 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_210.pth saving......
[2022-11-13 16:41:56 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_210.pth saved !!!
[2022-11-13 16:41:58 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.706 (1.706)	Loss 0.6908 (0.6908)	Acc@1 83.496 (83.496)	Acc@5 96.191 (96.191)	Mem 23872MB
[2022-11-13 16:42:09 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.058 Acc@5 96.038
[2022-11-13 16:42:09 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.1%
[2022-11-13 16:42:11 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.113 (2.113)	Loss 0.7687 (0.7687)	Acc@1 81.543 (81.543)	Acc@5 96.191 (96.191)	Mem 23872MB
[2022-11-13 16:42:22 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.062 Acc@5 96.566
[2022-11-13 16:42:22 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.1%
[2022-11-13 16:42:22 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.06% at 210 epoch
[2022-11-13 16:42:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][0/1251]	eta 0:53:27 lr 0.000210	time 2.5642 (2.5642)	loss 2.8880 (2.8880)	grad_norm 2.0403 (2.0403)	mem 23872MB
[2022-11-13 16:43:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][50/1251]	eta 0:15:46 lr 0.000210	time 0.7445 (0.7885)	loss 2.8624 (2.9438)	grad_norm 1.9144 (2.0008)	mem 23872MB
[2022-11-13 16:43:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][100/1251]	eta 0:14:45 lr 0.000210	time 0.7443 (0.7698)	loss 2.6138 (2.9521)	grad_norm 1.9164 (2.0649)	mem 23872MB
[2022-11-13 16:44:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][150/1251]	eta 0:14:01 lr 0.000209	time 0.7468 (0.7640)	loss 3.2472 (2.9200)	grad_norm 2.1549 (2.0928)	mem 23872MB
[2022-11-13 16:44:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][200/1251]	eta 0:13:19 lr 0.000209	time 0.7395 (0.7607)	loss 3.0359 (2.9210)	grad_norm 2.2920 (2.1408)	mem 23872MB
[2022-11-13 16:45:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][250/1251]	eta 0:12:38 lr 0.000209	time 0.7413 (0.7582)	loss 2.2228 (2.9086)	grad_norm 2.8614 (2.1466)	mem 23872MB
[2022-11-13 16:46:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][300/1251]	eta 0:12:00 lr 0.000209	time 0.8412 (0.7573)	loss 2.8436 (2.9018)	grad_norm 2.1397 (2.1437)	mem 23872MB
[2022-11-13 16:46:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][350/1251]	eta 0:11:21 lr 0.000209	time 0.7433 (0.7562)	loss 3.4069 (2.9040)	grad_norm 2.0752 (2.1367)	mem 23872MB
[2022-11-13 16:47:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][400/1251]	eta 0:10:43 lr 0.000209	time 0.7427 (0.7556)	loss 2.8096 (2.8928)	grad_norm 2.1326 (2.1433)	mem 23872MB
[2022-11-13 16:48:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][450/1251]	eta 0:10:04 lr 0.000208	time 0.7426 (0.7548)	loss 3.2054 (2.8875)	grad_norm 2.4206 (2.1419)	mem 23872MB
[2022-11-13 16:48:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][500/1251]	eta 0:09:26 lr 0.000208	time 0.7433 (0.7543)	loss 2.7416 (2.8871)	grad_norm 2.3074 (2.1400)	mem 23872MB
[2022-11-13 16:49:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][550/1251]	eta 0:08:48 lr 0.000208	time 0.7429 (0.7540)	loss 2.9914 (2.8890)	grad_norm 1.9396 (2.1495)	mem 23872MB
[2022-11-13 16:49:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][600/1251]	eta 0:08:10 lr 0.000208	time 0.7404 (0.7538)	loss 2.8445 (2.8821)	grad_norm 1.8209 (2.1554)	mem 23872MB
[2022-11-13 16:50:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][650/1251]	eta 0:07:32 lr 0.000208	time 0.7438 (0.7535)	loss 3.4126 (2.8872)	grad_norm 1.8462 (2.1487)	mem 23872MB
[2022-11-13 16:51:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][700/1251]	eta 0:06:55 lr 0.000208	time 0.7471 (0.7532)	loss 3.3633 (2.8927)	grad_norm 2.3363 (2.1545)	mem 23872MB
[2022-11-13 16:51:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][750/1251]	eta 0:06:17 lr 0.000207	time 0.8296 (0.7531)	loss 2.6910 (2.8942)	grad_norm 2.0013 (2.1569)	mem 23872MB
[2022-11-13 16:52:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][800/1251]	eta 0:05:39 lr 0.000207	time 0.7411 (0.7528)	loss 3.2125 (2.8943)	grad_norm 2.1601 (2.1506)	mem 23872MB
[2022-11-13 16:53:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][850/1251]	eta 0:05:01 lr 0.000207	time 0.7430 (0.7527)	loss 2.8825 (2.8915)	grad_norm 2.2826 (2.1492)	mem 23872MB
[2022-11-13 16:53:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][900/1251]	eta 0:04:24 lr 0.000207	time 0.7567 (0.7525)	loss 2.2879 (2.8927)	grad_norm 7.9471 (2.1541)	mem 23872MB
[2022-11-13 16:54:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][950/1251]	eta 0:03:46 lr 0.000207	time 0.7408 (0.7525)	loss 2.7469 (2.9015)	grad_norm 2.6450 (2.1556)	mem 23872MB
[2022-11-13 16:54:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][1000/1251]	eta 0:03:08 lr 0.000207	time 0.7362 (0.7524)	loss 3.1281 (2.9020)	grad_norm 2.0659 (2.1655)	mem 23872MB
[2022-11-13 16:55:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][1050/1251]	eta 0:02:31 lr 0.000206	time 0.7386 (0.7523)	loss 2.6163 (2.9035)	grad_norm 2.0408 (2.1661)	mem 23872MB
[2022-11-13 16:56:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][1100/1251]	eta 0:01:53 lr 0.000206	time 0.7414 (0.7522)	loss 2.9502 (2.8997)	grad_norm 2.1111 (2.1635)	mem 23872MB
[2022-11-13 16:56:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][1150/1251]	eta 0:01:15 lr 0.000206	time 0.7414 (0.7521)	loss 2.5273 (2.8993)	grad_norm 2.1588 (2.1642)	mem 23872MB
[2022-11-13 16:57:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][1200/1251]	eta 0:00:38 lr 0.000206	time 0.7393 (0.7519)	loss 1.8990 (2.8999)	grad_norm 2.7636 (2.1678)	mem 23872MB
[2022-11-13 16:58:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [211/300][1250/1251]	eta 0:00:00 lr 0.000206	time 0.7287 (0.7518)	loss 2.7233 (2.8988)	grad_norm 2.3479 (2.1656)	mem 23872MB
[2022-11-13 16:58:02 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 211 training takes 0:15:40
[2022-11-13 16:58:02 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_211.pth saving......
[2022-11-13 16:58:03 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_211.pth saved !!!
[2022-11-13 16:58:05 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.824 (1.824)	Loss 0.7343 (0.7343)	Acc@1 83.008 (83.008)	Acc@5 95.996 (95.996)	Mem 23872MB
[2022-11-13 16:58:16 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.996 Acc@5 96.076
[2022-11-13 16:58:16 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.0%
[2022-11-13 16:58:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.977 (1.977)	Loss 0.6529 (0.6529)	Acc@1 84.961 (84.961)	Acc@5 97.070 (97.070)	Mem 23872MB
[2022-11-13 16:58:29 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.114 Acc@5 96.568
[2022-11-13 16:58:29 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.1%
[2022-11-13 16:58:29 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.11% at 211 epoch
[2022-11-13 16:58:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][0/1251]	eta 0:54:06 lr 0.000206	time 2.5950 (2.5950)	loss 2.8204 (2.8204)	grad_norm 2.5704 (2.5704)	mem 23872MB
[2022-11-13 16:59:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][50/1251]	eta 0:15:50 lr 0.000206	time 0.7466 (0.7910)	loss 3.3002 (2.9683)	grad_norm 2.9304 (2.2036)	mem 23872MB
[2022-11-13 16:59:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][100/1251]	eta 0:14:48 lr 0.000205	time 0.7437 (0.7720)	loss 2.6807 (2.9305)	grad_norm 3.4620 (2.1651)	mem 23872MB
[2022-11-13 17:00:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][150/1251]	eta 0:14:02 lr 0.000205	time 0.7579 (0.7653)	loss 2.6748 (2.9124)	grad_norm 1.7009 (2.1790)	mem 23872MB
[2022-11-13 17:01:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][200/1251]	eta 0:13:22 lr 0.000205	time 0.7447 (0.7633)	loss 2.1851 (2.9202)	grad_norm 2.0350 (2.1825)	mem 23872MB
[2022-11-13 17:01:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][250/1251]	eta 0:12:40 lr 0.000205	time 0.7412 (0.7600)	loss 3.2625 (2.9205)	grad_norm 2.0640 (2.1842)	mem 23872MB
[2022-11-13 17:02:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][300/1251]	eta 0:12:02 lr 0.000205	time 0.7536 (0.7594)	loss 2.2329 (2.9292)	grad_norm 2.6653 (2.1782)	mem 23872MB
[2022-11-13 17:02:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][350/1251]	eta 0:11:23 lr 0.000205	time 0.7370 (0.7581)	loss 2.9846 (2.9279)	grad_norm 2.1605 (2.1687)	mem 23872MB
[2022-11-13 17:03:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][400/1251]	eta 0:10:44 lr 0.000204	time 0.7432 (0.7573)	loss 3.1776 (2.9124)	grad_norm 2.1797 (2.1785)	mem 23872MB
[2022-11-13 17:04:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][450/1251]	eta 0:10:05 lr 0.000204	time 0.7491 (0.7563)	loss 2.5645 (2.9090)	grad_norm 2.0013 (inf)	mem 23872MB
[2022-11-13 17:04:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][500/1251]	eta 0:09:27 lr 0.000204	time 0.7473 (0.7556)	loss 2.8638 (2.9090)	grad_norm 5.9333 (inf)	mem 23872MB
[2022-11-13 17:05:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][550/1251]	eta 0:08:49 lr 0.000204	time 0.7389 (0.7550)	loss 1.9617 (2.9089)	grad_norm 2.1191 (inf)	mem 23872MB
[2022-11-13 17:06:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][600/1251]	eta 0:08:11 lr 0.000204	time 0.7385 (0.7549)	loss 3.3332 (2.8992)	grad_norm 2.0145 (inf)	mem 23872MB
[2022-11-13 17:06:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][650/1251]	eta 0:07:33 lr 0.000204	time 0.7467 (0.7544)	loss 2.7581 (2.8982)	grad_norm 2.1832 (inf)	mem 23872MB
[2022-11-13 17:07:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][700/1251]	eta 0:06:55 lr 0.000203	time 0.7435 (0.7542)	loss 2.3480 (2.9003)	grad_norm 1.7646 (inf)	mem 23872MB
[2022-11-13 17:07:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][750/1251]	eta 0:06:17 lr 0.000203	time 0.7402 (0.7539)	loss 2.2895 (2.9006)	grad_norm 1.8707 (inf)	mem 23872MB
[2022-11-13 17:08:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][800/1251]	eta 0:05:39 lr 0.000203	time 0.7590 (0.7538)	loss 3.0453 (2.8984)	grad_norm 2.0719 (inf)	mem 23872MB
[2022-11-13 17:09:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][850/1251]	eta 0:05:02 lr 0.000203	time 0.7429 (0.7536)	loss 3.1841 (2.8975)	grad_norm 1.9457 (inf)	mem 23872MB
[2022-11-13 17:09:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][900/1251]	eta 0:04:24 lr 0.000203	time 0.7427 (0.7534)	loss 2.6697 (2.8965)	grad_norm 1.6982 (inf)	mem 23872MB
[2022-11-13 17:10:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][950/1251]	eta 0:03:46 lr 0.000203	time 0.7432 (0.7534)	loss 1.8290 (2.8950)	grad_norm 2.4793 (inf)	mem 23872MB
[2022-11-13 17:11:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][1000/1251]	eta 0:03:09 lr 0.000202	time 0.7485 (0.7534)	loss 3.3454 (2.8999)	grad_norm 1.7333 (inf)	mem 23872MB
[2022-11-13 17:11:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][1050/1251]	eta 0:02:31 lr 0.000202	time 0.7455 (0.7532)	loss 3.0263 (2.9012)	grad_norm 1.8552 (inf)	mem 23872MB
[2022-11-13 17:12:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][1100/1251]	eta 0:01:53 lr 0.000202	time 0.7466 (0.7531)	loss 3.2187 (2.9020)	grad_norm 1.8772 (inf)	mem 23872MB
[2022-11-13 17:12:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][1150/1251]	eta 0:01:16 lr 0.000202	time 0.7432 (0.7530)	loss 3.0923 (2.9039)	grad_norm 2.3833 (inf)	mem 23872MB
[2022-11-13 17:13:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][1200/1251]	eta 0:00:38 lr 0.000202	time 0.7442 (0.7529)	loss 2.4754 (2.9010)	grad_norm 2.2596 (inf)	mem 23872MB
[2022-11-13 17:14:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [212/300][1250/1251]	eta 0:00:00 lr 0.000202	time 0.7290 (0.7526)	loss 3.3845 (2.9008)	grad_norm 1.9277 (inf)	mem 23872MB
[2022-11-13 17:14:11 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 212 training takes 0:15:41
[2022-11-13 17:14:11 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_212.pth saving......
[2022-11-13 17:14:12 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_212.pth saved !!!
[2022-11-13 17:14:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.744 (1.744)	Loss 0.7731 (0.7731)	Acc@1 81.445 (81.445)	Acc@5 95.898 (95.898)	Mem 23872MB
[2022-11-13 17:14:24 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 81.854 Acc@5 96.082
[2022-11-13 17:14:24 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 81.9%
[2022-11-13 17:14:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.916 (1.916)	Loss 0.8099 (0.8099)	Acc@1 81.152 (81.152)	Acc@5 94.824 (94.824)	Mem 23872MB
[2022-11-13 17:14:37 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.136 Acc@5 96.586
[2022-11-13 17:14:37 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.1%
[2022-11-13 17:14:37 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.14% at 212 epoch
[2022-11-13 17:14:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][0/1251]	eta 0:55:32 lr 0.000202	time 2.6637 (2.6637)	loss 2.3185 (2.3185)	grad_norm 2.4259 (2.4259)	mem 23872MB
[2022-11-13 17:15:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][50/1251]	eta 0:15:42 lr 0.000201	time 0.7396 (0.7847)	loss 2.2967 (2.8460)	grad_norm 2.0958 (2.1901)	mem 23872MB
[2022-11-13 17:15:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][100/1251]	eta 0:14:45 lr 0.000201	time 0.7435 (0.7691)	loss 3.3594 (2.9015)	grad_norm 2.4369 (2.1675)	mem 23872MB
[2022-11-13 17:16:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][150/1251]	eta 0:13:59 lr 0.000201	time 0.7462 (0.7624)	loss 1.8729 (2.8769)	grad_norm 2.0271 (2.1520)	mem 23872MB
[2022-11-13 17:17:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][200/1251]	eta 0:13:18 lr 0.000201	time 0.7365 (0.7599)	loss 2.7231 (2.9045)	grad_norm 2.0795 (2.1827)	mem 23872MB
[2022-11-13 17:17:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][250/1251]	eta 0:12:38 lr 0.000201	time 0.7422 (0.7579)	loss 2.8547 (2.9335)	grad_norm 1.7702 (2.1827)	mem 23872MB
[2022-11-13 17:18:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][300/1251]	eta 0:11:59 lr 0.000201	time 0.8280 (0.7565)	loss 1.9276 (2.9170)	grad_norm 2.1406 (2.1803)	mem 23872MB
[2022-11-13 17:19:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][350/1251]	eta 0:11:21 lr 0.000200	time 0.7409 (0.7560)	loss 3.0086 (2.9069)	grad_norm 1.9502 (2.1843)	mem 23872MB
[2022-11-13 17:19:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][400/1251]	eta 0:10:42 lr 0.000200	time 0.7513 (0.7552)	loss 2.4179 (2.8999)	grad_norm 2.3987 (2.1830)	mem 23872MB
[2022-11-13 17:20:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][450/1251]	eta 0:10:04 lr 0.000200	time 0.7449 (0.7549)	loss 3.0474 (2.9017)	grad_norm 1.9565 (2.2002)	mem 23872MB
[2022-11-13 17:20:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][500/1251]	eta 0:09:26 lr 0.000200	time 0.7390 (0.7544)	loss 3.3097 (2.8914)	grad_norm 2.0366 (2.2024)	mem 23872MB
[2022-11-13 17:21:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][550/1251]	eta 0:08:48 lr 0.000200	time 0.7421 (0.7540)	loss 3.2978 (2.8969)	grad_norm 2.0800 (2.1943)	mem 23872MB
[2022-11-13 17:22:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][600/1251]	eta 0:08:10 lr 0.000200	time 0.7450 (0.7538)	loss 2.4270 (2.8973)	grad_norm 1.9899 (2.1932)	mem 23872MB
[2022-11-13 17:22:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][650/1251]	eta 0:07:33 lr 0.000199	time 0.7387 (0.7538)	loss 2.6931 (2.8963)	grad_norm 1.9991 (2.1931)	mem 23872MB
[2022-11-13 17:23:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][700/1251]	eta 0:06:55 lr 0.000199	time 0.8321 (0.7534)	loss 3.4321 (2.8940)	grad_norm 1.9775 (2.1999)	mem 23872MB
[2022-11-13 17:24:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][750/1251]	eta 0:06:17 lr 0.000199	time 0.7405 (0.7533)	loss 2.7985 (2.8933)	grad_norm 2.4943 (2.2141)	mem 23872MB
[2022-11-13 17:24:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][800/1251]	eta 0:05:39 lr 0.000199	time 0.7410 (0.7532)	loss 2.1519 (2.8948)	grad_norm 2.2273 (2.2132)	mem 23872MB
[2022-11-13 17:25:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][850/1251]	eta 0:05:01 lr 0.000199	time 0.7393 (0.7529)	loss 2.8378 (2.8920)	grad_norm 1.7456 (2.2087)	mem 23872MB
[2022-11-13 17:25:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][900/1251]	eta 0:04:24 lr 0.000199	time 0.7423 (0.7528)	loss 3.4057 (2.8886)	grad_norm 2.3105 (2.2039)	mem 23872MB
[2022-11-13 17:26:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][950/1251]	eta 0:03:46 lr 0.000199	time 0.7398 (0.7526)	loss 3.0514 (2.8831)	grad_norm 2.4398 (2.2013)	mem 23872MB
[2022-11-13 17:27:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][1000/1251]	eta 0:03:08 lr 0.000198	time 0.7426 (0.7526)	loss 3.1890 (2.8850)	grad_norm 2.1392 (2.1986)	mem 23872MB
[2022-11-13 17:27:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][1050/1251]	eta 0:02:31 lr 0.000198	time 0.7485 (0.7525)	loss 2.7840 (2.8852)	grad_norm 2.0898 (2.2073)	mem 23872MB
[2022-11-13 17:28:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][1100/1251]	eta 0:01:53 lr 0.000198	time 0.8355 (0.7524)	loss 2.9329 (2.8863)	grad_norm 2.3772 (2.2075)	mem 23872MB
[2022-11-13 17:29:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][1150/1251]	eta 0:01:15 lr 0.000198	time 0.7400 (0.7524)	loss 3.2551 (2.8823)	grad_norm 3.1856 (2.2071)	mem 23872MB
[2022-11-13 17:29:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][1200/1251]	eta 0:00:38 lr 0.000198	time 0.7397 (0.7522)	loss 2.3663 (2.8771)	grad_norm 2.3460 (2.2100)	mem 23872MB
[2022-11-13 17:30:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [213/300][1250/1251]	eta 0:00:00 lr 0.000198	time 0.7337 (0.7519)	loss 3.2821 (2.8803)	grad_norm 2.1779 (2.2064)	mem 23872MB
[2022-11-13 17:30:18 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 213 training takes 0:15:40
[2022-11-13 17:30:18 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_213.pth saving......
[2022-11-13 17:30:19 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_213.pth saved !!!
[2022-11-13 17:30:21 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.670 (1.670)	Loss 0.7673 (0.7673)	Acc@1 81.152 (81.152)	Acc@5 96.289 (96.289)	Mem 23872MB
[2022-11-13 17:30:32 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.008 Acc@5 96.104
[2022-11-13 17:30:32 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.0%
[2022-11-13 17:30:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.946 (1.946)	Loss 0.7209 (0.7209)	Acc@1 82.910 (82.910)	Acc@5 95.898 (95.898)	Mem 23872MB
[2022-11-13 17:30:45 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.192 Acc@5 96.590
[2022-11-13 17:30:45 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.2%
[2022-11-13 17:30:45 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.19% at 213 epoch
[2022-11-13 17:30:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][0/1251]	eta 0:53:05 lr 0.000198	time 2.5463 (2.5463)	loss 3.4046 (3.4046)	grad_norm 2.0577 (2.0577)	mem 23872MB
[2022-11-13 17:31:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][50/1251]	eta 0:15:41 lr 0.000197	time 0.7448 (0.7842)	loss 2.4403 (2.9204)	grad_norm 1.7438 (2.4253)	mem 23872MB
[2022-11-13 17:32:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][100/1251]	eta 0:14:44 lr 0.000197	time 0.7385 (0.7681)	loss 2.6664 (2.9023)	grad_norm 1.9311 (2.3171)	mem 23872MB
[2022-11-13 17:32:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][150/1251]	eta 0:13:59 lr 0.000197	time 0.7428 (0.7625)	loss 3.3545 (2.9084)	grad_norm 2.5558 (2.2744)	mem 23872MB
[2022-11-13 17:33:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][200/1251]	eta 0:13:18 lr 0.000197	time 0.7442 (0.7600)	loss 3.0353 (2.9244)	grad_norm 2.6791 (2.2756)	mem 23872MB
[2022-11-13 17:33:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][250/1251]	eta 0:12:38 lr 0.000197	time 0.7399 (0.7578)	loss 2.2564 (2.9170)	grad_norm 2.0408 (2.2730)	mem 23872MB
[2022-11-13 17:34:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][300/1251]	eta 0:11:59 lr 0.000197	time 0.8270 (0.7569)	loss 3.1944 (2.9100)	grad_norm 1.9341 (2.2678)	mem 23872MB
[2022-11-13 17:35:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][350/1251]	eta 0:11:21 lr 0.000196	time 0.7440 (0.7561)	loss 2.3007 (2.9104)	grad_norm 2.1033 (2.2348)	mem 23872MB
[2022-11-13 17:35:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][400/1251]	eta 0:10:42 lr 0.000196	time 0.7392 (0.7555)	loss 2.1076 (2.9154)	grad_norm 2.2023 (2.2336)	mem 23872MB
[2022-11-13 17:36:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][450/1251]	eta 0:10:04 lr 0.000196	time 0.7427 (0.7550)	loss 3.4770 (2.9167)	grad_norm 3.4620 (2.2246)	mem 23872MB
[2022-11-13 17:37:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][500/1251]	eta 0:09:26 lr 0.000196	time 0.7470 (0.7545)	loss 3.2076 (2.9074)	grad_norm 2.1481 (2.2242)	mem 23872MB
[2022-11-13 17:37:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][550/1251]	eta 0:08:48 lr 0.000196	time 0.7387 (0.7546)	loss 2.1086 (2.9117)	grad_norm 2.1117 (2.2133)	mem 23872MB
[2022-11-13 17:38:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][600/1251]	eta 0:08:10 lr 0.000196	time 0.7554 (0.7541)	loss 3.0140 (2.9066)	grad_norm 1.8973 (2.2211)	mem 23872MB
[2022-11-13 17:38:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][650/1251]	eta 0:07:33 lr 0.000195	time 0.7427 (0.7539)	loss 2.7218 (2.9083)	grad_norm 2.0628 (2.2250)	mem 23872MB
[2022-11-13 17:39:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][700/1251]	eta 0:06:55 lr 0.000195	time 0.8197 (0.7539)	loss 3.4409 (2.9084)	grad_norm 4.1784 (2.2317)	mem 23872MB
[2022-11-13 17:40:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][750/1251]	eta 0:06:17 lr 0.000195	time 0.7406 (0.7536)	loss 2.7471 (2.9075)	grad_norm 1.8216 (2.2327)	mem 23872MB
[2022-11-13 17:40:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][800/1251]	eta 0:05:39 lr 0.000195	time 0.7436 (0.7534)	loss 1.6400 (2.9011)	grad_norm 2.1229 (2.2315)	mem 23872MB
[2022-11-13 17:41:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][850/1251]	eta 0:05:02 lr 0.000195	time 0.7428 (0.7535)	loss 2.0153 (2.8988)	grad_norm 2.2310 (2.2283)	mem 23872MB
[2022-11-13 17:42:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][900/1251]	eta 0:04:24 lr 0.000195	time 0.7434 (0.7532)	loss 2.2238 (2.8936)	grad_norm 1.8593 (2.2265)	mem 23872MB
[2022-11-13 17:42:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][950/1251]	eta 0:03:46 lr 0.000194	time 0.7436 (0.7533)	loss 2.4980 (2.8962)	grad_norm 2.0145 (2.2433)	mem 23872MB
[2022-11-13 17:43:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][1000/1251]	eta 0:03:09 lr 0.000194	time 0.7437 (0.7531)	loss 3.2662 (2.8976)	grad_norm 1.9610 (2.2381)	mem 23872MB
[2022-11-13 17:43:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][1050/1251]	eta 0:02:31 lr 0.000194	time 0.7410 (0.7532)	loss 3.3660 (2.8941)	grad_norm 2.1225 (2.2353)	mem 23872MB
[2022-11-13 17:44:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][1100/1251]	eta 0:01:53 lr 0.000194	time 0.8274 (0.7531)	loss 3.1619 (2.8883)	grad_norm 2.0688 (2.2308)	mem 23872MB
[2022-11-13 17:45:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][1150/1251]	eta 0:01:16 lr 0.000194	time 0.7387 (0.7529)	loss 2.5815 (2.8834)	grad_norm 1.9391 (2.2270)	mem 23872MB
[2022-11-13 17:45:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][1200/1251]	eta 0:00:38 lr 0.000194	time 0.7451 (0.7528)	loss 3.3114 (2.8846)	grad_norm 2.0402 (2.2241)	mem 23872MB
[2022-11-13 17:46:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [214/300][1250/1251]	eta 0:00:00 lr 0.000193	time 0.7397 (0.7526)	loss 2.8993 (2.8864)	grad_norm 2.1737 (2.2222)	mem 23872MB
[2022-11-13 17:46:26 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 214 training takes 0:15:41
[2022-11-13 17:46:26 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_214.pth saving......
[2022-11-13 17:46:27 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_214.pth saved !!!
[2022-11-13 17:46:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.717 (1.717)	Loss 0.7825 (0.7825)	Acc@1 80.957 (80.957)	Acc@5 96.289 (96.289)	Mem 23872MB
[2022-11-13 17:46:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.056 Acc@5 96.200
[2022-11-13 17:46:40 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.1%
[2022-11-13 17:46:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.057 (2.057)	Loss 0.6781 (0.6781)	Acc@1 83.984 (83.984)	Acc@5 97.266 (97.266)	Mem 23872MB
[2022-11-13 17:46:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.244 Acc@5 96.622
[2022-11-13 17:46:53 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.2%
[2022-11-13 17:46:53 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.24% at 214 epoch
[2022-11-13 17:46:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][0/1251]	eta 0:51:54 lr 0.000193	time 2.4894 (2.4894)	loss 3.0955 (3.0955)	grad_norm 2.0447 (2.0447)	mem 23874MB
[2022-11-13 17:47:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][50/1251]	eta 0:15:44 lr 0.000193	time 0.7518 (0.7860)	loss 2.7177 (2.8781)	grad_norm 1.8133 (2.1389)	mem 23874MB
[2022-11-13 17:48:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][100/1251]	eta 0:14:45 lr 0.000193	time 0.7431 (0.7696)	loss 2.7930 (2.8696)	grad_norm 2.2393 (2.1773)	mem 23874MB
[2022-11-13 17:48:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][150/1251]	eta 0:14:00 lr 0.000193	time 0.7408 (0.7635)	loss 3.2123 (2.8725)	grad_norm 1.8493 (2.1773)	mem 23874MB
[2022-11-13 17:49:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][200/1251]	eta 0:13:19 lr 0.000193	time 0.7431 (0.7605)	loss 2.4742 (2.8519)	grad_norm 2.3141 (2.1978)	mem 23874MB
[2022-11-13 17:50:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][250/1251]	eta 0:12:38 lr 0.000193	time 0.7411 (0.7579)	loss 3.0793 (2.8789)	grad_norm 2.1011 (2.1976)	mem 23874MB
[2022-11-13 17:50:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][300/1251]	eta 0:12:00 lr 0.000193	time 0.8392 (0.7573)	loss 3.2363 (2.8835)	grad_norm 2.5709 (2.2019)	mem 23874MB
[2022-11-13 17:51:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][350/1251]	eta 0:11:21 lr 0.000192	time 0.7445 (0.7560)	loss 1.7995 (2.8921)	grad_norm 2.0235 (inf)	mem 23874MB
[2022-11-13 17:51:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][400/1251]	eta 0:10:42 lr 0.000192	time 0.7432 (0.7555)	loss 2.9897 (2.8964)	grad_norm 2.6525 (inf)	mem 23874MB
[2022-11-13 17:52:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][450/1251]	eta 0:10:04 lr 0.000192	time 0.7398 (0.7550)	loss 2.7138 (2.8908)	grad_norm 1.9767 (inf)	mem 23874MB
[2022-11-13 17:53:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][500/1251]	eta 0:09:26 lr 0.000192	time 0.7433 (0.7544)	loss 3.0066 (2.8858)	grad_norm 1.9089 (inf)	mem 23874MB
[2022-11-13 17:53:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][550/1251]	eta 0:08:48 lr 0.000192	time 0.7390 (0.7544)	loss 3.0934 (2.8808)	grad_norm 1.9588 (inf)	mem 23874MB
[2022-11-13 17:54:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][600/1251]	eta 0:08:10 lr 0.000192	time 0.7419 (0.7537)	loss 3.3274 (2.8861)	grad_norm 2.3761 (inf)	mem 23874MB
[2022-11-13 17:55:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][650/1251]	eta 0:07:33 lr 0.000191	time 0.7425 (0.7538)	loss 3.0803 (2.8932)	grad_norm 2.0717 (inf)	mem 23874MB
[2022-11-13 17:55:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][700/1251]	eta 0:06:55 lr 0.000191	time 0.8358 (0.7535)	loss 3.0790 (2.8959)	grad_norm 2.6325 (inf)	mem 23874MB
[2022-11-13 17:56:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][750/1251]	eta 0:06:17 lr 0.000191	time 0.8146 (0.7535)	loss 3.2581 (2.9015)	grad_norm 2.2143 (inf)	mem 23874MB
[2022-11-13 17:56:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][800/1251]	eta 0:05:39 lr 0.000191	time 0.7430 (0.7533)	loss 2.8576 (2.9035)	grad_norm 1.7690 (inf)	mem 23874MB
[2022-11-13 17:57:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][850/1251]	eta 0:05:02 lr 0.000191	time 0.7447 (0.7532)	loss 3.1521 (2.9054)	grad_norm 2.5951 (inf)	mem 23874MB
[2022-11-13 17:58:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][900/1251]	eta 0:04:24 lr 0.000191	time 0.7437 (0.7528)	loss 2.0701 (2.9024)	grad_norm 2.9705 (inf)	mem 23874MB
[2022-11-13 17:58:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][950/1251]	eta 0:03:46 lr 0.000190	time 0.7449 (0.7528)	loss 3.1198 (2.8928)	grad_norm 2.5292 (inf)	mem 23874MB
[2022-11-13 17:59:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][1000/1251]	eta 0:03:08 lr 0.000190	time 0.7491 (0.7526)	loss 3.1687 (2.8911)	grad_norm 1.8881 (inf)	mem 23874MB
[2022-11-13 18:00:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][1050/1251]	eta 0:02:31 lr 0.000190	time 0.7464 (0.7527)	loss 2.7682 (2.8908)	grad_norm 1.9651 (inf)	mem 23874MB
[2022-11-13 18:00:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][1100/1251]	eta 0:01:53 lr 0.000190	time 0.8374 (0.7527)	loss 3.2651 (2.8912)	grad_norm 2.1901 (inf)	mem 23874MB
[2022-11-13 18:01:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][1150/1251]	eta 0:01:16 lr 0.000190	time 0.8184 (0.7525)	loss 3.1137 (2.8911)	grad_norm 1.9009 (inf)	mem 23874MB
[2022-11-13 18:01:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][1200/1251]	eta 0:00:38 lr 0.000190	time 0.7466 (0.7524)	loss 2.7911 (2.8901)	grad_norm 1.9149 (inf)	mem 23874MB
[2022-11-13 18:02:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [215/300][1250/1251]	eta 0:00:00 lr 0.000189	time 0.7283 (0.7523)	loss 3.0810 (2.8909)	grad_norm 2.3157 (inf)	mem 23874MB
[2022-11-13 18:02:34 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 215 training takes 0:15:41
[2022-11-13 18:02:34 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_215.pth saving......
[2022-11-13 18:02:35 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_215.pth saved !!!
[2022-11-13 18:02:37 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.627 (1.627)	Loss 0.7072 (0.7072)	Acc@1 82.422 (82.422)	Acc@5 96.484 (96.484)	Mem 23874MB
[2022-11-13 18:02:48 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.230 Acc@5 96.186
[2022-11-13 18:02:48 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.2%
[2022-11-13 18:02:50 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.971 (1.971)	Loss 0.6524 (0.6524)	Acc@1 83.984 (83.984)	Acc@5 96.973 (96.973)	Mem 23874MB
[2022-11-13 18:03:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.272 Acc@5 96.642
[2022-11-13 18:03:00 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.3%
[2022-11-13 18:03:00 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.27% at 215 epoch
[2022-11-13 18:03:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][0/1251]	eta 0:51:18 lr 0.000189	time 2.4608 (2.4608)	loss 3.3711 (3.3711)	grad_norm 2.7772 (2.7772)	mem 23874MB
[2022-11-13 18:03:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][50/1251]	eta 0:15:48 lr 0.000189	time 0.7435 (0.7900)	loss 2.8883 (2.7928)	grad_norm 2.0590 (2.1822)	mem 23874MB
[2022-11-13 18:04:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][100/1251]	eta 0:14:47 lr 0.000189	time 0.7415 (0.7708)	loss 2.7589 (2.8335)	grad_norm 1.9793 (2.1744)	mem 23874MB
[2022-11-13 18:04:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][150/1251]	eta 0:14:00 lr 0.000189	time 0.7454 (0.7638)	loss 2.6725 (2.8559)	grad_norm 2.1780 (2.1650)	mem 23874MB
[2022-11-13 18:05:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][200/1251]	eta 0:13:19 lr 0.000189	time 0.7414 (0.7605)	loss 3.2029 (2.8739)	grad_norm 2.4124 (2.2261)	mem 23874MB
[2022-11-13 18:06:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][250/1251]	eta 0:12:38 lr 0.000189	time 0.7394 (0.7579)	loss 3.2671 (2.8788)	grad_norm 1.8737 (2.2147)	mem 23874MB
[2022-11-13 18:06:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][300/1251]	eta 0:11:59 lr 0.000189	time 0.7401 (0.7569)	loss 3.1879 (2.8818)	grad_norm 2.0334 (2.2141)	mem 23874MB
[2022-11-13 18:07:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][350/1251]	eta 0:11:21 lr 0.000188	time 0.7465 (0.7561)	loss 2.7761 (2.8778)	grad_norm 1.9648 (2.2081)	mem 23874MB
[2022-11-13 18:08:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][400/1251]	eta 0:10:43 lr 0.000188	time 0.7398 (0.7557)	loss 2.8265 (2.8715)	grad_norm 1.9748 (2.2080)	mem 23874MB
[2022-11-13 18:08:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][450/1251]	eta 0:10:04 lr 0.000188	time 0.7320 (0.7550)	loss 2.7270 (2.8643)	grad_norm 2.0199 (2.2126)	mem 23874MB
[2022-11-13 18:09:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][500/1251]	eta 0:09:26 lr 0.000188	time 0.7445 (0.7543)	loss 3.5318 (2.8641)	grad_norm 2.6410 (2.2194)	mem 23874MB
[2022-11-13 18:09:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][550/1251]	eta 0:08:48 lr 0.000188	time 0.7414 (0.7540)	loss 3.1092 (2.8692)	grad_norm 2.6589 (2.2205)	mem 23874MB
[2022-11-13 18:10:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][600/1251]	eta 0:08:10 lr 0.000188	time 0.7421 (0.7538)	loss 3.1582 (2.8671)	grad_norm 2.6387 (2.2328)	mem 23874MB
[2022-11-13 18:11:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][650/1251]	eta 0:07:32 lr 0.000187	time 0.7414 (0.7537)	loss 2.0023 (2.8684)	grad_norm 2.3970 (2.2307)	mem 23874MB
[2022-11-13 18:11:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][700/1251]	eta 0:06:55 lr 0.000187	time 0.7433 (0.7537)	loss 2.8008 (2.8714)	grad_norm 1.9135 (2.2453)	mem 23874MB
[2022-11-13 18:12:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][750/1251]	eta 0:06:17 lr 0.000187	time 0.8254 (0.7536)	loss 2.9631 (2.8730)	grad_norm 2.0646 (2.2454)	mem 23874MB
[2022-11-13 18:13:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][800/1251]	eta 0:05:39 lr 0.000187	time 0.7445 (0.7534)	loss 2.6148 (2.8701)	grad_norm 2.0084 (2.2439)	mem 23874MB
[2022-11-13 18:13:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][850/1251]	eta 0:05:02 lr 0.000187	time 0.7522 (0.7536)	loss 2.7475 (2.8752)	grad_norm 2.0680 (2.2515)	mem 23874MB
[2022-11-13 18:14:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][900/1251]	eta 0:04:24 lr 0.000187	time 0.7392 (0.7534)	loss 3.0017 (2.8772)	grad_norm 1.9421 (2.2452)	mem 23874MB
[2022-11-13 18:14:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][950/1251]	eta 0:03:46 lr 0.000186	time 0.7451 (0.7535)	loss 3.3243 (2.8718)	grad_norm 3.1712 (2.2537)	mem 23874MB
[2022-11-13 18:15:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][1000/1251]	eta 0:03:09 lr 0.000186	time 0.7378 (0.7533)	loss 3.0462 (2.8696)	grad_norm 2.2677 (2.2553)	mem 23874MB
[2022-11-13 18:16:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][1050/1251]	eta 0:02:31 lr 0.000186	time 0.7464 (0.7533)	loss 3.2078 (2.8666)	grad_norm 2.0661 (2.2562)	mem 23874MB
[2022-11-13 18:16:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][1100/1251]	eta 0:01:53 lr 0.000186	time 0.7413 (0.7532)	loss 2.7990 (2.8670)	grad_norm 1.9306 (2.2544)	mem 23874MB
[2022-11-13 18:17:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][1150/1251]	eta 0:01:16 lr 0.000186	time 0.7412 (0.7531)	loss 2.3072 (2.8698)	grad_norm 2.1048 (2.2525)	mem 23874MB
[2022-11-13 18:18:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][1200/1251]	eta 0:00:38 lr 0.000186	time 0.7464 (0.7532)	loss 2.1711 (2.8690)	grad_norm 2.1914 (2.2571)	mem 23874MB
[2022-11-13 18:18:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [216/300][1250/1251]	eta 0:00:00 lr 0.000186	time 0.7324 (0.7531)	loss 3.0405 (2.8689)	grad_norm 2.2975 (2.2556)	mem 23874MB
[2022-11-13 18:18:43 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 216 training takes 0:15:42
[2022-11-13 18:18:43 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_216.pth saving......
[2022-11-13 18:18:44 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_216.pth saved !!!
[2022-11-13 18:18:46 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.695 (1.695)	Loss 0.8397 (0.8397)	Acc@1 80.859 (80.859)	Acc@5 95.215 (95.215)	Mem 23874MB
[2022-11-13 18:18:56 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.236 Acc@5 96.134
[2022-11-13 18:18:56 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.2%
[2022-11-13 18:18:58 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.981 (1.981)	Loss 0.6845 (0.6845)	Acc@1 84.277 (84.277)	Acc@5 95.703 (95.703)	Mem 23874MB
[2022-11-13 18:19:09 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.282 Acc@5 96.652
[2022-11-13 18:19:09 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.3%
[2022-11-13 18:19:09 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.28% at 216 epoch
[2022-11-13 18:19:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][0/1251]	eta 0:52:58 lr 0.000185	time 2.5411 (2.5411)	loss 2.0317 (2.0317)	grad_norm 2.3467 (2.3467)	mem 23874MB
[2022-11-13 18:19:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][50/1251]	eta 0:15:47 lr 0.000185	time 0.7410 (0.7892)	loss 3.1731 (2.8218)	grad_norm 1.9726 (2.3190)	mem 23874MB
[2022-11-13 18:20:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][100/1251]	eta 0:14:49 lr 0.000185	time 0.7442 (0.7724)	loss 2.3460 (2.8080)	grad_norm 1.8677 (2.3449)	mem 23874MB
[2022-11-13 18:21:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][150/1251]	eta 0:14:03 lr 0.000185	time 0.7454 (0.7665)	loss 3.5301 (2.8298)	grad_norm 2.0071 (2.3133)	mem 23874MB
[2022-11-13 18:21:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][200/1251]	eta 0:13:21 lr 0.000185	time 0.7510 (0.7626)	loss 2.4665 (2.8219)	grad_norm 1.8702 (2.2955)	mem 23874MB
[2022-11-13 18:22:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][250/1251]	eta 0:12:41 lr 0.000185	time 0.7439 (0.7608)	loss 3.0403 (2.8455)	grad_norm 1.8910 (2.2779)	mem 23874MB
[2022-11-13 18:22:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][300/1251]	eta 0:12:02 lr 0.000185	time 0.7520 (0.7592)	loss 3.0885 (2.8251)	grad_norm 2.1351 (2.2761)	mem 23874MB
[2022-11-13 18:23:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][350/1251]	eta 0:11:23 lr 0.000184	time 0.7485 (0.7585)	loss 2.2629 (2.8337)	grad_norm 1.8761 (2.2736)	mem 23874MB
[2022-11-13 18:24:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][400/1251]	eta 0:10:45 lr 0.000184	time 0.7398 (0.7580)	loss 3.5003 (2.8385)	grad_norm 2.4178 (2.2749)	mem 23874MB
[2022-11-13 18:24:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][450/1251]	eta 0:10:06 lr 0.000184	time 0.7441 (0.7571)	loss 3.3903 (2.8438)	grad_norm 2.0935 (2.2669)	mem 23874MB
[2022-11-13 18:25:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][500/1251]	eta 0:09:28 lr 0.000184	time 0.7428 (0.7567)	loss 3.4511 (2.8451)	grad_norm 3.4533 (2.2634)	mem 23874MB
[2022-11-13 18:26:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][550/1251]	eta 0:08:50 lr 0.000184	time 0.7444 (0.7563)	loss 3.1318 (2.8443)	grad_norm 2.7310 (2.2636)	mem 23874MB
[2022-11-13 18:26:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][600/1251]	eta 0:08:12 lr 0.000184	time 0.7490 (0.7560)	loss 3.1678 (2.8463)	grad_norm 2.1342 (2.2571)	mem 23874MB
[2022-11-13 18:27:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][650/1251]	eta 0:07:34 lr 0.000183	time 0.7519 (0.7559)	loss 2.9351 (2.8450)	grad_norm 2.6028 (2.2529)	mem 23874MB
[2022-11-13 18:27:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][700/1251]	eta 0:06:56 lr 0.000183	time 0.7394 (0.7556)	loss 3.5091 (2.8473)	grad_norm 2.2321 (2.2527)	mem 23874MB
[2022-11-13 18:28:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][750/1251]	eta 0:06:18 lr 0.000183	time 0.7303 (0.7554)	loss 2.4163 (2.8454)	grad_norm 2.2539 (2.2406)	mem 23874MB
[2022-11-13 18:29:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][800/1251]	eta 0:05:40 lr 0.000183	time 0.7465 (0.7553)	loss 2.5949 (2.8408)	grad_norm 2.7530 (2.2463)	mem 23874MB
[2022-11-13 18:29:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][850/1251]	eta 0:05:02 lr 0.000183	time 0.7427 (0.7549)	loss 3.1587 (2.8436)	grad_norm 1.7426 (2.2362)	mem 23874MB
[2022-11-13 18:30:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][900/1251]	eta 0:04:24 lr 0.000183	time 0.7473 (0.7549)	loss 2.4155 (2.8468)	grad_norm 2.4947 (2.2325)	mem 23874MB
[2022-11-13 18:31:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][950/1251]	eta 0:03:47 lr 0.000183	time 0.7546 (0.7546)	loss 2.7960 (2.8450)	grad_norm 2.4542 (2.2275)	mem 23874MB
[2022-11-13 18:31:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][1000/1251]	eta 0:03:09 lr 0.000182	time 0.7413 (0.7545)	loss 3.0762 (2.8459)	grad_norm 2.0197 (2.2248)	mem 23874MB
[2022-11-13 18:32:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][1050/1251]	eta 0:02:31 lr 0.000182	time 0.7421 (0.7545)	loss 2.4451 (2.8477)	grad_norm 1.9243 (2.2217)	mem 23874MB
[2022-11-13 18:33:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][1100/1251]	eta 0:01:53 lr 0.000182	time 0.7419 (0.7542)	loss 3.1747 (2.8466)	grad_norm 2.1208 (2.2181)	mem 23874MB
[2022-11-13 18:33:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][1150/1251]	eta 0:01:16 lr 0.000182	time 0.7426 (0.7540)	loss 3.2181 (2.8473)	grad_norm 1.9428 (2.2228)	mem 23874MB
[2022-11-13 18:34:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][1200/1251]	eta 0:00:38 lr 0.000182	time 0.7427 (0.7540)	loss 2.6424 (2.8471)	grad_norm 2.2029 (2.2203)	mem 23874MB
[2022-11-13 18:34:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [217/300][1250/1251]	eta 0:00:00 lr 0.000182	time 0.7290 (0.7537)	loss 3.1908 (2.8468)	grad_norm 2.7810 (2.2204)	mem 23874MB
[2022-11-13 18:34:52 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 217 training takes 0:15:42
[2022-11-13 18:34:52 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_217.pth saving......
[2022-11-13 18:34:53 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_217.pth saved !!!
[2022-11-13 18:34:55 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.748 (1.748)	Loss 0.7315 (0.7315)	Acc@1 82.324 (82.324)	Acc@5 95.801 (95.801)	Mem 23874MB
[2022-11-13 18:35:06 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.216 Acc@5 96.158
[2022-11-13 18:35:06 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.2%
[2022-11-13 18:35:08 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.886 (1.886)	Loss 0.7045 (0.7045)	Acc@1 83.984 (83.984)	Acc@5 97.168 (97.168)	Mem 23874MB
[2022-11-13 18:35:19 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.302 Acc@5 96.654
[2022-11-13 18:35:19 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.3%
[2022-11-13 18:35:19 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.30% at 217 epoch
[2022-11-13 18:35:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][0/1251]	eta 0:52:58 lr 0.000182	time 2.5405 (2.5405)	loss 3.2119 (3.2119)	grad_norm 2.1578 (2.1578)	mem 23874MB
[2022-11-13 18:35:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][50/1251]	eta 0:15:45 lr 0.000181	time 0.7459 (0.7871)	loss 3.0376 (2.8378)	grad_norm 2.0965 (2.1745)	mem 23874MB
[2022-11-13 18:36:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][100/1251]	eta 0:14:45 lr 0.000181	time 0.7471 (0.7689)	loss 2.4635 (2.8885)	grad_norm 1.8310 (2.1758)	mem 23874MB
[2022-11-13 18:37:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][150/1251]	eta 0:13:59 lr 0.000181	time 0.7428 (0.7625)	loss 2.6096 (2.8771)	grad_norm 1.9045 (2.2375)	mem 23874MB
[2022-11-13 18:37:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][200/1251]	eta 0:13:17 lr 0.000181	time 0.7498 (0.7592)	loss 2.9888 (2.8504)	grad_norm 2.4290 (2.2477)	mem 23874MB
[2022-11-13 18:38:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][250/1251]	eta 0:12:38 lr 0.000181	time 0.7441 (0.7577)	loss 3.3635 (2.8556)	grad_norm 2.6758 (2.2596)	mem 23874MB
[2022-11-13 18:39:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][300/1251]	eta 0:11:59 lr 0.000181	time 0.7526 (0.7563)	loss 2.4442 (2.8518)	grad_norm 2.6846 (2.2628)	mem 23874MB
[2022-11-13 18:39:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][350/1251]	eta 0:11:21 lr 0.000180	time 0.7411 (0.7558)	loss 2.2983 (2.8399)	grad_norm 1.8099 (2.2768)	mem 23874MB
[2022-11-13 18:40:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][400/1251]	eta 0:10:42 lr 0.000180	time 0.7526 (0.7548)	loss 3.2053 (2.8526)	grad_norm 2.8896 (inf)	mem 23874MB
[2022-11-13 18:40:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][450/1251]	eta 0:10:04 lr 0.000180	time 0.7429 (0.7547)	loss 2.8239 (2.8482)	grad_norm 2.1946 (inf)	mem 23874MB
[2022-11-13 18:41:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][500/1251]	eta 0:09:26 lr 0.000180	time 0.7422 (0.7543)	loss 3.4461 (2.8490)	grad_norm 2.4526 (inf)	mem 23874MB
[2022-11-13 18:42:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][550/1251]	eta 0:08:48 lr 0.000180	time 0.7392 (0.7538)	loss 1.9240 (2.8456)	grad_norm 2.1350 (inf)	mem 23874MB
[2022-11-13 18:42:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][600/1251]	eta 0:08:10 lr 0.000180	time 0.7295 (0.7536)	loss 2.8162 (2.8572)	grad_norm 2.6659 (inf)	mem 23874MB
[2022-11-13 18:43:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][650/1251]	eta 0:07:32 lr 0.000180	time 0.7426 (0.7534)	loss 2.0067 (2.8536)	grad_norm 1.7778 (inf)	mem 23874MB
[2022-11-13 18:44:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][700/1251]	eta 0:06:54 lr 0.000179	time 0.7248 (0.7529)	loss 2.8598 (2.8602)	grad_norm 2.3222 (inf)	mem 23874MB
[2022-11-13 18:44:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][750/1251]	eta 0:06:17 lr 0.000179	time 0.7399 (0.7528)	loss 2.9574 (2.8695)	grad_norm 1.9866 (inf)	mem 23874MB
[2022-11-13 18:45:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][800/1251]	eta 0:05:39 lr 0.000179	time 0.7397 (0.7524)	loss 2.1052 (2.8718)	grad_norm 2.0523 (inf)	mem 23874MB
[2022-11-13 18:45:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][850/1251]	eta 0:05:01 lr 0.000179	time 0.7464 (0.7524)	loss 2.9737 (2.8692)	grad_norm 2.0991 (inf)	mem 23874MB
[2022-11-13 18:46:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][900/1251]	eta 0:04:24 lr 0.000179	time 0.7401 (0.7523)	loss 2.3637 (2.8723)	grad_norm 2.0781 (inf)	mem 23874MB
[2022-11-13 18:47:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][950/1251]	eta 0:03:46 lr 0.000179	time 0.7455 (0.7523)	loss 2.6331 (2.8662)	grad_norm 2.9055 (inf)	mem 23874MB
[2022-11-13 18:47:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][1000/1251]	eta 0:03:08 lr 0.000178	time 0.7453 (0.7522)	loss 2.4318 (2.8675)	grad_norm 1.9595 (inf)	mem 23874MB
[2022-11-13 18:48:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][1050/1251]	eta 0:02:31 lr 0.000178	time 0.7496 (0.7523)	loss 2.0141 (2.8654)	grad_norm 1.9358 (inf)	mem 23874MB
[2022-11-13 18:49:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][1100/1251]	eta 0:01:53 lr 0.000178	time 0.8293 (0.7521)	loss 3.1891 (2.8657)	grad_norm 2.8808 (inf)	mem 23874MB
[2022-11-13 18:49:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][1150/1251]	eta 0:01:15 lr 0.000178	time 0.7467 (0.7520)	loss 3.2177 (2.8639)	grad_norm 2.0103 (inf)	mem 23874MB
[2022-11-13 18:50:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][1200/1251]	eta 0:00:38 lr 0.000178	time 0.7414 (0.7518)	loss 2.9171 (2.8635)	grad_norm 2.0781 (inf)	mem 23874MB
[2022-11-13 18:50:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [218/300][1250/1251]	eta 0:00:00 lr 0.000178	time 0.7292 (0.7517)	loss 3.2873 (2.8618)	grad_norm 2.0653 (inf)	mem 23874MB
[2022-11-13 18:50:59 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 218 training takes 0:15:40
[2022-11-13 18:50:59 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_218.pth saving......
[2022-11-13 18:51:00 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_218.pth saved !!!
[2022-11-13 18:51:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.653 (1.653)	Loss 0.6753 (0.6753)	Acc@1 83.301 (83.301)	Acc@5 96.973 (96.973)	Mem 23874MB
[2022-11-13 18:51:13 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.262 Acc@5 96.210
[2022-11-13 18:51:13 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.3%
[2022-11-13 18:51:15 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.943 (1.943)	Loss 0.6585 (0.6585)	Acc@1 83.984 (83.984)	Acc@5 96.582 (96.582)	Mem 23874MB
[2022-11-13 18:51:26 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.318 Acc@5 96.628
[2022-11-13 18:51:26 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.3%
[2022-11-13 18:51:26 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.32% at 218 epoch
[2022-11-13 18:51:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][0/1251]	eta 0:54:36 lr 0.000178	time 2.6191 (2.6191)	loss 2.8697 (2.8697)	grad_norm 1.9466 (1.9466)	mem 23874MB
[2022-11-13 18:52:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][50/1251]	eta 0:15:48 lr 0.000177	time 0.7402 (0.7894)	loss 3.3578 (2.9502)	grad_norm 2.1063 (2.1169)	mem 23874MB
[2022-11-13 18:52:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][100/1251]	eta 0:14:47 lr 0.000177	time 0.8253 (0.7714)	loss 3.0797 (2.8535)	grad_norm 2.3247 (2.2476)	mem 23874MB
[2022-11-13 18:53:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][150/1251]	eta 0:14:02 lr 0.000177	time 0.7649 (0.7653)	loss 2.9144 (2.8723)	grad_norm 1.9420 (2.2134)	mem 23874MB
[2022-11-13 18:53:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][200/1251]	eta 0:13:20 lr 0.000177	time 0.7419 (0.7615)	loss 3.2975 (2.8400)	grad_norm 2.3329 (2.2127)	mem 23874MB
[2022-11-13 18:54:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][250/1251]	eta 0:12:40 lr 0.000177	time 0.7427 (0.7595)	loss 2.8957 (2.8358)	grad_norm 2.3583 (2.2119)	mem 23874MB
[2022-11-13 18:55:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][300/1251]	eta 0:12:01 lr 0.000177	time 0.8231 (0.7584)	loss 3.1553 (2.8369)	grad_norm 2.2628 (2.2171)	mem 23874MB
[2022-11-13 18:55:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][350/1251]	eta 0:11:22 lr 0.000177	time 0.7446 (0.7573)	loss 1.8951 (2.8479)	grad_norm 2.3761 (2.2227)	mem 23874MB
[2022-11-13 18:56:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][400/1251]	eta 0:10:43 lr 0.000176	time 0.7401 (0.7563)	loss 2.3502 (2.8331)	grad_norm 2.0920 (2.2288)	mem 23874MB
[2022-11-13 18:57:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][450/1251]	eta 0:10:05 lr 0.000176	time 0.7440 (0.7559)	loss 3.0441 (2.8451)	grad_norm 3.1988 (2.2352)	mem 23874MB
[2022-11-13 18:57:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][500/1251]	eta 0:09:27 lr 0.000176	time 0.7456 (0.7551)	loss 2.1161 (2.8425)	grad_norm 2.2806 (2.2364)	mem 23874MB
[2022-11-13 18:58:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][550/1251]	eta 0:08:49 lr 0.000176	time 0.7406 (0.7550)	loss 2.2367 (2.8386)	grad_norm 3.5227 (2.2541)	mem 23874MB
[2022-11-13 18:58:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][600/1251]	eta 0:08:11 lr 0.000176	time 0.7482 (0.7546)	loss 2.9139 (2.8453)	grad_norm 2.3551 (2.2598)	mem 23874MB
[2022-11-13 18:59:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][650/1251]	eta 0:07:33 lr 0.000176	time 0.7460 (0.7546)	loss 3.4619 (2.8402)	grad_norm 2.0863 (2.2599)	mem 23874MB
[2022-11-13 19:00:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][700/1251]	eta 0:06:55 lr 0.000175	time 0.8355 (0.7545)	loss 3.1176 (2.8487)	grad_norm 2.4471 (2.2599)	mem 23874MB
[2022-11-13 19:00:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][750/1251]	eta 0:06:17 lr 0.000175	time 0.7419 (0.7543)	loss 3.2686 (2.8510)	grad_norm 2.5471 (2.2831)	mem 23874MB
[2022-11-13 19:01:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][800/1251]	eta 0:05:39 lr 0.000175	time 0.7406 (0.7539)	loss 2.8738 (2.8534)	grad_norm 2.2553 (2.2847)	mem 23874MB
[2022-11-13 19:02:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][850/1251]	eta 0:05:02 lr 0.000175	time 0.7391 (0.7539)	loss 2.4290 (2.8563)	grad_norm 2.0970 (2.2804)	mem 23874MB
[2022-11-13 19:02:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][900/1251]	eta 0:04:24 lr 0.000175	time 0.7410 (0.7538)	loss 2.4432 (2.8511)	grad_norm 2.2913 (2.2763)	mem 23874MB
[2022-11-13 19:03:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][950/1251]	eta 0:03:46 lr 0.000175	time 0.7429 (0.7537)	loss 2.5148 (2.8509)	grad_norm 2.1216 (2.2788)	mem 23874MB
[2022-11-13 19:04:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][1000/1251]	eta 0:03:09 lr 0.000175	time 0.7412 (0.7535)	loss 3.1787 (2.8571)	grad_norm 1.9149 (2.2789)	mem 23874MB
[2022-11-13 19:04:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][1050/1251]	eta 0:02:31 lr 0.000174	time 0.7483 (0.7533)	loss 1.9907 (2.8513)	grad_norm 2.2500 (2.2764)	mem 23874MB
[2022-11-13 19:05:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][1100/1251]	eta 0:01:53 lr 0.000174	time 0.8263 (0.7533)	loss 3.0054 (2.8498)	grad_norm 1.9197 (2.2775)	mem 23874MB
[2022-11-13 19:05:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][1150/1251]	eta 0:01:16 lr 0.000174	time 0.7403 (0.7531)	loss 3.3236 (2.8531)	grad_norm 2.7163 (2.2767)	mem 23874MB
[2022-11-13 19:06:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][1200/1251]	eta 0:00:38 lr 0.000174	time 0.7404 (0.7531)	loss 3.2528 (2.8540)	grad_norm 2.2327 (2.2751)	mem 23874MB
[2022-11-13 19:07:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [219/300][1250/1251]	eta 0:00:00 lr 0.000174	time 0.7302 (0.7529)	loss 3.0610 (2.8533)	grad_norm 2.1753 (2.2701)	mem 23874MB
[2022-11-13 19:07:08 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 219 training takes 0:15:42
[2022-11-13 19:07:08 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_219.pth saving......
[2022-11-13 19:07:09 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_219.pth saved !!!
[2022-11-13 19:07:11 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.735 (1.735)	Loss 0.8365 (0.8365)	Acc@1 80.469 (80.469)	Acc@5 95.410 (95.410)	Mem 23874MB
[2022-11-13 19:07:21 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.184 Acc@5 96.124
[2022-11-13 19:07:21 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.2%
[2022-11-13 19:07:23 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.948 (1.948)	Loss 0.7839 (0.7839)	Acc@1 81.445 (81.445)	Acc@5 95.898 (95.898)	Mem 23874MB
[2022-11-13 19:07:34 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.362 Acc@5 96.640
[2022-11-13 19:07:34 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.4%
[2022-11-13 19:07:34 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.36% at 219 epoch
[2022-11-13 19:07:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][0/1251]	eta 0:51:59 lr 0.000174	time 2.4939 (2.4939)	loss 2.8646 (2.8646)	grad_norm 2.7058 (2.7058)	mem 23874MB
[2022-11-13 19:08:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][50/1251]	eta 0:15:48 lr 0.000174	time 0.7429 (0.7895)	loss 1.9034 (2.9045)	grad_norm 2.3086 (2.2574)	mem 23874MB
[2022-11-13 19:08:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][100/1251]	eta 0:14:48 lr 0.000173	time 0.7507 (0.7719)	loss 2.8586 (2.8855)	grad_norm 2.4684 (2.2840)	mem 23874MB
[2022-11-13 19:09:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][150/1251]	eta 0:14:01 lr 0.000173	time 0.8178 (0.7647)	loss 3.1095 (2.8678)	grad_norm 2.0625 (2.2715)	mem 23874MB
[2022-11-13 19:10:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][200/1251]	eta 0:13:19 lr 0.000173	time 0.8211 (0.7611)	loss 2.9772 (2.8354)	grad_norm 2.2297 (2.2850)	mem 23874MB
[2022-11-13 19:10:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][250/1251]	eta 0:12:39 lr 0.000173	time 0.7535 (0.7589)	loss 2.9763 (2.8300)	grad_norm 2.2346 (2.3298)	mem 23874MB
[2022-11-13 19:11:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][300/1251]	eta 0:12:00 lr 0.000173	time 0.8339 (0.7581)	loss 2.7892 (2.8371)	grad_norm 3.1237 (2.3340)	mem 23874MB
[2022-11-13 19:12:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][350/1251]	eta 0:11:21 lr 0.000173	time 0.7429 (0.7567)	loss 3.1919 (2.8307)	grad_norm 2.2449 (2.3375)	mem 23874MB
[2022-11-13 19:12:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][400/1251]	eta 0:10:43 lr 0.000173	time 0.8083 (0.7564)	loss 3.0903 (2.8327)	grad_norm 1.9098 (2.3402)	mem 23874MB
[2022-11-13 19:13:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][450/1251]	eta 0:10:05 lr 0.000172	time 0.7380 (0.7555)	loss 2.6423 (2.8406)	grad_norm 2.6270 (2.3397)	mem 23874MB
[2022-11-13 19:13:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][500/1251]	eta 0:09:27 lr 0.000172	time 0.7440 (0.7550)	loss 3.0064 (2.8442)	grad_norm 2.2506 (2.3345)	mem 23874MB
[2022-11-13 19:14:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][550/1251]	eta 0:08:48 lr 0.000172	time 0.7392 (0.7545)	loss 3.2402 (2.8472)	grad_norm 1.8811 (2.3321)	mem 23874MB
[2022-11-13 19:15:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][600/1251]	eta 0:08:10 lr 0.000172	time 0.7405 (0.7541)	loss 3.1529 (2.8603)	grad_norm 1.8621 (2.3330)	mem 23874MB
[2022-11-13 19:15:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][650/1251]	eta 0:07:33 lr 0.000172	time 0.7440 (0.7539)	loss 1.9032 (2.8580)	grad_norm 2.1593 (2.3289)	mem 23874MB
[2022-11-13 19:16:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][700/1251]	eta 0:06:55 lr 0.000172	time 0.8212 (0.7537)	loss 3.3025 (2.8602)	grad_norm 2.4708 (2.3226)	mem 23874MB
[2022-11-13 19:17:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][750/1251]	eta 0:06:17 lr 0.000171	time 0.7516 (0.7535)	loss 2.4819 (2.8601)	grad_norm 2.0903 (2.3196)	mem 23874MB
[2022-11-13 19:17:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][800/1251]	eta 0:05:39 lr 0.000171	time 0.7542 (0.7534)	loss 2.9769 (2.8603)	grad_norm 2.0033 (2.3139)	mem 23874MB
[2022-11-13 19:18:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][850/1251]	eta 0:05:02 lr 0.000171	time 0.7411 (0.7532)	loss 2.1526 (2.8603)	grad_norm 2.9938 (2.3125)	mem 23874MB
[2022-11-13 19:18:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][900/1251]	eta 0:04:24 lr 0.000171	time 0.7372 (0.7531)	loss 3.0872 (2.8595)	grad_norm 2.2594 (2.3161)	mem 23874MB
[2022-11-13 19:19:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][950/1251]	eta 0:03:46 lr 0.000171	time 0.7421 (0.7530)	loss 3.3560 (2.8563)	grad_norm 2.1726 (2.3144)	mem 23874MB
[2022-11-13 19:20:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][1000/1251]	eta 0:03:09 lr 0.000171	time 0.7477 (0.7530)	loss 3.0652 (2.8548)	grad_norm 2.5107 (2.3160)	mem 23874MB
[2022-11-13 19:20:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][1050/1251]	eta 0:02:31 lr 0.000171	time 0.7470 (0.7529)	loss 3.4360 (2.8522)	grad_norm 2.3899 (2.3198)	mem 23874MB
[2022-11-13 19:21:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][1100/1251]	eta 0:01:53 lr 0.000170	time 0.8278 (0.7529)	loss 3.0632 (2.8458)	grad_norm 2.4091 (2.3204)	mem 23874MB
[2022-11-13 19:22:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][1150/1251]	eta 0:01:16 lr 0.000170	time 0.7388 (0.7527)	loss 2.6968 (2.8449)	grad_norm 2.1435 (2.3169)	mem 23874MB
[2022-11-13 19:22:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][1200/1251]	eta 0:00:38 lr 0.000170	time 0.7434 (0.7527)	loss 2.9230 (2.8480)	grad_norm 2.3519 (2.3232)	mem 23874MB
[2022-11-13 19:23:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [220/300][1250/1251]	eta 0:00:00 lr 0.000170	time 0.7311 (0.7525)	loss 2.8090 (2.8491)	grad_norm 1.9616 (2.3189)	mem 23874MB
[2022-11-13 19:23:16 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 220 training takes 0:15:41
[2022-11-13 19:23:16 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_220.pth saving......
[2022-11-13 19:23:17 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_220.pth saved !!!
[2022-11-13 19:23:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.670 (1.670)	Loss 0.7465 (0.7465)	Acc@1 82.227 (82.227)	Acc@5 96.094 (96.094)	Mem 23874MB
[2022-11-13 19:23:29 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.178 Acc@5 96.110
[2022-11-13 19:23:29 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.2%
[2022-11-13 19:23:31 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.948 (1.948)	Loss 0.6842 (0.6842)	Acc@1 84.375 (84.375)	Acc@5 96.777 (96.777)	Mem 23874MB
[2022-11-13 19:23:42 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.402 Acc@5 96.628
[2022-11-13 19:23:42 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.4%
[2022-11-13 19:23:42 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.40% at 220 epoch
[2022-11-13 19:23:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][0/1251]	eta 0:51:48 lr 0.000170	time 2.4848 (2.4848)	loss 2.8009 (2.8009)	grad_norm 2.0309 (2.0309)	mem 23874MB
[2022-11-13 19:24:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][50/1251]	eta 0:15:47 lr 0.000170	time 0.7537 (0.7890)	loss 3.1298 (2.7126)	grad_norm 6.5254 (2.4089)	mem 23874MB
[2022-11-13 19:24:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][100/1251]	eta 0:14:45 lr 0.000170	time 0.7396 (0.7691)	loss 3.0500 (2.8375)	grad_norm 2.4188 (2.4384)	mem 23874MB
[2022-11-13 19:25:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][150/1251]	eta 0:14:00 lr 0.000169	time 0.7345 (0.7635)	loss 2.5994 (2.8225)	grad_norm 1.9866 (2.4491)	mem 23874MB
[2022-11-13 19:26:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][200/1251]	eta 0:13:18 lr 0.000169	time 0.7558 (0.7601)	loss 3.0260 (2.8076)	grad_norm 2.2674 (nan)	mem 23874MB
[2022-11-13 19:26:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][250/1251]	eta 0:12:38 lr 0.000169	time 0.7458 (0.7581)	loss 2.0945 (2.7991)	grad_norm 2.1950 (nan)	mem 23874MB
[2022-11-13 19:27:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][300/1251]	eta 0:11:59 lr 0.000169	time 0.7399 (0.7565)	loss 2.8254 (2.7990)	grad_norm 2.0870 (nan)	mem 23874MB
[2022-11-13 19:28:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][350/1251]	eta 0:11:20 lr 0.000169	time 0.7408 (0.7557)	loss 2.9680 (2.7862)	grad_norm 1.9764 (nan)	mem 23874MB
[2022-11-13 19:28:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][400/1251]	eta 0:10:42 lr 0.000169	time 0.7290 (0.7550)	loss 2.8986 (2.7906)	grad_norm 2.5719 (nan)	mem 23874MB
[2022-11-13 19:29:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][450/1251]	eta 0:10:04 lr 0.000169	time 0.7566 (0.7545)	loss 3.1689 (2.8056)	grad_norm 3.0907 (nan)	mem 23874MB
[2022-11-13 19:30:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][500/1251]	eta 0:09:26 lr 0.000168	time 0.7458 (0.7543)	loss 2.5889 (2.8136)	grad_norm 2.4724 (nan)	mem 23874MB
[2022-11-13 19:30:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][550/1251]	eta 0:08:48 lr 0.000168	time 0.9170 (0.7540)	loss 2.8491 (2.8186)	grad_norm 2.6457 (nan)	mem 23874MB
[2022-11-13 19:31:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][600/1251]	eta 0:08:10 lr 0.000168	time 0.7445 (0.7537)	loss 3.5077 (2.8176)	grad_norm 2.0587 (nan)	mem 23874MB
[2022-11-13 19:31:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][650/1251]	eta 0:07:32 lr 0.000168	time 0.8418 (0.7534)	loss 2.7694 (2.8230)	grad_norm 1.9481 (nan)	mem 23874MB
[2022-11-13 19:32:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][700/1251]	eta 0:06:55 lr 0.000168	time 0.7419 (0.7532)	loss 2.9919 (2.8240)	grad_norm 2.0389 (nan)	mem 23874MB
[2022-11-13 19:33:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][750/1251]	eta 0:06:17 lr 0.000168	time 0.7459 (0.7533)	loss 1.8523 (2.8295)	grad_norm 1.7987 (nan)	mem 23874MB
[2022-11-13 19:33:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][800/1251]	eta 0:05:39 lr 0.000168	time 0.7241 (0.7532)	loss 2.9426 (2.8284)	grad_norm 2.2856 (nan)	mem 23874MB
[2022-11-13 19:34:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][850/1251]	eta 0:05:01 lr 0.000167	time 0.7423 (0.7529)	loss 3.1936 (2.8267)	grad_norm 1.8964 (nan)	mem 23874MB
[2022-11-13 19:35:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][900/1251]	eta 0:04:24 lr 0.000167	time 0.7421 (0.7528)	loss 3.2488 (2.8239)	grad_norm 2.2763 (nan)	mem 23874MB
[2022-11-13 19:35:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][950/1251]	eta 0:03:46 lr 0.000167	time 0.7413 (0.7526)	loss 2.8092 (2.8245)	grad_norm 2.2143 (nan)	mem 23874MB
[2022-11-13 19:36:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][1000/1251]	eta 0:03:08 lr 0.000167	time 0.7447 (0.7526)	loss 3.2587 (2.8310)	grad_norm 2.4372 (nan)	mem 23874MB
[2022-11-13 19:36:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][1050/1251]	eta 0:02:31 lr 0.000167	time 0.7406 (0.7525)	loss 2.8524 (2.8322)	grad_norm 1.8380 (nan)	mem 23874MB
[2022-11-13 19:37:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][1100/1251]	eta 0:01:53 lr 0.000167	time 0.7448 (0.7525)	loss 3.0565 (2.8363)	grad_norm 2.3217 (nan)	mem 23874MB
[2022-11-13 19:38:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][1150/1251]	eta 0:01:15 lr 0.000166	time 0.7353 (0.7524)	loss 3.3270 (2.8395)	grad_norm 2.4214 (nan)	mem 23874MB
[2022-11-13 19:38:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][1200/1251]	eta 0:00:38 lr 0.000166	time 0.8217 (0.7524)	loss 2.9032 (2.8438)	grad_norm 1.8737 (nan)	mem 23874MB
[2022-11-13 19:39:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [221/300][1250/1251]	eta 0:00:00 lr 0.000166	time 0.7304 (0.7522)	loss 3.0156 (2.8459)	grad_norm 1.8808 (nan)	mem 23874MB
[2022-11-13 19:39:23 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 221 training takes 0:15:41
[2022-11-13 19:39:23 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_221.pth saving......
[2022-11-13 19:39:24 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_221.pth saved !!!
[2022-11-13 19:39:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.689 (1.689)	Loss 0.7696 (0.7696)	Acc@1 81.738 (81.738)	Acc@5 95.605 (95.605)	Mem 23874MB
[2022-11-13 19:39:36 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.280 Acc@5 96.202
[2022-11-13 19:39:36 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.3%
[2022-11-13 19:39:38 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.901 (1.901)	Loss 0.7643 (0.7643)	Acc@1 80.957 (80.957)	Acc@5 96.484 (96.484)	Mem 23874MB
[2022-11-13 19:39:49 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.428 Acc@5 96.662
[2022-11-13 19:39:49 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.4%
[2022-11-13 19:39:49 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.43% at 221 epoch
[2022-11-13 19:39:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][0/1251]	eta 0:54:16 lr 0.000166	time 2.6033 (2.6033)	loss 3.0063 (3.0063)	grad_norm 2.3130 (2.3130)	mem 23874MB
[2022-11-13 19:40:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][50/1251]	eta 0:15:49 lr 0.000166	time 0.7402 (0.7903)	loss 2.6938 (2.7647)	grad_norm 4.4796 (2.2059)	mem 23874MB
[2022-11-13 19:41:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][100/1251]	eta 0:14:47 lr 0.000166	time 0.7405 (0.7708)	loss 2.7998 (2.7966)	grad_norm 2.1949 (2.3169)	mem 23874MB
[2022-11-13 19:41:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][150/1251]	eta 0:14:01 lr 0.000166	time 0.7421 (0.7640)	loss 3.0785 (2.8454)	grad_norm 2.1292 (2.3115)	mem 23874MB
[2022-11-13 19:42:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][200/1251]	eta 0:13:19 lr 0.000166	time 0.7449 (0.7604)	loss 3.2710 (2.8048)	grad_norm 2.2803 (2.3277)	mem 23874MB
[2022-11-13 19:43:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][250/1251]	eta 0:12:39 lr 0.000165	time 0.7397 (0.7591)	loss 2.4153 (2.8126)	grad_norm 1.8426 (2.3204)	mem 23874MB
[2022-11-13 19:43:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][300/1251]	eta 0:11:59 lr 0.000165	time 0.7398 (0.7570)	loss 3.0603 (2.8235)	grad_norm 2.0757 (2.3304)	mem 23874MB
[2022-11-13 19:44:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][350/1251]	eta 0:11:21 lr 0.000165	time 0.7361 (0.7562)	loss 3.2739 (2.8192)	grad_norm 1.9805 (2.3074)	mem 23874MB
[2022-11-13 19:44:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][400/1251]	eta 0:10:42 lr 0.000165	time 0.7450 (0.7556)	loss 3.1129 (2.8124)	grad_norm 1.8476 (2.2956)	mem 23874MB
[2022-11-13 19:45:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][450/1251]	eta 0:10:05 lr 0.000165	time 0.7436 (0.7555)	loss 2.8835 (2.8126)	grad_norm 2.2959 (2.2898)	mem 23874MB
[2022-11-13 19:46:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][500/1251]	eta 0:09:26 lr 0.000165	time 0.7459 (0.7548)	loss 2.8916 (2.8187)	grad_norm 2.5228 (2.2884)	mem 23874MB
[2022-11-13 19:46:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][550/1251]	eta 0:08:48 lr 0.000164	time 0.7968 (0.7546)	loss 3.1019 (2.8098)	grad_norm 2.7919 (2.2868)	mem 23874MB
[2022-11-13 19:47:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][600/1251]	eta 0:08:10 lr 0.000164	time 0.7472 (0.7539)	loss 3.3096 (2.8104)	grad_norm 2.2637 (2.2854)	mem 23874MB
[2022-11-13 19:48:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][650/1251]	eta 0:07:33 lr 0.000164	time 0.7512 (0.7541)	loss 1.7837 (2.8133)	grad_norm 2.7759 (2.2999)	mem 23874MB
[2022-11-13 19:48:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][700/1251]	eta 0:06:55 lr 0.000164	time 0.7461 (0.7538)	loss 3.1541 (2.8149)	grad_norm 2.3718 (2.3014)	mem 23874MB
[2022-11-13 19:49:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][750/1251]	eta 0:06:17 lr 0.000164	time 0.7434 (0.7538)	loss 2.5973 (2.8208)	grad_norm 2.6434 (2.3073)	mem 23874MB
[2022-11-13 19:49:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][800/1251]	eta 0:05:39 lr 0.000164	time 0.7431 (0.7535)	loss 2.8721 (2.8199)	grad_norm 2.8489 (2.3123)	mem 23874MB
[2022-11-13 19:50:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][850/1251]	eta 0:05:02 lr 0.000164	time 0.7413 (0.7533)	loss 3.3291 (2.8214)	grad_norm 2.1532 (2.3103)	mem 23874MB
[2022-11-13 19:51:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][900/1251]	eta 0:04:24 lr 0.000163	time 0.7419 (0.7531)	loss 3.0750 (2.8135)	grad_norm 2.0553 (2.3049)	mem 23874MB
[2022-11-13 19:51:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][950/1251]	eta 0:03:46 lr 0.000163	time 0.7410 (0.7531)	loss 3.1571 (2.8160)	grad_norm 2.2274 (2.2993)	mem 23874MB
[2022-11-13 19:52:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][1000/1251]	eta 0:03:09 lr 0.000163	time 0.7429 (0.7530)	loss 3.2877 (2.8147)	grad_norm 2.0131 (2.3042)	mem 23874MB
[2022-11-13 19:53:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][1050/1251]	eta 0:02:31 lr 0.000163	time 0.7499 (0.7531)	loss 2.7096 (2.8155)	grad_norm 2.0024 (2.3049)	mem 23874MB
[2022-11-13 19:53:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][1100/1251]	eta 0:01:53 lr 0.000163	time 0.7426 (0.7527)	loss 3.2858 (2.8211)	grad_norm 3.2413 (2.3068)	mem 23874MB
[2022-11-13 19:54:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][1150/1251]	eta 0:01:16 lr 0.000163	time 0.7474 (0.7528)	loss 2.8250 (2.8208)	grad_norm 1.8501 (2.3072)	mem 23874MB
[2022-11-13 19:54:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][1200/1251]	eta 0:00:38 lr 0.000163	time 0.7417 (0.7526)	loss 3.1919 (2.8229)	grad_norm 2.1330 (2.3035)	mem 23874MB
[2022-11-13 19:55:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [222/300][1250/1251]	eta 0:00:00 lr 0.000162	time 0.7297 (0.7525)	loss 2.6607 (2.8229)	grad_norm 2.1884 (2.3069)	mem 23874MB
[2022-11-13 19:55:31 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 222 training takes 0:15:41
[2022-11-13 19:55:31 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_222.pth saving......
[2022-11-13 19:55:32 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_222.pth saved !!!
[2022-11-13 19:55:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.737 (1.737)	Loss 0.7855 (0.7855)	Acc@1 81.445 (81.445)	Acc@5 96.094 (96.094)	Mem 23874MB
[2022-11-13 19:55:44 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.356 Acc@5 96.228
[2022-11-13 19:55:44 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.4%
[2022-11-13 19:55:46 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.918 (1.918)	Loss 0.7043 (0.7043)	Acc@1 83.594 (83.594)	Acc@5 97.168 (97.168)	Mem 23874MB
[2022-11-13 19:55:57 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.470 Acc@5 96.684
[2022-11-13 19:55:57 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.5%
[2022-11-13 19:55:57 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.47% at 222 epoch
[2022-11-13 19:56:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][0/1251]	eta 0:51:26 lr 0.000162	time 2.4676 (2.4676)	loss 2.6918 (2.6918)	grad_norm 2.3243 (2.3243)	mem 23874MB
[2022-11-13 19:56:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][50/1251]	eta 0:15:48 lr 0.000162	time 0.7432 (0.7898)	loss 2.9360 (2.7217)	grad_norm 2.2477 (2.3013)	mem 23874MB
[2022-11-13 19:57:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][100/1251]	eta 0:14:46 lr 0.000162	time 0.7402 (0.7699)	loss 2.9663 (2.7979)	grad_norm 2.1650 (2.3514)	mem 23874MB
[2022-11-13 19:57:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][150/1251]	eta 0:14:00 lr 0.000162	time 0.7419 (0.7636)	loss 2.8090 (2.8069)	grad_norm 2.0185 (2.3316)	mem 23874MB
[2022-11-13 19:58:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][200/1251]	eta 0:13:19 lr 0.000162	time 0.8161 (0.7603)	loss 1.9948 (2.8213)	grad_norm 2.3455 (2.3465)	mem 23874MB
[2022-11-13 19:59:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][250/1251]	eta 0:12:39 lr 0.000162	time 0.7488 (0.7592)	loss 2.5461 (2.8322)	grad_norm 2.1317 (2.3356)	mem 23874MB
[2022-11-13 19:59:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][300/1251]	eta 0:12:00 lr 0.000161	time 0.7549 (0.7580)	loss 2.2612 (2.8231)	grad_norm 3.2120 (2.3299)	mem 23874MB
[2022-11-13 20:00:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][350/1251]	eta 0:11:21 lr 0.000161	time 0.7377 (0.7569)	loss 2.6721 (2.8262)	grad_norm 2.0429 (2.3167)	mem 23874MB
[2022-11-13 20:01:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][400/1251]	eta 0:10:43 lr 0.000161	time 0.7410 (0.7565)	loss 2.1749 (2.8385)	grad_norm 2.6689 (2.3153)	mem 23874MB
[2022-11-13 20:01:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][450/1251]	eta 0:10:05 lr 0.000161	time 0.7402 (0.7563)	loss 3.2591 (2.8344)	grad_norm 1.8183 (2.3112)	mem 23874MB
[2022-11-13 20:02:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][500/1251]	eta 0:09:27 lr 0.000161	time 0.7405 (0.7554)	loss 2.9785 (2.8302)	grad_norm 2.1118 (2.3145)	mem 23874MB
[2022-11-13 20:02:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][550/1251]	eta 0:08:49 lr 0.000161	time 0.7412 (0.7554)	loss 2.7601 (2.8311)	grad_norm 2.9846 (2.3295)	mem 23874MB
[2022-11-13 20:03:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][600/1251]	eta 0:08:11 lr 0.000161	time 0.8214 (0.7549)	loss 2.7223 (2.8269)	grad_norm 2.8072 (2.3411)	mem 23874MB
[2022-11-13 20:04:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][650/1251]	eta 0:07:33 lr 0.000160	time 0.7429 (0.7548)	loss 3.1810 (2.8266)	grad_norm 1.8771 (2.3491)	mem 23874MB
[2022-11-13 20:04:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][700/1251]	eta 0:06:55 lr 0.000160	time 0.7437 (0.7546)	loss 3.2488 (2.8156)	grad_norm 2.0699 (2.3522)	mem 23874MB
[2022-11-13 20:05:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][750/1251]	eta 0:06:17 lr 0.000160	time 0.7414 (0.7543)	loss 2.9656 (2.8203)	grad_norm 1.9313 (2.3614)	mem 23874MB
[2022-11-13 20:06:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][800/1251]	eta 0:05:40 lr 0.000160	time 0.7466 (0.7542)	loss 2.8938 (2.8265)	grad_norm 2.5116 (2.3583)	mem 23874MB
[2022-11-13 20:06:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][850/1251]	eta 0:05:02 lr 0.000160	time 0.7473 (0.7541)	loss 3.3512 (2.8263)	grad_norm 2.4843 (2.3536)	mem 23874MB
[2022-11-13 20:07:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][900/1251]	eta 0:04:24 lr 0.000160	time 0.7424 (0.7538)	loss 2.9774 (2.8331)	grad_norm 2.1754 (2.3579)	mem 23874MB
[2022-11-13 20:07:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][950/1251]	eta 0:03:46 lr 0.000160	time 0.7358 (0.7537)	loss 3.2573 (2.8359)	grad_norm 2.1744 (2.3578)	mem 23874MB
[2022-11-13 20:08:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][1000/1251]	eta 0:03:09 lr 0.000159	time 0.8190 (0.7534)	loss 3.2459 (2.8442)	grad_norm 2.1294 (nan)	mem 23874MB
[2022-11-13 20:09:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][1050/1251]	eta 0:02:31 lr 0.000159	time 0.7416 (0.7535)	loss 3.1320 (2.8386)	grad_norm 2.0124 (nan)	mem 23874MB
[2022-11-13 20:09:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][1100/1251]	eta 0:01:53 lr 0.000159	time 0.7441 (0.7534)	loss 3.1828 (2.8390)	grad_norm 2.6681 (nan)	mem 23874MB
[2022-11-13 20:10:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][1150/1251]	eta 0:01:16 lr 0.000159	time 0.8359 (0.7532)	loss 2.5616 (2.8350)	grad_norm 1.9853 (nan)	mem 23874MB
[2022-11-13 20:11:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][1200/1251]	eta 0:00:38 lr 0.000159	time 0.7427 (0.7532)	loss 2.9335 (2.8386)	grad_norm 2.1039 (nan)	mem 23874MB
[2022-11-13 20:11:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [223/300][1250/1251]	eta 0:00:00 lr 0.000159	time 0.7300 (0.7530)	loss 3.2285 (2.8402)	grad_norm 2.5635 (nan)	mem 23874MB
[2022-11-13 20:11:39 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 223 training takes 0:15:42
[2022-11-13 20:11:40 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_223.pth saving......
[2022-11-13 20:11:41 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_223.pth saved !!!
[2022-11-13 20:11:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.761 (1.761)	Loss 0.6396 (0.6396)	Acc@1 83.789 (83.789)	Acc@5 97.656 (97.656)	Mem 23874MB
[2022-11-13 20:11:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.586 Acc@5 96.310
[2022-11-13 20:11:53 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.6%
[2022-11-13 20:11:55 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.003 (2.003)	Loss 0.6987 (0.6987)	Acc@1 83.496 (83.496)	Acc@5 96.484 (96.484)	Mem 23874MB
[2022-11-13 20:12:06 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.434 Acc@5 96.676
[2022-11-13 20:12:06 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.4%
[2022-11-13 20:12:06 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.47% at 222 epoch
[2022-11-13 20:12:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][0/1251]	eta 0:53:19 lr 0.000159	time 2.5573 (2.5573)	loss 2.5352 (2.5352)	grad_norm 2.1892 (2.1892)	mem 23874MB
[2022-11-13 20:12:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][50/1251]	eta 0:15:48 lr 0.000159	time 0.7408 (0.7900)	loss 2.2379 (2.7395)	grad_norm 1.7886 (2.3668)	mem 23874MB
[2022-11-13 20:13:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][100/1251]	eta 0:14:49 lr 0.000158	time 0.7470 (0.7726)	loss 2.8030 (2.7418)	grad_norm 3.2885 (2.3452)	mem 23874MB
[2022-11-13 20:14:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][150/1251]	eta 0:14:03 lr 0.000158	time 0.7468 (0.7658)	loss 2.7466 (2.7704)	grad_norm 1.9026 (2.3669)	mem 23874MB
[2022-11-13 20:14:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][200/1251]	eta 0:13:21 lr 0.000158	time 0.7404 (0.7622)	loss 2.7298 (2.8064)	grad_norm 2.2442 (2.3577)	mem 23874MB
[2022-11-13 20:15:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][250/1251]	eta 0:12:41 lr 0.000158	time 0.7422 (0.7609)	loss 2.7608 (2.8114)	grad_norm 2.2338 (2.3796)	mem 23874MB
[2022-11-13 20:15:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][300/1251]	eta 0:12:02 lr 0.000158	time 0.7464 (0.7593)	loss 1.7981 (2.8073)	grad_norm 3.6586 (2.4094)	mem 23874MB
[2022-11-13 20:16:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][350/1251]	eta 0:11:23 lr 0.000158	time 0.7437 (0.7583)	loss 3.1947 (2.8227)	grad_norm 2.2398 (2.3887)	mem 23874MB
[2022-11-13 20:17:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][400/1251]	eta 0:10:45 lr 0.000157	time 0.7409 (0.7580)	loss 3.1290 (2.8160)	grad_norm 1.9074 (2.3779)	mem 23874MB
[2022-11-13 20:17:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][450/1251]	eta 0:10:06 lr 0.000157	time 0.7429 (0.7569)	loss 3.5099 (2.8179)	grad_norm 2.3140 (2.3734)	mem 23874MB
[2022-11-13 20:18:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][500/1251]	eta 0:09:28 lr 0.000157	time 0.7440 (0.7566)	loss 3.0928 (2.8099)	grad_norm 2.0202 (2.3624)	mem 23874MB
[2022-11-13 20:19:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][550/1251]	eta 0:08:50 lr 0.000157	time 0.7577 (0.7561)	loss 2.6326 (2.8054)	grad_norm 2.4402 (2.3526)	mem 23874MB
[2022-11-13 20:19:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][600/1251]	eta 0:08:12 lr 0.000157	time 0.8167 (0.7560)	loss 3.0788 (2.8095)	grad_norm 2.0038 (2.3550)	mem 23874MB
[2022-11-13 20:20:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][650/1251]	eta 0:07:34 lr 0.000157	time 0.7429 (0.7560)	loss 3.2631 (2.8059)	grad_norm 4.0252 (2.3613)	mem 23874MB
[2022-11-13 20:20:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][700/1251]	eta 0:06:56 lr 0.000157	time 0.7418 (0.7552)	loss 3.5059 (2.8054)	grad_norm 2.4391 (2.3629)	mem 23874MB
[2022-11-13 20:21:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][750/1251]	eta 0:06:18 lr 0.000156	time 0.7404 (0.7551)	loss 3.1011 (2.8061)	grad_norm 2.2088 (2.3632)	mem 23874MB
[2022-11-13 20:22:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][800/1251]	eta 0:05:40 lr 0.000156	time 0.7384 (0.7548)	loss 2.6874 (2.8077)	grad_norm 2.4159 (2.3660)	mem 23874MB
[2022-11-13 20:22:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][850/1251]	eta 0:05:02 lr 0.000156	time 0.7433 (0.7547)	loss 2.9191 (2.8131)	grad_norm 2.5182 (2.3591)	mem 23874MB
[2022-11-13 20:23:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][900/1251]	eta 0:04:24 lr 0.000156	time 0.7393 (0.7544)	loss 3.1576 (2.8076)	grad_norm 2.2567 (2.3469)	mem 23874MB
[2022-11-13 20:24:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][950/1251]	eta 0:03:47 lr 0.000156	time 0.8554 (0.7545)	loss 2.7427 (2.8052)	grad_norm 2.0179 (2.3413)	mem 23874MB
[2022-11-13 20:24:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][1000/1251]	eta 0:03:09 lr 0.000156	time 0.7413 (0.7541)	loss 3.3582 (2.8031)	grad_norm 2.0205 (2.3384)	mem 23874MB
[2022-11-13 20:25:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][1050/1251]	eta 0:02:31 lr 0.000156	time 0.7431 (0.7540)	loss 3.1205 (2.8005)	grad_norm 2.2546 (2.3403)	mem 23874MB
[2022-11-13 20:25:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][1100/1251]	eta 0:01:53 lr 0.000155	time 0.7469 (0.7540)	loss 2.7676 (2.8007)	grad_norm 1.9401 (2.3404)	mem 23874MB
[2022-11-13 20:26:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][1150/1251]	eta 0:01:16 lr 0.000155	time 0.7474 (0.7539)	loss 3.1872 (2.8009)	grad_norm 3.0166 (2.3429)	mem 23874MB
[2022-11-13 20:27:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][1200/1251]	eta 0:00:38 lr 0.000155	time 0.7474 (0.7539)	loss 2.6625 (2.8031)	grad_norm 2.1480 (2.3397)	mem 23874MB
[2022-11-13 20:27:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [224/300][1250/1251]	eta 0:00:00 lr 0.000155	time 0.7287 (0.7536)	loss 3.0273 (2.8060)	grad_norm 2.1999 (2.3357)	mem 23874MB
[2022-11-13 20:27:49 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 224 training takes 0:15:42
[2022-11-13 20:27:49 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_224.pth saving......
[2022-11-13 20:27:50 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_224.pth saved !!!
[2022-11-13 20:27:52 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.976 (1.976)	Loss 0.7170 (0.7170)	Acc@1 83.203 (83.203)	Acc@5 96.289 (96.289)	Mem 23874MB
[2022-11-13 20:28:03 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.782 Acc@5 96.270
[2022-11-13 20:28:03 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.8%
[2022-11-13 20:28:05 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.882 (1.882)	Loss 0.6756 (0.6756)	Acc@1 83.984 (83.984)	Acc@5 96.484 (96.484)	Mem 23874MB
[2022-11-13 20:28:15 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.458 Acc@5 96.686
[2022-11-13 20:28:15 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.5%
[2022-11-13 20:28:15 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.47% at 222 epoch
[2022-11-13 20:28:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][0/1251]	eta 0:51:29 lr 0.000155	time 2.4695 (2.4695)	loss 2.3396 (2.3396)	grad_norm 1.9407 (1.9407)	mem 23874MB
[2022-11-13 20:28:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][50/1251]	eta 0:15:45 lr 0.000155	time 0.7453 (0.7873)	loss 3.1601 (2.8378)	grad_norm 2.1699 (2.3845)	mem 23874MB
[2022-11-13 20:29:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][100/1251]	eta 0:14:46 lr 0.000155	time 0.7440 (0.7701)	loss 3.0834 (2.8360)	grad_norm 2.2456 (2.3852)	mem 23874MB
[2022-11-13 20:30:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][150/1251]	eta 0:14:01 lr 0.000155	time 0.7472 (0.7646)	loss 3.2570 (2.8503)	grad_norm 2.3396 (2.3744)	mem 23874MB
[2022-11-13 20:30:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][200/1251]	eta 0:13:20 lr 0.000154	time 0.7510 (0.7618)	loss 2.8026 (2.8374)	grad_norm 2.5632 (2.3618)	mem 23874MB
[2022-11-13 20:31:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][250/1251]	eta 0:12:39 lr 0.000154	time 0.7418 (0.7592)	loss 2.9314 (2.8204)	grad_norm 2.1502 (2.3353)	mem 23874MB
[2022-11-13 20:32:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][300/1251]	eta 0:12:01 lr 0.000154	time 0.8280 (0.7582)	loss 2.8228 (2.8327)	grad_norm 2.7050 (2.3336)	mem 23874MB
[2022-11-13 20:32:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][350/1251]	eta 0:11:21 lr 0.000154	time 0.7403 (0.7567)	loss 2.6262 (2.8262)	grad_norm 2.1104 (2.3206)	mem 23874MB
[2022-11-13 20:33:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][400/1251]	eta 0:10:43 lr 0.000154	time 0.7461 (0.7562)	loss 2.3841 (2.8226)	grad_norm 1.9836 (2.3366)	mem 23874MB
[2022-11-13 20:33:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][450/1251]	eta 0:10:05 lr 0.000154	time 0.7431 (0.7554)	loss 3.2644 (2.8315)	grad_norm 2.3327 (2.3339)	mem 23874MB
[2022-11-13 20:34:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][500/1251]	eta 0:09:26 lr 0.000154	time 0.7451 (0.7549)	loss 2.9817 (2.8293)	grad_norm 2.2223 (2.3389)	mem 23874MB
[2022-11-13 20:35:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][550/1251]	eta 0:08:49 lr 0.000153	time 0.7411 (0.7546)	loss 2.7023 (2.8332)	grad_norm 2.3478 (2.3312)	mem 23874MB
[2022-11-13 20:35:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][600/1251]	eta 0:08:11 lr 0.000153	time 0.8511 (0.7545)	loss 3.2750 (2.8347)	grad_norm 2.1054 (2.3399)	mem 23874MB
[2022-11-13 20:36:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][650/1251]	eta 0:07:33 lr 0.000153	time 0.7466 (0.7540)	loss 2.9390 (2.8374)	grad_norm 2.2049 (2.3319)	mem 23874MB
[2022-11-13 20:37:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][700/1251]	eta 0:06:55 lr 0.000153	time 0.8307 (0.7540)	loss 2.4356 (2.8394)	grad_norm 2.2612 (inf)	mem 23874MB
[2022-11-13 20:37:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][750/1251]	eta 0:06:17 lr 0.000153	time 0.7437 (0.7539)	loss 3.2060 (2.8356)	grad_norm 2.2057 (inf)	mem 23874MB
[2022-11-13 20:38:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][800/1251]	eta 0:05:39 lr 0.000153	time 0.7460 (0.7536)	loss 2.3537 (2.8373)	grad_norm 2.6094 (inf)	mem 23874MB
[2022-11-13 20:38:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][850/1251]	eta 0:05:02 lr 0.000153	time 0.7444 (0.7535)	loss 2.7456 (2.8442)	grad_norm 2.6459 (inf)	mem 23874MB
[2022-11-13 20:39:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][900/1251]	eta 0:04:24 lr 0.000152	time 0.7448 (0.7532)	loss 2.6449 (2.8454)	grad_norm 2.1115 (inf)	mem 23874MB
[2022-11-13 20:40:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][950/1251]	eta 0:03:46 lr 0.000152	time 0.7406 (0.7532)	loss 1.8671 (2.8410)	grad_norm 2.4282 (inf)	mem 23874MB
[2022-11-13 20:40:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][1000/1251]	eta 0:03:09 lr 0.000152	time 0.7404 (0.7532)	loss 3.4019 (2.8404)	grad_norm 2.8562 (inf)	mem 23874MB
[2022-11-13 20:41:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][1050/1251]	eta 0:02:31 lr 0.000152	time 0.7384 (0.7531)	loss 3.0524 (2.8386)	grad_norm 2.1809 (inf)	mem 23874MB
[2022-11-13 20:42:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][1100/1251]	eta 0:01:53 lr 0.000152	time 0.8243 (0.7532)	loss 3.2386 (2.8344)	grad_norm 2.3536 (inf)	mem 23874MB
[2022-11-13 20:42:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][1150/1251]	eta 0:01:16 lr 0.000152	time 0.7472 (0.7530)	loss 2.3521 (2.8344)	grad_norm 2.2884 (inf)	mem 23874MB
[2022-11-13 20:43:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][1200/1251]	eta 0:00:38 lr 0.000151	time 0.7407 (0.7529)	loss 2.9577 (2.8323)	grad_norm 2.4253 (inf)	mem 23874MB
[2022-11-13 20:43:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [225/300][1250/1251]	eta 0:00:00 lr 0.000151	time 0.7285 (0.7528)	loss 2.4932 (2.8306)	grad_norm 4.7997 (inf)	mem 23874MB
[2022-11-13 20:43:57 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 225 training takes 0:15:41
[2022-11-13 20:43:57 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_225.pth saving......
[2022-11-13 20:43:58 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_225.pth saved !!!
[2022-11-13 20:44:00 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.722 (1.722)	Loss 0.7824 (0.7824)	Acc@1 81.738 (81.738)	Acc@5 95.996 (95.996)	Mem 23874MB
[2022-11-13 20:44:11 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.322 Acc@5 96.316
[2022-11-13 20:44:11 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.3%
[2022-11-13 20:44:13 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.990 (1.990)	Loss 0.7459 (0.7459)	Acc@1 82.910 (82.910)	Acc@5 96.387 (96.387)	Mem 23874MB
[2022-11-13 20:44:24 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.506 Acc@5 96.674
[2022-11-13 20:44:24 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.5%
[2022-11-13 20:44:24 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.51% at 225 epoch
[2022-11-13 20:44:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][0/1251]	eta 0:51:46 lr 0.000151	time 2.4835 (2.4835)	loss 3.5151 (3.5151)	grad_norm 14.4542 (14.4542)	mem 23874MB
[2022-11-13 20:45:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][50/1251]	eta 0:15:45 lr 0.000151	time 0.7399 (0.7873)	loss 2.5090 (2.8148)	grad_norm 2.2456 (2.9630)	mem 23874MB
[2022-11-13 20:45:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][100/1251]	eta 0:14:48 lr 0.000151	time 0.7617 (0.7721)	loss 2.9178 (2.7819)	grad_norm 2.2691 (2.6074)	mem 23874MB
[2022-11-13 20:46:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][150/1251]	eta 0:14:02 lr 0.000151	time 0.8197 (0.7655)	loss 2.0975 (2.7939)	grad_norm 2.3956 (2.5256)	mem 23874MB
[2022-11-13 20:46:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][200/1251]	eta 0:13:20 lr 0.000151	time 0.7454 (0.7620)	loss 2.8715 (2.7946)	grad_norm 2.3194 (2.4537)	mem 23874MB
[2022-11-13 20:47:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][250/1251]	eta 0:12:40 lr 0.000151	time 0.7408 (0.7600)	loss 2.6746 (2.7898)	grad_norm 2.4489 (2.4452)	mem 23874MB
[2022-11-13 20:48:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][300/1251]	eta 0:12:01 lr 0.000150	time 0.7416 (0.7584)	loss 2.5655 (2.7930)	grad_norm 2.3436 (2.4584)	mem 23874MB
[2022-11-13 20:48:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][350/1251]	eta 0:11:22 lr 0.000150	time 0.7457 (0.7577)	loss 2.2814 (2.8043)	grad_norm 2.6132 (2.4293)	mem 23874MB
[2022-11-13 20:49:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][400/1251]	eta 0:10:44 lr 0.000150	time 0.7408 (0.7571)	loss 3.0104 (2.8040)	grad_norm 1.9682 (2.4036)	mem 23874MB
[2022-11-13 20:50:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][450/1251]	eta 0:10:05 lr 0.000150	time 0.7498 (0.7565)	loss 2.9448 (2.8052)	grad_norm 2.3432 (2.3930)	mem 23874MB
[2022-11-13 20:50:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][500/1251]	eta 0:09:27 lr 0.000150	time 0.7415 (0.7560)	loss 3.0840 (2.8053)	grad_norm 2.0763 (2.3857)	mem 23874MB
[2022-11-13 20:51:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][550/1251]	eta 0:08:49 lr 0.000150	time 0.7462 (0.7555)	loss 2.2153 (2.8006)	grad_norm 2.1412 (2.3834)	mem 23874MB
[2022-11-13 20:51:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][600/1251]	eta 0:08:11 lr 0.000150	time 0.7409 (0.7555)	loss 2.7454 (2.8028)	grad_norm 2.1294 (2.3893)	mem 23874MB
[2022-11-13 20:52:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][650/1251]	eta 0:07:33 lr 0.000149	time 0.7460 (0.7552)	loss 3.0887 (2.8025)	grad_norm 2.3491 (2.3779)	mem 23874MB
[2022-11-13 20:53:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][700/1251]	eta 0:06:55 lr 0.000149	time 0.7439 (0.7548)	loss 2.9347 (2.8014)	grad_norm 1.9754 (2.3779)	mem 23874MB
[2022-11-13 20:53:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][750/1251]	eta 0:06:18 lr 0.000149	time 0.8155 (0.7546)	loss 1.8779 (2.8041)	grad_norm 2.0982 (2.3727)	mem 23874MB
[2022-11-13 20:54:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][800/1251]	eta 0:05:40 lr 0.000149	time 0.7375 (0.7544)	loss 2.9253 (2.8032)	grad_norm 2.4383 (2.3779)	mem 23874MB
[2022-11-13 20:55:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][850/1251]	eta 0:05:02 lr 0.000149	time 0.7485 (0.7544)	loss 2.8744 (2.8022)	grad_norm 2.0777 (2.3779)	mem 23874MB
[2022-11-13 20:55:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][900/1251]	eta 0:04:24 lr 0.000149	time 0.7439 (0.7541)	loss 2.8768 (2.7989)	grad_norm 2.4989 (2.3728)	mem 23874MB
[2022-11-13 20:56:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][950/1251]	eta 0:03:47 lr 0.000149	time 0.7427 (0.7542)	loss 3.0509 (2.7980)	grad_norm 2.1194 (2.3670)	mem 23874MB
[2022-11-13 20:56:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][1000/1251]	eta 0:03:09 lr 0.000148	time 0.8163 (0.7542)	loss 2.9924 (2.8004)	grad_norm 2.1303 (2.3648)	mem 23874MB
[2022-11-13 20:57:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][1050/1251]	eta 0:02:31 lr 0.000148	time 0.7409 (0.7540)	loss 3.0841 (2.8006)	grad_norm 2.6028 (2.3660)	mem 23874MB
[2022-11-13 20:58:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][1100/1251]	eta 0:01:53 lr 0.000148	time 0.7473 (0.7540)	loss 3.1991 (2.8075)	grad_norm 3.9772 (2.3681)	mem 23874MB
[2022-11-13 20:58:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][1150/1251]	eta 0:01:16 lr 0.000148	time 0.7441 (0.7539)	loss 2.5477 (2.8072)	grad_norm 3.3874 (2.3661)	mem 23874MB
[2022-11-13 20:59:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][1200/1251]	eta 0:00:38 lr 0.000148	time 0.7396 (0.7539)	loss 3.0676 (2.8100)	grad_norm 2.0083 (2.3681)	mem 23874MB
[2022-11-13 21:00:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [226/300][1250/1251]	eta 0:00:00 lr 0.000148	time 0.7272 (0.7537)	loss 3.0570 (2.8088)	grad_norm 2.1944 (2.3647)	mem 23874MB
[2022-11-13 21:00:07 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 226 training takes 0:15:43
[2022-11-13 21:00:07 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_226.pth saving......
[2022-11-13 21:00:08 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_226.pth saved !!!
[2022-11-13 21:00:10 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.808 (1.808)	Loss 0.7618 (0.7618)	Acc@1 82.617 (82.617)	Acc@5 95.703 (95.703)	Mem 23874MB
[2022-11-13 21:00:21 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.536 Acc@5 96.286
[2022-11-13 21:00:21 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.5%
[2022-11-13 21:00:22 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.959 (1.959)	Loss 0.5882 (0.5882)	Acc@1 86.035 (86.035)	Acc@5 97.266 (97.266)	Mem 23874MB
[2022-11-13 21:00:33 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.518 Acc@5 96.684
[2022-11-13 21:00:33 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.5%
[2022-11-13 21:00:33 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.52% at 226 epoch
[2022-11-13 21:00:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][0/1251]	eta 0:53:02 lr 0.000148	time 2.5440 (2.5440)	loss 1.8323 (1.8323)	grad_norm 2.2788 (2.2788)	mem 23874MB
[2022-11-13 21:01:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][50/1251]	eta 0:15:50 lr 0.000148	time 0.8230 (0.7916)	loss 3.1566 (2.7980)	grad_norm 2.0910 (2.2937)	mem 23874MB
[2022-11-13 21:01:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][100/1251]	eta 0:14:46 lr 0.000147	time 0.7473 (0.7705)	loss 2.7767 (2.8322)	grad_norm 1.8961 (2.3619)	mem 23874MB
[2022-11-13 21:02:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][150/1251]	eta 0:14:02 lr 0.000147	time 0.7418 (0.7648)	loss 2.2803 (2.7802)	grad_norm 2.1428 (2.3650)	mem 23874MB
[2022-11-13 21:03:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][200/1251]	eta 0:13:20 lr 0.000147	time 0.7504 (0.7617)	loss 3.0354 (2.7869)	grad_norm 2.0212 (2.3578)	mem 23874MB
[2022-11-13 21:03:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][250/1251]	eta 0:12:39 lr 0.000147	time 0.7425 (0.7591)	loss 2.9551 (2.7840)	grad_norm 2.2580 (2.3508)	mem 23874MB
[2022-11-13 21:04:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][300/1251]	eta 0:12:00 lr 0.000147	time 0.7443 (0.7577)	loss 2.5873 (2.7934)	grad_norm 2.0853 (2.3468)	mem 23874MB
[2022-11-13 21:04:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][350/1251]	eta 0:11:21 lr 0.000147	time 0.7419 (0.7565)	loss 1.9782 (2.7927)	grad_norm 2.3420 (2.3414)	mem 23874MB
[2022-11-13 21:05:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][400/1251]	eta 0:10:43 lr 0.000147	time 0.7404 (0.7561)	loss 3.1139 (2.7950)	grad_norm 2.0265 (2.3335)	mem 23874MB
[2022-11-13 21:06:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][450/1251]	eta 0:10:05 lr 0.000146	time 0.7444 (0.7554)	loss 2.0589 (2.7912)	grad_norm 2.2088 (2.3375)	mem 23874MB
[2022-11-13 21:06:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][500/1251]	eta 0:09:27 lr 0.000146	time 0.7436 (0.7552)	loss 2.8223 (2.7904)	grad_norm 2.1709 (2.3533)	mem 23874MB
[2022-11-13 21:07:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][550/1251]	eta 0:08:49 lr 0.000146	time 0.7418 (0.7547)	loss 2.5164 (2.8012)	grad_norm 2.3651 (2.3552)	mem 23874MB
[2022-11-13 21:08:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][600/1251]	eta 0:08:11 lr 0.000146	time 0.8164 (0.7546)	loss 3.3849 (2.8099)	grad_norm 2.9846 (2.3587)	mem 23874MB
[2022-11-13 21:08:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][650/1251]	eta 0:07:33 lr 0.000146	time 0.7422 (0.7543)	loss 2.8961 (2.8091)	grad_norm 2.0556 (2.3575)	mem 23874MB
[2022-11-13 21:09:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][700/1251]	eta 0:06:55 lr 0.000146	time 0.7417 (0.7541)	loss 3.2677 (2.8071)	grad_norm 2.2949 (2.3485)	mem 23874MB
[2022-11-13 21:09:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][750/1251]	eta 0:06:17 lr 0.000146	time 0.8417 (0.7538)	loss 2.9936 (2.8082)	grad_norm 2.6529 (2.3532)	mem 23874MB
[2022-11-13 21:10:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][800/1251]	eta 0:05:39 lr 0.000145	time 0.7499 (0.7536)	loss 2.9849 (2.8083)	grad_norm 2.1370 (2.3491)	mem 23874MB
[2022-11-13 21:11:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][850/1251]	eta 0:05:02 lr 0.000145	time 0.7506 (0.7534)	loss 2.2137 (2.8040)	grad_norm 2.1282 (2.3482)	mem 23874MB
[2022-11-13 21:11:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][900/1251]	eta 0:04:24 lr 0.000145	time 0.7421 (0.7533)	loss 2.8289 (2.8085)	grad_norm 2.2138 (2.3505)	mem 23874MB
[2022-11-13 21:12:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][950/1251]	eta 0:03:46 lr 0.000145	time 0.8289 (0.7531)	loss 3.2010 (2.8124)	grad_norm 2.2618 (2.3546)	mem 23874MB
[2022-11-13 21:13:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][1000/1251]	eta 0:03:09 lr 0.000145	time 0.7420 (0.7530)	loss 2.1136 (2.8078)	grad_norm 2.3052 (inf)	mem 23874MB
[2022-11-13 21:13:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][1050/1251]	eta 0:02:31 lr 0.000145	time 0.7473 (0.7528)	loss 3.0889 (2.8106)	grad_norm 2.5484 (inf)	mem 23874MB
[2022-11-13 21:14:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][1100/1251]	eta 0:01:53 lr 0.000145	time 0.7404 (0.7527)	loss 3.2359 (2.8098)	grad_norm 2.3382 (inf)	mem 23874MB
[2022-11-13 21:15:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][1150/1251]	eta 0:01:16 lr 0.000144	time 0.7449 (0.7527)	loss 2.7842 (2.8063)	grad_norm 2.2706 (inf)	mem 23874MB
[2022-11-13 21:15:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][1200/1251]	eta 0:00:38 lr 0.000144	time 0.7427 (0.7527)	loss 2.0069 (2.8097)	grad_norm 1.7915 (inf)	mem 23874MB
[2022-11-13 21:16:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [227/300][1250/1251]	eta 0:00:00 lr 0.000144	time 0.7318 (0.7525)	loss 2.5972 (2.8091)	grad_norm 1.8502 (inf)	mem 23874MB
[2022-11-13 21:16:15 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 227 training takes 0:15:41
[2022-11-13 21:16:15 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_227.pth saving......
[2022-11-13 21:16:16 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_227.pth saved !!!
[2022-11-13 21:16:18 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.666 (1.666)	Loss 0.7298 (0.7298)	Acc@1 83.301 (83.301)	Acc@5 96.875 (96.875)	Mem 23874MB
[2022-11-13 21:16:29 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.506 Acc@5 96.322
[2022-11-13 21:16:29 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.5%
[2022-11-13 21:16:31 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.088 (2.088)	Loss 0.6801 (0.6801)	Acc@1 83.496 (83.496)	Acc@5 97.461 (97.461)	Mem 23874MB
[2022-11-13 21:16:41 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.552 Acc@5 96.698
[2022-11-13 21:16:41 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.6%
[2022-11-13 21:16:41 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.55% at 227 epoch
[2022-11-13 21:16:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][0/1251]	eta 0:54:14 lr 0.000144	time 2.6014 (2.6014)	loss 3.1172 (3.1172)	grad_norm 2.5822 (2.5822)	mem 23874MB
[2022-11-13 21:17:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][50/1251]	eta 0:15:55 lr 0.000144	time 0.7471 (0.7957)	loss 3.3769 (2.8160)	grad_norm 2.1112 (2.3461)	mem 23874MB
[2022-11-13 21:18:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][100/1251]	eta 0:14:50 lr 0.000144	time 0.7487 (0.7735)	loss 2.9142 (2.8133)	grad_norm 4.4852 (2.4087)	mem 23874MB
[2022-11-13 21:18:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][150/1251]	eta 0:14:03 lr 0.000144	time 0.7432 (0.7657)	loss 2.8371 (2.8408)	grad_norm 2.2317 (2.3972)	mem 23874MB
[2022-11-13 21:19:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][200/1251]	eta 0:13:21 lr 0.000144	time 0.7389 (0.7622)	loss 2.1113 (2.8313)	grad_norm 1.9646 (2.3888)	mem 23874MB
[2022-11-13 21:19:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][250/1251]	eta 0:12:40 lr 0.000143	time 0.7384 (0.7598)	loss 2.4956 (2.8337)	grad_norm 1.9215 (2.3935)	mem 23874MB
[2022-11-13 21:20:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][300/1251]	eta 0:12:00 lr 0.000143	time 0.7403 (0.7580)	loss 2.8405 (2.8290)	grad_norm 2.0047 (2.4023)	mem 23874MB
[2022-11-13 21:21:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][350/1251]	eta 0:11:21 lr 0.000143	time 0.7414 (0.7566)	loss 2.1022 (2.8252)	grad_norm 2.1734 (2.4132)	mem 23874MB
[2022-11-13 21:21:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][400/1251]	eta 0:10:43 lr 0.000143	time 0.7384 (0.7558)	loss 2.8801 (2.8186)	grad_norm 1.9610 (2.4166)	mem 23874MB
[2022-11-13 21:22:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][450/1251]	eta 0:10:05 lr 0.000143	time 0.7406 (0.7555)	loss 3.0290 (2.8205)	grad_norm 2.0744 (2.4232)	mem 23874MB
[2022-11-13 21:23:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][500/1251]	eta 0:09:26 lr 0.000143	time 0.7415 (0.7547)	loss 1.8908 (2.8111)	grad_norm 1.8151 (2.4151)	mem 23874MB
[2022-11-13 21:23:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][550/1251]	eta 0:08:48 lr 0.000143	time 0.7431 (0.7543)	loss 3.0486 (2.8008)	grad_norm 2.4136 (2.4028)	mem 23874MB
[2022-11-13 21:24:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][600/1251]	eta 0:08:10 lr 0.000142	time 0.7394 (0.7539)	loss 2.6716 (2.8069)	grad_norm 2.3205 (2.3975)	mem 23874MB
[2022-11-13 21:24:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][650/1251]	eta 0:07:32 lr 0.000142	time 0.7385 (0.7535)	loss 3.1971 (2.8077)	grad_norm 2.4037 (2.3943)	mem 23874MB
[2022-11-13 21:25:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][700/1251]	eta 0:06:54 lr 0.000142	time 0.7407 (0.7531)	loss 3.1687 (2.8038)	grad_norm 1.8247 (2.3911)	mem 23874MB
[2022-11-13 21:26:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][750/1251]	eta 0:06:17 lr 0.000142	time 0.7410 (0.7529)	loss 3.0968 (2.8126)	grad_norm 1.9094 (2.3817)	mem 23874MB
[2022-11-13 21:26:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][800/1251]	eta 0:05:39 lr 0.000142	time 0.7570 (0.7526)	loss 3.1165 (2.8204)	grad_norm 3.4045 (2.3805)	mem 23874MB
[2022-11-13 21:27:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][850/1251]	eta 0:05:01 lr 0.000142	time 0.7337 (0.7525)	loss 2.6329 (2.8255)	grad_norm 2.0175 (2.3876)	mem 23874MB
[2022-11-13 21:27:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][900/1251]	eta 0:04:24 lr 0.000142	time 0.7480 (0.7523)	loss 1.7769 (2.8241)	grad_norm 2.1870 (2.3859)	mem 23874MB
[2022-11-13 21:28:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][950/1251]	eta 0:03:46 lr 0.000141	time 0.7276 (0.7523)	loss 2.8973 (2.8198)	grad_norm 2.0993 (2.3829)	mem 23874MB
[2022-11-13 21:29:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][1000/1251]	eta 0:03:08 lr 0.000141	time 0.7488 (0.7523)	loss 3.1321 (2.8189)	grad_norm 2.6362 (2.3843)	mem 23874MB
[2022-11-13 21:29:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][1050/1251]	eta 0:02:31 lr 0.000141	time 0.7345 (0.7522)	loss 2.2802 (2.8249)	grad_norm 2.0805 (2.3860)	mem 23874MB
[2022-11-13 21:30:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][1100/1251]	eta 0:01:53 lr 0.000141	time 0.7460 (0.7522)	loss 3.0410 (2.8303)	grad_norm 2.2636 (2.3943)	mem 23874MB
[2022-11-13 21:31:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][1150/1251]	eta 0:01:15 lr 0.000141	time 0.7376 (0.7519)	loss 3.2601 (2.8314)	grad_norm 2.4803 (2.3932)	mem 23874MB
[2022-11-13 21:31:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][1200/1251]	eta 0:00:38 lr 0.000141	time 0.7438 (0.7518)	loss 3.1574 (2.8272)	grad_norm 1.9797 (2.3887)	mem 23874MB
[2022-11-13 21:32:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [228/300][1250/1251]	eta 0:00:00 lr 0.000141	time 0.7300 (0.7516)	loss 2.2735 (2.8293)	grad_norm 2.2825 (2.3980)	mem 23874MB
[2022-11-13 21:32:22 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 228 training takes 0:15:40
[2022-11-13 21:32:22 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_228.pth saving......
[2022-11-13 21:32:23 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_228.pth saved !!!
[2022-11-13 21:32:25 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.657 (1.657)	Loss 0.7614 (0.7614)	Acc@1 82.129 (82.129)	Acc@5 95.605 (95.605)	Mem 23874MB
[2022-11-13 21:32:35 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.670 Acc@5 96.318
[2022-11-13 21:32:35 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.7%
[2022-11-13 21:32:37 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.878 (1.878)	Loss 0.6660 (0.6660)	Acc@1 83.984 (83.984)	Acc@5 96.680 (96.680)	Mem 23874MB
[2022-11-13 21:32:48 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.562 Acc@5 96.672
[2022-11-13 21:32:48 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.6%
[2022-11-13 21:32:48 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.56% at 228 epoch
[2022-11-13 21:32:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][0/1251]	eta 0:55:58 lr 0.000141	time 2.6847 (2.6847)	loss 3.4058 (3.4058)	grad_norm 2.4835 (2.4835)	mem 23874MB
[2022-11-13 21:33:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][50/1251]	eta 0:15:47 lr 0.000140	time 0.7439 (0.7886)	loss 2.0818 (2.7067)	grad_norm 3.4765 (2.3719)	mem 23874MB
[2022-11-13 21:34:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][100/1251]	eta 0:14:47 lr 0.000140	time 0.7451 (0.7707)	loss 3.1589 (2.7821)	grad_norm 2.3734 (2.3609)	mem 23874MB
[2022-11-13 21:34:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][150/1251]	eta 0:14:00 lr 0.000140	time 0.7471 (0.7638)	loss 2.9520 (2.8090)	grad_norm 2.1679 (2.3358)	mem 23874MB
[2022-11-13 21:35:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][200/1251]	eta 0:13:19 lr 0.000140	time 0.7449 (0.7610)	loss 3.0639 (2.8163)	grad_norm 2.5586 (2.3627)	mem 23874MB
[2022-11-13 21:35:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][250/1251]	eta 0:12:40 lr 0.000140	time 0.7390 (0.7597)	loss 3.1082 (2.8294)	grad_norm 2.0820 (2.3701)	mem 23874MB
[2022-11-13 21:36:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][300/1251]	eta 0:12:00 lr 0.000140	time 0.7513 (0.7577)	loss 3.1907 (2.8150)	grad_norm 2.3626 (2.3567)	mem 23874MB
[2022-11-13 21:37:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][350/1251]	eta 0:11:22 lr 0.000140	time 0.7471 (0.7571)	loss 3.4470 (2.8147)	grad_norm 2.2729 (2.4001)	mem 23874MB
[2022-11-13 21:37:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][400/1251]	eta 0:10:43 lr 0.000140	time 0.7410 (0.7564)	loss 3.1437 (2.8128)	grad_norm 2.3679 (2.4094)	mem 23874MB
[2022-11-13 21:38:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][450/1251]	eta 0:10:05 lr 0.000139	time 0.7454 (0.7560)	loss 3.0465 (2.8174)	grad_norm 2.7116 (2.4218)	mem 23874MB
[2022-11-13 21:39:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][500/1251]	eta 0:09:27 lr 0.000139	time 0.7451 (0.7555)	loss 2.8885 (2.8187)	grad_norm 2.3159 (2.4142)	mem 23874MB
[2022-11-13 21:39:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][550/1251]	eta 0:08:49 lr 0.000139	time 0.7397 (0.7549)	loss 2.4695 (2.8206)	grad_norm 2.5595 (2.4229)	mem 23874MB
[2022-11-13 21:40:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][600/1251]	eta 0:08:11 lr 0.000139	time 0.7461 (0.7548)	loss 3.0098 (2.8182)	grad_norm 2.4859 (2.4164)	mem 23874MB
[2022-11-13 21:41:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][650/1251]	eta 0:07:33 lr 0.000139	time 0.7424 (0.7546)	loss 2.9984 (2.8174)	grad_norm 2.3189 (2.4180)	mem 23874MB
[2022-11-13 21:41:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][700/1251]	eta 0:06:55 lr 0.000139	time 0.7433 (0.7539)	loss 2.3720 (2.8168)	grad_norm 2.0657 (2.4152)	mem 23874MB
[2022-11-13 21:42:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][750/1251]	eta 0:06:17 lr 0.000139	time 0.8434 (0.7538)	loss 2.4859 (2.8105)	grad_norm 2.4428 (2.4234)	mem 23874MB
[2022-11-13 21:42:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][800/1251]	eta 0:05:39 lr 0.000138	time 0.7416 (0.7536)	loss 2.9683 (2.8102)	grad_norm 2.6902 (2.4192)	mem 23874MB
[2022-11-13 21:43:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][850/1251]	eta 0:05:02 lr 0.000138	time 0.7404 (0.7535)	loss 2.4446 (2.8147)	grad_norm 1.9060 (2.4151)	mem 23874MB
[2022-11-13 21:44:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][900/1251]	eta 0:04:24 lr 0.000138	time 0.7442 (0.7533)	loss 2.9409 (2.8188)	grad_norm 2.0838 (2.4103)	mem 23874MB
[2022-11-13 21:44:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][950/1251]	eta 0:03:46 lr 0.000138	time 0.7427 (0.7532)	loss 2.7424 (2.8208)	grad_norm 2.4967 (2.4197)	mem 23874MB
[2022-11-13 21:45:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][1000/1251]	eta 0:03:09 lr 0.000138	time 0.8156 (0.7531)	loss 2.4464 (2.8182)	grad_norm 2.0235 (2.4152)	mem 23874MB
[2022-11-13 21:46:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][1050/1251]	eta 0:02:31 lr 0.000138	time 0.7437 (0.7531)	loss 1.8276 (2.8196)	grad_norm 1.8004 (2.4205)	mem 23874MB
[2022-11-13 21:46:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][1100/1251]	eta 0:01:53 lr 0.000138	time 0.7378 (0.7529)	loss 2.5057 (2.8212)	grad_norm 2.3426 (2.4233)	mem 23874MB
[2022-11-13 21:47:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][1150/1251]	eta 0:01:16 lr 0.000137	time 0.7429 (0.7528)	loss 2.6018 (2.8220)	grad_norm 2.2351 (inf)	mem 23874MB
[2022-11-13 21:47:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][1200/1251]	eta 0:00:38 lr 0.000137	time 0.7292 (0.7528)	loss 3.2871 (2.8213)	grad_norm 3.2116 (inf)	mem 23874MB
[2022-11-13 21:48:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [229/300][1250/1251]	eta 0:00:00 lr 0.000137	time 0.7304 (0.7525)	loss 2.9815 (2.8175)	grad_norm 2.2461 (inf)	mem 23874MB
[2022-11-13 21:48:30 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 229 training takes 0:15:41
[2022-11-13 21:48:30 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_229.pth saving......
[2022-11-13 21:48:31 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_229.pth saved !!!
[2022-11-13 21:48:33 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.699 (1.699)	Loss 0.6530 (0.6530)	Acc@1 84.766 (84.766)	Acc@5 97.168 (97.168)	Mem 23874MB
[2022-11-13 21:48:43 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.644 Acc@5 96.350
[2022-11-13 21:48:43 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.6%
[2022-11-13 21:48:45 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.951 (1.951)	Loss 0.6360 (0.6360)	Acc@1 83.984 (83.984)	Acc@5 97.266 (97.266)	Mem 23874MB
[2022-11-13 21:48:56 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.576 Acc@5 96.692
[2022-11-13 21:48:56 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.6%
[2022-11-13 21:48:56 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.58% at 229 epoch
[2022-11-13 21:48:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][0/1251]	eta 0:53:06 lr 0.000137	time 2.5471 (2.5471)	loss 2.9853 (2.9853)	grad_norm 2.4586 (2.4586)	mem 23874MB
[2022-11-13 21:49:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][50/1251]	eta 0:15:42 lr 0.000137	time 0.8174 (0.7851)	loss 3.1992 (2.8216)	grad_norm 2.3416 (2.3757)	mem 23874MB
[2022-11-13 21:50:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][100/1251]	eta 0:14:43 lr 0.000137	time 0.7426 (0.7674)	loss 3.2796 (2.8063)	grad_norm 2.9558 (2.4594)	mem 23874MB
[2022-11-13 21:50:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][150/1251]	eta 0:13:59 lr 0.000137	time 0.7562 (0.7622)	loss 1.9184 (2.8090)	grad_norm 2.1973 (2.4811)	mem 23874MB
[2022-11-13 21:51:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][200/1251]	eta 0:13:17 lr 0.000137	time 0.7404 (0.7585)	loss 3.3001 (2.8092)	grad_norm 2.2913 (2.4650)	mem 23874MB
[2022-11-13 21:52:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][250/1251]	eta 0:12:37 lr 0.000136	time 0.7298 (0.7572)	loss 3.0616 (2.8130)	grad_norm 2.6151 (2.5276)	mem 23874MB
[2022-11-13 21:52:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][300/1251]	eta 0:11:58 lr 0.000136	time 0.8170 (0.7558)	loss 3.2921 (2.8204)	grad_norm 2.3036 (2.4979)	mem 23874MB
[2022-11-13 21:53:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][350/1251]	eta 0:11:20 lr 0.000136	time 0.7438 (0.7549)	loss 2.0466 (2.8291)	grad_norm 2.7731 (2.4898)	mem 23874MB
[2022-11-13 21:53:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][400/1251]	eta 0:10:41 lr 0.000136	time 0.7445 (0.7542)	loss 2.9096 (2.8146)	grad_norm 2.4011 (2.4769)	mem 23874MB
[2022-11-13 21:54:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][450/1251]	eta 0:10:03 lr 0.000136	time 0.7512 (0.7537)	loss 2.6181 (2.8051)	grad_norm 2.3359 (2.4777)	mem 23874MB
[2022-11-13 21:55:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][500/1251]	eta 0:09:25 lr 0.000136	time 0.7425 (0.7531)	loss 3.0760 (2.8056)	grad_norm 1.8904 (2.4766)	mem 23874MB
[2022-11-13 21:55:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][550/1251]	eta 0:08:47 lr 0.000136	time 0.7417 (0.7531)	loss 2.8142 (2.8080)	grad_norm 2.1167 (2.4791)	mem 23874MB
[2022-11-13 21:56:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][600/1251]	eta 0:08:09 lr 0.000135	time 0.7422 (0.7526)	loss 2.9222 (2.8124)	grad_norm 2.2401 (2.4842)	mem 23874MB
[2022-11-13 21:57:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][650/1251]	eta 0:07:32 lr 0.000135	time 0.7375 (0.7525)	loss 2.9835 (2.8069)	grad_norm 3.7505 (2.4780)	mem 23874MB
[2022-11-13 21:57:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][700/1251]	eta 0:06:54 lr 0.000135	time 0.8386 (0.7524)	loss 3.2607 (2.8063)	grad_norm 2.8942 (2.4721)	mem 23874MB
[2022-11-13 21:58:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][750/1251]	eta 0:06:16 lr 0.000135	time 0.7450 (0.7520)	loss 2.4074 (2.8087)	grad_norm 2.0103 (2.4773)	mem 23874MB
[2022-11-13 21:58:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][800/1251]	eta 0:05:39 lr 0.000135	time 0.7499 (0.7520)	loss 3.0980 (2.8165)	grad_norm 2.4971 (2.4874)	mem 23874MB
[2022-11-13 21:59:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][850/1251]	eta 0:05:01 lr 0.000135	time 0.7479 (0.7519)	loss 3.0127 (2.8146)	grad_norm 2.2723 (2.4797)	mem 23874MB
[2022-11-13 22:00:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][900/1251]	eta 0:04:23 lr 0.000135	time 0.7392 (0.7518)	loss 3.0739 (2.8105)	grad_norm 2.2776 (2.4739)	mem 23874MB
[2022-11-13 22:00:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][950/1251]	eta 0:03:46 lr 0.000135	time 0.7468 (0.7518)	loss 3.0012 (2.8115)	grad_norm 2.2654 (2.4736)	mem 23874MB
[2022-11-13 22:01:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][1000/1251]	eta 0:03:08 lr 0.000134	time 0.7402 (0.7515)	loss 3.0837 (2.8156)	grad_norm 2.0334 (2.4663)	mem 23874MB
[2022-11-13 22:02:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][1050/1251]	eta 0:02:31 lr 0.000134	time 0.7431 (0.7516)	loss 2.2079 (2.8170)	grad_norm 2.2214 (2.4598)	mem 23874MB
[2022-11-13 22:02:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][1100/1251]	eta 0:01:53 lr 0.000134	time 0.8263 (0.7515)	loss 3.0888 (2.8188)	grad_norm 1.9697 (2.4570)	mem 23874MB
[2022-11-13 22:03:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][1150/1251]	eta 0:01:15 lr 0.000134	time 0.7425 (0.7515)	loss 1.8761 (2.8140)	grad_norm 2.6396 (2.4576)	mem 23874MB
[2022-11-13 22:03:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][1200/1251]	eta 0:00:38 lr 0.000134	time 0.7515 (0.7515)	loss 3.0319 (2.8101)	grad_norm 2.0411 (2.4557)	mem 23874MB
[2022-11-13 22:04:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [230/300][1250/1251]	eta 0:00:00 lr 0.000134	time 0.7304 (0.7512)	loss 3.2221 (2.8089)	grad_norm 2.1052 (2.4534)	mem 23874MB
[2022-11-13 22:04:36 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 230 training takes 0:15:39
[2022-11-13 22:04:36 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_230.pth saving......
[2022-11-13 22:04:37 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_230.pth saved !!!
[2022-11-13 22:04:39 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.759 (1.759)	Loss 0.7457 (0.7457)	Acc@1 82.324 (82.324)	Acc@5 96.582 (96.582)	Mem 23874MB
[2022-11-13 22:04:50 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.486 Acc@5 96.180
[2022-11-13 22:04:50 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.5%
[2022-11-13 22:04:52 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.016 (2.016)	Loss 0.6962 (0.6962)	Acc@1 83.984 (83.984)	Acc@5 96.777 (96.777)	Mem 23874MB
[2022-11-13 22:05:03 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.614 Acc@5 96.700
[2022-11-13 22:05:03 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.6%
[2022-11-13 22:05:03 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.61% at 230 epoch
[2022-11-13 22:05:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][0/1251]	eta 0:52:35 lr 0.000134	time 2.5224 (2.5224)	loss 3.1115 (3.1115)	grad_norm 2.1253 (2.1253)	mem 23874MB
[2022-11-13 22:05:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][50/1251]	eta 0:15:43 lr 0.000134	time 0.7423 (0.7857)	loss 2.5660 (2.7914)	grad_norm 2.4065 (2.3962)	mem 23874MB
[2022-11-13 22:06:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][100/1251]	eta 0:14:45 lr 0.000133	time 0.7427 (0.7691)	loss 3.3052 (2.8473)	grad_norm 3.1451 (2.3924)	mem 23874MB
[2022-11-13 22:06:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][150/1251]	eta 0:13:59 lr 0.000133	time 0.7475 (0.7627)	loss 3.2209 (2.8014)	grad_norm 2.7905 (2.4163)	mem 23874MB
[2022-11-13 22:07:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][200/1251]	eta 0:13:18 lr 0.000133	time 0.8343 (0.7601)	loss 3.0586 (2.7914)	grad_norm 2.7638 (2.4461)	mem 23874MB
[2022-11-13 22:08:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][250/1251]	eta 0:12:38 lr 0.000133	time 0.7399 (0.7580)	loss 2.2657 (2.7792)	grad_norm 2.5137 (2.4602)	mem 23874MB
[2022-11-13 22:08:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][300/1251]	eta 0:11:59 lr 0.000133	time 0.7414 (0.7569)	loss 2.7863 (2.7834)	grad_norm 3.2807 (2.4701)	mem 23874MB
[2022-11-13 22:09:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][350/1251]	eta 0:11:20 lr 0.000133	time 0.7431 (0.7556)	loss 2.0036 (2.7795)	grad_norm 2.1687 (2.4521)	mem 23874MB
[2022-11-13 22:10:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][400/1251]	eta 0:10:42 lr 0.000133	time 0.7439 (0.7550)	loss 2.2597 (2.7745)	grad_norm 2.2157 (2.4530)	mem 23874MB
[2022-11-13 22:10:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][450/1251]	eta 0:10:04 lr 0.000132	time 0.7410 (0.7546)	loss 2.6178 (2.7587)	grad_norm 3.0919 (2.4504)	mem 23874MB
[2022-11-13 22:11:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][500/1251]	eta 0:09:26 lr 0.000132	time 0.7432 (0.7543)	loss 2.8791 (2.7626)	grad_norm 2.1937 (2.4452)	mem 23874MB
[2022-11-13 22:11:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][550/1251]	eta 0:08:48 lr 0.000132	time 0.7413 (0.7538)	loss 2.9155 (2.7768)	grad_norm 2.0995 (2.4400)	mem 23874MB
[2022-11-13 22:12:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][600/1251]	eta 0:08:10 lr 0.000132	time 0.8355 (0.7536)	loss 3.1350 (2.7798)	grad_norm 2.6545 (2.4436)	mem 23874MB
[2022-11-13 22:13:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][650/1251]	eta 0:07:32 lr 0.000132	time 0.7356 (0.7534)	loss 2.5703 (2.7796)	grad_norm 2.0068 (2.4392)	mem 23874MB
[2022-11-13 22:13:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][700/1251]	eta 0:06:55 lr 0.000132	time 0.7376 (0.7532)	loss 2.5752 (2.7795)	grad_norm 3.3956 (2.4395)	mem 23874MB
[2022-11-13 22:14:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][750/1251]	eta 0:06:17 lr 0.000132	time 0.7417 (0.7529)	loss 2.8398 (2.7836)	grad_norm 3.0491 (2.4503)	mem 23874MB
[2022-11-13 22:15:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][800/1251]	eta 0:05:39 lr 0.000132	time 0.7564 (0.7528)	loss 2.6674 (2.7832)	grad_norm 2.4465 (2.4692)	mem 23874MB
[2022-11-13 22:15:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][850/1251]	eta 0:05:01 lr 0.000131	time 0.7424 (0.7528)	loss 3.2608 (2.7775)	grad_norm 2.7308 (2.4683)	mem 23874MB
[2022-11-13 22:16:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][900/1251]	eta 0:04:24 lr 0.000131	time 0.7402 (0.7526)	loss 2.3764 (2.7773)	grad_norm 2.2279 (2.4678)	mem 23874MB
[2022-11-13 22:16:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][950/1251]	eta 0:03:46 lr 0.000131	time 0.7404 (0.7525)	loss 2.6617 (2.7877)	grad_norm 2.5024 (2.4745)	mem 23874MB
[2022-11-13 22:17:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][1000/1251]	eta 0:03:08 lr 0.000131	time 0.8450 (0.7524)	loss 3.0847 (2.7898)	grad_norm 2.4352 (2.4766)	mem 23874MB
[2022-11-13 22:18:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][1050/1251]	eta 0:02:31 lr 0.000131	time 0.7436 (0.7524)	loss 2.9238 (2.7865)	grad_norm 2.2702 (2.4702)	mem 23874MB
[2022-11-13 22:18:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][1100/1251]	eta 0:01:53 lr 0.000131	time 0.7490 (0.7523)	loss 2.6141 (2.7919)	grad_norm 2.2424 (2.4666)	mem 23874MB
[2022-11-13 22:19:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][1150/1251]	eta 0:01:15 lr 0.000131	time 0.7425 (0.7522)	loss 2.7752 (2.7923)	grad_norm 2.3531 (2.4614)	mem 23874MB
[2022-11-13 22:20:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][1200/1251]	eta 0:00:38 lr 0.000130	time 0.7415 (0.7522)	loss 2.2551 (2.7958)	grad_norm 2.1287 (2.4600)	mem 23874MB
[2022-11-13 22:20:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [231/300][1250/1251]	eta 0:00:00 lr 0.000130	time 0.7302 (0.7521)	loss 2.0404 (2.7915)	grad_norm 2.2853 (2.4556)	mem 23874MB
[2022-11-13 22:20:44 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 231 training takes 0:15:41
[2022-11-13 22:20:44 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_231.pth saving......
[2022-11-13 22:20:45 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_231.pth saved !!!
[2022-11-13 22:20:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.711 (1.711)	Loss 0.8000 (0.8000)	Acc@1 81.348 (81.348)	Acc@5 95.410 (95.410)	Mem 23874MB
[2022-11-13 22:20:58 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.736 Acc@5 96.348
[2022-11-13 22:20:58 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.7%
[2022-11-13 22:20:59 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.838 (1.838)	Loss 0.6254 (0.6254)	Acc@1 85.449 (85.449)	Acc@5 97.266 (97.266)	Mem 23874MB
[2022-11-13 22:21:10 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.600 Acc@5 96.712
[2022-11-13 22:21:10 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.6%
[2022-11-13 22:21:10 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.61% at 230 epoch
[2022-11-13 22:21:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][0/1251]	eta 0:52:51 lr 0.000130	time 2.5352 (2.5352)	loss 3.1577 (3.1577)	grad_norm 2.2536 (2.2536)	mem 23874MB
[2022-11-13 22:21:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][50/1251]	eta 0:15:48 lr 0.000130	time 0.7505 (0.7899)	loss 2.2157 (2.7748)	grad_norm 2.3022 (2.2758)	mem 23874MB
[2022-11-13 22:22:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][100/1251]	eta 0:14:49 lr 0.000130	time 0.7418 (0.7724)	loss 3.0819 (2.8018)	grad_norm 2.7614 (2.4154)	mem 23874MB
[2022-11-13 22:23:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][150/1251]	eta 0:14:02 lr 0.000130	time 0.7451 (0.7657)	loss 2.7978 (2.8268)	grad_norm 2.3761 (2.4158)	mem 23874MB
[2022-11-13 22:23:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][200/1251]	eta 0:13:20 lr 0.000130	time 0.7498 (0.7615)	loss 2.6179 (2.8513)	grad_norm 2.8245 (2.4256)	mem 23874MB
[2022-11-13 22:24:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][250/1251]	eta 0:12:41 lr 0.000130	time 0.7422 (0.7603)	loss 2.6937 (2.8472)	grad_norm 2.0176 (2.4304)	mem 23874MB
[2022-11-13 22:24:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][300/1251]	eta 0:12:00 lr 0.000129	time 0.7479 (0.7581)	loss 2.1705 (2.8296)	grad_norm 2.2839 (2.4454)	mem 23874MB
[2022-11-13 22:25:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][350/1251]	eta 0:11:22 lr 0.000129	time 0.7456 (0.7574)	loss 1.7912 (2.8110)	grad_norm 2.1778 (2.4278)	mem 23874MB
[2022-11-13 22:26:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][400/1251]	eta 0:10:44 lr 0.000129	time 0.7397 (0.7569)	loss 2.4745 (2.7962)	grad_norm 2.3235 (2.4345)	mem 23874MB
[2022-11-13 22:26:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][450/1251]	eta 0:10:05 lr 0.000129	time 0.7449 (0.7559)	loss 2.6675 (2.8000)	grad_norm 1.9565 (2.4406)	mem 23874MB
[2022-11-13 22:27:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][500/1251]	eta 0:09:27 lr 0.000129	time 0.7478 (0.7556)	loss 2.5466 (2.7980)	grad_norm 2.2838 (inf)	mem 23874MB
[2022-11-13 22:28:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][550/1251]	eta 0:08:49 lr 0.000129	time 0.7437 (0.7552)	loss 2.8576 (2.7950)	grad_norm 2.3978 (inf)	mem 23874MB
[2022-11-13 22:28:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][600/1251]	eta 0:08:11 lr 0.000129	time 0.7433 (0.7547)	loss 3.1111 (2.8057)	grad_norm 2.1470 (inf)	mem 23874MB
[2022-11-13 22:29:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][650/1251]	eta 0:07:33 lr 0.000129	time 0.8187 (0.7547)	loss 2.5167 (2.7943)	grad_norm 2.5208 (inf)	mem 23874MB
[2022-11-13 22:29:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][700/1251]	eta 0:06:55 lr 0.000128	time 0.7395 (0.7541)	loss 2.9144 (2.7915)	grad_norm 2.0263 (inf)	mem 23874MB
[2022-11-13 22:30:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][750/1251]	eta 0:06:17 lr 0.000128	time 0.7447 (0.7539)	loss 2.5717 (2.7863)	grad_norm 2.4648 (inf)	mem 23874MB
[2022-11-13 22:31:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][800/1251]	eta 0:05:40 lr 0.000128	time 0.7480 (0.7539)	loss 2.9529 (2.7875)	grad_norm 2.2668 (inf)	mem 23874MB
[2022-11-13 22:31:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][850/1251]	eta 0:05:02 lr 0.000128	time 0.7404 (0.7534)	loss 1.9288 (2.7848)	grad_norm 2.3778 (inf)	mem 23874MB
[2022-11-13 22:32:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][900/1251]	eta 0:04:24 lr 0.000128	time 0.7507 (0.7535)	loss 3.2898 (2.7794)	grad_norm 2.6601 (inf)	mem 23874MB
[2022-11-13 22:33:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][950/1251]	eta 0:03:46 lr 0.000128	time 0.7414 (0.7532)	loss 2.3067 (2.7777)	grad_norm 2.4582 (inf)	mem 23874MB
[2022-11-13 22:33:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][1000/1251]	eta 0:03:08 lr 0.000128	time 0.7435 (0.7530)	loss 2.2654 (2.7752)	grad_norm 2.2486 (inf)	mem 23874MB
[2022-11-13 22:34:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][1050/1251]	eta 0:02:31 lr 0.000127	time 0.7433 (0.7529)	loss 3.0744 (2.7763)	grad_norm 2.2795 (inf)	mem 23874MB
[2022-11-13 22:34:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][1100/1251]	eta 0:01:53 lr 0.000127	time 0.7449 (0.7527)	loss 3.0105 (2.7717)	grad_norm 2.3514 (inf)	mem 23874MB
[2022-11-13 22:35:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][1150/1251]	eta 0:01:16 lr 0.000127	time 0.7409 (0.7526)	loss 2.5793 (2.7735)	grad_norm 2.2835 (inf)	mem 23874MB
[2022-11-13 22:36:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][1200/1251]	eta 0:00:38 lr 0.000127	time 0.7467 (0.7526)	loss 2.8545 (2.7732)	grad_norm 2.2432 (inf)	mem 23874MB
[2022-11-13 22:36:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [232/300][1250/1251]	eta 0:00:00 lr 0.000127	time 0.7327 (0.7522)	loss 2.8417 (2.7781)	grad_norm 4.9755 (inf)	mem 23874MB
[2022-11-13 22:36:51 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 232 training takes 0:15:41
[2022-11-13 22:36:52 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_232.pth saving......
[2022-11-13 22:36:53 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_232.pth saved !!!
[2022-11-13 22:36:54 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.769 (1.769)	Loss 0.7202 (0.7202)	Acc@1 84.082 (84.082)	Acc@5 96.484 (96.484)	Mem 23874MB
[2022-11-13 22:37:05 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.526 Acc@5 96.340
[2022-11-13 22:37:05 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.5%
[2022-11-13 22:37:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.999 (1.999)	Loss 0.6896 (0.6896)	Acc@1 83.594 (83.594)	Acc@5 97.363 (97.363)	Mem 23874MB
[2022-11-13 22:37:18 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.634 Acc@5 96.708
[2022-11-13 22:37:18 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.6%
[2022-11-13 22:37:18 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.63% at 232 epoch
[2022-11-13 22:37:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][0/1251]	eta 0:55:17 lr 0.000127	time 2.6521 (2.6521)	loss 2.8526 (2.8526)	grad_norm 2.5428 (2.5428)	mem 23874MB
[2022-11-13 22:37:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][50/1251]	eta 0:15:40 lr 0.000127	time 0.7439 (0.7833)	loss 1.9923 (2.7042)	grad_norm 2.5855 (2.5119)	mem 23874MB
[2022-11-13 22:38:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][100/1251]	eta 0:14:45 lr 0.000127	time 0.7540 (0.7695)	loss 2.9550 (2.7114)	grad_norm 16.4410 (2.6074)	mem 23874MB
[2022-11-13 22:39:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][150/1251]	eta 0:14:00 lr 0.000127	time 0.7420 (0.7636)	loss 2.7589 (2.7240)	grad_norm 2.3561 (2.5515)	mem 23874MB
[2022-11-13 22:39:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][200/1251]	eta 0:13:18 lr 0.000126	time 0.7462 (0.7602)	loss 2.1873 (2.7365)	grad_norm 2.2680 (2.5965)	mem 23874MB
[2022-11-13 22:40:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][250/1251]	eta 0:12:39 lr 0.000126	time 0.7396 (0.7587)	loss 2.9198 (2.7546)	grad_norm 1.9233 (2.5885)	mem 23874MB
[2022-11-13 22:41:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][300/1251]	eta 0:11:59 lr 0.000126	time 0.8135 (0.7571)	loss 2.9529 (2.7515)	grad_norm 2.0821 (2.5592)	mem 23874MB
[2022-11-13 22:41:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][350/1251]	eta 0:11:21 lr 0.000126	time 0.7475 (0.7564)	loss 2.9973 (2.7618)	grad_norm 2.7583 (2.5467)	mem 23874MB
[2022-11-13 22:42:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][400/1251]	eta 0:10:43 lr 0.000126	time 0.7426 (0.7559)	loss 2.8311 (2.7660)	grad_norm 2.3543 (2.5404)	mem 23874MB
[2022-11-13 22:42:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][450/1251]	eta 0:10:04 lr 0.000126	time 0.7460 (0.7549)	loss 1.9055 (2.7604)	grad_norm 2.0463 (2.5330)	mem 23874MB
[2022-11-13 22:43:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][500/1251]	eta 0:09:26 lr 0.000126	time 0.7565 (0.7547)	loss 2.2538 (2.7657)	grad_norm 19.0074 (2.5496)	mem 23874MB
[2022-11-13 22:44:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][550/1251]	eta 0:08:48 lr 0.000125	time 0.7401 (0.7546)	loss 1.9428 (2.7592)	grad_norm 2.7228 (2.5382)	mem 23874MB
[2022-11-13 22:44:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][600/1251]	eta 0:08:10 lr 0.000125	time 0.7438 (0.7538)	loss 2.8391 (2.7645)	grad_norm 2.5586 (2.5345)	mem 23874MB
[2022-11-13 22:45:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][650/1251]	eta 0:07:33 lr 0.000125	time 0.7397 (0.7538)	loss 3.2212 (2.7662)	grad_norm 2.8662 (2.5275)	mem 23874MB
[2022-11-13 22:46:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][700/1251]	eta 0:06:55 lr 0.000125	time 0.8180 (0.7533)	loss 3.0134 (2.7704)	grad_norm 2.6360 (2.5297)	mem 23874MB
[2022-11-13 22:46:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][750/1251]	eta 0:06:17 lr 0.000125	time 0.7384 (0.7532)	loss 3.0028 (2.7721)	grad_norm 2.6679 (2.5193)	mem 23874MB
[2022-11-13 22:47:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][800/1251]	eta 0:05:39 lr 0.000125	time 0.7406 (0.7529)	loss 2.8957 (2.7743)	grad_norm 1.9303 (2.5268)	mem 23874MB
[2022-11-13 22:47:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][850/1251]	eta 0:05:01 lr 0.000125	time 0.7381 (0.7527)	loss 3.0252 (2.7714)	grad_norm 2.6338 (2.5184)	mem 23874MB
[2022-11-13 22:48:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][900/1251]	eta 0:04:24 lr 0.000125	time 0.7430 (0.7526)	loss 3.2950 (2.7747)	grad_norm 2.3522 (2.5133)	mem 23874MB
[2022-11-13 22:49:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][950/1251]	eta 0:03:46 lr 0.000124	time 0.7468 (0.7527)	loss 2.2177 (2.7700)	grad_norm 2.4091 (2.5047)	mem 23874MB
[2022-11-13 22:49:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][1000/1251]	eta 0:03:08 lr 0.000124	time 0.7433 (0.7525)	loss 3.3586 (2.7714)	grad_norm 2.8881 (2.5062)	mem 23874MB
[2022-11-13 22:50:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][1050/1251]	eta 0:02:31 lr 0.000124	time 0.7411 (0.7525)	loss 3.3149 (2.7756)	grad_norm 3.0206 (2.5029)	mem 23874MB
[2022-11-13 22:51:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][1100/1251]	eta 0:01:53 lr 0.000124	time 0.8141 (0.7524)	loss 2.4595 (2.7723)	grad_norm 3.0820 (2.4941)	mem 23874MB
[2022-11-13 22:51:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][1150/1251]	eta 0:01:15 lr 0.000124	time 0.7464 (0.7523)	loss 2.4480 (2.7764)	grad_norm 2.0952 (2.4950)	mem 23874MB
[2022-11-13 22:52:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][1200/1251]	eta 0:00:38 lr 0.000124	time 0.7411 (0.7522)	loss 2.9355 (2.7754)	grad_norm 2.5798 (2.4930)	mem 23874MB
[2022-11-13 22:52:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [233/300][1250/1251]	eta 0:00:00 lr 0.000124	time 0.7297 (0.7520)	loss 2.9428 (2.7733)	grad_norm 2.0298 (2.4908)	mem 23874MB
[2022-11-13 22:52:59 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 233 training takes 0:15:40
[2022-11-13 22:52:59 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_233.pth saving......
[2022-11-13 22:53:00 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_233.pth saved !!!
[2022-11-13 22:53:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.678 (1.678)	Loss 0.7619 (0.7619)	Acc@1 82.031 (82.031)	Acc@5 96.191 (96.191)	Mem 23874MB
[2022-11-13 22:53:13 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.570 Acc@5 96.318
[2022-11-13 22:53:13 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.6%
[2022-11-13 22:53:15 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.933 (1.933)	Loss 0.6834 (0.6834)	Acc@1 84.961 (84.961)	Acc@5 96.582 (96.582)	Mem 23874MB
[2022-11-13 22:53:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.616 Acc@5 96.720
[2022-11-13 22:53:25 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.6%
[2022-11-13 22:53:25 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.63% at 232 epoch
[2022-11-13 22:53:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][0/1251]	eta 0:57:34 lr 0.000124	time 2.7612 (2.7612)	loss 3.1903 (3.1903)	grad_norm 1.9176 (1.9176)	mem 23874MB
[2022-11-13 22:54:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][50/1251]	eta 0:15:49 lr 0.000123	time 0.7420 (0.7909)	loss 3.0980 (2.7988)	grad_norm 2.8355 (2.4506)	mem 23874MB
[2022-11-13 22:54:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][100/1251]	eta 0:14:45 lr 0.000123	time 0.7530 (0.7695)	loss 2.1208 (2.7620)	grad_norm 2.0658 (2.4124)	mem 23874MB
[2022-11-13 22:55:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][150/1251]	eta 0:14:00 lr 0.000123	time 0.7406 (0.7630)	loss 2.1171 (2.7834)	grad_norm 1.9576 (2.4134)	mem 23874MB
[2022-11-13 22:55:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][200/1251]	eta 0:13:19 lr 0.000123	time 0.7501 (0.7602)	loss 3.4621 (2.8009)	grad_norm 2.4018 (2.4420)	mem 23874MB
[2022-11-13 22:56:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][250/1251]	eta 0:12:37 lr 0.000123	time 0.7421 (0.7570)	loss 2.3503 (2.7830)	grad_norm 2.6578 (2.4311)	mem 23874MB
[2022-11-13 22:57:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][300/1251]	eta 0:11:59 lr 0.000123	time 0.7409 (0.7562)	loss 2.7250 (2.7737)	grad_norm 2.1245 (2.4333)	mem 23874MB
[2022-11-13 22:57:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][350/1251]	eta 0:11:20 lr 0.000123	time 0.7409 (0.7550)	loss 2.5292 (2.7794)	grad_norm 1.9776 (2.4293)	mem 23874MB
[2022-11-13 22:58:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][400/1251]	eta 0:10:41 lr 0.000123	time 0.7396 (0.7540)	loss 3.0243 (2.7826)	grad_norm 2.2987 (2.4145)	mem 23874MB
[2022-11-13 22:59:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][450/1251]	eta 0:10:03 lr 0.000122	time 0.7433 (0.7533)	loss 2.0074 (2.7707)	grad_norm 2.1302 (2.4035)	mem 23874MB
[2022-11-13 22:59:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][500/1251]	eta 0:09:25 lr 0.000122	time 0.7400 (0.7527)	loss 1.8138 (2.7608)	grad_norm 1.9997 (2.4012)	mem 23874MB
[2022-11-13 23:00:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][550/1251]	eta 0:08:47 lr 0.000122	time 0.7402 (0.7526)	loss 1.9142 (2.7573)	grad_norm 2.3981 (2.4001)	mem 23874MB
[2022-11-13 23:00:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][600/1251]	eta 0:08:09 lr 0.000122	time 0.7452 (0.7522)	loss 1.8753 (2.7684)	grad_norm 2.3507 (inf)	mem 23874MB
[2022-11-13 23:01:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][650/1251]	eta 0:07:31 lr 0.000122	time 0.7461 (0.7519)	loss 2.0043 (2.7712)	grad_norm 2.7502 (inf)	mem 23874MB
[2022-11-13 23:02:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][700/1251]	eta 0:06:54 lr 0.000122	time 0.7424 (0.7514)	loss 2.5777 (2.7757)	grad_norm 2.1538 (inf)	mem 23874MB
[2022-11-13 23:02:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][750/1251]	eta 0:06:16 lr 0.000122	time 0.7397 (0.7515)	loss 2.5130 (2.7839)	grad_norm 2.2298 (inf)	mem 23874MB
[2022-11-13 23:03:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][800/1251]	eta 0:05:38 lr 0.000121	time 0.7394 (0.7512)	loss 2.9939 (2.7757)	grad_norm 2.2156 (inf)	mem 23874MB
[2022-11-13 23:04:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][850/1251]	eta 0:05:01 lr 0.000121	time 0.7439 (0.7512)	loss 2.0826 (2.7765)	grad_norm 3.2672 (inf)	mem 23874MB
[2022-11-13 23:04:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][900/1251]	eta 0:04:23 lr 0.000121	time 0.7370 (0.7508)	loss 2.4471 (2.7758)	grad_norm 2.1340 (inf)	mem 23874MB
[2022-11-13 23:05:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][950/1251]	eta 0:03:45 lr 0.000121	time 0.7433 (0.7508)	loss 2.8101 (2.7763)	grad_norm 2.5935 (inf)	mem 23874MB
[2022-11-13 23:05:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][1000/1251]	eta 0:03:08 lr 0.000121	time 0.7482 (0.7507)	loss 3.0505 (2.7768)	grad_norm 2.1632 (inf)	mem 23874MB
[2022-11-13 23:06:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][1050/1251]	eta 0:02:30 lr 0.000121	time 0.7411 (0.7506)	loss 3.0986 (2.7743)	grad_norm 2.3081 (inf)	mem 23874MB
[2022-11-13 23:07:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][1100/1251]	eta 0:01:53 lr 0.000121	time 0.7401 (0.7504)	loss 2.7340 (2.7717)	grad_norm 2.5227 (inf)	mem 23874MB
[2022-11-13 23:07:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][1150/1251]	eta 0:01:15 lr 0.000121	time 0.7486 (0.7503)	loss 2.7456 (2.7698)	grad_norm 2.0238 (inf)	mem 23874MB
[2022-11-13 23:08:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][1200/1251]	eta 0:00:38 lr 0.000120	time 0.7360 (0.7501)	loss 3.4010 (2.7701)	grad_norm 2.6393 (inf)	mem 23874MB
[2022-11-13 23:09:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [234/300][1250/1251]	eta 0:00:00 lr 0.000120	time 0.7282 (0.7499)	loss 2.8922 (2.7686)	grad_norm 2.0677 (inf)	mem 23874MB
[2022-11-13 23:09:04 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 234 training takes 0:15:38
[2022-11-13 23:09:04 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_234.pth saving......
[2022-11-13 23:09:05 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_234.pth saved !!!
[2022-11-13 23:09:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.733 (1.733)	Loss 0.7638 (0.7638)	Acc@1 81.152 (81.152)	Acc@5 96.191 (96.191)	Mem 23874MB
[2022-11-13 23:09:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.752 Acc@5 96.246
[2022-11-13 23:09:17 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.8%
[2022-11-13 23:09:19 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.990 (1.990)	Loss 0.6802 (0.6802)	Acc@1 83.301 (83.301)	Acc@5 96.484 (96.484)	Mem 23874MB
[2022-11-13 23:09:30 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.640 Acc@5 96.714
[2022-11-13 23:09:30 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.6%
[2022-11-13 23:09:30 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.64% at 234 epoch
[2022-11-13 23:09:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][0/1251]	eta 0:52:10 lr 0.000120	time 2.5020 (2.5020)	loss 1.8861 (1.8861)	grad_norm 2.1675 (2.1675)	mem 23874MB
[2022-11-13 23:10:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][50/1251]	eta 0:15:46 lr 0.000120	time 0.7382 (0.7883)	loss 3.1151 (2.6355)	grad_norm 2.4772 (2.4658)	mem 23874MB
[2022-11-13 23:10:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][100/1251]	eta 0:14:43 lr 0.000120	time 0.7404 (0.7680)	loss 3.1466 (2.6881)	grad_norm 2.4998 (2.4077)	mem 23874MB
[2022-11-13 23:11:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][150/1251]	eta 0:14:00 lr 0.000120	time 0.7412 (0.7630)	loss 2.4166 (2.7230)	grad_norm 2.0981 (2.3950)	mem 23874MB
[2022-11-13 23:12:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][200/1251]	eta 0:13:18 lr 0.000120	time 0.7429 (0.7595)	loss 2.8953 (2.7403)	grad_norm 2.2613 (2.4398)	mem 23874MB
[2022-11-13 23:12:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][250/1251]	eta 0:12:37 lr 0.000120	time 0.7394 (0.7569)	loss 3.1572 (2.7336)	grad_norm 3.0664 (2.4514)	mem 23874MB
[2022-11-13 23:13:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][300/1251]	eta 0:11:59 lr 0.000120	time 0.8296 (0.7564)	loss 2.7663 (2.7518)	grad_norm 2.1631 (2.4437)	mem 23874MB
[2022-11-13 23:13:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][350/1251]	eta 0:11:20 lr 0.000119	time 0.7387 (0.7555)	loss 2.6074 (2.7629)	grad_norm 2.0329 (2.4527)	mem 23874MB
[2022-11-13 23:14:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][400/1251]	eta 0:10:42 lr 0.000119	time 0.7374 (0.7549)	loss 2.7158 (2.7602)	grad_norm 3.3211 (2.4446)	mem 23874MB
[2022-11-13 23:15:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][450/1251]	eta 0:10:04 lr 0.000119	time 0.7385 (0.7546)	loss 2.8595 (2.7647)	grad_norm 2.1394 (2.4788)	mem 23874MB
[2022-11-13 23:15:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][500/1251]	eta 0:09:26 lr 0.000119	time 0.7430 (0.7538)	loss 2.5817 (2.7627)	grad_norm 2.4237 (2.4686)	mem 23874MB
[2022-11-13 23:16:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][550/1251]	eta 0:08:48 lr 0.000119	time 0.7401 (0.7537)	loss 2.1754 (2.7565)	grad_norm 2.8158 (2.4837)	mem 23874MB
[2022-11-13 23:17:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][600/1251]	eta 0:08:10 lr 0.000119	time 0.7384 (0.7534)	loss 3.0356 (2.7586)	grad_norm 4.4242 (2.5007)	mem 23874MB
[2022-11-13 23:17:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][650/1251]	eta 0:07:32 lr 0.000119	time 0.7466 (0.7532)	loss 2.1964 (2.7653)	grad_norm 2.3503 (2.4982)	mem 23874MB
[2022-11-13 23:18:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][700/1251]	eta 0:06:54 lr 0.000118	time 0.8289 (0.7531)	loss 3.0469 (2.7684)	grad_norm 2.5087 (2.4999)	mem 23874MB
[2022-11-13 23:18:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][750/1251]	eta 0:06:17 lr 0.000118	time 0.8438 (0.7530)	loss 3.0985 (2.7743)	grad_norm 1.9973 (2.4945)	mem 23874MB
[2022-11-13 23:19:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][800/1251]	eta 0:05:39 lr 0.000118	time 0.7491 (0.7527)	loss 2.4960 (2.7688)	grad_norm 2.1103 (2.4909)	mem 23874MB
[2022-11-13 23:20:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][850/1251]	eta 0:05:01 lr 0.000118	time 0.7403 (0.7527)	loss 2.1462 (2.7732)	grad_norm 2.6617 (2.4970)	mem 23874MB
[2022-11-13 23:20:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][900/1251]	eta 0:04:24 lr 0.000118	time 0.7450 (0.7525)	loss 3.3262 (2.7755)	grad_norm 2.3469 (2.4937)	mem 23874MB
[2022-11-13 23:21:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][950/1251]	eta 0:03:46 lr 0.000118	time 0.7376 (0.7524)	loss 2.7001 (2.7783)	grad_norm 2.2538 (inf)	mem 23874MB
[2022-11-13 23:22:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][1000/1251]	eta 0:03:08 lr 0.000118	time 0.7371 (0.7523)	loss 2.8905 (2.7783)	grad_norm 2.1440 (inf)	mem 23874MB
[2022-11-13 23:22:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][1050/1251]	eta 0:02:31 lr 0.000118	time 0.7445 (0.7522)	loss 2.9059 (2.7785)	grad_norm 2.0312 (inf)	mem 23874MB
[2022-11-13 23:23:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][1100/1251]	eta 0:01:53 lr 0.000117	time 0.8377 (0.7522)	loss 1.7873 (2.7770)	grad_norm 2.4071 (inf)	mem 23874MB
[2022-11-13 23:23:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][1150/1251]	eta 0:01:15 lr 0.000117	time 0.7393 (0.7521)	loss 2.9373 (2.7748)	grad_norm 2.3051 (inf)	mem 23874MB
[2022-11-13 23:24:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][1200/1251]	eta 0:00:38 lr 0.000117	time 0.7438 (0.7520)	loss 3.3654 (2.7753)	grad_norm 2.1506 (inf)	mem 23874MB
[2022-11-13 23:25:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [235/300][1250/1251]	eta 0:00:00 lr 0.000117	time 0.7352 (0.7519)	loss 2.4600 (2.7745)	grad_norm 2.0443 (inf)	mem 23874MB
[2022-11-13 23:25:11 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 235 training takes 0:15:40
[2022-11-13 23:25:11 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_235.pth saving......
[2022-11-13 23:25:12 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_235.pth saved !!!
[2022-11-13 23:25:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.745 (1.745)	Loss 0.7399 (0.7399)	Acc@1 82.422 (82.422)	Acc@5 96.094 (96.094)	Mem 23874MB
[2022-11-13 23:25:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.876 Acc@5 96.396
[2022-11-13 23:25:25 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.9%
[2022-11-13 23:25:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.880 (1.880)	Loss 0.6922 (0.6922)	Acc@1 83.301 (83.301)	Acc@5 96.680 (96.680)	Mem 23874MB
[2022-11-13 23:25:37 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.604 Acc@5 96.712
[2022-11-13 23:25:37 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.6%
[2022-11-13 23:25:37 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.64% at 234 epoch
[2022-11-13 23:25:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][0/1251]	eta 0:51:59 lr 0.000117	time 2.4934 (2.4934)	loss 2.8081 (2.8081)	grad_norm 2.1480 (2.1480)	mem 23874MB
[2022-11-13 23:26:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][50/1251]	eta 0:15:46 lr 0.000117	time 0.7426 (0.7884)	loss 3.0591 (2.7257)	grad_norm 2.4498 (2.7818)	mem 23874MB
[2022-11-13 23:26:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][100/1251]	eta 0:14:45 lr 0.000117	time 0.7495 (0.7697)	loss 2.3891 (2.7401)	grad_norm 6.3260 (2.6700)	mem 23874MB
[2022-11-13 23:27:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][150/1251]	eta 0:14:01 lr 0.000117	time 0.7440 (0.7645)	loss 3.0232 (2.7757)	grad_norm 2.4517 (2.6186)	mem 23874MB
[2022-11-13 23:28:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][200/1251]	eta 0:13:18 lr 0.000117	time 0.8161 (0.7600)	loss 2.6483 (2.7729)	grad_norm 2.1686 (2.6126)	mem 23874MB
[2022-11-13 23:28:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][250/1251]	eta 0:12:38 lr 0.000116	time 0.7406 (0.7580)	loss 3.3181 (2.7816)	grad_norm 2.5020 (2.5655)	mem 23874MB
[2022-11-13 23:29:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][300/1251]	eta 0:12:00 lr 0.000116	time 0.7406 (0.7576)	loss 2.8898 (2.7936)	grad_norm 2.2899 (2.5451)	mem 23874MB
[2022-11-13 23:30:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][350/1251]	eta 0:11:21 lr 0.000116	time 0.7592 (0.7564)	loss 3.2482 (2.7878)	grad_norm 2.1811 (2.5348)	mem 23874MB
[2022-11-13 23:30:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][400/1251]	eta 0:10:43 lr 0.000116	time 0.7461 (0.7564)	loss 2.2081 (2.7795)	grad_norm 2.4426 (2.5113)	mem 23874MB
[2022-11-13 23:31:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][450/1251]	eta 0:10:05 lr 0.000116	time 0.7402 (0.7555)	loss 3.2479 (2.7920)	grad_norm 2.3916 (2.5144)	mem 23874MB
[2022-11-13 23:31:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][500/1251]	eta 0:09:26 lr 0.000116	time 0.7416 (0.7548)	loss 2.5453 (2.7951)	grad_norm 2.6292 (2.5022)	mem 23874MB
[2022-11-13 23:32:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][550/1251]	eta 0:08:48 lr 0.000116	time 0.7430 (0.7545)	loss 1.8179 (2.7911)	grad_norm 2.2112 (2.4899)	mem 23874MB
[2022-11-13 23:33:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][600/1251]	eta 0:08:10 lr 0.000116	time 0.7408 (0.7542)	loss 2.0312 (2.7824)	grad_norm 2.2743 (2.4860)	mem 23874MB
[2022-11-13 23:33:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][650/1251]	eta 0:07:33 lr 0.000115	time 0.7396 (0.7541)	loss 1.8917 (2.7859)	grad_norm 2.4441 (2.4834)	mem 23874MB
[2022-11-13 23:34:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][700/1251]	eta 0:06:55 lr 0.000115	time 0.7382 (0.7541)	loss 2.7495 (2.7819)	grad_norm 2.4671 (2.4832)	mem 23874MB
[2022-11-13 23:35:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][750/1251]	eta 0:06:17 lr 0.000115	time 0.7420 (0.7535)	loss 2.9538 (2.7769)	grad_norm 2.2960 (2.4884)	mem 23874MB
[2022-11-13 23:35:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][800/1251]	eta 0:05:39 lr 0.000115	time 0.7431 (0.7536)	loss 2.6706 (2.7727)	grad_norm 2.7710 (2.4860)	mem 23874MB
[2022-11-13 23:36:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][850/1251]	eta 0:05:02 lr 0.000115	time 0.7425 (0.7534)	loss 3.1484 (2.7724)	grad_norm 2.2206 (2.4814)	mem 23874MB
[2022-11-13 23:36:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][900/1251]	eta 0:04:24 lr 0.000115	time 0.7423 (0.7532)	loss 3.1177 (2.7680)	grad_norm 2.1099 (2.4887)	mem 23874MB
[2022-11-13 23:37:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][950/1251]	eta 0:03:46 lr 0.000115	time 0.7437 (0.7531)	loss 3.1983 (2.7672)	grad_norm 2.3502 (2.4891)	mem 23874MB
[2022-11-13 23:38:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][1000/1251]	eta 0:03:08 lr 0.000115	time 0.7394 (0.7529)	loss 2.9473 (2.7690)	grad_norm 2.3668 (2.4865)	mem 23874MB
[2022-11-13 23:38:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][1050/1251]	eta 0:02:31 lr 0.000114	time 0.7317 (0.7527)	loss 1.9505 (2.7716)	grad_norm 2.3587 (2.4982)	mem 23874MB
[2022-11-13 23:39:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][1100/1251]	eta 0:01:53 lr 0.000114	time 0.7408 (0.7527)	loss 2.7769 (2.7751)	grad_norm 2.6350 (2.4976)	mem 23874MB
[2022-11-13 23:40:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][1150/1251]	eta 0:01:16 lr 0.000114	time 0.7400 (0.7525)	loss 2.9525 (2.7776)	grad_norm 2.3717 (2.4950)	mem 23874MB
[2022-11-13 23:40:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][1200/1251]	eta 0:00:38 lr 0.000114	time 0.7426 (0.7526)	loss 3.3333 (2.7750)	grad_norm 2.3874 (2.4956)	mem 23874MB
[2022-11-13 23:41:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [236/300][1250/1251]	eta 0:00:00 lr 0.000114	time 0.7949 (0.7523)	loss 3.1729 (2.7768)	grad_norm 2.8374 (2.4992)	mem 23874MB
[2022-11-13 23:41:19 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 236 training takes 0:15:41
[2022-11-13 23:41:19 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_236.pth saving......
[2022-11-13 23:41:20 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_236.pth saved !!!
[2022-11-13 23:41:22 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.754 (1.754)	Loss 0.7850 (0.7850)	Acc@1 80.859 (80.859)	Acc@5 96.484 (96.484)	Mem 23874MB
[2022-11-13 23:41:32 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.774 Acc@5 96.336
[2022-11-13 23:41:32 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.8%
[2022-11-13 23:41:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.977 (1.977)	Loss 0.7249 (0.7249)	Acc@1 83.594 (83.594)	Acc@5 97.070 (97.070)	Mem 23874MB
[2022-11-13 23:41:45 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.622 Acc@5 96.712
[2022-11-13 23:41:45 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.6%
[2022-11-13 23:41:45 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.64% at 234 epoch
[2022-11-13 23:41:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][0/1251]	eta 0:52:33 lr 0.000114	time 2.5204 (2.5204)	loss 3.0084 (3.0084)	grad_norm 3.3506 (3.3506)	mem 23874MB
[2022-11-13 23:42:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][50/1251]	eta 0:15:44 lr 0.000114	time 0.7429 (0.7866)	loss 2.0669 (2.7443)	grad_norm 2.8560 (2.6281)	mem 23874MB
[2022-11-13 23:43:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][100/1251]	eta 0:14:45 lr 0.000114	time 0.7393 (0.7691)	loss 2.5332 (2.7340)	grad_norm 2.2710 (2.5964)	mem 23874MB
[2022-11-13 23:43:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][150/1251]	eta 0:13:58 lr 0.000113	time 0.7385 (0.7617)	loss 2.7890 (2.7354)	grad_norm 2.4386 (2.5416)	mem 23874MB
[2022-11-13 23:44:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][200/1251]	eta 0:13:18 lr 0.000113	time 0.7645 (0.7597)	loss 2.0222 (2.7247)	grad_norm 2.2021 (2.6098)	mem 23874MB
[2022-11-13 23:44:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][250/1251]	eta 0:12:38 lr 0.000113	time 0.7395 (0.7578)	loss 3.1383 (2.7223)	grad_norm 3.6560 (2.5710)	mem 23874MB
[2022-11-13 23:45:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][300/1251]	eta 0:11:59 lr 0.000113	time 0.7488 (0.7564)	loss 2.9013 (2.7369)	grad_norm 2.7652 (2.5341)	mem 23874MB
[2022-11-13 23:46:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][350/1251]	eta 0:11:20 lr 0.000113	time 0.7466 (0.7554)	loss 2.1106 (2.7319)	grad_norm 2.3750 (2.5342)	mem 23874MB
[2022-11-13 23:46:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][400/1251]	eta 0:10:42 lr 0.000113	time 0.7441 (0.7549)	loss 2.2941 (2.7262)	grad_norm 2.3552 (2.5290)	mem 23874MB
[2022-11-13 23:47:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][450/1251]	eta 0:10:04 lr 0.000113	time 0.7442 (0.7543)	loss 3.1377 (2.7218)	grad_norm 2.2825 (2.5149)	mem 23874MB
[2022-11-13 23:48:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][500/1251]	eta 0:09:26 lr 0.000113	time 0.7393 (0.7541)	loss 2.7996 (2.7185)	grad_norm 2.0088 (2.4995)	mem 23874MB
[2022-11-13 23:48:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][550/1251]	eta 0:08:48 lr 0.000112	time 0.7448 (0.7534)	loss 3.0531 (2.7171)	grad_norm 2.6484 (2.5097)	mem 23874MB
[2022-11-13 23:49:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][600/1251]	eta 0:08:10 lr 0.000112	time 0.7555 (0.7536)	loss 2.4554 (2.7186)	grad_norm 2.1215 (2.5058)	mem 23874MB
[2022-11-13 23:49:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][650/1251]	eta 0:07:32 lr 0.000112	time 0.7378 (0.7532)	loss 2.5626 (2.7224)	grad_norm 2.2119 (2.5030)	mem 23874MB
[2022-11-13 23:50:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][700/1251]	eta 0:06:54 lr 0.000112	time 0.7409 (0.7529)	loss 3.2287 (2.7310)	grad_norm 2.5806 (nan)	mem 23874MB
[2022-11-13 23:51:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][750/1251]	eta 0:06:17 lr 0.000112	time 0.7463 (0.7526)	loss 3.1992 (2.7362)	grad_norm 2.7895 (nan)	mem 23874MB
[2022-11-13 23:51:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][800/1251]	eta 0:05:39 lr 0.000112	time 0.7411 (0.7525)	loss 2.1140 (2.7390)	grad_norm 2.2538 (nan)	mem 23874MB
[2022-11-13 23:52:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][850/1251]	eta 0:05:01 lr 0.000112	time 0.7390 (0.7521)	loss 2.7248 (2.7423)	grad_norm 2.0509 (nan)	mem 23874MB
[2022-11-13 23:53:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][900/1251]	eta 0:04:23 lr 0.000112	time 0.7404 (0.7521)	loss 2.3869 (2.7425)	grad_norm 2.7014 (nan)	mem 23874MB
[2022-11-13 23:53:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][950/1251]	eta 0:03:46 lr 0.000111	time 0.7452 (0.7518)	loss 3.3202 (2.7448)	grad_norm 3.0684 (nan)	mem 23874MB
[2022-11-13 23:54:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][1000/1251]	eta 0:03:08 lr 0.000111	time 0.7425 (0.7518)	loss 2.9263 (2.7436)	grad_norm 2.2546 (nan)	mem 23874MB
[2022-11-13 23:54:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][1050/1251]	eta 0:02:31 lr 0.000111	time 0.7373 (0.7518)	loss 2.9367 (2.7475)	grad_norm 2.7712 (nan)	mem 23874MB
[2022-11-13 23:55:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][1100/1251]	eta 0:01:53 lr 0.000111	time 0.7408 (0.7516)	loss 2.7493 (2.7468)	grad_norm 2.4575 (nan)	mem 23874MB
[2022-11-13 23:56:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][1150/1251]	eta 0:01:15 lr 0.000111	time 0.7374 (0.7515)	loss 1.7044 (2.7481)	grad_norm 2.1675 (nan)	mem 23874MB
[2022-11-13 23:56:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][1200/1251]	eta 0:00:38 lr 0.000111	time 0.7414 (0.7515)	loss 2.9963 (2.7495)	grad_norm 2.3841 (nan)	mem 23874MB
[2022-11-13 23:57:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [237/300][1250/1251]	eta 0:00:00 lr 0.000111	time 0.7271 (0.7513)	loss 2.9474 (2.7517)	grad_norm 2.0562 (nan)	mem 23874MB
[2022-11-13 23:57:25 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 237 training takes 0:15:39
[2022-11-13 23:57:25 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_237.pth saving......
[2022-11-13 23:57:26 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_237.pth saved !!!
[2022-11-13 23:57:28 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.745 (1.745)	Loss 0.6966 (0.6966)	Acc@1 84.180 (84.180)	Acc@5 96.973 (96.973)	Mem 23874MB
[2022-11-13 23:57:39 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.870 Acc@5 96.380
[2022-11-13 23:57:39 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.9%
[2022-11-13 23:57:41 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.887 (1.887)	Loss 0.6235 (0.6235)	Acc@1 84.375 (84.375)	Acc@5 97.656 (97.656)	Mem 23874MB
[2022-11-13 23:57:52 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.666 Acc@5 96.714
[2022-11-13 23:57:52 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.7%
[2022-11-13 23:57:52 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.67% at 237 epoch
[2022-11-13 23:57:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][0/1251]	eta 0:53:31 lr 0.000111	time 2.5673 (2.5673)	loss 3.0626 (3.0626)	grad_norm 2.5692 (2.5692)	mem 23874MB
[2022-11-13 23:58:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][50/1251]	eta 0:15:45 lr 0.000111	time 0.7447 (0.7871)	loss 3.0469 (2.7190)	grad_norm 2.2791 (2.5426)	mem 23874MB
[2022-11-13 23:59:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][100/1251]	eta 0:14:44 lr 0.000110	time 0.7411 (0.7681)	loss 2.9409 (2.6994)	grad_norm 3.3374 (2.5325)	mem 23874MB
[2022-11-13 23:59:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][150/1251]	eta 0:13:58 lr 0.000110	time 0.7584 (0.7620)	loss 3.0510 (2.7059)	grad_norm 2.6679 (2.5334)	mem 23874MB
[2022-11-14 00:00:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][200/1251]	eta 0:13:17 lr 0.000110	time 0.7459 (0.7586)	loss 3.2536 (2.7137)	grad_norm 2.6576 (2.5377)	mem 23874MB
[2022-11-14 00:01:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][250/1251]	eta 0:12:38 lr 0.000110	time 0.8119 (0.7573)	loss 3.1314 (2.7119)	grad_norm 2.8324 (2.5457)	mem 23874MB
[2022-11-14 00:01:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][300/1251]	eta 0:11:58 lr 0.000110	time 0.7455 (0.7557)	loss 2.9429 (2.7236)	grad_norm 2.3688 (2.5826)	mem 23874MB
[2022-11-14 00:02:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][350/1251]	eta 0:11:20 lr 0.000110	time 0.7412 (0.7555)	loss 3.1453 (2.7275)	grad_norm 2.4367 (2.5545)	mem 23874MB
[2022-11-14 00:02:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][400/1251]	eta 0:10:41 lr 0.000110	time 0.7449 (0.7540)	loss 2.4203 (2.7185)	grad_norm 2.2047 (2.5515)	mem 23874MB
[2022-11-14 00:03:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][450/1251]	eta 0:10:04 lr 0.000110	time 0.7424 (0.7541)	loss 2.5500 (2.7133)	grad_norm 2.5819 (2.5431)	mem 23874MB
[2022-11-14 00:04:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][500/1251]	eta 0:09:25 lr 0.000109	time 0.8546 (0.7535)	loss 2.9644 (2.7159)	grad_norm 2.6957 (2.5403)	mem 23874MB
[2022-11-14 00:04:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][550/1251]	eta 0:08:48 lr 0.000109	time 0.7453 (0.7533)	loss 3.0527 (2.7278)	grad_norm 2.5807 (2.5450)	mem 23874MB
[2022-11-14 00:05:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][600/1251]	eta 0:08:10 lr 0.000109	time 0.7316 (0.7529)	loss 2.3742 (2.7264)	grad_norm 3.3449 (2.5361)	mem 23874MB
[2022-11-14 00:06:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][650/1251]	eta 0:07:32 lr 0.000109	time 0.8241 (0.7528)	loss 2.7529 (2.7305)	grad_norm 3.1917 (2.5494)	mem 23874MB
[2022-11-14 00:06:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][700/1251]	eta 0:06:54 lr 0.000109	time 0.7434 (0.7524)	loss 3.1106 (2.7394)	grad_norm 2.2874 (2.5495)	mem 23874MB
[2022-11-14 00:07:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][750/1251]	eta 0:06:16 lr 0.000109	time 0.7416 (0.7524)	loss 2.7837 (2.7463)	grad_norm 2.5746 (2.5532)	mem 23874MB
[2022-11-14 00:07:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][800/1251]	eta 0:05:39 lr 0.000109	time 0.7401 (0.7520)	loss 2.9183 (2.7465)	grad_norm 2.3286 (2.5701)	mem 23874MB
[2022-11-14 00:08:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][850/1251]	eta 0:05:01 lr 0.000109	time 0.7446 (0.7521)	loss 3.1716 (2.7498)	grad_norm 2.5468 (2.5647)	mem 23874MB
[2022-11-14 00:09:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][900/1251]	eta 0:04:23 lr 0.000108	time 0.8151 (0.7520)	loss 2.9753 (2.7536)	grad_norm 2.4657 (2.5674)	mem 23874MB
[2022-11-14 00:09:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][950/1251]	eta 0:03:46 lr 0.000108	time 0.7445 (0.7519)	loss 2.8994 (2.7544)	grad_norm 2.6714 (2.5662)	mem 23874MB
[2022-11-14 00:10:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][1000/1251]	eta 0:03:08 lr 0.000108	time 0.7465 (0.7518)	loss 3.0905 (2.7504)	grad_norm 2.1182 (2.5666)	mem 23874MB
[2022-11-14 00:11:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][1050/1251]	eta 0:02:31 lr 0.000108	time 0.8159 (0.7518)	loss 1.8764 (2.7484)	grad_norm 2.9441 (2.5676)	mem 23874MB
[2022-11-14 00:11:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][1100/1251]	eta 0:01:53 lr 0.000108	time 0.7467 (0.7516)	loss 2.5555 (2.7482)	grad_norm 2.2925 (2.5828)	mem 23874MB
[2022-11-14 00:12:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][1150/1251]	eta 0:01:15 lr 0.000108	time 0.7483 (0.7517)	loss 2.9586 (2.7504)	grad_norm 3.0583 (2.5753)	mem 23874MB
[2022-11-14 00:12:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][1200/1251]	eta 0:00:38 lr 0.000108	time 0.7388 (0.7514)	loss 3.2829 (2.7494)	grad_norm 2.6569 (2.5725)	mem 23874MB
[2022-11-14 00:13:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [238/300][1250/1251]	eta 0:00:00 lr 0.000108	time 0.7339 (0.7513)	loss 2.9680 (2.7478)	grad_norm 2.1467 (2.5693)	mem 23874MB
[2022-11-14 00:13:32 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 238 training takes 0:15:40
[2022-11-14 00:13:32 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_238.pth saving......
[2022-11-14 00:13:33 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_238.pth saved !!!
[2022-11-14 00:13:35 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.765 (1.765)	Loss 0.7412 (0.7412)	Acc@1 82.812 (82.812)	Acc@5 96.484 (96.484)	Mem 23874MB
[2022-11-14 00:13:45 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.986 Acc@5 96.366
[2022-11-14 00:13:45 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.0%
[2022-11-14 00:13:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.935 (1.935)	Loss 0.7301 (0.7301)	Acc@1 82.617 (82.617)	Acc@5 95.898 (95.898)	Mem 23874MB
[2022-11-14 00:13:58 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.640 Acc@5 96.718
[2022-11-14 00:13:58 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.6%
[2022-11-14 00:13:58 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.67% at 237 epoch
[2022-11-14 00:14:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][0/1251]	eta 0:51:54 lr 0.000108	time 2.4897 (2.4897)	loss 3.2625 (3.2625)	grad_norm 3.4179 (3.4179)	mem 23874MB
[2022-11-14 00:14:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][50/1251]	eta 0:15:49 lr 0.000107	time 0.7356 (0.7903)	loss 2.0389 (2.6972)	grad_norm 2.8060 (2.4875)	mem 23874MB
[2022-11-14 00:15:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][100/1251]	eta 0:14:46 lr 0.000107	time 0.7424 (0.7703)	loss 1.9177 (2.7094)	grad_norm 2.2957 (2.5271)	mem 23874MB
[2022-11-14 00:15:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][150/1251]	eta 0:14:01 lr 0.000107	time 0.7402 (0.7639)	loss 2.9515 (2.7344)	grad_norm 2.7338 (2.5776)	mem 23874MB
[2022-11-14 00:16:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][200/1251]	eta 0:13:19 lr 0.000107	time 0.7436 (0.7604)	loss 2.0182 (2.7340)	grad_norm 2.2203 (2.5597)	mem 23874MB
[2022-11-14 00:17:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][250/1251]	eta 0:12:39 lr 0.000107	time 0.8064 (0.7589)	loss 2.5302 (2.7384)	grad_norm 2.4215 (2.5672)	mem 23874MB
[2022-11-14 00:17:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][300/1251]	eta 0:12:00 lr 0.000107	time 0.7379 (0.7573)	loss 2.8432 (2.7419)	grad_norm 3.8856 (2.5488)	mem 23874MB
[2022-11-14 00:18:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][350/1251]	eta 0:11:21 lr 0.000107	time 0.7490 (0.7565)	loss 2.1819 (2.7521)	grad_norm 2.1368 (2.5322)	mem 23874MB
[2022-11-14 00:19:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][400/1251]	eta 0:10:43 lr 0.000107	time 0.7398 (0.7558)	loss 3.3339 (2.7657)	grad_norm 2.6268 (2.5922)	mem 23874MB
[2022-11-14 00:19:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][450/1251]	eta 0:10:05 lr 0.000106	time 0.7489 (0.7558)	loss 2.5588 (2.7683)	grad_norm 2.2404 (2.5788)	mem 23874MB
[2022-11-14 00:20:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][500/1251]	eta 0:09:26 lr 0.000106	time 0.7384 (0.7549)	loss 2.8853 (2.7676)	grad_norm 1.9395 (2.5718)	mem 23874MB
[2022-11-14 00:20:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][550/1251]	eta 0:08:49 lr 0.000106	time 0.7444 (0.7548)	loss 2.3195 (2.7691)	grad_norm 2.1280 (2.5667)	mem 23874MB
[2022-11-14 00:21:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][600/1251]	eta 0:08:11 lr 0.000106	time 0.7504 (0.7544)	loss 2.8617 (2.7748)	grad_norm 3.4526 (2.5663)	mem 23874MB
[2022-11-14 00:22:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][650/1251]	eta 0:07:33 lr 0.000106	time 0.7394 (0.7544)	loss 2.9773 (2.7688)	grad_norm 2.6075 (2.5689)	mem 23874MB
[2022-11-14 00:22:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][700/1251]	eta 0:06:55 lr 0.000106	time 0.7465 (0.7542)	loss 3.0274 (2.7633)	grad_norm 2.4293 (2.5583)	mem 23874MB
[2022-11-14 00:23:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][750/1251]	eta 0:06:17 lr 0.000106	time 0.7346 (0.7540)	loss 2.1227 (2.7566)	grad_norm 3.6990 (2.5604)	mem 23874MB
[2022-11-14 00:24:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][800/1251]	eta 0:05:39 lr 0.000106	time 0.7418 (0.7538)	loss 3.0431 (2.7568)	grad_norm 2.1449 (2.5572)	mem 23874MB
[2022-11-14 00:24:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][850/1251]	eta 0:05:02 lr 0.000106	time 0.7464 (0.7538)	loss 2.9714 (2.7547)	grad_norm 2.6129 (2.5597)	mem 23874MB
[2022-11-14 00:25:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][900/1251]	eta 0:04:24 lr 0.000105	time 0.7449 (0.7535)	loss 2.2204 (2.7525)	grad_norm 2.5346 (2.5554)	mem 23874MB
[2022-11-14 00:25:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][950/1251]	eta 0:03:46 lr 0.000105	time 0.7424 (0.7537)	loss 3.1128 (2.7569)	grad_norm 2.3611 (2.5563)	mem 23874MB
[2022-11-14 00:26:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][1000/1251]	eta 0:03:09 lr 0.000105	time 0.7497 (0.7535)	loss 2.6285 (2.7572)	grad_norm 7.0403 (2.5613)	mem 23874MB
[2022-11-14 00:27:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][1050/1251]	eta 0:02:31 lr 0.000105	time 0.7388 (0.7535)	loss 2.7441 (2.7562)	grad_norm 2.1277 (2.5701)	mem 23874MB
[2022-11-14 00:27:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][1100/1251]	eta 0:01:53 lr 0.000105	time 0.7414 (0.7533)	loss 3.2729 (2.7536)	grad_norm 3.1182 (2.5682)	mem 23874MB
[2022-11-14 00:28:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][1150/1251]	eta 0:01:16 lr 0.000105	time 0.7391 (0.7532)	loss 3.0153 (2.7512)	grad_norm 2.2714 (2.5705)	mem 23874MB
[2022-11-14 00:29:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][1200/1251]	eta 0:00:38 lr 0.000105	time 0.7528 (0.7532)	loss 2.1185 (2.7512)	grad_norm 2.1419 (2.5625)	mem 23874MB
[2022-11-14 00:29:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [239/300][1250/1251]	eta 0:00:00 lr 0.000105	time 0.7385 (0.7530)	loss 2.6796 (2.7532)	grad_norm 1.9344 (2.5704)	mem 23874MB
[2022-11-14 00:29:40 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 239 training takes 0:15:42
[2022-11-14 00:29:41 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_239.pth saving......
[2022-11-14 00:29:42 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_239.pth saved !!!
[2022-11-14 00:29:44 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.940 (1.940)	Loss 0.7625 (0.7625)	Acc@1 82.031 (82.031)	Acc@5 95.996 (95.996)	Mem 23874MB
[2022-11-14 00:29:54 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.754 Acc@5 96.292
[2022-11-14 00:29:54 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.8%
[2022-11-14 00:29:56 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.972 (1.972)	Loss 0.7197 (0.7197)	Acc@1 82.812 (82.812)	Acc@5 96.582 (96.582)	Mem 23874MB
[2022-11-14 00:30:07 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.646 Acc@5 96.716
[2022-11-14 00:30:07 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.6%
[2022-11-14 00:30:07 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.67% at 237 epoch
[2022-11-14 00:30:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][0/1251]	eta 0:51:40 lr 0.000105	time 2.4787 (2.4787)	loss 3.1202 (3.1202)	grad_norm 2.1479 (2.1479)	mem 23874MB
[2022-11-14 00:30:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][50/1251]	eta 0:15:45 lr 0.000104	time 0.7451 (0.7874)	loss 3.0065 (2.8218)	grad_norm 2.3859 (2.7683)	mem 23874MB
[2022-11-14 00:31:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][100/1251]	eta 0:14:46 lr 0.000104	time 0.7419 (0.7704)	loss 2.9393 (2.8149)	grad_norm 2.2442 (2.7246)	mem 23874MB
[2022-11-14 00:32:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][150/1251]	eta 0:14:00 lr 0.000104	time 0.7506 (0.7635)	loss 2.7283 (2.7985)	grad_norm 2.6388 (2.6889)	mem 23874MB
[2022-11-14 00:32:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][200/1251]	eta 0:13:18 lr 0.000104	time 0.7406 (0.7600)	loss 3.1970 (2.7727)	grad_norm 2.6910 (2.6687)	mem 23874MB
[2022-11-14 00:33:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][250/1251]	eta 0:12:39 lr 0.000104	time 0.7480 (0.7590)	loss 2.7444 (2.7638)	grad_norm 2.6278 (2.6518)	mem 23874MB
[2022-11-14 00:33:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][300/1251]	eta 0:12:00 lr 0.000104	time 0.8250 (0.7577)	loss 2.8477 (2.7669)	grad_norm 2.6391 (2.6531)	mem 23874MB
[2022-11-14 00:34:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][350/1251]	eta 0:11:21 lr 0.000104	time 0.7423 (0.7569)	loss 2.9918 (2.7543)	grad_norm 2.4565 (2.6394)	mem 23874MB
[2022-11-14 00:35:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][400/1251]	eta 0:10:43 lr 0.000104	time 0.7528 (0.7560)	loss 3.1159 (2.7578)	grad_norm 2.4861 (2.6353)	mem 23874MB
[2022-11-14 00:35:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][450/1251]	eta 0:10:05 lr 0.000103	time 0.7442 (0.7556)	loss 3.1603 (2.7607)	grad_norm 2.4937 (2.6193)	mem 23874MB
[2022-11-14 00:36:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][500/1251]	eta 0:09:26 lr 0.000103	time 0.7443 (0.7547)	loss 2.6087 (2.7420)	grad_norm 2.6345 (2.6321)	mem 23874MB
[2022-11-14 00:37:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][550/1251]	eta 0:08:48 lr 0.000103	time 0.7411 (0.7544)	loss 3.0547 (2.7406)	grad_norm 4.6071 (2.6306)	mem 23874MB
[2022-11-14 00:37:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][600/1251]	eta 0:08:10 lr 0.000103	time 0.7487 (0.7540)	loss 2.8782 (2.7426)	grad_norm 5.1551 (2.6174)	mem 23874MB
[2022-11-14 00:38:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][650/1251]	eta 0:07:32 lr 0.000103	time 0.7389 (0.7537)	loss 2.6880 (2.7440)	grad_norm 2.3203 (2.6179)	mem 23874MB
[2022-11-14 00:38:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][700/1251]	eta 0:06:55 lr 0.000103	time 0.8325 (0.7535)	loss 2.8791 (2.7481)	grad_norm 2.4266 (2.6011)	mem 23874MB
[2022-11-14 00:39:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][750/1251]	eta 0:06:17 lr 0.000103	time 0.7440 (0.7533)	loss 2.8827 (2.7489)	grad_norm 2.1343 (2.5978)	mem 23874MB
[2022-11-14 00:40:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][800/1251]	eta 0:05:39 lr 0.000103	time 0.7408 (0.7529)	loss 2.1535 (2.7463)	grad_norm 2.4145 (2.5894)	mem 23874MB
[2022-11-14 00:40:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][850/1251]	eta 0:05:01 lr 0.000102	time 0.7438 (0.7528)	loss 3.3662 (2.7372)	grad_norm 2.2891 (2.5815)	mem 23874MB
[2022-11-14 00:41:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][900/1251]	eta 0:04:24 lr 0.000102	time 0.7414 (0.7528)	loss 2.4677 (2.7356)	grad_norm 2.2447 (2.5798)	mem 23874MB
[2022-11-14 00:42:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][950/1251]	eta 0:03:46 lr 0.000102	time 0.7499 (0.7525)	loss 2.6543 (2.7390)	grad_norm 2.4962 (2.5794)	mem 23874MB
[2022-11-14 00:42:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][1000/1251]	eta 0:03:08 lr 0.000102	time 0.7426 (0.7523)	loss 1.9687 (2.7415)	grad_norm 2.9624 (2.5809)	mem 23874MB
[2022-11-14 00:43:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][1050/1251]	eta 0:02:31 lr 0.000102	time 0.7432 (0.7523)	loss 2.3786 (2.7409)	grad_norm 2.4439 (2.5785)	mem 23874MB
[2022-11-14 00:43:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][1100/1251]	eta 0:01:53 lr 0.000102	time 0.8236 (0.7524)	loss 2.7597 (2.7406)	grad_norm 2.9618 (2.5775)	mem 23874MB
[2022-11-14 00:44:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][1150/1251]	eta 0:01:15 lr 0.000102	time 0.7440 (0.7521)	loss 3.1878 (2.7424)	grad_norm 2.6759 (2.5772)	mem 23874MB
[2022-11-14 00:45:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][1200/1251]	eta 0:00:38 lr 0.000102	time 0.7458 (0.7521)	loss 3.0051 (2.7451)	grad_norm 2.5081 (2.5737)	mem 23874MB
[2022-11-14 00:45:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [240/300][1250/1251]	eta 0:00:00 lr 0.000102	time 0.7298 (0.7518)	loss 2.6932 (2.7463)	grad_norm 2.1350 (2.5708)	mem 23874MB
[2022-11-14 00:45:48 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 240 training takes 0:15:40
[2022-11-14 00:45:48 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_240.pth saving......
[2022-11-14 00:45:49 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_240.pth saved !!!
[2022-11-14 00:45:51 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.683 (1.683)	Loss 0.7155 (0.7155)	Acc@1 84.375 (84.375)	Acc@5 95.996 (95.996)	Mem 23874MB
[2022-11-14 00:46:01 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.948 Acc@5 96.382
[2022-11-14 00:46:01 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.9%
[2022-11-14 00:46:03 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.943 (1.943)	Loss 0.6884 (0.6884)	Acc@1 83.789 (83.789)	Acc@5 96.289 (96.289)	Mem 23874MB
[2022-11-14 00:46:14 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.666 Acc@5 96.718
[2022-11-14 00:46:14 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.7%
[2022-11-14 00:46:14 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.67% at 240 epoch
[2022-11-14 00:46:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][0/1251]	eta 0:53:14 lr 0.000102	time 2.5539 (2.5539)	loss 2.9628 (2.9628)	grad_norm 2.4316 (2.4316)	mem 23874MB
[2022-11-14 00:46:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][50/1251]	eta 0:15:44 lr 0.000101	time 0.7459 (0.7862)	loss 2.7272 (2.7661)	grad_norm 2.7720 (2.5414)	mem 23874MB
[2022-11-14 00:47:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][100/1251]	eta 0:14:47 lr 0.000101	time 0.7438 (0.7707)	loss 3.0403 (2.7722)	grad_norm 2.7185 (2.6060)	mem 23874MB
[2022-11-14 00:48:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][150/1251]	eta 0:14:00 lr 0.000101	time 0.8140 (0.7638)	loss 2.9738 (2.7731)	grad_norm 2.6872 (2.5582)	mem 23874MB
[2022-11-14 00:48:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][200/1251]	eta 0:13:19 lr 0.000101	time 0.7416 (0.7608)	loss 2.9158 (2.7658)	grad_norm 3.1333 (2.5633)	mem 23874MB
[2022-11-14 00:49:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][250/1251]	eta 0:12:39 lr 0.000101	time 0.7442 (0.7585)	loss 1.7841 (2.7415)	grad_norm 2.0634 (2.5660)	mem 23874MB
[2022-11-14 00:50:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][300/1251]	eta 0:11:59 lr 0.000101	time 0.7415 (0.7569)	loss 2.0960 (2.7361)	grad_norm 2.4140 (2.5508)	mem 23874MB
[2022-11-14 00:50:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][350/1251]	eta 0:11:21 lr 0.000101	time 0.7391 (0.7561)	loss 3.0002 (2.7402)	grad_norm 2.6111 (2.5484)	mem 23874MB
[2022-11-14 00:51:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][400/1251]	eta 0:10:42 lr 0.000101	time 0.7434 (0.7555)	loss 2.5795 (2.7404)	grad_norm 2.3456 (2.5570)	mem 23874MB
[2022-11-14 00:51:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][450/1251]	eta 0:10:04 lr 0.000100	time 0.7431 (0.7549)	loss 2.8018 (2.7322)	grad_norm 2.8909 (2.5583)	mem 23874MB
[2022-11-14 00:52:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][500/1251]	eta 0:09:26 lr 0.000100	time 0.7390 (0.7545)	loss 2.2768 (2.7361)	grad_norm 2.3672 (2.5566)	mem 23874MB
[2022-11-14 00:53:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][550/1251]	eta 0:08:48 lr 0.000100	time 0.7390 (0.7538)	loss 3.0104 (2.7424)	grad_norm 2.1563 (2.5663)	mem 23874MB
[2022-11-14 00:53:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][600/1251]	eta 0:08:10 lr 0.000100	time 0.7388 (0.7534)	loss 2.9410 (2.7447)	grad_norm 2.5403 (2.5539)	mem 23874MB
[2022-11-14 00:54:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][650/1251]	eta 0:07:32 lr 0.000100	time 0.7443 (0.7532)	loss 3.0954 (2.7409)	grad_norm 2.5932 (2.5433)	mem 23874MB
[2022-11-14 00:55:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][700/1251]	eta 0:06:54 lr 0.000100	time 0.7406 (0.7529)	loss 1.9139 (2.7437)	grad_norm 2.4675 (2.5577)	mem 23874MB
[2022-11-14 00:55:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][750/1251]	eta 0:06:17 lr 0.000100	time 0.7369 (0.7529)	loss 2.6417 (2.7535)	grad_norm 2.9662 (2.5598)	mem 23874MB
[2022-11-14 00:56:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][800/1251]	eta 0:05:39 lr 0.000100	time 0.7385 (0.7526)	loss 2.6720 (2.7514)	grad_norm 3.3574 (2.5682)	mem 23874MB
[2022-11-14 00:56:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][850/1251]	eta 0:05:01 lr 0.000099	time 0.7450 (0.7525)	loss 2.9503 (2.7470)	grad_norm 2.5055 (2.5687)	mem 23874MB
[2022-11-14 00:57:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][900/1251]	eta 0:04:24 lr 0.000099	time 0.7424 (0.7522)	loss 2.8725 (2.7499)	grad_norm 2.7086 (2.5749)	mem 23874MB
[2022-11-14 00:58:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][950/1251]	eta 0:03:46 lr 0.000099	time 0.7323 (0.7522)	loss 3.0226 (2.7467)	grad_norm 2.5512 (2.5797)	mem 23874MB
[2022-11-14 00:58:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][1000/1251]	eta 0:03:08 lr 0.000099	time 0.7386 (0.7520)	loss 3.1973 (2.7483)	grad_norm 2.1576 (2.5765)	mem 23874MB
[2022-11-14 00:59:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][1050/1251]	eta 0:02:31 lr 0.000099	time 0.8368 (0.7520)	loss 3.0941 (2.7462)	grad_norm 2.4280 (2.5760)	mem 23874MB
[2022-11-14 01:00:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][1100/1251]	eta 0:01:53 lr 0.000099	time 0.7423 (0.7517)	loss 3.2214 (2.7487)	grad_norm 2.4394 (2.5850)	mem 23874MB
[2022-11-14 01:00:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][1150/1251]	eta 0:01:15 lr 0.000099	time 0.7384 (0.7517)	loss 2.9405 (2.7522)	grad_norm 2.5762 (2.5899)	mem 23874MB
[2022-11-14 01:01:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][1200/1251]	eta 0:00:38 lr 0.000099	time 0.7500 (0.7518)	loss 3.0527 (2.7516)	grad_norm 2.4277 (2.5848)	mem 23874MB
[2022-11-14 01:01:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [241/300][1250/1251]	eta 0:00:00 lr 0.000099	time 0.7305 (0.7514)	loss 2.9201 (2.7507)	grad_norm 2.9622 (2.5883)	mem 23874MB
[2022-11-14 01:01:54 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 241 training takes 0:15:40
[2022-11-14 01:01:54 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_241.pth saving......
[2022-11-14 01:01:55 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_241.pth saved !!!
[2022-11-14 01:01:57 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.853 (1.853)	Loss 0.7328 (0.7328)	Acc@1 83.691 (83.691)	Acc@5 95.410 (95.410)	Mem 23874MB
[2022-11-14 01:02:08 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.014 Acc@5 96.442
[2022-11-14 01:02:08 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.0%
[2022-11-14 01:02:10 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.023 (2.023)	Loss 0.6843 (0.6843)	Acc@1 84.668 (84.668)	Acc@5 97.168 (97.168)	Mem 23874MB
[2022-11-14 01:02:21 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.696 Acc@5 96.678
[2022-11-14 01:02:21 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.7%
[2022-11-14 01:02:21 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.70% at 241 epoch
[2022-11-14 01:02:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][0/1251]	eta 0:52:59 lr 0.000099	time 2.5417 (2.5417)	loss 2.8223 (2.8223)	grad_norm 2.0270 (2.0270)	mem 23874MB
[2022-11-14 01:03:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][50/1251]	eta 0:15:42 lr 0.000098	time 0.7395 (0.7846)	loss 2.8038 (2.7117)	grad_norm 2.4962 (2.4364)	mem 23874MB
[2022-11-14 01:03:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][100/1251]	eta 0:14:44 lr 0.000098	time 0.7468 (0.7683)	loss 1.9638 (2.7278)	grad_norm 2.5067 (2.5164)	mem 23874MB
[2022-11-14 01:04:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][150/1251]	eta 0:14:01 lr 0.000098	time 0.7423 (0.7642)	loss 3.0817 (2.7455)	grad_norm 2.2226 (2.5142)	mem 23874MB
[2022-11-14 01:04:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][200/1251]	eta 0:13:19 lr 0.000098	time 0.7452 (0.7603)	loss 3.2114 (2.7623)	grad_norm 2.3190 (2.5377)	mem 23874MB
[2022-11-14 01:05:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][250/1251]	eta 0:12:39 lr 0.000098	time 0.7436 (0.7592)	loss 3.1987 (2.7941)	grad_norm 2.7791 (2.5324)	mem 23874MB
[2022-11-14 01:06:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][300/1251]	eta 0:12:00 lr 0.000098	time 0.8231 (0.7574)	loss 2.7454 (2.7877)	grad_norm 2.1868 (2.5389)	mem 23874MB
[2022-11-14 01:06:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][350/1251]	eta 0:11:21 lr 0.000098	time 0.7500 (0.7569)	loss 1.7574 (2.7872)	grad_norm 2.3619 (2.5397)	mem 23874MB
[2022-11-14 01:07:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][400/1251]	eta 0:10:43 lr 0.000098	time 0.7370 (0.7562)	loss 3.0662 (2.7830)	grad_norm 2.3421 (2.5611)	mem 23874MB
[2022-11-14 01:08:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][450/1251]	eta 0:10:05 lr 0.000097	time 0.7440 (0.7555)	loss 2.9878 (2.7768)	grad_norm 2.3261 (inf)	mem 23874MB
[2022-11-14 01:08:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][500/1251]	eta 0:09:27 lr 0.000097	time 0.7491 (0.7553)	loss 3.1643 (2.7840)	grad_norm 2.0128 (inf)	mem 23874MB
[2022-11-14 01:09:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][550/1251]	eta 0:08:49 lr 0.000097	time 0.7112 (0.7548)	loss 2.0201 (2.7774)	grad_norm inf (inf)	mem 23874MB
[2022-11-14 01:09:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][600/1251]	eta 0:08:11 lr 0.000097	time 0.7503 (0.7544)	loss 3.1843 (2.7694)	grad_norm 9.1162 (inf)	mem 23874MB
[2022-11-14 01:10:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][650/1251]	eta 0:07:33 lr 0.000097	time 0.7407 (0.7542)	loss 3.1397 (2.7697)	grad_norm 2.3813 (inf)	mem 23874MB
[2022-11-14 01:11:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][700/1251]	eta 0:06:55 lr 0.000097	time 0.8133 (0.7541)	loss 3.1899 (2.7706)	grad_norm 3.1946 (inf)	mem 23874MB
[2022-11-14 01:11:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][750/1251]	eta 0:06:17 lr 0.000097	time 0.7505 (0.7539)	loss 3.1355 (2.7714)	grad_norm 2.3224 (inf)	mem 23874MB
[2022-11-14 01:12:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][800/1251]	eta 0:05:40 lr 0.000097	time 0.7444 (0.7540)	loss 2.9131 (2.7631)	grad_norm 2.4894 (inf)	mem 23874MB
[2022-11-14 01:13:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][850/1251]	eta 0:05:02 lr 0.000097	time 0.7444 (0.7537)	loss 2.9919 (2.7643)	grad_norm 2.6418 (inf)	mem 23874MB
[2022-11-14 01:13:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][900/1251]	eta 0:04:24 lr 0.000096	time 0.7480 (0.7537)	loss 2.5571 (2.7660)	grad_norm 3.7919 (inf)	mem 23874MB
[2022-11-14 01:14:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][950/1251]	eta 0:03:46 lr 0.000096	time 0.7461 (0.7536)	loss 3.2801 (2.7681)	grad_norm 2.9056 (inf)	mem 23874MB
[2022-11-14 01:14:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][1000/1251]	eta 0:03:09 lr 0.000096	time 0.7475 (0.7535)	loss 3.3450 (2.7665)	grad_norm 3.3362 (inf)	mem 23874MB
[2022-11-14 01:15:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][1050/1251]	eta 0:02:31 lr 0.000096	time 0.7395 (0.7535)	loss 2.9278 (2.7652)	grad_norm 2.4950 (inf)	mem 23874MB
[2022-11-14 01:16:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][1100/1251]	eta 0:01:53 lr 0.000096	time 0.8258 (0.7534)	loss 1.7698 (2.7621)	grad_norm 2.5435 (inf)	mem 23874MB
[2022-11-14 01:16:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][1150/1251]	eta 0:01:16 lr 0.000096	time 0.7392 (0.7532)	loss 3.2553 (2.7613)	grad_norm 2.5186 (inf)	mem 23874MB
[2022-11-14 01:17:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][1200/1251]	eta 0:00:38 lr 0.000096	time 0.7441 (0.7531)	loss 2.5532 (2.7566)	grad_norm 2.3691 (inf)	mem 23874MB
[2022-11-14 01:18:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [242/300][1250/1251]	eta 0:00:00 lr 0.000096	time 0.7289 (0.7529)	loss 3.0518 (2.7560)	grad_norm 2.2516 (inf)	mem 23874MB
[2022-11-14 01:18:03 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 242 training takes 0:15:42
[2022-11-14 01:18:03 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_242.pth saving......
[2022-11-14 01:18:04 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_242.pth saved !!!
[2022-11-14 01:18:06 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.786 (1.786)	Loss 0.6923 (0.6923)	Acc@1 85.352 (85.352)	Acc@5 96.875 (96.875)	Mem 23874MB
[2022-11-14 01:18:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.928 Acc@5 96.432
[2022-11-14 01:18:17 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 82.9%
[2022-11-14 01:18:19 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.955 (1.955)	Loss 0.7672 (0.7672)	Acc@1 81.348 (81.348)	Acc@5 95.898 (95.898)	Mem 23874MB
[2022-11-14 01:18:30 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.698 Acc@5 96.684
[2022-11-14 01:18:30 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.7%
[2022-11-14 01:18:30 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.70% at 242 epoch
[2022-11-14 01:18:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][0/1251]	eta 0:53:11 lr 0.000096	time 2.5510 (2.5510)	loss 2.4746 (2.4746)	grad_norm 2.3265 (2.3265)	mem 23874MB
[2022-11-14 01:19:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][50/1251]	eta 0:15:42 lr 0.000095	time 0.7390 (0.7846)	loss 2.6949 (2.7086)	grad_norm 2.5263 (2.4547)	mem 23874MB
[2022-11-14 01:19:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][100/1251]	eta 0:14:45 lr 0.000095	time 0.7454 (0.7692)	loss 2.4681 (2.7220)	grad_norm 2.1859 (2.5282)	mem 23874MB
[2022-11-14 01:20:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][150/1251]	eta 0:13:59 lr 0.000095	time 0.7442 (0.7626)	loss 2.6620 (2.7696)	grad_norm 2.3084 (2.6466)	mem 23874MB
[2022-11-14 01:21:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][200/1251]	eta 0:13:18 lr 0.000095	time 0.7400 (0.7602)	loss 1.8369 (2.7683)	grad_norm 2.5115 (2.6384)	mem 23874MB
[2022-11-14 01:21:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][250/1251]	eta 0:12:38 lr 0.000095	time 0.7434 (0.7581)	loss 2.3657 (2.7557)	grad_norm 2.4231 (2.6205)	mem 23874MB
[2022-11-14 01:22:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][300/1251]	eta 0:11:59 lr 0.000095	time 0.7536 (0.7568)	loss 3.2733 (2.7556)	grad_norm 3.6923 (2.6664)	mem 23874MB
[2022-11-14 01:22:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][350/1251]	eta 0:11:21 lr 0.000095	time 0.7442 (0.7560)	loss 3.1629 (2.7577)	grad_norm 2.5368 (2.6622)	mem 23874MB
[2022-11-14 01:23:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][400/1251]	eta 0:10:42 lr 0.000095	time 0.7487 (0.7552)	loss 3.0312 (2.7626)	grad_norm 2.6426 (2.6499)	mem 23874MB
[2022-11-14 01:24:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][450/1251]	eta 0:10:04 lr 0.000095	time 0.7456 (0.7547)	loss 2.6459 (2.7627)	grad_norm 2.4178 (2.6445)	mem 23874MB
[2022-11-14 01:24:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][500/1251]	eta 0:09:26 lr 0.000094	time 0.7442 (0.7541)	loss 2.7874 (2.7603)	grad_norm 2.6071 (2.6429)	mem 23874MB
[2022-11-14 01:25:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][550/1251]	eta 0:08:48 lr 0.000094	time 0.7395 (0.7537)	loss 3.0615 (2.7564)	grad_norm 2.4925 (2.6353)	mem 23874MB
[2022-11-14 01:26:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][600/1251]	eta 0:08:10 lr 0.000094	time 0.7296 (0.7533)	loss 3.1053 (2.7599)	grad_norm 3.0983 (2.6269)	mem 23874MB
[2022-11-14 01:26:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][650/1251]	eta 0:07:32 lr 0.000094	time 0.7450 (0.7531)	loss 2.9336 (2.7597)	grad_norm 3.0930 (2.6218)	mem 23874MB
[2022-11-14 01:27:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][700/1251]	eta 0:06:54 lr 0.000094	time 0.7536 (0.7528)	loss 2.2308 (2.7547)	grad_norm 9.3383 (2.6199)	mem 23874MB
[2022-11-14 01:27:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][750/1251]	eta 0:06:17 lr 0.000094	time 0.7426 (0.7528)	loss 3.2223 (2.7522)	grad_norm 4.5622 (2.6132)	mem 23874MB
[2022-11-14 01:28:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][800/1251]	eta 0:05:39 lr 0.000094	time 0.7412 (0.7525)	loss 2.9920 (2.7483)	grad_norm 2.9250 (2.6108)	mem 23874MB
[2022-11-14 01:29:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][850/1251]	eta 0:05:01 lr 0.000094	time 0.7433 (0.7524)	loss 1.9971 (2.7448)	grad_norm 2.6908 (2.6015)	mem 23874MB
[2022-11-14 01:29:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][900/1251]	eta 0:04:23 lr 0.000094	time 0.7451 (0.7521)	loss 2.6540 (2.7435)	grad_norm 2.6091 (2.6080)	mem 23874MB
[2022-11-14 01:30:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][950/1251]	eta 0:03:46 lr 0.000093	time 0.7390 (0.7521)	loss 2.9790 (2.7411)	grad_norm 2.3441 (2.6014)	mem 23874MB
[2022-11-14 01:31:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][1000/1251]	eta 0:03:08 lr 0.000093	time 0.7399 (0.7520)	loss 2.7471 (2.7376)	grad_norm 2.0901 (2.5949)	mem 23874MB
[2022-11-14 01:31:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][1050/1251]	eta 0:02:31 lr 0.000093	time 0.7467 (0.7520)	loss 2.1838 (2.7384)	grad_norm 2.5088 (2.5976)	mem 23874MB
[2022-11-14 01:32:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][1100/1251]	eta 0:01:53 lr 0.000093	time 0.7492 (0.7519)	loss 2.8629 (2.7410)	grad_norm 2.1456 (2.5970)	mem 23874MB
[2022-11-14 01:32:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][1150/1251]	eta 0:01:15 lr 0.000093	time 0.7374 (0.7519)	loss 3.0736 (2.7464)	grad_norm 2.7633 (2.6055)	mem 23874MB
[2022-11-14 01:33:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][1200/1251]	eta 0:00:38 lr 0.000093	time 0.7395 (0.7518)	loss 2.9432 (2.7482)	grad_norm 2.8754 (2.6069)	mem 23874MB
[2022-11-14 01:34:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [243/300][1250/1251]	eta 0:00:00 lr 0.000093	time 0.7288 (0.7517)	loss 1.9211 (2.7419)	grad_norm 2.2128 (2.6040)	mem 23874MB
[2022-11-14 01:34:10 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 243 training takes 0:15:40
[2022-11-14 01:34:10 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_243.pth saving......
[2022-11-14 01:34:11 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_243.pth saved !!!
[2022-11-14 01:34:13 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.798 (1.798)	Loss 0.6841 (0.6841)	Acc@1 82.910 (82.910)	Acc@5 96.777 (96.777)	Mem 23874MB
[2022-11-14 01:34:24 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.964 Acc@5 96.392
[2022-11-14 01:34:24 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.0%
[2022-11-14 01:34:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.892 (1.892)	Loss 0.7507 (0.7507)	Acc@1 82.422 (82.422)	Acc@5 96.094 (96.094)	Mem 23874MB
[2022-11-14 01:34:37 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.722 Acc@5 96.670
[2022-11-14 01:34:37 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.7%
[2022-11-14 01:34:37 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.72% at 243 epoch
[2022-11-14 01:34:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][0/1251]	eta 0:54:36 lr 0.000093	time 2.6193 (2.6193)	loss 1.8838 (1.8838)	grad_norm 2.7445 (2.7445)	mem 23874MB
[2022-11-14 01:35:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][50/1251]	eta 0:15:48 lr 0.000093	time 0.7372 (0.7900)	loss 2.1211 (2.7870)	grad_norm 2.0950 (2.9061)	mem 23874MB
[2022-11-14 01:35:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][100/1251]	eta 0:14:44 lr 0.000092	time 0.7437 (0.7681)	loss 3.3092 (2.7137)	grad_norm 2.2332 (2.6861)	mem 23874MB
[2022-11-14 01:36:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][150/1251]	eta 0:14:00 lr 0.000092	time 0.7375 (0.7633)	loss 2.9093 (2.7199)	grad_norm 2.3264 (2.6328)	mem 23874MB
[2022-11-14 01:37:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][200/1251]	eta 0:13:18 lr 0.000092	time 0.7443 (0.7600)	loss 2.6419 (2.7134)	grad_norm 2.2159 (2.6313)	mem 23874MB
[2022-11-14 01:37:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][250/1251]	eta 0:12:38 lr 0.000092	time 0.7382 (0.7580)	loss 2.9655 (2.7312)	grad_norm 2.1667 (2.5993)	mem 23874MB
[2022-11-14 01:38:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][300/1251]	eta 0:11:59 lr 0.000092	time 0.7503 (0.7567)	loss 3.1283 (2.7218)	grad_norm 6.3777 (2.5946)	mem 23874MB
[2022-11-14 01:39:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][350/1251]	eta 0:11:21 lr 0.000092	time 0.7427 (0.7559)	loss 2.5328 (2.7265)	grad_norm 3.0549 (2.6354)	mem 23874MB
[2022-11-14 01:39:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][400/1251]	eta 0:10:42 lr 0.000092	time 0.7423 (0.7548)	loss 3.1240 (2.7556)	grad_norm 2.6071 (2.6308)	mem 23874MB
[2022-11-14 01:40:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][450/1251]	eta 0:10:04 lr 0.000092	time 0.7412 (0.7546)	loss 2.3337 (2.7554)	grad_norm 2.5259 (2.6253)	mem 23874MB
[2022-11-14 01:40:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][500/1251]	eta 0:09:26 lr 0.000092	time 0.7407 (0.7537)	loss 3.2510 (2.7608)	grad_norm 2.8047 (2.6319)	mem 23874MB
[2022-11-14 01:41:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][550/1251]	eta 0:08:48 lr 0.000091	time 0.7405 (0.7538)	loss 3.1366 (2.7696)	grad_norm 2.1185 (2.6282)	mem 23874MB
[2022-11-14 01:42:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][600/1251]	eta 0:08:10 lr 0.000091	time 0.7451 (0.7537)	loss 3.1127 (2.7561)	grad_norm 2.4622 (2.6360)	mem 23874MB
[2022-11-14 01:42:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][650/1251]	eta 0:07:32 lr 0.000091	time 0.7413 (0.7533)	loss 2.9362 (2.7544)	grad_norm 2.5602 (2.6276)	mem 23874MB
[2022-11-14 01:43:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][700/1251]	eta 0:06:55 lr 0.000091	time 0.7453 (0.7534)	loss 2.9098 (2.7599)	grad_norm 2.6805 (2.6243)	mem 23874MB
[2022-11-14 01:44:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][750/1251]	eta 0:06:17 lr 0.000091	time 0.8246 (0.7532)	loss 2.7110 (2.7499)	grad_norm 2.1372 (2.6188)	mem 23874MB
[2022-11-14 01:44:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][800/1251]	eta 0:05:39 lr 0.000091	time 0.7409 (0.7530)	loss 2.3957 (2.7473)	grad_norm 2.3944 (2.6103)	mem 23874MB
[2022-11-14 01:45:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][850/1251]	eta 0:05:01 lr 0.000091	time 0.7399 (0.7530)	loss 2.0340 (2.7524)	grad_norm 2.8583 (2.6081)	mem 23874MB
[2022-11-14 01:45:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][900/1251]	eta 0:04:24 lr 0.000091	time 0.7442 (0.7528)	loss 2.9572 (2.7557)	grad_norm 2.4553 (2.6141)	mem 23874MB
[2022-11-14 01:46:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][950/1251]	eta 0:03:46 lr 0.000091	time 0.7386 (0.7527)	loss 2.9379 (2.7546)	grad_norm 2.2487 (2.6072)	mem 23874MB
[2022-11-14 01:47:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][1000/1251]	eta 0:03:08 lr 0.000090	time 0.7457 (0.7525)	loss 2.7226 (2.7538)	grad_norm 3.6000 (2.6081)	mem 23874MB
[2022-11-14 01:47:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][1050/1251]	eta 0:02:31 lr 0.000090	time 0.7393 (0.7524)	loss 2.8510 (2.7522)	grad_norm 2.2791 (2.6059)	mem 23874MB
[2022-11-14 01:48:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][1100/1251]	eta 0:01:53 lr 0.000090	time 0.7492 (0.7523)	loss 2.7212 (2.7500)	grad_norm 2.5534 (2.6039)	mem 23874MB
[2022-11-14 01:49:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][1150/1251]	eta 0:01:15 lr 0.000090	time 0.7427 (0.7522)	loss 2.6169 (2.7498)	grad_norm 2.3005 (2.6155)	mem 23874MB
[2022-11-14 01:49:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][1200/1251]	eta 0:00:38 lr 0.000090	time 0.7471 (0.7521)	loss 2.9530 (2.7414)	grad_norm 2.1387 (2.6171)	mem 23874MB
[2022-11-14 01:50:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [244/300][1250/1251]	eta 0:00:00 lr 0.000090	time 0.7301 (0.7520)	loss 2.9757 (2.7395)	grad_norm 4.0666 (2.6218)	mem 23874MB
[2022-11-14 01:50:18 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 244 training takes 0:15:40
[2022-11-14 01:50:18 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_244.pth saving......
[2022-11-14 01:50:19 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_244.pth saved !!!
[2022-11-14 01:50:21 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.755 (1.755)	Loss 0.6412 (0.6412)	Acc@1 82.617 (82.617)	Acc@5 98.047 (98.047)	Mem 23874MB
[2022-11-14 01:50:31 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.138 Acc@5 96.418
[2022-11-14 01:50:31 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.1%
[2022-11-14 01:50:33 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.963 (1.963)	Loss 0.6848 (0.6848)	Acc@1 83.984 (83.984)	Acc@5 96.875 (96.875)	Mem 23874MB
[2022-11-14 01:50:44 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.738 Acc@5 96.688
[2022-11-14 01:50:44 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.7%
[2022-11-14 01:50:44 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.74% at 244 epoch
[2022-11-14 01:50:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][0/1251]	eta 0:52:02 lr 0.000090	time 2.4959 (2.4959)	loss 3.1707 (3.1707)	grad_norm 2.0732 (2.0732)	mem 23874MB
[2022-11-14 01:51:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][50/1251]	eta 0:15:45 lr 0.000090	time 0.7433 (0.7870)	loss 2.9606 (2.7030)	grad_norm 2.2609 (2.7220)	mem 23874MB
[2022-11-14 01:52:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][100/1251]	eta 0:14:47 lr 0.000090	time 0.7414 (0.7712)	loss 2.6737 (2.7274)	grad_norm 2.5113 (2.7690)	mem 23874MB
[2022-11-14 01:52:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][150/1251]	eta 0:14:01 lr 0.000090	time 0.7437 (0.7640)	loss 2.8726 (2.7673)	grad_norm 2.4976 (2.6937)	mem 23874MB
[2022-11-14 01:53:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][200/1251]	eta 0:13:19 lr 0.000089	time 0.8116 (0.7606)	loss 2.8930 (2.7389)	grad_norm 2.6920 (2.7287)	mem 23874MB
[2022-11-14 01:53:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][250/1251]	eta 0:12:39 lr 0.000089	time 0.7560 (0.7587)	loss 2.9917 (2.7530)	grad_norm 5.5657 (2.7531)	mem 23874MB
[2022-11-14 01:54:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][300/1251]	eta 0:12:00 lr 0.000089	time 0.7554 (0.7580)	loss 3.2475 (2.7400)	grad_norm 3.1340 (2.7279)	mem 23874MB
[2022-11-14 01:55:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][350/1251]	eta 0:11:21 lr 0.000089	time 0.7408 (0.7567)	loss 3.1755 (2.7514)	grad_norm 2.8292 (2.7126)	mem 23874MB
[2022-11-14 01:55:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][400/1251]	eta 0:10:43 lr 0.000089	time 0.8320 (0.7561)	loss 3.0916 (2.7435)	grad_norm 3.0160 (2.6945)	mem 23874MB
[2022-11-14 01:56:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][450/1251]	eta 0:10:05 lr 0.000089	time 0.7457 (0.7555)	loss 2.4898 (2.7361)	grad_norm 2.7403 (2.6756)	mem 23874MB
[2022-11-14 01:57:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][500/1251]	eta 0:09:26 lr 0.000089	time 0.7403 (0.7550)	loss 1.6937 (2.7315)	grad_norm 2.5002 (2.6815)	mem 23874MB
[2022-11-14 01:57:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][550/1251]	eta 0:08:49 lr 0.000089	time 0.7429 (0.7548)	loss 1.9977 (2.7293)	grad_norm 2.6760 (2.6979)	mem 23874MB
[2022-11-14 01:58:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][600/1251]	eta 0:08:11 lr 0.000089	time 0.7420 (0.7544)	loss 3.4260 (2.7297)	grad_norm 2.7588 (2.6947)	mem 23874MB
[2022-11-14 01:58:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][650/1251]	eta 0:07:33 lr 0.000088	time 0.7475 (0.7542)	loss 2.8917 (2.7355)	grad_norm 2.6626 (2.6836)	mem 23874MB
[2022-11-14 01:59:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][700/1251]	eta 0:06:55 lr 0.000088	time 0.7432 (0.7542)	loss 2.6389 (2.7343)	grad_norm 2.4687 (2.7011)	mem 23874MB
[2022-11-14 02:00:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][750/1251]	eta 0:06:17 lr 0.000088	time 0.7400 (0.7538)	loss 3.1800 (2.7356)	grad_norm 2.1551 (2.6825)	mem 23874MB
[2022-11-14 02:00:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][800/1251]	eta 0:05:39 lr 0.000088	time 0.7448 (0.7538)	loss 1.8599 (2.7288)	grad_norm 2.4100 (2.6794)	mem 23874MB
[2022-11-14 02:01:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][850/1251]	eta 0:05:02 lr 0.000088	time 0.7443 (0.7537)	loss 3.0322 (2.7352)	grad_norm 2.4791 (2.6752)	mem 23874MB
[2022-11-14 02:02:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][900/1251]	eta 0:04:24 lr 0.000088	time 0.7393 (0.7537)	loss 2.5217 (2.7393)	grad_norm 2.2040 (2.6896)	mem 23874MB
[2022-11-14 02:02:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][950/1251]	eta 0:03:46 lr 0.000088	time 0.7397 (0.7536)	loss 3.1146 (2.7404)	grad_norm 3.7940 (2.6987)	mem 23874MB
[2022-11-14 02:03:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][1000/1251]	eta 0:03:09 lr 0.000088	time 0.7403 (0.7535)	loss 2.3470 (2.7384)	grad_norm 2.5755 (2.6995)	mem 23874MB
[2022-11-14 02:03:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][1050/1251]	eta 0:02:31 lr 0.000088	time 0.7562 (0.7535)	loss 1.8668 (2.7400)	grad_norm 2.4392 (2.6946)	mem 23874MB
[2022-11-14 02:04:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][1100/1251]	eta 0:01:53 lr 0.000087	time 0.7413 (0.7535)	loss 2.5281 (2.7405)	grad_norm 2.1090 (2.6867)	mem 23874MB
[2022-11-14 02:05:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][1150/1251]	eta 0:01:16 lr 0.000087	time 0.7414 (0.7534)	loss 2.9197 (2.7419)	grad_norm 2.5421 (inf)	mem 23874MB
[2022-11-14 02:05:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][1200/1251]	eta 0:00:38 lr 0.000087	time 0.7437 (0.7533)	loss 2.1696 (2.7405)	grad_norm 2.2000 (inf)	mem 23874MB
[2022-11-14 02:06:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [245/300][1250/1251]	eta 0:00:00 lr 0.000087	time 0.7300 (0.7530)	loss 3.1874 (2.7422)	grad_norm 2.2683 (inf)	mem 23874MB
[2022-11-14 02:06:26 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 245 training takes 0:15:42
[2022-11-14 02:06:26 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_245.pth saving......
[2022-11-14 02:06:27 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_245.pth saved !!!
[2022-11-14 02:06:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.662 (1.662)	Loss 0.6926 (0.6926)	Acc@1 83.105 (83.105)	Acc@5 97.168 (97.168)	Mem 23874MB
[2022-11-14 02:06:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 82.964 Acc@5 96.392
[2022-11-14 02:06:40 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.0%
[2022-11-14 02:06:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.835 (1.835)	Loss 0.6854 (0.6854)	Acc@1 82.715 (82.715)	Acc@5 97.168 (97.168)	Mem 23874MB
[2022-11-14 02:06:52 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.736 Acc@5 96.698
[2022-11-14 02:06:52 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.7%
[2022-11-14 02:06:52 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.74% at 244 epoch
[2022-11-14 02:06:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][0/1251]	eta 0:51:24 lr 0.000087	time 2.4657 (2.4657)	loss 2.9151 (2.9151)	grad_norm 2.4171 (2.4171)	mem 23874MB
[2022-11-14 02:07:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][50/1251]	eta 0:15:46 lr 0.000087	time 0.7465 (0.7877)	loss 2.5410 (2.8464)	grad_norm 2.6377 (2.8559)	mem 23874MB
[2022-11-14 02:08:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][100/1251]	eta 0:14:44 lr 0.000087	time 0.7369 (0.7689)	loss 2.9071 (2.7634)	grad_norm 2.4808 (2.6783)	mem 23874MB
[2022-11-14 02:08:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][150/1251]	eta 0:14:00 lr 0.000087	time 0.7372 (0.7631)	loss 2.7794 (2.7041)	grad_norm 2.5491 (2.6467)	mem 23874MB
[2022-11-14 02:09:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][200/1251]	eta 0:13:18 lr 0.000087	time 0.7434 (0.7600)	loss 1.7990 (2.7181)	grad_norm 2.1520 (2.6936)	mem 23874MB
[2022-11-14 02:10:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][250/1251]	eta 0:12:38 lr 0.000087	time 0.7498 (0.7580)	loss 2.7698 (2.7168)	grad_norm 2.3697 (2.6816)	mem 23874MB
[2022-11-14 02:10:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][300/1251]	eta 0:11:59 lr 0.000086	time 0.7418 (0.7569)	loss 2.7706 (2.7097)	grad_norm 2.0992 (2.6984)	mem 23874MB
[2022-11-14 02:11:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][350/1251]	eta 0:11:20 lr 0.000086	time 0.7479 (0.7558)	loss 2.8850 (2.7167)	grad_norm 2.2532 (2.6821)	mem 23874MB
[2022-11-14 02:11:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][400/1251]	eta 0:10:42 lr 0.000086	time 0.7393 (0.7554)	loss 2.5035 (2.7150)	grad_norm 2.1981 (2.7139)	mem 23874MB
[2022-11-14 02:12:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][450/1251]	eta 0:10:04 lr 0.000086	time 0.7438 (0.7548)	loss 2.9924 (2.7167)	grad_norm 2.4316 (2.6972)	mem 23874MB
[2022-11-14 02:13:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][500/1251]	eta 0:09:26 lr 0.000086	time 0.7479 (0.7545)	loss 2.7234 (2.7222)	grad_norm 19.4521 (2.7357)	mem 23874MB
[2022-11-14 02:13:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][550/1251]	eta 0:08:48 lr 0.000086	time 0.7398 (0.7542)	loss 2.0064 (2.7182)	grad_norm 2.2734 (2.7238)	mem 23874MB
[2022-11-14 02:14:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][600/1251]	eta 0:08:10 lr 0.000086	time 0.7434 (0.7540)	loss 2.4625 (2.7224)	grad_norm 2.0121 (2.7467)	mem 23874MB
[2022-11-14 02:15:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][650/1251]	eta 0:07:33 lr 0.000086	time 0.7664 (0.7538)	loss 2.9362 (2.7207)	grad_norm 2.7240 (2.7372)	mem 23874MB
[2022-11-14 02:15:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][700/1251]	eta 0:06:55 lr 0.000086	time 0.7377 (0.7534)	loss 2.2765 (2.7132)	grad_norm 2.4659 (2.7260)	mem 23874MB
[2022-11-14 02:16:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][750/1251]	eta 0:06:17 lr 0.000085	time 0.7436 (0.7533)	loss 2.3773 (2.7090)	grad_norm 2.3438 (2.7279)	mem 23874MB
[2022-11-14 02:16:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][800/1251]	eta 0:05:39 lr 0.000085	time 0.7471 (0.7532)	loss 2.5732 (2.7064)	grad_norm 3.0211 (2.7272)	mem 23874MB
[2022-11-14 02:17:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][850/1251]	eta 0:05:01 lr 0.000085	time 0.7426 (0.7530)	loss 2.7161 (2.7042)	grad_norm 3.7619 (2.7437)	mem 23874MB
[2022-11-14 02:18:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][900/1251]	eta 0:04:24 lr 0.000085	time 0.7456 (0.7529)	loss 3.0930 (2.7047)	grad_norm 3.3662 (2.7329)	mem 23874MB
[2022-11-14 02:18:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][950/1251]	eta 0:03:46 lr 0.000085	time 0.7464 (0.7527)	loss 3.0779 (2.7039)	grad_norm 4.0826 (2.7201)	mem 23874MB
[2022-11-14 02:19:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][1000/1251]	eta 0:03:08 lr 0.000085	time 0.7453 (0.7527)	loss 2.2029 (2.7070)	grad_norm 2.5377 (2.7142)	mem 23874MB
[2022-11-14 02:20:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][1050/1251]	eta 0:02:31 lr 0.000085	time 0.7530 (0.7525)	loss 2.8545 (2.7104)	grad_norm 2.0392 (2.7072)	mem 23874MB
[2022-11-14 02:20:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][1100/1251]	eta 0:01:53 lr 0.000085	time 0.8170 (0.7524)	loss 2.9536 (2.7067)	grad_norm 2.2544 (2.6997)	mem 23874MB
[2022-11-14 02:21:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][1150/1251]	eta 0:01:15 lr 0.000085	time 0.7395 (0.7522)	loss 2.9562 (2.7052)	grad_norm 2.9909 (2.6934)	mem 23874MB
[2022-11-14 02:21:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][1200/1251]	eta 0:00:38 lr 0.000084	time 0.7433 (0.7522)	loss 2.7703 (2.7078)	grad_norm 2.7639 (2.6892)	mem 23874MB
[2022-11-14 02:22:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [246/300][1250/1251]	eta 0:00:00 lr 0.000084	time 0.7286 (0.7519)	loss 1.8996 (2.7064)	grad_norm 2.5295 (2.6913)	mem 23874MB
[2022-11-14 02:22:33 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 246 training takes 0:15:40
[2022-11-14 02:22:33 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_246.pth saving......
[2022-11-14 02:22:35 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_246.pth saved !!!
[2022-11-14 02:22:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.717 (1.717)	Loss 0.7472 (0.7472)	Acc@1 82.227 (82.227)	Acc@5 96.191 (96.191)	Mem 23874MB
[2022-11-14 02:22:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.154 Acc@5 96.376
[2022-11-14 02:22:47 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.2%
[2022-11-14 02:22:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.987 (1.987)	Loss 0.6388 (0.6388)	Acc@1 85.547 (85.547)	Acc@5 96.973 (96.973)	Mem 23874MB
[2022-11-14 02:23:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.718 Acc@5 96.702
[2022-11-14 02:23:00 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.7%
[2022-11-14 02:23:00 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.74% at 244 epoch
[2022-11-14 02:23:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][0/1251]	eta 0:52:22 lr 0.000084	time 2.5117 (2.5117)	loss 3.2008 (3.2008)	grad_norm 1.9045 (1.9045)	mem 23874MB
[2022-11-14 02:23:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][50/1251]	eta 0:15:42 lr 0.000084	time 0.7401 (0.7848)	loss 2.1388 (2.8195)	grad_norm 2.3430 (2.6198)	mem 23874MB
[2022-11-14 02:24:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][100/1251]	eta 0:14:43 lr 0.000084	time 0.7428 (0.7675)	loss 3.0051 (2.7789)	grad_norm 2.6115 (2.6613)	mem 23874MB
[2022-11-14 02:24:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][150/1251]	eta 0:13:58 lr 0.000084	time 0.7422 (0.7620)	loss 3.0374 (2.7626)	grad_norm 2.9279 (2.6616)	mem 23874MB
[2022-11-14 02:25:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][200/1251]	eta 0:13:17 lr 0.000084	time 0.7470 (0.7586)	loss 3.1640 (2.7426)	grad_norm 3.8693 (2.6559)	mem 23874MB
[2022-11-14 02:26:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][250/1251]	eta 0:12:37 lr 0.000084	time 0.7590 (0.7571)	loss 2.2512 (2.7297)	grad_norm 2.2265 (2.6054)	mem 23874MB
[2022-11-14 02:26:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][300/1251]	eta 0:11:59 lr 0.000084	time 0.7414 (0.7562)	loss 2.8861 (2.7435)	grad_norm 2.1882 (2.6264)	mem 23874MB
[2022-11-14 02:27:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][350/1251]	eta 0:11:20 lr 0.000084	time 0.7411 (0.7555)	loss 2.3003 (2.7209)	grad_norm 2.5409 (2.6159)	mem 23874MB
[2022-11-14 02:28:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][400/1251]	eta 0:10:42 lr 0.000083	time 0.7434 (0.7550)	loss 2.2697 (2.7162)	grad_norm 2.9387 (2.6006)	mem 23874MB
[2022-11-14 02:28:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][450/1251]	eta 0:10:04 lr 0.000083	time 0.7410 (0.7546)	loss 3.3346 (2.7245)	grad_norm 2.5564 (2.5971)	mem 23874MB
[2022-11-14 02:29:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][500/1251]	eta 0:09:26 lr 0.000083	time 0.7419 (0.7540)	loss 2.8110 (2.7262)	grad_norm 3.0922 (2.6018)	mem 23874MB
[2022-11-14 02:29:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][550/1251]	eta 0:08:48 lr 0.000083	time 0.7409 (0.7536)	loss 3.1023 (2.7236)	grad_norm 2.1894 (2.6137)	mem 23874MB
[2022-11-14 02:30:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][600/1251]	eta 0:08:10 lr 0.000083	time 0.7417 (0.7532)	loss 2.6554 (2.7187)	grad_norm 3.0471 (2.6287)	mem 23874MB
[2022-11-14 02:31:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][650/1251]	eta 0:07:32 lr 0.000083	time 0.7585 (0.7531)	loss 2.8117 (2.7096)	grad_norm 2.6548 (2.6204)	mem 23874MB
[2022-11-14 02:31:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][700/1251]	eta 0:06:54 lr 0.000083	time 0.7393 (0.7528)	loss 3.2947 (2.7066)	grad_norm 3.3392 (2.6235)	mem 23874MB
[2022-11-14 02:32:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][750/1251]	eta 0:06:17 lr 0.000083	time 0.7413 (0.7526)	loss 3.1008 (2.7040)	grad_norm 2.4852 (2.6258)	mem 23874MB
[2022-11-14 02:33:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][800/1251]	eta 0:05:39 lr 0.000083	time 0.7368 (0.7525)	loss 3.2471 (2.7114)	grad_norm 2.5615 (2.6267)	mem 23874MB
[2022-11-14 02:33:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][850/1251]	eta 0:05:01 lr 0.000082	time 0.7410 (0.7525)	loss 3.0833 (2.7122)	grad_norm 2.8416 (2.6401)	mem 23874MB
[2022-11-14 02:34:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][900/1251]	eta 0:04:24 lr 0.000082	time 0.7411 (0.7523)	loss 2.9042 (2.7089)	grad_norm 2.1219 (2.6342)	mem 23874MB
[2022-11-14 02:34:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][950/1251]	eta 0:03:46 lr 0.000082	time 0.7390 (0.7523)	loss 3.1730 (2.7019)	grad_norm 2.0938 (2.6355)	mem 23874MB
[2022-11-14 02:35:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][1000/1251]	eta 0:03:08 lr 0.000082	time 0.7375 (0.7520)	loss 2.2423 (2.6992)	grad_norm 2.4599 (2.6318)	mem 23874MB
[2022-11-14 02:36:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][1050/1251]	eta 0:02:31 lr 0.000082	time 0.7541 (0.7520)	loss 2.8030 (2.6994)	grad_norm 3.0350 (2.6452)	mem 23874MB
[2022-11-14 02:36:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][1100/1251]	eta 0:01:53 lr 0.000082	time 0.7432 (0.7519)	loss 2.8850 (2.7032)	grad_norm 2.1870 (2.6481)	mem 23874MB
[2022-11-14 02:37:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][1150/1251]	eta 0:01:15 lr 0.000082	time 0.7438 (0.7519)	loss 3.1346 (2.7066)	grad_norm 2.3935 (2.6460)	mem 23874MB
[2022-11-14 02:38:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][1200/1251]	eta 0:00:38 lr 0.000082	time 0.7411 (0.7517)	loss 1.8286 (2.7034)	grad_norm 2.0977 (2.6416)	mem 23874MB
[2022-11-14 02:38:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [247/300][1250/1251]	eta 0:00:00 lr 0.000082	time 0.7294 (0.7516)	loss 2.9748 (2.7041)	grad_norm 3.0080 (2.6468)	mem 23874MB
[2022-11-14 02:38:40 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 247 training takes 0:15:40
[2022-11-14 02:38:40 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_247.pth saving......
[2022-11-14 02:38:41 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_247.pth saved !!!
[2022-11-14 02:38:43 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.717 (1.717)	Loss 0.7171 (0.7171)	Acc@1 83.105 (83.105)	Acc@5 96.875 (96.875)	Mem 23874MB
[2022-11-14 02:38:54 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.026 Acc@5 96.408
[2022-11-14 02:38:54 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.0%
[2022-11-14 02:38:56 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.936 (1.936)	Loss 0.6861 (0.6861)	Acc@1 83.691 (83.691)	Acc@5 96.777 (96.777)	Mem 23874MB
[2022-11-14 02:39:07 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.760 Acc@5 96.692
[2022-11-14 02:39:07 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.8%
[2022-11-14 02:39:07 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.76% at 247 epoch
[2022-11-14 02:39:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][0/1251]	eta 0:52:55 lr 0.000082	time 2.5386 (2.5386)	loss 2.5538 (2.5538)	grad_norm 2.2869 (2.2869)	mem 23874MB
[2022-11-14 02:39:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][50/1251]	eta 0:15:48 lr 0.000081	time 0.7410 (0.7898)	loss 1.9101 (2.7307)	grad_norm 2.5160 (2.6533)	mem 23874MB
[2022-11-14 02:40:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][100/1251]	eta 0:14:47 lr 0.000081	time 0.7452 (0.7710)	loss 2.5395 (2.7089)	grad_norm 2.3276 (2.7624)	mem 23874MB
[2022-11-14 02:41:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][150/1251]	eta 0:14:01 lr 0.000081	time 0.7388 (0.7645)	loss 3.0163 (2.7009)	grad_norm 2.0178 (inf)	mem 23874MB
[2022-11-14 02:41:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][200/1251]	eta 0:13:20 lr 0.000081	time 0.7376 (0.7616)	loss 1.9133 (2.7033)	grad_norm 2.9281 (inf)	mem 23874MB
[2022-11-14 02:42:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][250/1251]	eta 0:12:40 lr 0.000081	time 0.7405 (0.7595)	loss 2.4278 (2.6980)	grad_norm 2.6548 (inf)	mem 23874MB
[2022-11-14 02:42:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][300/1251]	eta 0:12:01 lr 0.000081	time 0.7392 (0.7582)	loss 2.7981 (2.7055)	grad_norm 1.9640 (inf)	mem 23874MB
[2022-11-14 02:43:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][350/1251]	eta 0:11:22 lr 0.000081	time 0.7447 (0.7573)	loss 2.5661 (2.7159)	grad_norm 2.5313 (inf)	mem 23874MB
[2022-11-14 02:44:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][400/1251]	eta 0:10:44 lr 0.000081	time 0.7411 (0.7570)	loss 3.0347 (2.7196)	grad_norm 2.6048 (inf)	mem 23874MB
[2022-11-14 02:44:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][450/1251]	eta 0:10:05 lr 0.000081	time 0.7356 (0.7563)	loss 1.8662 (2.7272)	grad_norm 2.9799 (inf)	mem 23874MB
[2022-11-14 02:45:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][500/1251]	eta 0:09:27 lr 0.000081	time 0.7420 (0.7559)	loss 2.0072 (2.7266)	grad_norm 2.7407 (inf)	mem 23874MB
[2022-11-14 02:46:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][550/1251]	eta 0:08:49 lr 0.000080	time 0.7455 (0.7554)	loss 2.4809 (2.7292)	grad_norm 3.7028 (inf)	mem 23874MB
[2022-11-14 02:46:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][600/1251]	eta 0:08:11 lr 0.000080	time 0.7427 (0.7550)	loss 2.5471 (2.7334)	grad_norm 3.1349 (inf)	mem 23874MB
[2022-11-14 02:47:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][650/1251]	eta 0:07:33 lr 0.000080	time 0.7562 (0.7549)	loss 3.0803 (2.7268)	grad_norm 3.3989 (inf)	mem 23874MB
[2022-11-14 02:47:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][700/1251]	eta 0:06:55 lr 0.000080	time 0.8086 (0.7547)	loss 1.8384 (2.7209)	grad_norm 2.9441 (inf)	mem 23874MB
[2022-11-14 02:48:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][750/1251]	eta 0:06:17 lr 0.000080	time 0.7464 (0.7544)	loss 1.9983 (2.7187)	grad_norm 2.1900 (inf)	mem 23874MB
[2022-11-14 02:49:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][800/1251]	eta 0:05:40 lr 0.000080	time 0.7448 (0.7542)	loss 2.3070 (2.7126)	grad_norm 2.4581 (inf)	mem 23874MB
[2022-11-14 02:49:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][850/1251]	eta 0:05:02 lr 0.000080	time 0.7455 (0.7540)	loss 3.1395 (2.7115)	grad_norm 2.6293 (inf)	mem 23874MB
[2022-11-14 02:50:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][900/1251]	eta 0:04:24 lr 0.000080	time 0.7402 (0.7539)	loss 2.5028 (2.7118)	grad_norm 2.1636 (inf)	mem 23874MB
[2022-11-14 02:51:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][950/1251]	eta 0:03:46 lr 0.000080	time 0.7317 (0.7538)	loss 2.7208 (2.7164)	grad_norm 2.0752 (inf)	mem 23874MB
[2022-11-14 02:51:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][1000/1251]	eta 0:03:09 lr 0.000079	time 0.7426 (0.7535)	loss 2.8476 (2.7177)	grad_norm 2.6443 (inf)	mem 23874MB
[2022-11-14 02:52:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][1050/1251]	eta 0:02:31 lr 0.000079	time 0.7528 (0.7534)	loss 2.6206 (2.7153)	grad_norm 2.3247 (inf)	mem 23874MB
[2022-11-14 02:52:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][1100/1251]	eta 0:01:53 lr 0.000079	time 0.8313 (0.7533)	loss 3.1087 (2.7136)	grad_norm 2.4296 (inf)	mem 23874MB
[2022-11-14 02:53:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][1150/1251]	eta 0:01:16 lr 0.000079	time 0.7423 (0.7530)	loss 3.0542 (2.7141)	grad_norm 2.6017 (inf)	mem 23874MB
[2022-11-14 02:54:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][1200/1251]	eta 0:00:38 lr 0.000079	time 0.7463 (0.7529)	loss 2.7331 (2.7135)	grad_norm 2.4845 (inf)	mem 23874MB
[2022-11-14 02:54:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [248/300][1250/1251]	eta 0:00:00 lr 0.000079	time 0.7285 (0.7528)	loss 2.5968 (2.7126)	grad_norm 2.4955 (inf)	mem 23874MB
[2022-11-14 02:54:49 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 248 training takes 0:15:41
[2022-11-14 02:54:49 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_248.pth saving......
[2022-11-14 02:54:50 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_248.pth saved !!!
[2022-11-14 02:54:52 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.729 (1.729)	Loss 0.8180 (0.8180)	Acc@1 81.250 (81.250)	Acc@5 94.629 (94.629)	Mem 23874MB
[2022-11-14 02:55:02 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.132 Acc@5 96.454
[2022-11-14 02:55:02 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.1%
[2022-11-14 02:55:04 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.981 (1.981)	Loss 0.6589 (0.6589)	Acc@1 84.473 (84.473)	Acc@5 96.777 (96.777)	Mem 23874MB
[2022-11-14 02:55:15 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.788 Acc@5 96.700
[2022-11-14 02:55:15 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.8%
[2022-11-14 02:55:15 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.79% at 248 epoch
[2022-11-14 02:55:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][0/1251]	eta 0:53:04 lr 0.000079	time 2.5457 (2.5457)	loss 3.2252 (3.2252)	grad_norm 3.0121 (3.0121)	mem 23874MB
[2022-11-14 02:55:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][50/1251]	eta 0:15:50 lr 0.000079	time 0.7403 (0.7918)	loss 2.7436 (2.7984)	grad_norm 2.6618 (2.5806)	mem 23874MB
[2022-11-14 02:56:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][100/1251]	eta 0:14:47 lr 0.000079	time 0.7482 (0.7706)	loss 2.8386 (2.7986)	grad_norm 2.3659 (2.5920)	mem 23874MB
[2022-11-14 02:57:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][150/1251]	eta 0:14:01 lr 0.000079	time 0.7394 (0.7642)	loss 2.6902 (2.7705)	grad_norm 2.3423 (2.6215)	mem 23874MB
[2022-11-14 02:57:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][200/1251]	eta 0:13:19 lr 0.000079	time 0.7422 (0.7609)	loss 2.8447 (2.7463)	grad_norm 2.1801 (inf)	mem 23874MB
[2022-11-14 02:58:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][250/1251]	eta 0:12:39 lr 0.000078	time 0.7600 (0.7592)	loss 3.0274 (2.7382)	grad_norm 2.6976 (inf)	mem 23874MB
[2022-11-14 02:59:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][300/1251]	eta 0:12:01 lr 0.000078	time 0.7259 (0.7582)	loss 2.4035 (2.7249)	grad_norm 2.8884 (inf)	mem 23874MB
[2022-11-14 02:59:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][350/1251]	eta 0:11:22 lr 0.000078	time 0.7447 (0.7570)	loss 2.2640 (2.7311)	grad_norm 2.3776 (inf)	mem 23874MB
[2022-11-14 03:00:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][400/1251]	eta 0:10:43 lr 0.000078	time 0.7415 (0.7558)	loss 2.7311 (2.7310)	grad_norm 2.4432 (inf)	mem 23874MB
[2022-11-14 03:00:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][450/1251]	eta 0:10:04 lr 0.000078	time 0.7458 (0.7552)	loss 2.6490 (2.7258)	grad_norm 2.7623 (inf)	mem 23874MB
[2022-11-14 03:01:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][500/1251]	eta 0:09:26 lr 0.000078	time 0.7502 (0.7548)	loss 1.7186 (2.7220)	grad_norm 8.2279 (inf)	mem 23874MB
[2022-11-14 03:02:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][550/1251]	eta 0:08:48 lr 0.000078	time 0.8255 (0.7546)	loss 2.5892 (2.7251)	grad_norm 2.1855 (inf)	mem 23874MB
[2022-11-14 03:02:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][600/1251]	eta 0:08:11 lr 0.000078	time 0.7422 (0.7544)	loss 2.8501 (2.7184)	grad_norm 2.2173 (inf)	mem 23874MB
[2022-11-14 03:03:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][650/1251]	eta 0:07:33 lr 0.000078	time 0.7493 (0.7538)	loss 2.8737 (2.7197)	grad_norm 2.4684 (inf)	mem 23874MB
[2022-11-14 03:04:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][700/1251]	eta 0:06:55 lr 0.000077	time 0.7404 (0.7536)	loss 2.8652 (2.7198)	grad_norm 2.4955 (inf)	mem 23874MB
[2022-11-14 03:04:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][750/1251]	eta 0:06:17 lr 0.000077	time 0.7398 (0.7536)	loss 3.1507 (2.7141)	grad_norm 2.5502 (inf)	mem 23874MB
[2022-11-14 03:05:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][800/1251]	eta 0:05:39 lr 0.000077	time 0.7417 (0.7532)	loss 3.2216 (2.7146)	grad_norm 2.7508 (inf)	mem 23874MB
[2022-11-14 03:05:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][850/1251]	eta 0:05:02 lr 0.000077	time 0.7451 (0.7532)	loss 2.9684 (2.7133)	grad_norm 3.1772 (inf)	mem 23874MB
[2022-11-14 03:06:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][900/1251]	eta 0:04:24 lr 0.000077	time 0.7604 (0.7530)	loss 2.7919 (2.7103)	grad_norm 2.3229 (inf)	mem 23874MB
[2022-11-14 03:07:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][950/1251]	eta 0:03:46 lr 0.000077	time 0.7380 (0.7528)	loss 2.9448 (2.7074)	grad_norm 2.3583 (inf)	mem 23874MB
[2022-11-14 03:07:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][1000/1251]	eta 0:03:08 lr 0.000077	time 0.7393 (0.7527)	loss 3.1302 (2.7064)	grad_norm 2.2777 (inf)	mem 23874MB
[2022-11-14 03:08:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][1050/1251]	eta 0:02:31 lr 0.000077	time 0.7411 (0.7526)	loss 2.6700 (2.7101)	grad_norm 3.3055 (inf)	mem 23874MB
[2022-11-14 03:09:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][1100/1251]	eta 0:01:53 lr 0.000077	time 0.7417 (0.7527)	loss 1.9628 (2.7075)	grad_norm 4.4174 (inf)	mem 23874MB
[2022-11-14 03:09:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][1150/1251]	eta 0:01:16 lr 0.000077	time 0.7383 (0.7525)	loss 2.7113 (2.7073)	grad_norm 2.7484 (inf)	mem 23874MB
[2022-11-14 03:10:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][1200/1251]	eta 0:00:38 lr 0.000076	time 0.7386 (0.7524)	loss 2.8039 (2.7035)	grad_norm 2.4265 (inf)	mem 23874MB
[2022-11-14 03:10:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [249/300][1250/1251]	eta 0:00:00 lr 0.000076	time 0.7346 (0.7521)	loss 2.8418 (2.7051)	grad_norm 2.4796 (inf)	mem 23874MB
[2022-11-14 03:10:56 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 249 training takes 0:15:41
[2022-11-14 03:10:56 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_249.pth saving......
[2022-11-14 03:10:57 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_249.pth saved !!!
[2022-11-14 03:10:59 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.776 (1.776)	Loss 0.6265 (0.6265)	Acc@1 84.668 (84.668)	Acc@5 97.949 (97.949)	Mem 23874MB
[2022-11-14 03:11:10 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.170 Acc@5 96.514
[2022-11-14 03:11:10 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.2%
[2022-11-14 03:11:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.971 (1.971)	Loss 0.6611 (0.6611)	Acc@1 84.473 (84.473)	Acc@5 97.266 (97.266)	Mem 23874MB
[2022-11-14 03:11:23 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.810 Acc@5 96.716
[2022-11-14 03:11:23 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.8%
[2022-11-14 03:11:23 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.81% at 249 epoch
[2022-11-14 03:11:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][0/1251]	eta 0:53:34 lr 0.000076	time 2.5693 (2.5693)	loss 1.8626 (1.8626)	grad_norm 2.1387 (2.1387)	mem 23874MB
[2022-11-14 03:12:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][50/1251]	eta 0:15:45 lr 0.000076	time 0.7385 (0.7873)	loss 2.6896 (2.7149)	grad_norm 2.8656 (2.7347)	mem 23874MB
[2022-11-14 03:12:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][100/1251]	eta 0:14:48 lr 0.000076	time 0.7558 (0.7717)	loss 3.0251 (2.7575)	grad_norm 5.2173 (2.7075)	mem 23874MB
[2022-11-14 03:13:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][150/1251]	eta 0:14:02 lr 0.000076	time 0.7426 (0.7656)	loss 2.9091 (2.7467)	grad_norm 2.4444 (2.7763)	mem 23874MB
[2022-11-14 03:13:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][200/1251]	eta 0:13:21 lr 0.000076	time 0.7515 (0.7622)	loss 3.1485 (2.7196)	grad_norm 2.6370 (2.7692)	mem 23874MB
[2022-11-14 03:14:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][250/1251]	eta 0:12:41 lr 0.000076	time 0.7455 (0.7603)	loss 2.4866 (2.7254)	grad_norm 2.6547 (2.7465)	mem 23874MB
[2022-11-14 03:15:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][300/1251]	eta 0:12:00 lr 0.000076	time 0.7452 (0.7580)	loss 2.8384 (2.7171)	grad_norm 2.8339 (2.7241)	mem 23874MB
[2022-11-14 03:15:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][350/1251]	eta 0:11:22 lr 0.000076	time 0.7452 (0.7575)	loss 2.3011 (2.6967)	grad_norm 3.0520 (2.7506)	mem 23874MB
[2022-11-14 03:16:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][400/1251]	eta 0:10:44 lr 0.000075	time 0.7410 (0.7570)	loss 3.2141 (2.6951)	grad_norm 2.4861 (2.7499)	mem 23874MB
[2022-11-14 03:17:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][450/1251]	eta 0:10:05 lr 0.000075	time 0.7372 (0.7565)	loss 2.9445 (2.6927)	grad_norm 2.2600 (2.7372)	mem 23874MB
[2022-11-14 03:17:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][500/1251]	eta 0:09:27 lr 0.000075	time 0.7436 (0.7559)	loss 3.2349 (2.6997)	grad_norm 1.9945 (2.7258)	mem 23874MB
[2022-11-14 03:18:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][550/1251]	eta 0:08:49 lr 0.000075	time 0.8223 (0.7553)	loss 2.9015 (2.7038)	grad_norm 2.4386 (2.7210)	mem 23874MB
[2022-11-14 03:18:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][600/1251]	eta 0:08:11 lr 0.000075	time 0.7462 (0.7550)	loss 2.3318 (2.7044)	grad_norm 2.3675 (2.7280)	mem 23874MB
[2022-11-14 03:19:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][650/1251]	eta 0:07:33 lr 0.000075	time 0.7422 (0.7551)	loss 2.5756 (2.6976)	grad_norm 2.3996 (2.7470)	mem 23874MB
[2022-11-14 03:20:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][700/1251]	eta 0:06:55 lr 0.000075	time 0.7383 (0.7546)	loss 2.8015 (2.6970)	grad_norm 2.6502 (2.7370)	mem 23874MB
[2022-11-14 03:20:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][750/1251]	eta 0:06:18 lr 0.000075	time 0.8196 (0.7545)	loss 3.1194 (2.6951)	grad_norm 2.3672 (2.7317)	mem 23874MB
[2022-11-14 03:21:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][800/1251]	eta 0:05:40 lr 0.000075	time 0.7493 (0.7542)	loss 3.0069 (2.7000)	grad_norm 2.4329 (2.7322)	mem 23874MB
[2022-11-14 03:22:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][850/1251]	eta 0:05:02 lr 0.000075	time 0.7381 (0.7539)	loss 2.9257 (2.6949)	grad_norm 2.3586 (2.7229)	mem 23874MB
[2022-11-14 03:22:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][900/1251]	eta 0:04:24 lr 0.000074	time 0.7441 (0.7537)	loss 3.0605 (2.6955)	grad_norm 2.9333 (2.7232)	mem 23874MB
[2022-11-14 03:23:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][950/1251]	eta 0:03:46 lr 0.000074	time 0.7379 (0.7537)	loss 2.5529 (2.6978)	grad_norm 2.1455 (2.7221)	mem 23874MB
[2022-11-14 03:23:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][1000/1251]	eta 0:03:09 lr 0.000074	time 0.7519 (0.7536)	loss 2.1855 (2.6958)	grad_norm 2.7663 (2.7233)	mem 23874MB
[2022-11-14 03:24:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][1050/1251]	eta 0:02:31 lr 0.000074	time 0.7450 (0.7535)	loss 3.0703 (2.6914)	grad_norm 2.4950 (2.7270)	mem 23874MB
[2022-11-14 03:25:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][1100/1251]	eta 0:01:53 lr 0.000074	time 0.7414 (0.7532)	loss 2.0128 (2.6878)	grad_norm 3.4046 (2.7318)	mem 23874MB
[2022-11-14 03:25:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][1150/1251]	eta 0:01:16 lr 0.000074	time 0.7455 (0.7531)	loss 3.0892 (2.6919)	grad_norm 2.4330 (2.7396)	mem 23874MB
[2022-11-14 03:26:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][1200/1251]	eta 0:00:38 lr 0.000074	time 0.7407 (0.7531)	loss 3.1125 (2.6916)	grad_norm 2.3870 (2.7440)	mem 23874MB
[2022-11-14 03:27:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [250/300][1250/1251]	eta 0:00:00 lr 0.000074	time 0.7313 (0.7528)	loss 3.0849 (2.6966)	grad_norm 2.9344 (2.7421)	mem 23874MB
[2022-11-14 03:27:05 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 250 training takes 0:15:41
[2022-11-14 03:27:05 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_250.pth saving......
[2022-11-14 03:27:06 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_250.pth saved !!!
[2022-11-14 03:27:08 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.672 (1.672)	Loss 0.6747 (0.6747)	Acc@1 84.961 (84.961)	Acc@5 97.070 (97.070)	Mem 23874MB
[2022-11-14 03:27:18 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.306 Acc@5 96.460
[2022-11-14 03:27:18 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.3%
[2022-11-14 03:27:20 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.933 (1.933)	Loss 0.7048 (0.7048)	Acc@1 83.398 (83.398)	Acc@5 96.582 (96.582)	Mem 23874MB
[2022-11-14 03:27:31 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.828 Acc@5 96.708
[2022-11-14 03:27:31 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.8%
[2022-11-14 03:27:31 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.83% at 250 epoch
[2022-11-14 03:27:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][0/1251]	eta 0:51:49 lr 0.000074	time 2.4857 (2.4857)	loss 3.2289 (3.2289)	grad_norm 2.3873 (2.3873)	mem 23874MB
[2022-11-14 03:28:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][50/1251]	eta 0:15:43 lr 0.000074	time 0.7458 (0.7856)	loss 2.6811 (2.6223)	grad_norm 2.5380 (2.8067)	mem 23874MB
[2022-11-14 03:28:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][100/1251]	eta 0:14:42 lr 0.000074	time 0.7514 (0.7666)	loss 2.8377 (2.6606)	grad_norm 3.2180 (2.7153)	mem 23874MB
[2022-11-14 03:29:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][150/1251]	eta 0:13:59 lr 0.000073	time 0.7418 (0.7622)	loss 2.8735 (2.6458)	grad_norm 2.2697 (2.7178)	mem 23874MB
[2022-11-14 03:30:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][200/1251]	eta 0:13:16 lr 0.000073	time 0.7451 (0.7581)	loss 2.9621 (2.6750)	grad_norm 2.2861 (2.7665)	mem 23874MB
[2022-11-14 03:30:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][250/1251]	eta 0:12:37 lr 0.000073	time 0.7406 (0.7569)	loss 2.4224 (2.6782)	grad_norm 2.7456 (2.7393)	mem 23874MB
[2022-11-14 03:31:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][300/1251]	eta 0:11:58 lr 0.000073	time 0.8161 (0.7558)	loss 2.8753 (2.6771)	grad_norm 2.4685 (2.7333)	mem 23874MB
[2022-11-14 03:31:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][350/1251]	eta 0:11:20 lr 0.000073	time 0.7400 (0.7551)	loss 2.1776 (2.6781)	grad_norm 2.3208 (2.7343)	mem 23874MB
[2022-11-14 03:32:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][400/1251]	eta 0:10:41 lr 0.000073	time 0.7388 (0.7540)	loss 2.7708 (2.6792)	grad_norm 2.3026 (2.7233)	mem 23874MB
[2022-11-14 03:33:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][450/1251]	eta 0:10:03 lr 0.000073	time 0.7423 (0.7536)	loss 2.8896 (2.6874)	grad_norm 2.6787 (2.7292)	mem 23874MB
[2022-11-14 03:33:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][500/1251]	eta 0:09:25 lr 0.000073	time 0.7491 (0.7532)	loss 2.5573 (2.6785)	grad_norm 2.3977 (2.7486)	mem 23874MB
[2022-11-14 03:34:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][550/1251]	eta 0:08:47 lr 0.000073	time 0.8282 (0.7530)	loss 2.9746 (2.6864)	grad_norm 2.4067 (2.7408)	mem 23874MB
[2022-11-14 03:35:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][600/1251]	eta 0:08:09 lr 0.000073	time 0.7404 (0.7527)	loss 2.2040 (2.6954)	grad_norm 2.1849 (2.7237)	mem 23874MB
[2022-11-14 03:35:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][650/1251]	eta 0:07:32 lr 0.000072	time 0.7420 (0.7522)	loss 2.6091 (2.7019)	grad_norm 2.5734 (2.7231)	mem 23874MB
[2022-11-14 03:36:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][700/1251]	eta 0:06:54 lr 0.000072	time 0.8168 (0.7520)	loss 3.0373 (2.6965)	grad_norm 2.4506 (2.7122)	mem 23874MB
[2022-11-14 03:36:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][750/1251]	eta 0:06:16 lr 0.000072	time 0.7428 (0.7519)	loss 3.1228 (2.6981)	grad_norm 2.1447 (2.7122)	mem 23874MB
[2022-11-14 03:37:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][800/1251]	eta 0:05:39 lr 0.000072	time 0.7389 (0.7517)	loss 2.6691 (2.7025)	grad_norm 2.4962 (2.7206)	mem 23874MB
[2022-11-14 03:38:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][850/1251]	eta 0:05:01 lr 0.000072	time 0.7324 (0.7515)	loss 1.9910 (2.7003)	grad_norm 2.5066 (2.7296)	mem 23874MB
[2022-11-14 03:38:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][900/1251]	eta 0:04:23 lr 0.000072	time 0.7379 (0.7514)	loss 1.9581 (2.6911)	grad_norm 2.5158 (2.7380)	mem 23874MB
[2022-11-14 03:39:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][950/1251]	eta 0:03:46 lr 0.000072	time 0.7398 (0.7512)	loss 2.9978 (2.6904)	grad_norm 2.0879 (2.7388)	mem 23874MB
[2022-11-14 03:40:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][1000/1251]	eta 0:03:08 lr 0.000072	time 0.7433 (0.7512)	loss 3.1209 (2.6879)	grad_norm 2.6455 (2.7696)	mem 23874MB
[2022-11-14 03:40:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][1050/1251]	eta 0:02:30 lr 0.000072	time 0.7404 (0.7511)	loss 3.0797 (2.6875)	grad_norm 2.1371 (2.7646)	mem 23874MB
[2022-11-14 03:41:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][1100/1251]	eta 0:01:53 lr 0.000072	time 0.8273 (0.7512)	loss 3.0766 (2.6898)	grad_norm 2.5414 (2.7761)	mem 23874MB
[2022-11-14 03:41:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][1150/1251]	eta 0:01:15 lr 0.000071	time 0.7413 (0.7510)	loss 2.6072 (2.6892)	grad_norm 2.3163 (2.7716)	mem 23874MB
[2022-11-14 03:42:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][1200/1251]	eta 0:00:38 lr 0.000071	time 0.7409 (0.7509)	loss 2.8053 (2.6929)	grad_norm 2.7675 (2.7815)	mem 23874MB
[2022-11-14 03:43:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [251/300][1250/1251]	eta 0:00:00 lr 0.000071	time 0.7301 (0.7506)	loss 2.6586 (2.6948)	grad_norm 3.8193 (2.7840)	mem 23874MB
[2022-11-14 03:43:10 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 251 training takes 0:15:39
[2022-11-14 03:43:10 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_251.pth saving......
[2022-11-14 03:43:12 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_251.pth saved !!!
[2022-11-14 03:43:13 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.786 (1.786)	Loss 0.6307 (0.6307)	Acc@1 85.254 (85.254)	Acc@5 97.656 (97.656)	Mem 23874MB
[2022-11-14 03:43:24 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.300 Acc@5 96.542
[2022-11-14 03:43:24 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.3%
[2022-11-14 03:43:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.986 (1.986)	Loss 0.6215 (0.6215)	Acc@1 83.984 (83.984)	Acc@5 97.656 (97.656)	Mem 23874MB
[2022-11-14 03:43:37 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.828 Acc@5 96.706
[2022-11-14 03:43:37 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.8%
[2022-11-14 03:43:37 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.83% at 250 epoch
[2022-11-14 03:43:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][0/1251]	eta 0:52:42 lr 0.000071	time 2.5276 (2.5276)	loss 3.2940 (3.2940)	grad_norm 2.6145 (2.6145)	mem 23874MB
[2022-11-14 03:44:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][50/1251]	eta 0:15:42 lr 0.000071	time 0.7438 (0.7850)	loss 2.8620 (2.7419)	grad_norm 2.3482 (2.7230)	mem 23874MB
[2022-11-14 03:44:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][100/1251]	eta 0:14:45 lr 0.000071	time 0.7570 (0.7692)	loss 2.0903 (2.7334)	grad_norm 2.9337 (2.7670)	mem 23874MB
[2022-11-14 03:45:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][150/1251]	eta 0:14:00 lr 0.000071	time 0.8269 (0.7633)	loss 2.3825 (2.7087)	grad_norm 2.4611 (2.8047)	mem 23874MB
[2022-11-14 03:46:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][200/1251]	eta 0:13:19 lr 0.000071	time 0.7460 (0.7603)	loss 2.9836 (2.7168)	grad_norm 2.4141 (2.7783)	mem 23874MB
[2022-11-14 03:46:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][250/1251]	eta 0:12:39 lr 0.000071	time 0.7358 (0.7584)	loss 2.2602 (2.7027)	grad_norm 2.8606 (2.7827)	mem 23874MB
[2022-11-14 03:47:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][300/1251]	eta 0:11:59 lr 0.000071	time 0.7424 (0.7570)	loss 2.9060 (2.6967)	grad_norm 2.4559 (2.7600)	mem 23874MB
[2022-11-14 03:48:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][350/1251]	eta 0:11:21 lr 0.000071	time 0.7421 (0.7561)	loss 2.6641 (2.6989)	grad_norm 2.3046 (2.8064)	mem 23874MB
[2022-11-14 03:48:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][400/1251]	eta 0:10:42 lr 0.000070	time 0.7405 (0.7553)	loss 2.7219 (2.6950)	grad_norm 2.3938 (2.8316)	mem 23874MB
[2022-11-14 03:49:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][450/1251]	eta 0:10:04 lr 0.000070	time 0.7391 (0.7551)	loss 2.4197 (2.6874)	grad_norm 2.1766 (2.8182)	mem 23874MB
[2022-11-14 03:49:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][500/1251]	eta 0:09:26 lr 0.000070	time 0.7444 (0.7548)	loss 3.1578 (2.6980)	grad_norm 4.9825 (2.8282)	mem 23874MB
[2022-11-14 03:50:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][550/1251]	eta 0:08:48 lr 0.000070	time 0.8346 (0.7544)	loss 2.0587 (2.6958)	grad_norm 3.1101 (2.8445)	mem 23874MB
[2022-11-14 03:51:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][600/1251]	eta 0:08:10 lr 0.000070	time 0.8085 (0.7540)	loss 3.0756 (2.6983)	grad_norm 2.1001 (2.8350)	mem 23874MB
[2022-11-14 03:51:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][650/1251]	eta 0:07:32 lr 0.000070	time 0.7484 (0.7537)	loss 2.4722 (2.7022)	grad_norm 2.6549 (2.8326)	mem 23874MB
[2022-11-14 03:52:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][700/1251]	eta 0:06:55 lr 0.000070	time 0.7509 (0.7537)	loss 2.6448 (2.7023)	grad_norm 2.6763 (2.8161)	mem 23874MB
[2022-11-14 03:53:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][750/1251]	eta 0:06:17 lr 0.000070	time 0.7348 (0.7537)	loss 2.8914 (2.7019)	grad_norm 3.2172 (inf)	mem 23874MB
[2022-11-14 03:53:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][800/1251]	eta 0:05:39 lr 0.000070	time 0.7457 (0.7534)	loss 2.8723 (2.7076)	grad_norm 2.4896 (inf)	mem 23874MB
[2022-11-14 03:54:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][850/1251]	eta 0:05:02 lr 0.000070	time 0.7401 (0.7534)	loss 2.1994 (2.7005)	grad_norm 3.1629 (inf)	mem 23874MB
[2022-11-14 03:54:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][900/1251]	eta 0:04:24 lr 0.000069	time 0.7425 (0.7530)	loss 2.8909 (2.7002)	grad_norm 3.0379 (inf)	mem 23874MB
[2022-11-14 03:55:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][950/1251]	eta 0:03:46 lr 0.000069	time 0.7435 (0.7530)	loss 3.0390 (2.7015)	grad_norm 2.5406 (inf)	mem 23874MB
[2022-11-14 03:56:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][1000/1251]	eta 0:03:08 lr 0.000069	time 0.7392 (0.7529)	loss 2.6089 (2.6970)	grad_norm 2.6270 (inf)	mem 23874MB
[2022-11-14 03:56:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][1050/1251]	eta 0:02:31 lr 0.000069	time 0.7441 (0.7529)	loss 2.4084 (2.6936)	grad_norm 2.0107 (inf)	mem 23874MB
[2022-11-14 03:57:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][1100/1251]	eta 0:01:53 lr 0.000069	time 0.7403 (0.7527)	loss 2.9716 (2.6950)	grad_norm 2.3470 (inf)	mem 23874MB
[2022-11-14 03:58:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][1150/1251]	eta 0:01:16 lr 0.000069	time 0.7360 (0.7525)	loss 2.7053 (2.6906)	grad_norm 2.1853 (inf)	mem 23874MB
[2022-11-14 03:58:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][1200/1251]	eta 0:00:38 lr 0.000069	time 0.7381 (0.7523)	loss 2.7571 (2.6929)	grad_norm 2.5623 (inf)	mem 23874MB
[2022-11-14 03:59:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [252/300][1250/1251]	eta 0:00:00 lr 0.000069	time 0.7301 (0.7522)	loss 2.9311 (2.6936)	grad_norm 2.3058 (inf)	mem 23874MB
[2022-11-14 03:59:18 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 252 training takes 0:15:41
[2022-11-14 03:59:18 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_252.pth saving......
[2022-11-14 03:59:19 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_252.pth saved !!!
[2022-11-14 03:59:21 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.701 (1.701)	Loss 0.7762 (0.7762)	Acc@1 81.738 (81.738)	Acc@5 95.898 (95.898)	Mem 23874MB
[2022-11-14 03:59:32 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.158 Acc@5 96.498
[2022-11-14 03:59:32 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.2%
[2022-11-14 03:59:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.903 (1.903)	Loss 0.7665 (0.7665)	Acc@1 81.152 (81.152)	Acc@5 95.508 (95.508)	Mem 23874MB
[2022-11-14 03:59:44 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.828 Acc@5 96.700
[2022-11-14 03:59:44 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.8%
[2022-11-14 03:59:44 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.83% at 250 epoch
[2022-11-14 03:59:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][0/1251]	eta 0:56:02 lr 0.000069	time 2.6881 (2.6881)	loss 2.8641 (2.8641)	grad_norm 3.9614 (3.9614)	mem 23874MB
[2022-11-14 04:00:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][50/1251]	eta 0:15:51 lr 0.000069	time 0.7377 (0.7922)	loss 1.7178 (2.6940)	grad_norm 2.6078 (2.7287)	mem 23874MB
[2022-11-14 04:01:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][100/1251]	eta 0:14:46 lr 0.000069	time 0.7429 (0.7706)	loss 2.4541 (2.6417)	grad_norm 2.2918 (2.7013)	mem 23874MB
[2022-11-14 04:01:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][150/1251]	eta 0:14:01 lr 0.000068	time 0.7643 (0.7645)	loss 3.1416 (2.6480)	grad_norm 2.6054 (2.6836)	mem 23874MB
[2022-11-14 04:02:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][200/1251]	eta 0:13:19 lr 0.000068	time 0.8192 (0.7604)	loss 2.8245 (2.6437)	grad_norm 2.2613 (2.6507)	mem 23874MB
[2022-11-14 04:02:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][250/1251]	eta 0:12:38 lr 0.000068	time 0.7406 (0.7579)	loss 2.7850 (2.6498)	grad_norm 2.9478 (2.7134)	mem 23874MB
[2022-11-14 04:03:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][300/1251]	eta 0:11:59 lr 0.000068	time 0.7412 (0.7569)	loss 2.7640 (2.6624)	grad_norm 2.2363 (2.7349)	mem 23874MB
[2022-11-14 04:04:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][350/1251]	eta 0:11:21 lr 0.000068	time 0.7438 (0.7563)	loss 3.0185 (2.6589)	grad_norm 3.0569 (2.7372)	mem 23874MB
[2022-11-14 04:04:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][400/1251]	eta 0:10:42 lr 0.000068	time 0.7456 (0.7552)	loss 2.7979 (2.6693)	grad_norm 2.2828 (2.7401)	mem 23874MB
[2022-11-14 04:05:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][450/1251]	eta 0:10:04 lr 0.000068	time 0.7544 (0.7551)	loss 2.9434 (2.6762)	grad_norm 6.7380 (2.7593)	mem 23874MB
[2022-11-14 04:06:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][500/1251]	eta 0:09:26 lr 0.000068	time 0.7450 (0.7544)	loss 2.4230 (2.6735)	grad_norm 2.5396 (2.7625)	mem 23874MB
[2022-11-14 04:06:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][550/1251]	eta 0:08:48 lr 0.000068	time 0.7432 (0.7544)	loss 2.9062 (2.6708)	grad_norm 3.1181 (2.7667)	mem 23874MB
[2022-11-14 04:07:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][600/1251]	eta 0:08:10 lr 0.000068	time 0.7361 (0.7539)	loss 3.0944 (2.6757)	grad_norm 2.5022 (2.7664)	mem 23874MB
[2022-11-14 04:07:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][650/1251]	eta 0:07:32 lr 0.000067	time 0.7422 (0.7537)	loss 3.1306 (2.6855)	grad_norm 2.0578 (2.7695)	mem 23874MB
[2022-11-14 04:08:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][700/1251]	eta 0:06:55 lr 0.000067	time 0.7490 (0.7533)	loss 3.0153 (2.6838)	grad_norm 2.3596 (2.7638)	mem 23874MB
[2022-11-14 04:09:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][750/1251]	eta 0:06:17 lr 0.000067	time 0.7416 (0.7532)	loss 2.6292 (2.6858)	grad_norm 2.1173 (2.7537)	mem 23874MB
[2022-11-14 04:09:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][800/1251]	eta 0:05:39 lr 0.000067	time 0.8272 (0.7529)	loss 2.9169 (2.6848)	grad_norm 2.4698 (2.7555)	mem 23874MB
[2022-11-14 04:10:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][850/1251]	eta 0:05:01 lr 0.000067	time 0.7409 (0.7530)	loss 2.9038 (2.6795)	grad_norm 2.2775 (2.7551)	mem 23874MB
[2022-11-14 04:11:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][900/1251]	eta 0:04:24 lr 0.000067	time 0.7404 (0.7527)	loss 1.6534 (2.6818)	grad_norm 2.1891 (2.7552)	mem 23874MB
[2022-11-14 04:11:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][950/1251]	eta 0:03:46 lr 0.000067	time 0.7446 (0.7525)	loss 2.3721 (2.6787)	grad_norm 2.5683 (2.7574)	mem 23874MB
[2022-11-14 04:12:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][1000/1251]	eta 0:03:08 lr 0.000067	time 0.7399 (0.7525)	loss 3.0055 (2.6759)	grad_norm 3.1442 (2.7619)	mem 23874MB
[2022-11-14 04:12:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][1050/1251]	eta 0:02:31 lr 0.000067	time 0.7442 (0.7523)	loss 2.9686 (2.6755)	grad_norm 2.6470 (2.7610)	mem 23874MB
[2022-11-14 04:13:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][1100/1251]	eta 0:01:53 lr 0.000067	time 0.7413 (0.7522)	loss 2.5671 (2.6746)	grad_norm 2.4925 (2.7566)	mem 23874MB
[2022-11-14 04:14:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][1150/1251]	eta 0:01:15 lr 0.000067	time 0.7498 (0.7522)	loss 3.0186 (2.6747)	grad_norm 2.6681 (2.7631)	mem 23874MB
[2022-11-14 04:14:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][1200/1251]	eta 0:00:38 lr 0.000066	time 0.7425 (0.7521)	loss 2.7390 (2.6755)	grad_norm 2.7578 (2.7674)	mem 23874MB
[2022-11-14 04:15:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [253/300][1250/1251]	eta 0:00:00 lr 0.000066	time 0.7279 (0.7520)	loss 2.9034 (2.6780)	grad_norm 2.5194 (2.7687)	mem 23874MB
[2022-11-14 04:15:25 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 253 training takes 0:15:40
[2022-11-14 04:15:26 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_253.pth saving......
[2022-11-14 04:15:27 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_253.pth saved !!!
[2022-11-14 04:15:28 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.683 (1.683)	Loss 0.8055 (0.8055)	Acc@1 81.836 (81.836)	Acc@5 95.703 (95.703)	Mem 23874MB
[2022-11-14 04:15:39 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.314 Acc@5 96.464
[2022-11-14 04:15:39 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.3%
[2022-11-14 04:15:41 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.995 (1.995)	Loss 0.6609 (0.6609)	Acc@1 85.547 (85.547)	Acc@5 96.582 (96.582)	Mem 23874MB
[2022-11-14 04:15:52 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.860 Acc@5 96.714
[2022-11-14 04:15:52 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 04:15:52 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.86% at 253 epoch
[2022-11-14 04:15:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][0/1251]	eta 0:52:34 lr 0.000066	time 2.5213 (2.5213)	loss 2.1150 (2.1150)	grad_norm 2.4384 (2.4384)	mem 23874MB
[2022-11-14 04:16:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][50/1251]	eta 0:15:49 lr 0.000066	time 0.7494 (0.7910)	loss 2.9880 (2.5795)	grad_norm 2.7504 (2.7702)	mem 23874MB
[2022-11-14 04:17:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][100/1251]	eta 0:14:46 lr 0.000066	time 0.7414 (0.7701)	loss 2.8757 (2.6004)	grad_norm 2.4414 (2.7256)	mem 23874MB
[2022-11-14 04:17:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][150/1251]	eta 0:14:01 lr 0.000066	time 0.7360 (0.7645)	loss 2.8461 (2.6677)	grad_norm 2.7696 (2.7500)	mem 23874MB
[2022-11-14 04:18:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][200/1251]	eta 0:13:19 lr 0.000066	time 0.7454 (0.7603)	loss 2.6666 (2.6570)	grad_norm 2.1077 (2.7295)	mem 23874MB
[2022-11-14 04:19:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][250/1251]	eta 0:12:38 lr 0.000066	time 0.7586 (0.7577)	loss 2.8296 (2.6609)	grad_norm 2.4930 (2.7316)	mem 23874MB
[2022-11-14 04:19:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][300/1251]	eta 0:12:00 lr 0.000066	time 0.7434 (0.7575)	loss 1.7148 (2.6506)	grad_norm 2.7894 (2.7460)	mem 23874MB
[2022-11-14 04:20:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][350/1251]	eta 0:11:21 lr 0.000066	time 0.7399 (0.7560)	loss 2.3167 (2.6703)	grad_norm 2.5744 (2.7514)	mem 23874MB
[2022-11-14 04:20:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][400/1251]	eta 0:10:43 lr 0.000066	time 0.7442 (0.7557)	loss 2.6339 (2.6722)	grad_norm 2.7494 (2.7776)	mem 23874MB
[2022-11-14 04:21:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][450/1251]	eta 0:10:04 lr 0.000065	time 0.7385 (0.7552)	loss 2.4702 (2.6766)	grad_norm 2.7265 (2.8320)	mem 23874MB
[2022-11-14 04:22:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][500/1251]	eta 0:09:26 lr 0.000065	time 0.7433 (0.7546)	loss 2.8199 (2.6758)	grad_norm 2.1771 (2.8408)	mem 23874MB
[2022-11-14 04:22:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][550/1251]	eta 0:08:48 lr 0.000065	time 0.7410 (0.7543)	loss 2.9404 (2.6823)	grad_norm 3.1826 (2.8337)	mem 23874MB
[2022-11-14 04:23:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][600/1251]	eta 0:08:10 lr 0.000065	time 0.8298 (0.7540)	loss 2.4992 (2.6793)	grad_norm 2.8332 (2.8383)	mem 23874MB
[2022-11-14 04:24:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][650/1251]	eta 0:07:33 lr 0.000065	time 0.7412 (0.7538)	loss 2.4742 (2.6770)	grad_norm 2.3440 (2.8245)	mem 23874MB
[2022-11-14 04:24:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][700/1251]	eta 0:06:55 lr 0.000065	time 0.7441 (0.7536)	loss 2.4610 (2.6765)	grad_norm 2.5635 (2.8212)	mem 23874MB
[2022-11-14 04:25:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][750/1251]	eta 0:06:17 lr 0.000065	time 0.7488 (0.7532)	loss 1.6117 (2.6773)	grad_norm 2.8679 (2.8268)	mem 23874MB
[2022-11-14 04:25:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][800/1251]	eta 0:05:39 lr 0.000065	time 0.7453 (0.7532)	loss 1.9473 (2.6831)	grad_norm 3.0231 (2.8288)	mem 23874MB
[2022-11-14 04:26:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][850/1251]	eta 0:05:01 lr 0.000065	time 0.7438 (0.7531)	loss 2.7052 (2.6850)	grad_norm 2.4035 (2.8193)	mem 23874MB
[2022-11-14 04:27:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][900/1251]	eta 0:04:24 lr 0.000065	time 0.7456 (0.7530)	loss 2.7158 (2.6868)	grad_norm 2.8180 (2.8242)	mem 23874MB
[2022-11-14 04:27:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][950/1251]	eta 0:03:46 lr 0.000065	time 0.7569 (0.7529)	loss 3.0700 (2.6848)	grad_norm 2.5299 (2.8208)	mem 23874MB
[2022-11-14 04:28:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][1000/1251]	eta 0:03:08 lr 0.000064	time 0.8226 (0.7528)	loss 2.0360 (2.6862)	grad_norm 2.9256 (2.8210)	mem 23874MB
[2022-11-14 04:29:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][1050/1251]	eta 0:02:31 lr 0.000064	time 0.7239 (0.7527)	loss 2.9228 (2.6868)	grad_norm 2.1693 (2.8134)	mem 23874MB
[2022-11-14 04:29:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][1100/1251]	eta 0:01:53 lr 0.000064	time 0.7502 (0.7526)	loss 2.6518 (2.6845)	grad_norm 6.3424 (2.8207)	mem 23874MB
[2022-11-14 04:30:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][1150/1251]	eta 0:01:16 lr 0.000064	time 0.7390 (0.7525)	loss 3.2743 (2.6880)	grad_norm 2.9352 (2.8232)	mem 23874MB
[2022-11-14 04:30:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][1200/1251]	eta 0:00:38 lr 0.000064	time 0.7438 (0.7525)	loss 2.5719 (2.6876)	grad_norm 2.4261 (2.8141)	mem 23874MB
[2022-11-14 04:31:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [254/300][1250/1251]	eta 0:00:00 lr 0.000064	time 0.7305 (0.7522)	loss 3.1005 (2.6884)	grad_norm 2.6686 (2.8309)	mem 23874MB
[2022-11-14 04:31:33 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 254 training takes 0:15:41
[2022-11-14 04:31:33 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_254.pth saving......
[2022-11-14 04:31:34 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_254.pth saved !!!
[2022-11-14 04:31:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.807 (1.807)	Loss 0.7102 (0.7102)	Acc@1 84.277 (84.277)	Acc@5 95.898 (95.898)	Mem 23874MB
[2022-11-14 04:31:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.246 Acc@5 96.442
[2022-11-14 04:31:47 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.2%
[2022-11-14 04:31:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.090 (2.090)	Loss 0.6572 (0.6572)	Acc@1 84.961 (84.961)	Acc@5 96.973 (96.973)	Mem 23874MB
[2022-11-14 04:32:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.878 Acc@5 96.714
[2022-11-14 04:32:00 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 04:32:00 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.88% at 254 epoch
[2022-11-14 04:32:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][0/1251]	eta 0:54:11 lr 0.000064	time 2.5991 (2.5991)	loss 1.7388 (1.7388)	grad_norm 2.4306 (2.4306)	mem 23874MB
[2022-11-14 04:32:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][50/1251]	eta 0:15:49 lr 0.000064	time 0.8492 (0.7906)	loss 3.0300 (2.6930)	grad_norm 6.4940 (2.7017)	mem 23874MB
[2022-11-14 04:33:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][100/1251]	eta 0:14:48 lr 0.000064	time 0.7434 (0.7719)	loss 2.3457 (2.7240)	grad_norm 2.4179 (2.8716)	mem 23874MB
[2022-11-14 04:33:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][150/1251]	eta 0:14:01 lr 0.000064	time 0.7467 (0.7646)	loss 3.1582 (2.6997)	grad_norm 3.1738 (2.9395)	mem 23874MB
[2022-11-14 04:34:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][200/1251]	eta 0:13:20 lr 0.000064	time 0.7434 (0.7614)	loss 3.0410 (2.7032)	grad_norm 2.5495 (2.8928)	mem 23874MB
[2022-11-14 04:35:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][250/1251]	eta 0:12:40 lr 0.000063	time 0.7404 (0.7593)	loss 1.7725 (2.6910)	grad_norm 2.2981 (2.8947)	mem 23874MB
[2022-11-14 04:35:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][300/1251]	eta 0:12:00 lr 0.000063	time 0.7434 (0.7581)	loss 2.5838 (2.6959)	grad_norm 3.1452 (2.8837)	mem 23874MB
[2022-11-14 04:36:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][350/1251]	eta 0:11:22 lr 0.000063	time 0.7427 (0.7572)	loss 2.4452 (2.6876)	grad_norm 2.9269 (2.8523)	mem 23874MB
[2022-11-14 04:37:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][400/1251]	eta 0:10:43 lr 0.000063	time 0.7429 (0.7565)	loss 2.6718 (2.6753)	grad_norm 2.1337 (2.8158)	mem 23874MB
[2022-11-14 04:37:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][450/1251]	eta 0:10:05 lr 0.000063	time 0.7367 (0.7555)	loss 2.9948 (2.6816)	grad_norm 2.3383 (2.8161)	mem 23874MB
[2022-11-14 04:38:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][500/1251]	eta 0:09:27 lr 0.000063	time 0.7406 (0.7550)	loss 3.1632 (2.6796)	grad_norm 2.4514 (2.8378)	mem 23874MB
[2022-11-14 04:38:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][550/1251]	eta 0:08:49 lr 0.000063	time 0.7426 (0.7547)	loss 2.0493 (2.6797)	grad_norm 2.6327 (2.8281)	mem 23874MB
[2022-11-14 04:39:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][600/1251]	eta 0:08:11 lr 0.000063	time 0.7394 (0.7545)	loss 2.8297 (2.6813)	grad_norm 2.3468 (2.8143)	mem 23874MB
[2022-11-14 04:40:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][650/1251]	eta 0:07:33 lr 0.000063	time 0.7460 (0.7544)	loss 2.6126 (2.6815)	grad_norm 3.0099 (2.8023)	mem 23874MB
[2022-11-14 04:40:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][700/1251]	eta 0:06:55 lr 0.000063	time 0.7428 (0.7539)	loss 1.9635 (2.6811)	grad_norm 2.4020 (2.7949)	mem 23874MB
[2022-11-14 04:41:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][750/1251]	eta 0:06:17 lr 0.000063	time 0.7472 (0.7540)	loss 2.7394 (2.6772)	grad_norm 2.1826 (2.7975)	mem 23874MB
[2022-11-14 04:42:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][800/1251]	eta 0:05:40 lr 0.000062	time 0.7467 (0.7539)	loss 2.5681 (2.6784)	grad_norm 2.3710 (2.7895)	mem 23874MB
[2022-11-14 04:42:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][850/1251]	eta 0:05:02 lr 0.000062	time 0.7398 (0.7536)	loss 2.7458 (2.6782)	grad_norm 2.3448 (2.7828)	mem 23874MB
[2022-11-14 04:43:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][900/1251]	eta 0:04:24 lr 0.000062	time 0.7384 (0.7535)	loss 2.4751 (2.6774)	grad_norm 2.2696 (2.7819)	mem 23874MB
[2022-11-14 04:43:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][950/1251]	eta 0:03:46 lr 0.000062	time 0.7426 (0.7533)	loss 2.3259 (2.6765)	grad_norm 3.2151 (2.7785)	mem 23874MB
[2022-11-14 04:44:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][1000/1251]	eta 0:03:09 lr 0.000062	time 0.7377 (0.7533)	loss 1.9502 (2.6714)	grad_norm 3.3091 (2.7792)	mem 23874MB
[2022-11-14 04:45:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][1050/1251]	eta 0:02:31 lr 0.000062	time 0.7463 (0.7532)	loss 3.1596 (2.6692)	grad_norm 3.1637 (2.7746)	mem 23874MB
[2022-11-14 04:45:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][1100/1251]	eta 0:01:53 lr 0.000062	time 0.7405 (0.7531)	loss 3.0174 (2.6739)	grad_norm 2.4795 (2.7718)	mem 23874MB
[2022-11-14 04:46:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][1150/1251]	eta 0:01:16 lr 0.000062	time 0.7455 (0.7532)	loss 2.7515 (2.6768)	grad_norm 2.3938 (2.7670)	mem 23874MB
[2022-11-14 04:47:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][1200/1251]	eta 0:00:38 lr 0.000062	time 0.7419 (0.7530)	loss 2.3851 (2.6773)	grad_norm 2.5246 (2.7599)	mem 23874MB
[2022-11-14 04:47:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [255/300][1250/1251]	eta 0:00:00 lr 0.000062	time 0.7301 (0.7528)	loss 3.0808 (2.6758)	grad_norm 2.7072 (2.7560)	mem 23874MB
[2022-11-14 04:47:41 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 255 training takes 0:15:41
[2022-11-14 04:47:42 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_255.pth saving......
[2022-11-14 04:47:43 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_255.pth saved !!!
[2022-11-14 04:47:44 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.796 (1.796)	Loss 0.7052 (0.7052)	Acc@1 82.324 (82.324)	Acc@5 96.875 (96.875)	Mem 23874MB
[2022-11-14 04:47:55 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.316 Acc@5 96.548
[2022-11-14 04:47:55 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.3%
[2022-11-14 04:47:57 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.091 (2.091)	Loss 0.6937 (0.6937)	Acc@1 84.375 (84.375)	Acc@5 96.973 (96.973)	Mem 23874MB
[2022-11-14 04:48:08 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.880 Acc@5 96.716
[2022-11-14 04:48:08 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 04:48:08 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.88% at 255 epoch
[2022-11-14 04:48:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][0/1251]	eta 0:51:24 lr 0.000062	time 2.4655 (2.4655)	loss 2.6987 (2.6987)	grad_norm 2.7590 (2.7590)	mem 23874MB
[2022-11-14 04:48:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][50/1251]	eta 0:15:45 lr 0.000062	time 0.7434 (0.7870)	loss 2.9548 (2.6561)	grad_norm 2.3103 (2.8575)	mem 23874MB
[2022-11-14 04:49:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][100/1251]	eta 0:14:43 lr 0.000061	time 0.7445 (0.7676)	loss 3.1369 (2.6315)	grad_norm 2.9076 (inf)	mem 23874MB
[2022-11-14 04:50:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][150/1251]	eta 0:13:59 lr 0.000061	time 0.7464 (0.7621)	loss 3.2532 (2.7017)	grad_norm 2.7743 (inf)	mem 23874MB
[2022-11-14 04:50:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][200/1251]	eta 0:13:18 lr 0.000061	time 0.7555 (0.7593)	loss 2.8617 (2.6935)	grad_norm 2.4799 (inf)	mem 23874MB
[2022-11-14 04:51:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][250/1251]	eta 0:12:38 lr 0.000061	time 0.7387 (0.7576)	loss 3.0529 (2.6936)	grad_norm 2.6148 (inf)	mem 23874MB
[2022-11-14 04:51:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][300/1251]	eta 0:11:59 lr 0.000061	time 0.7405 (0.7567)	loss 2.7516 (2.6830)	grad_norm 3.1973 (inf)	mem 23874MB
[2022-11-14 04:52:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][350/1251]	eta 0:11:20 lr 0.000061	time 0.7398 (0.7554)	loss 3.1888 (2.6766)	grad_norm 2.8457 (inf)	mem 23874MB
[2022-11-14 04:53:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][400/1251]	eta 0:10:42 lr 0.000061	time 0.7406 (0.7548)	loss 3.0410 (2.6864)	grad_norm 2.3895 (inf)	mem 23874MB
[2022-11-14 04:53:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][450/1251]	eta 0:10:04 lr 0.000061	time 0.8228 (0.7547)	loss 3.2049 (2.6917)	grad_norm 3.4807 (inf)	mem 23874MB
[2022-11-14 04:54:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][500/1251]	eta 0:09:26 lr 0.000061	time 0.7469 (0.7540)	loss 2.9923 (2.7003)	grad_norm 2.4598 (inf)	mem 23874MB
[2022-11-14 04:55:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][550/1251]	eta 0:08:48 lr 0.000061	time 0.7421 (0.7539)	loss 2.9297 (2.6959)	grad_norm 2.7661 (inf)	mem 23874MB
[2022-11-14 04:55:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][600/1251]	eta 0:08:10 lr 0.000061	time 0.7342 (0.7536)	loss 2.5972 (2.6908)	grad_norm 2.4066 (inf)	mem 23874MB
[2022-11-14 04:56:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][650/1251]	eta 0:07:32 lr 0.000060	time 0.7420 (0.7534)	loss 3.0194 (2.6867)	grad_norm 2.4721 (inf)	mem 23874MB
[2022-11-14 04:56:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][700/1251]	eta 0:06:54 lr 0.000060	time 0.7421 (0.7530)	loss 2.5662 (2.6824)	grad_norm 2.2217 (inf)	mem 23874MB
[2022-11-14 04:57:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][750/1251]	eta 0:06:17 lr 0.000060	time 0.7439 (0.7529)	loss 1.7282 (2.6838)	grad_norm 2.6821 (inf)	mem 23874MB
[2022-11-14 04:58:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][800/1251]	eta 0:05:39 lr 0.000060	time 0.7432 (0.7526)	loss 3.0395 (2.6767)	grad_norm 3.5612 (inf)	mem 23874MB
[2022-11-14 04:58:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][850/1251]	eta 0:05:01 lr 0.000060	time 0.7401 (0.7528)	loss 2.9418 (2.6775)	grad_norm 3.9500 (inf)	mem 23874MB
[2022-11-14 04:59:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][900/1251]	eta 0:04:24 lr 0.000060	time 0.7444 (0.7525)	loss 2.9531 (2.6800)	grad_norm 3.2858 (inf)	mem 23874MB
[2022-11-14 05:00:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][950/1251]	eta 0:03:46 lr 0.000060	time 0.7517 (0.7524)	loss 3.0462 (2.6788)	grad_norm 2.3957 (inf)	mem 23874MB
[2022-11-14 05:00:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][1000/1251]	eta 0:03:08 lr 0.000060	time 0.7412 (0.7523)	loss 2.9500 (2.6803)	grad_norm 3.6176 (inf)	mem 23874MB
[2022-11-14 05:01:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][1050/1251]	eta 0:02:31 lr 0.000060	time 0.7382 (0.7522)	loss 1.7657 (2.6828)	grad_norm 3.3027 (inf)	mem 23874MB
[2022-11-14 05:01:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][1100/1251]	eta 0:01:53 lr 0.000060	time 0.7463 (0.7522)	loss 2.7254 (2.6873)	grad_norm 2.2145 (inf)	mem 23874MB
[2022-11-14 05:02:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][1150/1251]	eta 0:01:15 lr 0.000060	time 0.7473 (0.7520)	loss 2.9905 (2.6906)	grad_norm 2.7566 (inf)	mem 23874MB
[2022-11-14 05:03:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][1200/1251]	eta 0:00:38 lr 0.000059	time 0.7424 (0.7518)	loss 2.5903 (2.6916)	grad_norm 2.5855 (inf)	mem 23874MB
[2022-11-14 05:03:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [256/300][1250/1251]	eta 0:00:00 lr 0.000059	time 0.7318 (0.7518)	loss 2.9771 (2.6903)	grad_norm 2.8961 (inf)	mem 23874MB
[2022-11-14 05:03:49 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 256 training takes 0:15:40
[2022-11-14 05:03:49 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_256.pth saving......
[2022-11-14 05:03:50 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_256.pth saved !!!
[2022-11-14 05:03:52 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.707 (1.707)	Loss 0.7172 (0.7172)	Acc@1 82.910 (82.910)	Acc@5 96.973 (96.973)	Mem 23874MB
[2022-11-14 05:04:03 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.388 Acc@5 96.552
[2022-11-14 05:04:03 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.4%
[2022-11-14 05:04:04 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.867 (1.867)	Loss 0.6078 (0.6078)	Acc@1 86.719 (86.719)	Acc@5 97.461 (97.461)	Mem 23874MB
[2022-11-14 05:04:15 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.886 Acc@5 96.696
[2022-11-14 05:04:15 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 05:04:15 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.89% at 256 epoch
[2022-11-14 05:04:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][0/1251]	eta 0:52:21 lr 0.000059	time 2.5108 (2.5108)	loss 3.1122 (3.1122)	grad_norm 2.4982 (2.4982)	mem 23874MB
[2022-11-14 05:04:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][50/1251]	eta 0:15:47 lr 0.000059	time 0.7432 (0.7889)	loss 2.9833 (2.6030)	grad_norm 34.1309 (3.2927)	mem 23874MB
[2022-11-14 05:05:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][100/1251]	eta 0:14:45 lr 0.000059	time 0.7493 (0.7692)	loss 2.3188 (2.6509)	grad_norm 2.7878 (3.0910)	mem 23874MB
[2022-11-14 05:06:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][150/1251]	eta 0:14:00 lr 0.000059	time 0.7475 (0.7636)	loss 3.1071 (2.6213)	grad_norm 2.8249 (2.9380)	mem 23874MB
[2022-11-14 05:06:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][200/1251]	eta 0:13:19 lr 0.000059	time 0.7380 (0.7603)	loss 2.9781 (2.6256)	grad_norm 2.5467 (3.0309)	mem 23874MB
[2022-11-14 05:07:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][250/1251]	eta 0:12:39 lr 0.000059	time 0.7442 (0.7587)	loss 3.0425 (2.6317)	grad_norm 2.4829 (3.1271)	mem 23874MB
[2022-11-14 05:08:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][300/1251]	eta 0:12:00 lr 0.000059	time 0.7391 (0.7574)	loss 1.7600 (2.6448)	grad_norm 2.4884 (3.0654)	mem 23874MB
[2022-11-14 05:08:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][350/1251]	eta 0:11:21 lr 0.000059	time 0.7423 (0.7565)	loss 2.4721 (2.6396)	grad_norm 2.9326 (3.0182)	mem 23874MB
[2022-11-14 05:09:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][400/1251]	eta 0:10:43 lr 0.000059	time 0.7386 (0.7559)	loss 3.0815 (2.6455)	grad_norm 2.4260 (3.0070)	mem 23874MB
[2022-11-14 05:09:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][450/1251]	eta 0:10:04 lr 0.000059	time 0.7362 (0.7551)	loss 2.7539 (2.6488)	grad_norm 2.9729 (2.9837)	mem 23874MB
[2022-11-14 05:10:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][500/1251]	eta 0:09:26 lr 0.000058	time 0.7405 (0.7546)	loss 2.7527 (2.6518)	grad_norm 4.7160 (2.9706)	mem 23874MB
[2022-11-14 05:11:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][550/1251]	eta 0:08:48 lr 0.000058	time 0.7475 (0.7545)	loss 3.1895 (2.6674)	grad_norm 4.8838 (2.9642)	mem 23874MB
[2022-11-14 05:11:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][600/1251]	eta 0:08:10 lr 0.000058	time 0.7440 (0.7542)	loss 3.1366 (2.6614)	grad_norm 3.0156 (2.9558)	mem 23874MB
[2022-11-14 05:12:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][650/1251]	eta 0:07:33 lr 0.000058	time 0.7444 (0.7538)	loss 2.8060 (2.6634)	grad_norm 2.6864 (2.9450)	mem 23874MB
[2022-11-14 05:13:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][700/1251]	eta 0:06:55 lr 0.000058	time 0.7370 (0.7535)	loss 2.4092 (2.6614)	grad_norm 2.5362 (2.9435)	mem 23874MB
[2022-11-14 05:13:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][750/1251]	eta 0:06:17 lr 0.000058	time 0.7417 (0.7531)	loss 2.9403 (2.6599)	grad_norm 2.7208 (2.9380)	mem 23874MB
[2022-11-14 05:14:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][800/1251]	eta 0:05:39 lr 0.000058	time 0.7426 (0.7529)	loss 2.0579 (2.6588)	grad_norm 2.4952 (2.9417)	mem 23874MB
[2022-11-14 05:14:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][850/1251]	eta 0:05:01 lr 0.000058	time 0.7441 (0.7527)	loss 3.1836 (2.6558)	grad_norm 3.0409 (2.9487)	mem 23874MB
[2022-11-14 05:15:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][900/1251]	eta 0:04:24 lr 0.000058	time 0.7412 (0.7526)	loss 2.8784 (2.6582)	grad_norm 2.6414 (2.9428)	mem 23874MB
[2022-11-14 05:16:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][950/1251]	eta 0:03:46 lr 0.000058	time 0.7397 (0.7526)	loss 2.1122 (2.6549)	grad_norm 2.5719 (2.9331)	mem 23874MB
[2022-11-14 05:16:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][1000/1251]	eta 0:03:08 lr 0.000058	time 0.7402 (0.7523)	loss 1.9347 (2.6553)	grad_norm 2.9918 (2.9250)	mem 23874MB
[2022-11-14 05:17:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][1050/1251]	eta 0:02:31 lr 0.000057	time 0.7446 (0.7522)	loss 2.5943 (2.6540)	grad_norm 3.2890 (2.9188)	mem 23874MB
[2022-11-14 05:18:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][1100/1251]	eta 0:01:53 lr 0.000057	time 0.8269 (0.7522)	loss 2.4375 (2.6538)	grad_norm 2.4977 (inf)	mem 23874MB
[2022-11-14 05:18:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][1150/1251]	eta 0:01:15 lr 0.000057	time 0.7479 (0.7521)	loss 2.2640 (2.6543)	grad_norm 2.8394 (inf)	mem 23874MB
[2022-11-14 05:19:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][1200/1251]	eta 0:00:38 lr 0.000057	time 0.7533 (0.7521)	loss 2.7833 (2.6545)	grad_norm 3.5077 (inf)	mem 23874MB
[2022-11-14 05:19:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [257/300][1250/1251]	eta 0:00:00 lr 0.000057	time 0.8162 (0.7519)	loss 2.6254 (2.6494)	grad_norm 2.5772 (inf)	mem 23874MB
[2022-11-14 05:19:56 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 257 training takes 0:15:40
[2022-11-14 05:19:56 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_257.pth saving......
[2022-11-14 05:19:57 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_257.pth saved !!!
[2022-11-14 05:19:59 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.759 (1.759)	Loss 0.7355 (0.7355)	Acc@1 82.227 (82.227)	Acc@5 96.484 (96.484)	Mem 23874MB
[2022-11-14 05:20:10 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.244 Acc@5 96.538
[2022-11-14 05:20:10 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.2%
[2022-11-14 05:20:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.920 (1.920)	Loss 0.6785 (0.6785)	Acc@1 84.961 (84.961)	Acc@5 96.777 (96.777)	Mem 23874MB
[2022-11-14 05:20:22 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.888 Acc@5 96.700
[2022-11-14 05:20:22 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 05:20:22 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.89% at 257 epoch
[2022-11-14 05:20:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][0/1251]	eta 0:54:16 lr 0.000057	time 2.6031 (2.6031)	loss 1.6111 (1.6111)	grad_norm 2.7423 (2.7423)	mem 23874MB
[2022-11-14 05:21:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][50/1251]	eta 0:15:45 lr 0.000057	time 0.7422 (0.7873)	loss 3.1419 (2.6877)	grad_norm 2.9329 (3.0835)	mem 23874MB
[2022-11-14 05:21:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][100/1251]	eta 0:14:47 lr 0.000057	time 0.7426 (0.7707)	loss 3.1988 (2.6827)	grad_norm 2.6277 (3.0715)	mem 23874MB
[2022-11-14 05:22:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][150/1251]	eta 0:14:00 lr 0.000057	time 0.7475 (0.7637)	loss 3.0494 (2.7089)	grad_norm 2.3981 (3.0481)	mem 23874MB
[2022-11-14 05:22:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][200/1251]	eta 0:13:19 lr 0.000057	time 0.8111 (0.7611)	loss 2.7967 (2.6881)	grad_norm 2.4501 (2.9902)	mem 23874MB
[2022-11-14 05:23:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][250/1251]	eta 0:12:39 lr 0.000057	time 0.7429 (0.7587)	loss 3.0818 (2.6815)	grad_norm 2.7733 (2.9457)	mem 23874MB
[2022-11-14 05:24:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][300/1251]	eta 0:12:00 lr 0.000057	time 0.8179 (0.7573)	loss 2.6354 (2.6670)	grad_norm 2.4973 (2.9300)	mem 23874MB
[2022-11-14 05:24:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][350/1251]	eta 0:11:21 lr 0.000056	time 0.7502 (0.7565)	loss 2.8579 (2.6686)	grad_norm 2.2435 (2.9409)	mem 23874MB
[2022-11-14 05:25:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][400/1251]	eta 0:10:43 lr 0.000056	time 0.7418 (0.7557)	loss 3.0266 (2.6583)	grad_norm 2.6768 (2.9197)	mem 23874MB
[2022-11-14 05:26:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][450/1251]	eta 0:10:04 lr 0.000056	time 0.7403 (0.7551)	loss 2.8398 (2.6522)	grad_norm 2.5516 (2.8960)	mem 23874MB
[2022-11-14 05:26:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][500/1251]	eta 0:09:26 lr 0.000056	time 0.7430 (0.7547)	loss 1.7627 (2.6462)	grad_norm 2.6709 (2.8880)	mem 23874MB
[2022-11-14 05:27:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][550/1251]	eta 0:08:48 lr 0.000056	time 0.7473 (0.7544)	loss 3.1303 (2.6514)	grad_norm 3.0585 (2.8738)	mem 23874MB
[2022-11-14 05:27:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][600/1251]	eta 0:08:11 lr 0.000056	time 0.7393 (0.7543)	loss 2.5404 (2.6529)	grad_norm 2.3285 (2.8651)	mem 23874MB
[2022-11-14 05:28:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][650/1251]	eta 0:07:33 lr 0.000056	time 0.7442 (0.7540)	loss 2.2766 (2.6603)	grad_norm 3.2453 (2.8630)	mem 23874MB
[2022-11-14 05:29:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][700/1251]	eta 0:06:55 lr 0.000056	time 0.8249 (0.7537)	loss 2.7746 (2.6649)	grad_norm 2.2313 (2.8663)	mem 23874MB
[2022-11-14 05:29:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][750/1251]	eta 0:06:17 lr 0.000056	time 0.7417 (0.7535)	loss 1.9851 (2.6661)	grad_norm 2.6406 (2.8671)	mem 23874MB
[2022-11-14 05:30:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][800/1251]	eta 0:05:39 lr 0.000056	time 0.7473 (0.7534)	loss 2.9834 (2.6710)	grad_norm 3.2438 (2.8670)	mem 23874MB
[2022-11-14 05:31:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][850/1251]	eta 0:05:02 lr 0.000056	time 0.7434 (0.7531)	loss 2.3663 (2.6776)	grad_norm 2.4907 (2.8626)	mem 23874MB
[2022-11-14 05:31:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][900/1251]	eta 0:04:24 lr 0.000056	time 0.7398 (0.7532)	loss 2.8642 (2.6820)	grad_norm 2.6483 (2.8542)	mem 23874MB
[2022-11-14 05:32:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][950/1251]	eta 0:03:46 lr 0.000055	time 0.7433 (0.7530)	loss 3.0670 (2.6772)	grad_norm 2.6847 (2.8530)	mem 23874MB
[2022-11-14 05:32:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][1000/1251]	eta 0:03:08 lr 0.000055	time 0.7392 (0.7529)	loss 2.7938 (2.6789)	grad_norm 2.7686 (2.8607)	mem 23874MB
[2022-11-14 05:33:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][1050/1251]	eta 0:02:31 lr 0.000055	time 0.7420 (0.7527)	loss 2.8128 (2.6808)	grad_norm 2.2717 (2.8770)	mem 23874MB
[2022-11-14 05:34:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][1100/1251]	eta 0:01:53 lr 0.000055	time 0.8135 (0.7527)	loss 2.9390 (2.6827)	grad_norm 3.6708 (2.9010)	mem 23874MB
[2022-11-14 05:34:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][1150/1251]	eta 0:01:16 lr 0.000055	time 0.7424 (0.7526)	loss 2.7888 (2.6800)	grad_norm 2.4471 (2.8947)	mem 23874MB
[2022-11-14 05:35:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][1200/1251]	eta 0:00:38 lr 0.000055	time 0.7401 (0.7526)	loss 3.0700 (2.6829)	grad_norm 3.0555 (2.8905)	mem 23874MB
[2022-11-14 05:36:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [258/300][1250/1251]	eta 0:00:00 lr 0.000055	time 0.7277 (0.7523)	loss 2.5501 (2.6790)	grad_norm 2.8156 (2.8937)	mem 23874MB
[2022-11-14 05:36:04 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 258 training takes 0:15:41
[2022-11-14 05:36:04 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_258.pth saving......
[2022-11-14 05:36:05 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_258.pth saved !!!
[2022-11-14 05:36:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.651 (1.651)	Loss 0.6548 (0.6548)	Acc@1 83.984 (83.984)	Acc@5 96.680 (96.680)	Mem 23874MB
[2022-11-14 05:36:17 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.320 Acc@5 96.484
[2022-11-14 05:36:17 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.3%
[2022-11-14 05:36:19 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.912 (1.912)	Loss 0.6901 (0.6901)	Acc@1 83.594 (83.594)	Acc@5 96.387 (96.387)	Mem 23874MB
[2022-11-14 05:36:30 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.894 Acc@5 96.696
[2022-11-14 05:36:30 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 05:36:30 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.89% at 258 epoch
[2022-11-14 05:36:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][0/1251]	eta 0:52:50 lr 0.000055	time 2.5346 (2.5346)	loss 2.6781 (2.6781)	grad_norm 2.7042 (2.7042)	mem 23874MB
[2022-11-14 05:37:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][50/1251]	eta 0:15:47 lr 0.000055	time 0.7512 (0.7886)	loss 2.6392 (2.6859)	grad_norm 7.0903 (2.7210)	mem 23874MB
[2022-11-14 05:37:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][100/1251]	eta 0:14:47 lr 0.000055	time 0.7557 (0.7714)	loss 2.9773 (2.6579)	grad_norm 2.2857 (2.8044)	mem 23874MB
[2022-11-14 05:38:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][150/1251]	eta 0:14:02 lr 0.000055	time 0.7404 (0.7649)	loss 2.6500 (2.6735)	grad_norm 2.5927 (2.7707)	mem 23874MB
[2022-11-14 05:39:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][200/1251]	eta 0:13:20 lr 0.000055	time 0.7384 (0.7613)	loss 3.0957 (2.6823)	grad_norm 2.3319 (2.8333)	mem 23874MB
[2022-11-14 05:39:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][250/1251]	eta 0:12:39 lr 0.000054	time 0.7381 (0.7587)	loss 3.3269 (2.6746)	grad_norm 3.2394 (2.8126)	mem 23874MB
[2022-11-14 05:40:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][300/1251]	eta 0:12:00 lr 0.000054	time 0.7385 (0.7577)	loss 2.2341 (2.6532)	grad_norm 3.2947 (2.8870)	mem 23874MB
[2022-11-14 05:40:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][350/1251]	eta 0:11:21 lr 0.000054	time 0.7392 (0.7566)	loss 2.6447 (2.6440)	grad_norm 2.4535 (2.8977)	mem 23874MB
[2022-11-14 05:41:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][400/1251]	eta 0:10:43 lr 0.000054	time 0.7449 (0.7561)	loss 1.9370 (2.6404)	grad_norm 2.8428 (2.9176)	mem 23874MB
[2022-11-14 05:42:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][450/1251]	eta 0:10:04 lr 0.000054	time 0.7523 (0.7550)	loss 3.1143 (2.6384)	grad_norm 2.5332 (2.8913)	mem 23874MB
[2022-11-14 05:42:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][500/1251]	eta 0:09:26 lr 0.000054	time 0.7506 (0.7548)	loss 2.1911 (2.6407)	grad_norm 10.7736 (2.9014)	mem 23874MB
[2022-11-14 05:43:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][550/1251]	eta 0:08:48 lr 0.000054	time 0.7424 (0.7543)	loss 3.0556 (2.6471)	grad_norm 3.4620 (2.8818)	mem 23874MB
[2022-11-14 05:44:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][600/1251]	eta 0:08:10 lr 0.000054	time 0.7380 (0.7538)	loss 2.5918 (2.6451)	grad_norm 2.3906 (2.8672)	mem 23874MB
[2022-11-14 05:44:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][650/1251]	eta 0:07:32 lr 0.000054	time 0.7416 (0.7537)	loss 2.6951 (2.6539)	grad_norm 4.2643 (2.8765)	mem 23874MB
[2022-11-14 05:45:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][700/1251]	eta 0:06:55 lr 0.000054	time 0.7396 (0.7534)	loss 2.7830 (2.6497)	grad_norm 2.4616 (2.8707)	mem 23874MB
[2022-11-14 05:45:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][750/1251]	eta 0:06:17 lr 0.000054	time 0.7533 (0.7533)	loss 2.0319 (2.6488)	grad_norm 3.1075 (2.8611)	mem 23874MB
[2022-11-14 05:46:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][800/1251]	eta 0:05:39 lr 0.000054	time 0.7412 (0.7531)	loss 1.8415 (2.6438)	grad_norm 2.6003 (2.8706)	mem 23874MB
[2022-11-14 05:47:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][850/1251]	eta 0:05:01 lr 0.000053	time 0.7371 (0.7530)	loss 2.1717 (2.6427)	grad_norm 2.4872 (2.8732)	mem 23874MB
[2022-11-14 05:47:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][900/1251]	eta 0:04:24 lr 0.000053	time 0.7433 (0.7528)	loss 2.9760 (2.6460)	grad_norm 2.7362 (2.8737)	mem 23874MB
[2022-11-14 05:48:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][950/1251]	eta 0:03:46 lr 0.000053	time 0.7458 (0.7527)	loss 3.0721 (2.6513)	grad_norm 3.3530 (2.8852)	mem 23874MB
[2022-11-14 05:49:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][1000/1251]	eta 0:03:08 lr 0.000053	time 0.7387 (0.7526)	loss 3.1946 (2.6533)	grad_norm 2.5616 (2.8889)	mem 23874MB
[2022-11-14 05:49:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][1050/1251]	eta 0:02:31 lr 0.000053	time 0.7481 (0.7526)	loss 2.1945 (2.6526)	grad_norm 2.9185 (2.8963)	mem 23874MB
[2022-11-14 05:50:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][1100/1251]	eta 0:01:53 lr 0.000053	time 0.7467 (0.7525)	loss 2.7376 (2.6500)	grad_norm 2.5373 (2.8993)	mem 23874MB
[2022-11-14 05:50:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][1150/1251]	eta 0:01:15 lr 0.000053	time 0.7426 (0.7524)	loss 2.9234 (2.6518)	grad_norm 2.5940 (2.8981)	mem 23874MB
[2022-11-14 05:51:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][1200/1251]	eta 0:00:38 lr 0.000053	time 0.7473 (0.7523)	loss 2.0583 (2.6467)	grad_norm 2.5163 (2.9033)	mem 23874MB
[2022-11-14 05:52:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [259/300][1250/1251]	eta 0:00:00 lr 0.000053	time 0.7301 (0.7520)	loss 2.7932 (2.6465)	grad_norm 2.2863 (2.8935)	mem 23874MB
[2022-11-14 05:52:11 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 259 training takes 0:15:40
[2022-11-14 05:52:11 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_259.pth saving......
[2022-11-14 05:52:12 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_259.pth saved !!!
[2022-11-14 05:52:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.652 (1.652)	Loss 0.6890 (0.6890)	Acc@1 84.766 (84.766)	Acc@5 97.266 (97.266)	Mem 23874MB
[2022-11-14 05:52:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.288 Acc@5 96.478
[2022-11-14 05:52:25 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.3%
[2022-11-14 05:52:27 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.048 (2.048)	Loss 0.7560 (0.7560)	Acc@1 84.277 (84.277)	Acc@5 96.387 (96.387)	Mem 23874MB
[2022-11-14 05:52:38 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.880 Acc@5 96.708
[2022-11-14 05:52:38 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 05:52:38 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.89% at 258 epoch
[2022-11-14 05:52:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][0/1251]	eta 0:51:51 lr 0.000053	time 2.4873 (2.4873)	loss 2.6922 (2.6922)	grad_norm 3.3848 (3.3848)	mem 23874MB
[2022-11-14 05:53:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][50/1251]	eta 0:15:40 lr 0.000053	time 0.7383 (0.7834)	loss 2.0538 (2.6086)	grad_norm 2.8021 (2.8140)	mem 23874MB
[2022-11-14 05:53:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][100/1251]	eta 0:14:44 lr 0.000053	time 0.7426 (0.7684)	loss 1.9245 (2.6172)	grad_norm 2.8774 (2.8167)	mem 23874MB
[2022-11-14 05:54:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][150/1251]	eta 0:13:58 lr 0.000053	time 0.7429 (0.7618)	loss 2.4293 (2.6459)	grad_norm 2.7917 (2.8383)	mem 23874MB
[2022-11-14 05:55:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][200/1251]	eta 0:13:17 lr 0.000052	time 0.7694 (0.7589)	loss 2.1789 (2.6287)	grad_norm 6.8252 (2.8250)	mem 23874MB
[2022-11-14 05:55:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][250/1251]	eta 0:12:37 lr 0.000052	time 0.7402 (0.7570)	loss 2.7093 (2.6429)	grad_norm 2.6186 (2.8077)	mem 23874MB
[2022-11-14 05:56:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][300/1251]	eta 0:11:58 lr 0.000052	time 0.7392 (0.7554)	loss 2.9395 (2.6620)	grad_norm 2.9699 (2.8369)	mem 23874MB
[2022-11-14 05:57:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][350/1251]	eta 0:11:19 lr 0.000052	time 0.7432 (0.7546)	loss 2.9244 (2.6716)	grad_norm 2.3732 (2.8277)	mem 23874MB
[2022-11-14 05:57:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][400/1251]	eta 0:10:41 lr 0.000052	time 0.8270 (0.7540)	loss 2.9664 (2.6716)	grad_norm 2.8870 (2.8554)	mem 23874MB
[2022-11-14 05:58:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][450/1251]	eta 0:10:03 lr 0.000052	time 0.7450 (0.7534)	loss 2.5089 (2.6709)	grad_norm 3.3192 (2.8562)	mem 23874MB
[2022-11-14 05:58:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][500/1251]	eta 0:09:25 lr 0.000052	time 0.7433 (0.7533)	loss 3.2273 (2.6775)	grad_norm 2.8133 (2.8738)	mem 23874MB
[2022-11-14 05:59:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][550/1251]	eta 0:08:47 lr 0.000052	time 0.7432 (0.7527)	loss 2.3461 (2.6761)	grad_norm 2.4768 (2.8529)	mem 23874MB
[2022-11-14 06:00:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][600/1251]	eta 0:08:09 lr 0.000052	time 0.7445 (0.7525)	loss 2.6447 (2.6743)	grad_norm 4.4381 (inf)	mem 23874MB
[2022-11-14 06:00:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][650/1251]	eta 0:07:32 lr 0.000052	time 0.7480 (0.7523)	loss 2.7533 (2.6820)	grad_norm 2.6272 (inf)	mem 23874MB
[2022-11-14 06:01:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][700/1251]	eta 0:06:54 lr 0.000052	time 0.7413 (0.7521)	loss 2.0901 (2.6786)	grad_norm 2.6254 (inf)	mem 23874MB
[2022-11-14 06:02:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][750/1251]	eta 0:06:16 lr 0.000052	time 0.7472 (0.7520)	loss 3.0644 (2.6711)	grad_norm 2.7536 (inf)	mem 23874MB
[2022-11-14 06:02:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][800/1251]	eta 0:05:38 lr 0.000051	time 0.7426 (0.7516)	loss 3.0684 (2.6717)	grad_norm 2.1752 (inf)	mem 23874MB
[2022-11-14 06:03:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][850/1251]	eta 0:05:01 lr 0.000051	time 0.7423 (0.7515)	loss 2.9446 (2.6705)	grad_norm 3.4742 (inf)	mem 23874MB
[2022-11-14 06:03:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][900/1251]	eta 0:04:23 lr 0.000051	time 0.7481 (0.7512)	loss 2.8175 (2.6727)	grad_norm 2.7196 (inf)	mem 23874MB
[2022-11-14 06:04:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][950/1251]	eta 0:03:46 lr 0.000051	time 0.7414 (0.7512)	loss 2.7858 (2.6741)	grad_norm 2.3651 (inf)	mem 23874MB
[2022-11-14 06:05:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][1000/1251]	eta 0:03:08 lr 0.000051	time 0.7447 (0.7511)	loss 2.8250 (2.6798)	grad_norm 2.8149 (inf)	mem 23874MB
[2022-11-14 06:05:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][1050/1251]	eta 0:02:30 lr 0.000051	time 0.7451 (0.7511)	loss 2.4044 (2.6731)	grad_norm 3.2361 (inf)	mem 23874MB
[2022-11-14 06:06:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][1100/1251]	eta 0:01:53 lr 0.000051	time 0.8199 (0.7508)	loss 2.1634 (2.6737)	grad_norm 2.2247 (inf)	mem 23874MB
[2022-11-14 06:07:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][1150/1251]	eta 0:01:15 lr 0.000051	time 0.7395 (0.7508)	loss 3.1827 (2.6721)	grad_norm 2.6548 (inf)	mem 23874MB
[2022-11-14 06:07:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][1200/1251]	eta 0:00:38 lr 0.000051	time 0.7449 (0.7507)	loss 1.9834 (2.6726)	grad_norm 2.7138 (inf)	mem 23874MB
[2022-11-14 06:08:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [260/300][1250/1251]	eta 0:00:00 lr 0.000051	time 0.7374 (0.7505)	loss 2.9274 (2.6713)	grad_norm 2.5950 (inf)	mem 23874MB
[2022-11-14 06:08:17 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 260 training takes 0:15:39
[2022-11-14 06:08:17 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_260.pth saving......
[2022-11-14 06:08:18 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_260.pth saved !!!
[2022-11-14 06:08:20 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.661 (1.661)	Loss 0.6493 (0.6493)	Acc@1 85.840 (85.840)	Acc@5 97.168 (97.168)	Mem 23874MB
[2022-11-14 06:08:30 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.404 Acc@5 96.540
[2022-11-14 06:08:30 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.4%
[2022-11-14 06:08:32 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.964 (1.964)	Loss 0.7357 (0.7357)	Acc@1 83.203 (83.203)	Acc@5 96.289 (96.289)	Mem 23874MB
[2022-11-14 06:08:43 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.864 Acc@5 96.696
[2022-11-14 06:08:43 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 06:08:43 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.89% at 258 epoch
[2022-11-14 06:08:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][0/1251]	eta 0:58:08 lr 0.000051	time 2.7882 (2.7882)	loss 1.8361 (1.8361)	grad_norm 2.4198 (2.4198)	mem 23874MB
[2022-11-14 06:09:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][50/1251]	eta 0:15:51 lr 0.000051	time 0.7455 (0.7927)	loss 2.9190 (2.5533)	grad_norm 2.3887 (2.7667)	mem 23874MB
[2022-11-14 06:10:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][100/1251]	eta 0:14:46 lr 0.000051	time 0.7426 (0.7706)	loss 2.9089 (2.5989)	grad_norm 2.7659 (2.8423)	mem 23874MB
[2022-11-14 06:10:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][150/1251]	eta 0:14:01 lr 0.000050	time 0.7463 (0.7646)	loss 2.5301 (2.6300)	grad_norm 2.8804 (2.8584)	mem 23874MB
[2022-11-14 06:11:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][200/1251]	eta 0:13:19 lr 0.000050	time 0.7455 (0.7611)	loss 2.3316 (2.6342)	grad_norm 2.9071 (2.9741)	mem 23874MB
[2022-11-14 06:11:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][250/1251]	eta 0:12:39 lr 0.000050	time 0.7421 (0.7586)	loss 2.8945 (2.6161)	grad_norm 3.0891 (3.0045)	mem 23874MB
[2022-11-14 06:12:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][300/1251]	eta 0:12:00 lr 0.000050	time 0.7441 (0.7571)	loss 2.0045 (2.6190)	grad_norm 2.7534 (2.9755)	mem 23874MB
[2022-11-14 06:13:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][350/1251]	eta 0:11:21 lr 0.000050	time 0.7545 (0.7564)	loss 3.1736 (2.6216)	grad_norm 6.1555 (2.9511)	mem 23874MB
[2022-11-14 06:13:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][400/1251]	eta 0:10:42 lr 0.000050	time 0.7395 (0.7551)	loss 2.9792 (2.6383)	grad_norm 2.3297 (2.9316)	mem 23874MB
[2022-11-14 06:14:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][450/1251]	eta 0:10:04 lr 0.000050	time 0.7456 (0.7547)	loss 2.8607 (2.6435)	grad_norm 2.6853 (2.9182)	mem 23874MB
[2022-11-14 06:15:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][500/1251]	eta 0:09:26 lr 0.000050	time 0.7496 (0.7541)	loss 3.1146 (2.6424)	grad_norm 3.1661 (2.9131)	mem 23874MB
[2022-11-14 06:15:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][550/1251]	eta 0:08:48 lr 0.000050	time 0.7415 (0.7538)	loss 2.9217 (2.6463)	grad_norm 2.9098 (2.9204)	mem 23874MB
[2022-11-14 06:16:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][600/1251]	eta 0:08:10 lr 0.000050	time 0.7477 (0.7536)	loss 1.6618 (2.6389)	grad_norm 2.2262 (2.9060)	mem 23874MB
[2022-11-14 06:16:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][650/1251]	eta 0:07:32 lr 0.000050	time 0.7463 (0.7533)	loss 1.8907 (2.6389)	grad_norm 2.9088 (2.9302)	mem 23874MB
[2022-11-14 06:17:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][700/1251]	eta 0:06:54 lr 0.000050	time 0.7412 (0.7530)	loss 3.1399 (2.6435)	grad_norm 2.9086 (2.9300)	mem 23874MB
[2022-11-14 06:18:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][750/1251]	eta 0:06:17 lr 0.000049	time 0.8067 (0.7528)	loss 2.3520 (2.6428)	grad_norm 2.6338 (2.9232)	mem 23874MB
[2022-11-14 06:18:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][800/1251]	eta 0:05:39 lr 0.000049	time 0.7451 (0.7525)	loss 2.3648 (2.6461)	grad_norm 2.7660 (2.9215)	mem 23874MB
[2022-11-14 06:19:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][850/1251]	eta 0:05:01 lr 0.000049	time 0.7398 (0.7525)	loss 2.4119 (2.6448)	grad_norm 3.7695 (2.9159)	mem 23874MB
[2022-11-14 06:20:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][900/1251]	eta 0:04:24 lr 0.000049	time 0.7420 (0.7524)	loss 3.0065 (2.6425)	grad_norm 2.5934 (2.9276)	mem 23874MB
[2022-11-14 06:20:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][950/1251]	eta 0:03:46 lr 0.000049	time 0.7466 (0.7522)	loss 2.9985 (2.6451)	grad_norm 2.9303 (2.9265)	mem 23874MB
[2022-11-14 06:21:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][1000/1251]	eta 0:03:08 lr 0.000049	time 0.7430 (0.7521)	loss 2.6744 (2.6485)	grad_norm 2.7619 (2.9386)	mem 23874MB
[2022-11-14 06:21:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][1050/1251]	eta 0:02:31 lr 0.000049	time 0.7419 (0.7518)	loss 2.9048 (2.6460)	grad_norm 2.7254 (2.9495)	mem 23874MB
[2022-11-14 06:22:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][1100/1251]	eta 0:01:53 lr 0.000049	time 0.7473 (0.7519)	loss 2.9575 (2.6463)	grad_norm 2.6455 (2.9548)	mem 23874MB
[2022-11-14 06:23:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][1150/1251]	eta 0:01:15 lr 0.000049	time 0.7585 (0.7518)	loss 3.1049 (2.6445)	grad_norm 2.8386 (2.9542)	mem 23874MB
[2022-11-14 06:23:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][1200/1251]	eta 0:00:38 lr 0.000049	time 0.7385 (0.7517)	loss 2.9313 (2.6424)	grad_norm 2.8189 (2.9481)	mem 23874MB
[2022-11-14 06:24:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [261/300][1250/1251]	eta 0:00:00 lr 0.000049	time 0.7268 (0.7514)	loss 2.3965 (2.6387)	grad_norm 2.7762 (inf)	mem 23874MB
[2022-11-14 06:24:23 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 261 training takes 0:15:40
[2022-11-14 06:24:23 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_261.pth saving......
[2022-11-14 06:24:25 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_261.pth saved !!!
[2022-11-14 06:24:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.749 (1.749)	Loss 0.6291 (0.6291)	Acc@1 85.742 (85.742)	Acc@5 96.777 (96.777)	Mem 23874MB
[2022-11-14 06:24:37 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.408 Acc@5 96.510
[2022-11-14 06:24:37 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.4%
[2022-11-14 06:24:39 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.967 (1.967)	Loss 0.7400 (0.7400)	Acc@1 83.105 (83.105)	Acc@5 96.582 (96.582)	Mem 23874MB
[2022-11-14 06:24:50 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.878 Acc@5 96.702
[2022-11-14 06:24:50 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 06:24:50 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.89% at 258 epoch
[2022-11-14 06:24:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][0/1251]	eta 0:52:28 lr 0.000049	time 2.5166 (2.5166)	loss 2.9662 (2.9662)	grad_norm 2.4974 (2.4974)	mem 23874MB
[2022-11-14 06:25:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][50/1251]	eta 0:15:45 lr 0.000049	time 0.7407 (0.7876)	loss 2.8539 (2.5459)	grad_norm 2.3225 (3.0293)	mem 23874MB
[2022-11-14 06:26:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][100/1251]	eta 0:14:47 lr 0.000049	time 0.7435 (0.7707)	loss 3.0694 (2.6070)	grad_norm 2.7454 (2.9583)	mem 23874MB
[2022-11-14 06:26:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][150/1251]	eta 0:14:01 lr 0.000048	time 0.8361 (0.7640)	loss 2.7529 (2.6160)	grad_norm 4.2691 (2.9459)	mem 23874MB
[2022-11-14 06:27:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][200/1251]	eta 0:13:19 lr 0.000048	time 0.7378 (0.7605)	loss 2.5366 (2.6311)	grad_norm 2.4344 (2.9750)	mem 23874MB
[2022-11-14 06:28:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][250/1251]	eta 0:12:38 lr 0.000048	time 0.7417 (0.7580)	loss 3.0999 (2.6210)	grad_norm 4.1364 (2.9381)	mem 23874MB
[2022-11-14 06:28:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][300/1251]	eta 0:11:59 lr 0.000048	time 0.7467 (0.7566)	loss 2.9651 (2.6357)	grad_norm 2.7339 (2.9315)	mem 23874MB
[2022-11-14 06:29:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][350/1251]	eta 0:11:21 lr 0.000048	time 0.7467 (0.7559)	loss 3.0973 (2.6464)	grad_norm 2.8868 (2.9310)	mem 23874MB
[2022-11-14 06:29:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][400/1251]	eta 0:10:42 lr 0.000048	time 0.8248 (0.7556)	loss 2.4033 (2.6504)	grad_norm 3.0742 (2.9722)	mem 23874MB
[2022-11-14 06:30:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][450/1251]	eta 0:10:04 lr 0.000048	time 0.7495 (0.7547)	loss 2.1886 (2.6583)	grad_norm 2.4779 (2.9783)	mem 23874MB
[2022-11-14 06:31:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][500/1251]	eta 0:09:26 lr 0.000048	time 0.7380 (0.7541)	loss 1.7189 (2.6546)	grad_norm 2.6571 (2.9668)	mem 23874MB
[2022-11-14 06:31:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][550/1251]	eta 0:08:48 lr 0.000048	time 0.8422 (0.7537)	loss 2.5356 (2.6603)	grad_norm 2.5731 (2.9644)	mem 23874MB
[2022-11-14 06:32:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][600/1251]	eta 0:08:10 lr 0.000048	time 0.7447 (0.7534)	loss 2.9274 (2.6636)	grad_norm 3.0134 (2.9546)	mem 23874MB
[2022-11-14 06:33:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][650/1251]	eta 0:07:32 lr 0.000048	time 0.7413 (0.7532)	loss 1.9843 (2.6636)	grad_norm 2.5906 (2.9336)	mem 23874MB
[2022-11-14 06:33:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][700/1251]	eta 0:06:54 lr 0.000048	time 0.7398 (0.7530)	loss 2.9276 (2.6662)	grad_norm 2.6323 (2.9294)	mem 23874MB
[2022-11-14 06:34:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][750/1251]	eta 0:06:17 lr 0.000047	time 0.7416 (0.7529)	loss 2.8107 (2.6674)	grad_norm 2.8302 (2.9325)	mem 23874MB
[2022-11-14 06:34:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][800/1251]	eta 0:05:39 lr 0.000047	time 0.7463 (0.7527)	loss 1.8420 (2.6643)	grad_norm 2.4881 (2.9456)	mem 23874MB
[2022-11-14 06:35:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][850/1251]	eta 0:05:01 lr 0.000047	time 0.7439 (0.7526)	loss 3.1679 (2.6652)	grad_norm 2.8280 (2.9477)	mem 23874MB
[2022-11-14 06:36:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][900/1251]	eta 0:04:24 lr 0.000047	time 0.7394 (0.7524)	loss 2.5555 (2.6674)	grad_norm 2.3929 (2.9585)	mem 23874MB
[2022-11-14 06:36:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][950/1251]	eta 0:03:46 lr 0.000047	time 0.8619 (0.7524)	loss 2.6630 (2.6673)	grad_norm 2.9007 (2.9531)	mem 23874MB
[2022-11-14 06:37:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][1000/1251]	eta 0:03:08 lr 0.000047	time 0.7448 (0.7523)	loss 2.7178 (2.6652)	grad_norm 2.8264 (2.9581)	mem 23874MB
[2022-11-14 06:38:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][1050/1251]	eta 0:02:31 lr 0.000047	time 0.7303 (0.7521)	loss 3.0521 (2.6668)	grad_norm 3.6277 (2.9599)	mem 23874MB
[2022-11-14 06:38:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][1100/1251]	eta 0:01:53 lr 0.000047	time 0.7406 (0.7520)	loss 2.8468 (2.6656)	grad_norm 2.5158 (2.9608)	mem 23874MB
[2022-11-14 06:39:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][1150/1251]	eta 0:01:15 lr 0.000047	time 0.7423 (0.7517)	loss 3.0236 (2.6605)	grad_norm 2.8272 (2.9517)	mem 23874MB
[2022-11-14 06:39:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][1200/1251]	eta 0:00:38 lr 0.000047	time 0.7422 (0.7518)	loss 2.5347 (2.6579)	grad_norm 2.8711 (2.9574)	mem 23874MB
[2022-11-14 06:40:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [262/300][1250/1251]	eta 0:00:00 lr 0.000047	time 0.7271 (0.7514)	loss 2.0107 (2.6571)	grad_norm 2.4471 (2.9563)	mem 23874MB
[2022-11-14 06:40:30 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 262 training takes 0:15:40
[2022-11-14 06:40:30 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_262.pth saving......
[2022-11-14 06:40:31 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_262.pth saved !!!
[2022-11-14 06:40:33 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.816 (1.816)	Loss 0.8227 (0.8227)	Acc@1 81.738 (81.738)	Acc@5 95.508 (95.508)	Mem 23874MB
[2022-11-14 06:40:44 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.512 Acc@5 96.560
[2022-11-14 06:40:44 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.5%
[2022-11-14 06:40:46 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.960 (1.960)	Loss 0.7102 (0.7102)	Acc@1 82.812 (82.812)	Acc@5 96.777 (96.777)	Mem 23874MB
[2022-11-14 06:40:56 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.894 Acc@5 96.712
[2022-11-14 06:40:56 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 06:40:56 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.89% at 262 epoch
[2022-11-14 06:40:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][0/1251]	eta 0:52:22 lr 0.000047	time 2.5117 (2.5117)	loss 2.8787 (2.8787)	grad_norm 2.5551 (2.5551)	mem 23874MB
[2022-11-14 06:41:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][50/1251]	eta 0:15:40 lr 0.000047	time 0.7418 (0.7834)	loss 2.4694 (2.6463)	grad_norm 2.8597 (2.9200)	mem 23874MB
[2022-11-14 06:42:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][100/1251]	eta 0:14:43 lr 0.000047	time 0.7598 (0.7672)	loss 2.4510 (2.6149)	grad_norm 2.2186 (2.9086)	mem 23874MB
[2022-11-14 06:42:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][150/1251]	eta 0:13:58 lr 0.000046	time 0.7333 (0.7618)	loss 3.0590 (2.6087)	grad_norm 2.6758 (2.8787)	mem 23874MB
[2022-11-14 06:43:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][200/1251]	eta 0:13:17 lr 0.000046	time 0.7428 (0.7589)	loss 2.9499 (2.6335)	grad_norm 2.6964 (2.9391)	mem 23874MB
[2022-11-14 06:44:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][250/1251]	eta 0:12:38 lr 0.000046	time 0.7469 (0.7573)	loss 3.1012 (2.6480)	grad_norm 2.7388 (2.8994)	mem 23874MB
[2022-11-14 06:44:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][300/1251]	eta 0:11:58 lr 0.000046	time 0.8342 (0.7556)	loss 2.9296 (2.6542)	grad_norm 2.4989 (2.9163)	mem 23874MB
[2022-11-14 06:45:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][350/1251]	eta 0:11:20 lr 0.000046	time 0.7456 (0.7548)	loss 2.5221 (2.6677)	grad_norm 2.3707 (2.9301)	mem 23874MB
[2022-11-14 06:45:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][400/1251]	eta 0:10:41 lr 0.000046	time 0.7559 (0.7541)	loss 2.1109 (2.6564)	grad_norm 3.4822 (2.9189)	mem 23874MB
[2022-11-14 06:46:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][450/1251]	eta 0:10:03 lr 0.000046	time 0.7391 (0.7540)	loss 2.6914 (2.6433)	grad_norm 2.6488 (2.9201)	mem 23874MB
[2022-11-14 06:47:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][500/1251]	eta 0:09:25 lr 0.000046	time 0.7483 (0.7536)	loss 2.4861 (2.6513)	grad_norm 2.5963 (2.9182)	mem 23874MB
[2022-11-14 06:47:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][550/1251]	eta 0:08:47 lr 0.000046	time 0.8117 (0.7532)	loss 3.1053 (2.6608)	grad_norm 3.6938 (2.9315)	mem 23874MB
[2022-11-14 06:48:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][600/1251]	eta 0:08:10 lr 0.000046	time 0.7443 (0.7528)	loss 2.9141 (2.6586)	grad_norm 2.5117 (2.9246)	mem 23874MB
[2022-11-14 06:49:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][650/1251]	eta 0:07:32 lr 0.000046	time 0.7457 (0.7524)	loss 2.1433 (2.6545)	grad_norm 2.5593 (2.9186)	mem 23874MB
[2022-11-14 06:49:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][700/1251]	eta 0:06:54 lr 0.000046	time 0.8147 (0.7524)	loss 3.0258 (2.6575)	grad_norm 2.7763 (2.9359)	mem 23874MB
[2022-11-14 06:50:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][750/1251]	eta 0:06:16 lr 0.000046	time 0.7378 (0.7521)	loss 2.9286 (2.6526)	grad_norm 3.4621 (2.9381)	mem 23874MB
[2022-11-14 06:50:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][800/1251]	eta 0:05:39 lr 0.000045	time 0.7403 (0.7522)	loss 1.9370 (2.6551)	grad_norm 3.1741 (2.9353)	mem 23874MB
[2022-11-14 06:51:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][850/1251]	eta 0:05:01 lr 0.000045	time 0.7454 (0.7521)	loss 3.0954 (2.6541)	grad_norm 2.7326 (2.9379)	mem 23874MB
[2022-11-14 06:52:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][900/1251]	eta 0:04:23 lr 0.000045	time 0.7419 (0.7520)	loss 2.6466 (2.6575)	grad_norm 2.8493 (2.9313)	mem 23874MB
[2022-11-14 06:52:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][950/1251]	eta 0:03:46 lr 0.000045	time 0.7475 (0.7518)	loss 2.8221 (2.6636)	grad_norm 2.6515 (2.9320)	mem 23874MB
[2022-11-14 06:53:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][1000/1251]	eta 0:03:08 lr 0.000045	time 0.7429 (0.7518)	loss 2.0787 (2.6642)	grad_norm 2.5137 (2.9410)	mem 23874MB
[2022-11-14 06:54:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][1050/1251]	eta 0:02:31 lr 0.000045	time 0.7429 (0.7516)	loss 2.5007 (2.6626)	grad_norm 3.9796 (2.9379)	mem 23874MB
[2022-11-14 06:54:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][1100/1251]	eta 0:01:53 lr 0.000045	time 0.8358 (0.7516)	loss 3.1287 (2.6637)	grad_norm 2.2251 (2.9459)	mem 23874MB
[2022-11-14 06:55:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][1150/1251]	eta 0:01:15 lr 0.000045	time 0.7400 (0.7514)	loss 2.8630 (2.6640)	grad_norm 2.6063 (2.9413)	mem 23874MB
[2022-11-14 06:55:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][1200/1251]	eta 0:00:38 lr 0.000045	time 0.7445 (0.7515)	loss 2.7796 (2.6602)	grad_norm 2.6859 (2.9471)	mem 23874MB
[2022-11-14 06:56:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [263/300][1250/1251]	eta 0:00:00 lr 0.000045	time 0.7365 (0.7513)	loss 3.1791 (2.6614)	grad_norm 4.6347 (2.9453)	mem 23874MB
[2022-11-14 06:56:36 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 263 training takes 0:15:40
[2022-11-14 06:56:37 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_263.pth saving......
[2022-11-14 06:56:38 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_263.pth saved !!!
[2022-11-14 06:56:39 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.682 (1.682)	Loss 0.6348 (0.6348)	Acc@1 84.473 (84.473)	Acc@5 97.070 (97.070)	Mem 23874MB
[2022-11-14 06:56:50 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.548 Acc@5 96.588
[2022-11-14 06:56:50 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.5%
[2022-11-14 06:56:52 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.934 (1.934)	Loss 0.7005 (0.7005)	Acc@1 84.277 (84.277)	Acc@5 96.680 (96.680)	Mem 23874MB
[2022-11-14 06:57:03 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.934 Acc@5 96.718
[2022-11-14 06:57:03 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 06:57:03 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.93% at 263 epoch
[2022-11-14 06:57:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][0/1251]	eta 0:53:54 lr 0.000045	time 2.5853 (2.5853)	loss 2.6577 (2.6577)	grad_norm 2.9074 (2.9074)	mem 23874MB
[2022-11-14 06:57:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][50/1251]	eta 0:15:47 lr 0.000045	time 0.7502 (0.7890)	loss 3.0148 (2.6542)	grad_norm 2.2972 (2.8048)	mem 23874MB
[2022-11-14 06:58:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][100/1251]	eta 0:14:45 lr 0.000045	time 0.7400 (0.7694)	loss 2.9282 (2.6295)	grad_norm 2.8351 (2.8947)	mem 23874MB
[2022-11-14 06:58:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][150/1251]	eta 0:14:01 lr 0.000045	time 0.7480 (0.7639)	loss 2.7079 (2.6578)	grad_norm 2.7332 (2.9002)	mem 23874MB
[2022-11-14 06:59:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][200/1251]	eta 0:13:18 lr 0.000044	time 0.7473 (0.7597)	loss 2.6596 (2.6525)	grad_norm 2.3998 (2.8997)	mem 23874MB
[2022-11-14 07:00:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][250/1251]	eta 0:12:39 lr 0.000044	time 0.7442 (0.7583)	loss 2.9644 (2.6618)	grad_norm 2.6731 (2.9235)	mem 23874MB
[2022-11-14 07:00:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][300/1251]	eta 0:11:59 lr 0.000044	time 0.7421 (0.7562)	loss 3.1811 (2.6557)	grad_norm 2.7400 (2.9137)	mem 23874MB
[2022-11-14 07:01:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][350/1251]	eta 0:11:20 lr 0.000044	time 0.7391 (0.7556)	loss 1.8999 (2.6493)	grad_norm 2.7828 (2.8889)	mem 23874MB
[2022-11-14 07:02:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][400/1251]	eta 0:10:42 lr 0.000044	time 0.7292 (0.7548)	loss 2.9614 (2.6573)	grad_norm 2.9029 (2.8881)	mem 23874MB
[2022-11-14 07:02:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][450/1251]	eta 0:10:04 lr 0.000044	time 0.7381 (0.7543)	loss 2.7659 (2.6532)	grad_norm 2.5532 (2.9648)	mem 23874MB
[2022-11-14 07:03:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][500/1251]	eta 0:09:26 lr 0.000044	time 0.7434 (0.7539)	loss 2.7104 (2.6566)	grad_norm 2.4558 (2.9736)	mem 23874MB
[2022-11-14 07:03:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][550/1251]	eta 0:08:48 lr 0.000044	time 0.7390 (0.7534)	loss 2.9267 (2.6639)	grad_norm 2.4771 (2.9678)	mem 23874MB
[2022-11-14 07:04:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][600/1251]	eta 0:08:10 lr 0.000044	time 0.7449 (0.7530)	loss 2.8371 (2.6687)	grad_norm 2.6701 (2.9619)	mem 23874MB
[2022-11-14 07:05:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][650/1251]	eta 0:07:32 lr 0.000044	time 0.7405 (0.7530)	loss 2.5476 (2.6705)	grad_norm 2.5209 (2.9612)	mem 23874MB
[2022-11-14 07:05:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][700/1251]	eta 0:06:54 lr 0.000044	time 0.7481 (0.7525)	loss 2.9309 (2.6726)	grad_norm 2.4537 (2.9512)	mem 23874MB
[2022-11-14 07:06:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][750/1251]	eta 0:06:16 lr 0.000044	time 0.7406 (0.7525)	loss 2.8204 (2.6702)	grad_norm 2.7873 (2.9456)	mem 23874MB
[2022-11-14 07:07:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][800/1251]	eta 0:05:39 lr 0.000044	time 0.7426 (0.7522)	loss 2.1218 (2.6675)	grad_norm 3.4356 (2.9425)	mem 23874MB
[2022-11-14 07:07:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][850/1251]	eta 0:05:01 lr 0.000043	time 0.7380 (0.7522)	loss 1.7863 (2.6646)	grad_norm 2.1579 (2.9410)	mem 23874MB
[2022-11-14 07:08:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][900/1251]	eta 0:04:23 lr 0.000043	time 0.7444 (0.7518)	loss 1.7958 (2.6638)	grad_norm 2.9560 (2.9292)	mem 23874MB
[2022-11-14 07:08:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][950/1251]	eta 0:03:46 lr 0.000043	time 0.7439 (0.7518)	loss 2.6904 (2.6581)	grad_norm 3.0313 (2.9236)	mem 23874MB
[2022-11-14 07:09:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][1000/1251]	eta 0:03:08 lr 0.000043	time 0.7391 (0.7517)	loss 1.9349 (2.6538)	grad_norm 3.0548 (2.9188)	mem 23874MB
[2022-11-14 07:10:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][1050/1251]	eta 0:02:31 lr 0.000043	time 0.7477 (0.7516)	loss 3.1258 (2.6584)	grad_norm 2.8790 (2.9160)	mem 23874MB
[2022-11-14 07:10:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][1100/1251]	eta 0:01:53 lr 0.000043	time 0.7406 (0.7514)	loss 1.8069 (2.6587)	grad_norm 3.6995 (2.9144)	mem 23874MB
[2022-11-14 07:11:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][1150/1251]	eta 0:01:15 lr 0.000043	time 0.7405 (0.7512)	loss 2.8650 (2.6567)	grad_norm 3.8676 (2.9104)	mem 23874MB
[2022-11-14 07:12:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][1200/1251]	eta 0:00:38 lr 0.000043	time 0.7405 (0.7511)	loss 2.8267 (2.6574)	grad_norm 2.7470 (2.9133)	mem 23874MB
[2022-11-14 07:12:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [264/300][1250/1251]	eta 0:00:00 lr 0.000043	time 0.7283 (0.7510)	loss 2.8027 (2.6558)	grad_norm 2.4156 (2.9237)	mem 23874MB
[2022-11-14 07:12:43 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 264 training takes 0:15:39
[2022-11-14 07:12:43 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_264.pth saving......
[2022-11-14 07:12:44 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_264.pth saved !!!
[2022-11-14 07:12:45 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.691 (1.691)	Loss 0.7413 (0.7413)	Acc@1 83.008 (83.008)	Acc@5 96.582 (96.582)	Mem 23874MB
[2022-11-14 07:12:56 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.454 Acc@5 96.504
[2022-11-14 07:12:56 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.5%
[2022-11-14 07:12:58 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.899 (1.899)	Loss 0.6922 (0.6922)	Acc@1 83.203 (83.203)	Acc@5 96.777 (96.777)	Mem 23874MB
[2022-11-14 07:13:09 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.944 Acc@5 96.710
[2022-11-14 07:13:09 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 07:13:09 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 07:13:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][0/1251]	eta 0:52:08 lr 0.000043	time 2.5009 (2.5009)	loss 3.0378 (3.0378)	grad_norm 3.1375 (3.1375)	mem 23874MB
[2022-11-14 07:13:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][50/1251]	eta 0:15:50 lr 0.000043	time 0.7392 (0.7913)	loss 2.4163 (2.6041)	grad_norm 2.3696 (3.0558)	mem 23874MB
[2022-11-14 07:14:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][100/1251]	eta 0:14:45 lr 0.000043	time 0.7401 (0.7695)	loss 2.8996 (2.6101)	grad_norm 2.6476 (2.9342)	mem 23874MB
[2022-11-14 07:15:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][150/1251]	eta 0:14:01 lr 0.000043	time 0.7445 (0.7642)	loss 2.8301 (2.6104)	grad_norm 2.9128 (2.8777)	mem 23874MB
[2022-11-14 07:15:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][200/1251]	eta 0:13:18 lr 0.000043	time 0.7441 (0.7601)	loss 2.5525 (2.6089)	grad_norm 2.8779 (2.8649)	mem 23874MB
[2022-11-14 07:16:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][250/1251]	eta 0:12:39 lr 0.000043	time 0.7490 (0.7587)	loss 2.7251 (2.6185)	grad_norm 5.1008 (2.8940)	mem 23874MB
[2022-11-14 07:16:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][300/1251]	eta 0:12:00 lr 0.000042	time 0.7389 (0.7574)	loss 2.9798 (2.6296)	grad_norm 3.1904 (2.8822)	mem 23874MB
[2022-11-14 07:17:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][350/1251]	eta 0:11:21 lr 0.000042	time 0.7493 (0.7559)	loss 2.3412 (2.6255)	grad_norm 3.2325 (2.8747)	mem 23874MB
[2022-11-14 07:18:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][400/1251]	eta 0:10:42 lr 0.000042	time 0.7434 (0.7556)	loss 2.8350 (2.6312)	grad_norm 3.2729 (2.9030)	mem 23874MB
[2022-11-14 07:18:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][450/1251]	eta 0:10:04 lr 0.000042	time 0.7450 (0.7551)	loss 2.6312 (2.6300)	grad_norm 2.4832 (2.8982)	mem 23874MB
[2022-11-14 07:19:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][500/1251]	eta 0:09:26 lr 0.000042	time 0.7422 (0.7546)	loss 1.7862 (2.6311)	grad_norm 2.6386 (2.8893)	mem 23874MB
[2022-11-14 07:20:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][550/1251]	eta 0:08:48 lr 0.000042	time 0.7501 (0.7544)	loss 2.7907 (2.6393)	grad_norm 3.0940 (2.9008)	mem 23874MB
[2022-11-14 07:20:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][600/1251]	eta 0:08:10 lr 0.000042	time 0.7413 (0.7537)	loss 2.6406 (2.6427)	grad_norm 2.4426 (2.9145)	mem 23874MB
[2022-11-14 07:21:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][650/1251]	eta 0:07:32 lr 0.000042	time 0.7422 (0.7536)	loss 2.4573 (2.6422)	grad_norm 2.8503 (2.9126)	mem 23874MB
[2022-11-14 07:21:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][700/1251]	eta 0:06:55 lr 0.000042	time 0.7377 (0.7534)	loss 2.2483 (2.6406)	grad_norm 2.5217 (2.9027)	mem 23874MB
[2022-11-14 07:22:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][750/1251]	eta 0:06:17 lr 0.000042	time 0.7485 (0.7530)	loss 3.1822 (2.6418)	grad_norm 2.3783 (2.9035)	mem 23874MB
[2022-11-14 07:23:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][800/1251]	eta 0:05:39 lr 0.000042	time 0.7411 (0.7529)	loss 3.1574 (2.6477)	grad_norm 2.4445 (2.9127)	mem 23874MB
[2022-11-14 07:23:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][850/1251]	eta 0:05:01 lr 0.000042	time 0.7454 (0.7528)	loss 1.9552 (2.6498)	grad_norm 2.8868 (2.9104)	mem 23874MB
[2022-11-14 07:24:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][900/1251]	eta 0:04:24 lr 0.000042	time 0.7508 (0.7524)	loss 2.7407 (2.6560)	grad_norm 16.8863 (2.9242)	mem 23874MB
[2022-11-14 07:25:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][950/1251]	eta 0:03:46 lr 0.000041	time 0.7657 (0.7523)	loss 2.2627 (2.6567)	grad_norm 2.3825 (2.9261)	mem 23874MB
[2022-11-14 07:25:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][1000/1251]	eta 0:03:08 lr 0.000041	time 0.7419 (0.7522)	loss 3.0321 (2.6546)	grad_norm 2.8552 (2.9306)	mem 23874MB
[2022-11-14 07:26:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][1050/1251]	eta 0:02:31 lr 0.000041	time 0.7428 (0.7522)	loss 1.6115 (2.6571)	grad_norm 2.5996 (2.9275)	mem 23874MB
[2022-11-14 07:26:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][1100/1251]	eta 0:01:53 lr 0.000041	time 0.8130 (0.7522)	loss 2.4963 (2.6582)	grad_norm 4.5070 (2.9315)	mem 23874MB
[2022-11-14 07:27:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][1150/1251]	eta 0:01:15 lr 0.000041	time 0.7466 (0.7520)	loss 2.8603 (2.6545)	grad_norm 3.1101 (2.9427)	mem 23874MB
[2022-11-14 07:28:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][1200/1251]	eta 0:00:38 lr 0.000041	time 0.7442 (0.7520)	loss 1.8520 (2.6527)	grad_norm 2.6799 (2.9495)	mem 23874MB
[2022-11-14 07:28:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [265/300][1250/1251]	eta 0:00:00 lr 0.000041	time 0.7288 (0.7518)	loss 2.6402 (2.6502)	grad_norm 2.7118 (2.9524)	mem 23874MB
[2022-11-14 07:28:49 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 265 training takes 0:15:40
[2022-11-14 07:28:50 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_265.pth saving......
[2022-11-14 07:28:51 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_265.pth saved !!!
[2022-11-14 07:28:53 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.766 (1.766)	Loss 0.7393 (0.7393)	Acc@1 82.227 (82.227)	Acc@5 96.094 (96.094)	Mem 23874MB
[2022-11-14 07:29:03 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.466 Acc@5 96.458
[2022-11-14 07:29:03 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.5%
[2022-11-14 07:29:05 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.063 (2.063)	Loss 0.6513 (0.6513)	Acc@1 83.984 (83.984)	Acc@5 97.266 (97.266)	Mem 23874MB
[2022-11-14 07:29:16 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.904 Acc@5 96.724
[2022-11-14 07:29:16 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 07:29:16 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 07:29:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][0/1251]	eta 0:51:54 lr 0.000041	time 2.4897 (2.4897)	loss 2.9460 (2.9460)	grad_norm 3.0912 (3.0912)	mem 23874MB
[2022-11-14 07:29:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][50/1251]	eta 0:15:48 lr 0.000041	time 0.7493 (0.7894)	loss 2.2941 (2.6695)	grad_norm 2.4640 (3.2428)	mem 23874MB
[2022-11-14 07:30:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][100/1251]	eta 0:14:48 lr 0.000041	time 0.7468 (0.7723)	loss 3.0494 (2.6196)	grad_norm 2.7490 (3.0491)	mem 23874MB
[2022-11-14 07:31:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][150/1251]	eta 0:14:00 lr 0.000041	time 0.7416 (0.7632)	loss 2.8321 (2.6028)	grad_norm 2.9073 (3.0427)	mem 23874MB
[2022-11-14 07:31:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][200/1251]	eta 0:13:19 lr 0.000041	time 0.7463 (0.7607)	loss 2.7616 (2.5941)	grad_norm 3.2005 (3.0195)	mem 23874MB
[2022-11-14 07:32:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][250/1251]	eta 0:12:39 lr 0.000041	time 0.7513 (0.7586)	loss 2.4955 (2.6096)	grad_norm 3.5502 (3.0665)	mem 23874MB
[2022-11-14 07:33:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][300/1251]	eta 0:12:00 lr 0.000041	time 0.7459 (0.7574)	loss 2.7637 (2.6077)	grad_norm 2.9222 (3.0620)	mem 23874MB
[2022-11-14 07:33:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][350/1251]	eta 0:11:21 lr 0.000041	time 0.7404 (0.7565)	loss 3.2071 (2.6161)	grad_norm 3.1194 (3.0288)	mem 23874MB
[2022-11-14 07:34:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][400/1251]	eta 0:10:42 lr 0.000040	time 0.8296 (0.7555)	loss 2.8363 (2.6273)	grad_norm 3.1473 (3.0298)	mem 23874MB
[2022-11-14 07:34:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][450/1251]	eta 0:10:04 lr 0.000040	time 0.7425 (0.7548)	loss 2.2292 (2.6146)	grad_norm 2.8359 (3.0113)	mem 23874MB
[2022-11-14 07:35:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][500/1251]	eta 0:09:26 lr 0.000040	time 0.7472 (0.7546)	loss 2.4486 (2.6216)	grad_norm 3.4646 (3.0052)	mem 23874MB
[2022-11-14 07:36:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][550/1251]	eta 0:08:48 lr 0.000040	time 0.7404 (0.7541)	loss 2.9577 (2.6235)	grad_norm 2.2571 (2.9876)	mem 23874MB
[2022-11-14 07:36:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][600/1251]	eta 0:08:10 lr 0.000040	time 0.8280 (0.7542)	loss 2.3080 (2.6291)	grad_norm 3.1405 (3.0017)	mem 23874MB
[2022-11-14 07:37:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][650/1251]	eta 0:07:32 lr 0.000040	time 0.7418 (0.7537)	loss 2.9924 (2.6287)	grad_norm 2.3134 (3.0076)	mem 23874MB
[2022-11-14 07:38:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][700/1251]	eta 0:06:55 lr 0.000040	time 0.7432 (0.7533)	loss 3.0280 (2.6300)	grad_norm 2.8965 (3.0020)	mem 23874MB
[2022-11-14 07:38:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][750/1251]	eta 0:06:17 lr 0.000040	time 0.7456 (0.7533)	loss 2.8422 (2.6295)	grad_norm 4.9035 (3.0295)	mem 23874MB
[2022-11-14 07:39:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][800/1251]	eta 0:05:39 lr 0.000040	time 0.7389 (0.7530)	loss 1.8520 (2.6276)	grad_norm 3.2199 (3.0268)	mem 23874MB
[2022-11-14 07:39:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][850/1251]	eta 0:05:01 lr 0.000040	time 0.8415 (0.7530)	loss 2.8553 (2.6283)	grad_norm 2.5269 (3.0202)	mem 23874MB
[2022-11-14 07:40:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][900/1251]	eta 0:04:24 lr 0.000040	time 0.7499 (0.7529)	loss 2.3205 (2.6218)	grad_norm 2.7123 (3.0147)	mem 23874MB
[2022-11-14 07:41:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][950/1251]	eta 0:03:46 lr 0.000040	time 0.7465 (0.7527)	loss 2.8406 (2.6209)	grad_norm 2.7662 (3.0076)	mem 23874MB
[2022-11-14 07:41:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][1000/1251]	eta 0:03:08 lr 0.000040	time 0.7452 (0.7525)	loss 2.4131 (2.6209)	grad_norm 2.5250 (2.9970)	mem 23874MB
[2022-11-14 07:42:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][1050/1251]	eta 0:02:31 lr 0.000040	time 0.7391 (0.7525)	loss 3.0701 (2.6227)	grad_norm 2.7215 (3.0036)	mem 23874MB
[2022-11-14 07:43:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][1100/1251]	eta 0:01:53 lr 0.000039	time 0.7407 (0.7523)	loss 3.1655 (2.6237)	grad_norm 3.6940 (3.0055)	mem 23874MB
[2022-11-14 07:43:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][1150/1251]	eta 0:01:15 lr 0.000039	time 0.7434 (0.7523)	loss 3.1735 (2.6242)	grad_norm 2.6636 (3.0045)	mem 23874MB
[2022-11-14 07:44:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][1200/1251]	eta 0:00:38 lr 0.000039	time 0.7421 (0.7521)	loss 2.8396 (2.6248)	grad_norm 2.9794 (2.9955)	mem 23874MB
[2022-11-14 07:44:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [266/300][1250/1251]	eta 0:00:00 lr 0.000039	time 0.7292 (0.7519)	loss 2.9792 (2.6218)	grad_norm 2.6445 (3.0054)	mem 23874MB
[2022-11-14 07:44:57 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 266 training takes 0:15:40
[2022-11-14 07:44:57 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_266.pth saving......
[2022-11-14 07:44:58 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_266.pth saved !!!
[2022-11-14 07:45:00 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.816 (1.816)	Loss 0.7093 (0.7093)	Acc@1 83.887 (83.887)	Acc@5 96.875 (96.875)	Mem 23874MB
[2022-11-14 07:45:11 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.506 Acc@5 96.498
[2022-11-14 07:45:11 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.5%
[2022-11-14 07:45:13 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.921 (1.921)	Loss 0.6457 (0.6457)	Acc@1 83.887 (83.887)	Acc@5 97.461 (97.461)	Mem 23874MB
[2022-11-14 07:45:23 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.904 Acc@5 96.714
[2022-11-14 07:45:23 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 07:45:23 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 07:45:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][0/1251]	eta 0:52:50 lr 0.000039	time 2.5345 (2.5345)	loss 2.6794 (2.6794)	grad_norm 3.0821 (3.0821)	mem 23874MB
[2022-11-14 07:46:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][50/1251]	eta 0:15:45 lr 0.000039	time 0.7421 (0.7873)	loss 2.2938 (2.6262)	grad_norm 3.2716 (2.8266)	mem 23874MB
[2022-11-14 07:46:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][100/1251]	eta 0:14:46 lr 0.000039	time 0.8420 (0.7698)	loss 2.8732 (2.6288)	grad_norm 2.4876 (2.8635)	mem 23874MB
[2022-11-14 07:47:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][150/1251]	eta 0:14:01 lr 0.000039	time 0.7540 (0.7644)	loss 2.4168 (2.6211)	grad_norm 3.2651 (2.9079)	mem 23874MB
[2022-11-14 07:47:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][200/1251]	eta 0:13:18 lr 0.000039	time 0.7410 (0.7598)	loss 2.4399 (2.6129)	grad_norm 3.1258 (inf)	mem 23874MB
[2022-11-14 07:48:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][250/1251]	eta 0:12:38 lr 0.000039	time 0.7399 (0.7577)	loss 1.7663 (2.6198)	grad_norm 2.7634 (inf)	mem 23874MB
[2022-11-14 07:49:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][300/1251]	eta 0:11:59 lr 0.000039	time 0.7500 (0.7562)	loss 2.2398 (2.6027)	grad_norm 2.8473 (inf)	mem 23874MB
[2022-11-14 07:49:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][350/1251]	eta 0:11:20 lr 0.000039	time 0.7410 (0.7557)	loss 2.8316 (2.6071)	grad_norm 2.7656 (inf)	mem 23874MB
[2022-11-14 07:50:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][400/1251]	eta 0:10:42 lr 0.000039	time 0.7421 (0.7548)	loss 2.4073 (2.6093)	grad_norm 2.9999 (inf)	mem 23874MB
[2022-11-14 07:51:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][450/1251]	eta 0:10:04 lr 0.000039	time 0.7445 (0.7544)	loss 2.7932 (2.6261)	grad_norm 2.4851 (inf)	mem 23874MB
[2022-11-14 07:51:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][500/1251]	eta 0:09:26 lr 0.000039	time 0.7442 (0.7541)	loss 2.7652 (2.6266)	grad_norm 2.2454 (inf)	mem 23874MB
[2022-11-14 07:52:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][550/1251]	eta 0:08:48 lr 0.000038	time 0.7495 (0.7539)	loss 3.1321 (2.6304)	grad_norm 2.7333 (inf)	mem 23874MB
[2022-11-14 07:52:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][600/1251]	eta 0:08:10 lr 0.000038	time 0.7438 (0.7534)	loss 2.0403 (2.6263)	grad_norm 2.6600 (inf)	mem 23874MB
[2022-11-14 07:53:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][650/1251]	eta 0:07:32 lr 0.000038	time 0.7427 (0.7533)	loss 2.8778 (2.6359)	grad_norm 2.5385 (inf)	mem 23874MB
[2022-11-14 07:54:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][700/1251]	eta 0:06:54 lr 0.000038	time 0.7418 (0.7530)	loss 3.0810 (2.6334)	grad_norm 2.5958 (inf)	mem 23874MB
[2022-11-14 07:54:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][750/1251]	eta 0:06:17 lr 0.000038	time 0.7447 (0.7531)	loss 2.6610 (2.6284)	grad_norm 2.8238 (inf)	mem 23874MB
[2022-11-14 07:55:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][800/1251]	eta 0:05:39 lr 0.000038	time 0.7415 (0.7530)	loss 3.0657 (2.6273)	grad_norm 2.9934 (inf)	mem 23874MB
[2022-11-14 07:56:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][850/1251]	eta 0:05:01 lr 0.000038	time 0.7542 (0.7527)	loss 2.7131 (2.6252)	grad_norm 6.0092 (inf)	mem 23874MB
[2022-11-14 07:56:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][900/1251]	eta 0:04:24 lr 0.000038	time 0.7395 (0.7527)	loss 2.3896 (2.6221)	grad_norm 2.9513 (inf)	mem 23874MB
[2022-11-14 07:57:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][950/1251]	eta 0:03:46 lr 0.000038	time 0.7430 (0.7525)	loss 2.1688 (2.6219)	grad_norm 2.4306 (inf)	mem 23874MB
[2022-11-14 07:57:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][1000/1251]	eta 0:03:08 lr 0.000038	time 0.7416 (0.7523)	loss 3.0503 (2.6243)	grad_norm 2.7288 (inf)	mem 23874MB
[2022-11-14 07:58:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][1050/1251]	eta 0:02:31 lr 0.000038	time 0.7422 (0.7522)	loss 2.3161 (2.6233)	grad_norm 2.8820 (inf)	mem 23874MB
[2022-11-14 07:59:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][1100/1251]	eta 0:01:53 lr 0.000038	time 0.7427 (0.7520)	loss 2.3847 (2.6219)	grad_norm 2.3834 (inf)	mem 23874MB
[2022-11-14 07:59:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][1150/1251]	eta 0:01:15 lr 0.000038	time 0.7427 (0.7519)	loss 2.9733 (2.6252)	grad_norm 2.2564 (inf)	mem 23874MB
[2022-11-14 08:00:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][1200/1251]	eta 0:00:38 lr 0.000038	time 0.7444 (0.7520)	loss 2.3080 (2.6268)	grad_norm 2.2962 (inf)	mem 23874MB
[2022-11-14 08:01:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [267/300][1250/1251]	eta 0:00:00 lr 0.000038	time 0.7279 (0.7516)	loss 1.6144 (2.6238)	grad_norm 2.5890 (inf)	mem 23874MB
[2022-11-14 08:01:04 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 267 training takes 0:15:40
[2022-11-14 08:01:04 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_267.pth saving......
[2022-11-14 08:01:05 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_267.pth saved !!!
[2022-11-14 08:01:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.744 (1.744)	Loss 0.6586 (0.6586)	Acc@1 83.301 (83.301)	Acc@5 97.070 (97.070)	Mem 23874MB
[2022-11-14 08:01:18 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.478 Acc@5 96.530
[2022-11-14 08:01:18 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.5%
[2022-11-14 08:01:20 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.997 (1.997)	Loss 0.6972 (0.6972)	Acc@1 83.008 (83.008)	Acc@5 96.289 (96.289)	Mem 23874MB
[2022-11-14 08:01:31 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.918 Acc@5 96.706
[2022-11-14 08:01:31 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 08:01:31 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 08:01:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][0/1251]	eta 0:53:23 lr 0.000038	time 2.5611 (2.5611)	loss 2.7439 (2.7439)	grad_norm 2.8836 (2.8836)	mem 23874MB
[2022-11-14 08:02:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][50/1251]	eta 0:15:44 lr 0.000037	time 0.7487 (0.7866)	loss 2.7001 (2.6073)	grad_norm 2.6833 (3.3274)	mem 23874MB
[2022-11-14 08:02:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][100/1251]	eta 0:14:46 lr 0.000037	time 0.7440 (0.7705)	loss 3.1210 (2.6251)	grad_norm 2.9001 (3.1560)	mem 23874MB
[2022-11-14 08:03:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][150/1251]	eta 0:14:00 lr 0.000037	time 0.7386 (0.7637)	loss 2.5995 (2.5895)	grad_norm 2.5851 (3.0875)	mem 23874MB
[2022-11-14 08:04:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][200/1251]	eta 0:13:18 lr 0.000037	time 0.7429 (0.7599)	loss 2.6239 (2.6105)	grad_norm 3.3698 (3.2220)	mem 23874MB
[2022-11-14 08:04:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][250/1251]	eta 0:12:38 lr 0.000037	time 0.7375 (0.7575)	loss 2.5889 (2.6130)	grad_norm 2.6522 (3.1886)	mem 23874MB
[2022-11-14 08:05:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][300/1251]	eta 0:11:59 lr 0.000037	time 0.8251 (0.7561)	loss 2.5398 (2.6227)	grad_norm 2.3610 (3.1422)	mem 23874MB
[2022-11-14 08:05:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][350/1251]	eta 0:11:20 lr 0.000037	time 0.7402 (0.7549)	loss 1.8282 (2.6252)	grad_norm 3.2336 (3.1603)	mem 23874MB
[2022-11-14 08:06:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][400/1251]	eta 0:10:42 lr 0.000037	time 0.7499 (0.7546)	loss 2.6785 (2.6185)	grad_norm 5.3791 (3.1319)	mem 23874MB
[2022-11-14 08:07:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][450/1251]	eta 0:10:03 lr 0.000037	time 0.7411 (0.7538)	loss 2.7839 (2.6170)	grad_norm 2.9413 (3.0939)	mem 23874MB
[2022-11-14 08:07:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][500/1251]	eta 0:09:25 lr 0.000037	time 0.7450 (0.7534)	loss 2.4272 (2.6184)	grad_norm 3.1177 (3.0765)	mem 23874MB
[2022-11-14 08:08:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][550/1251]	eta 0:08:47 lr 0.000037	time 0.8101 (0.7530)	loss 2.9912 (2.6218)	grad_norm 2.8484 (3.0632)	mem 23874MB
[2022-11-14 08:09:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][600/1251]	eta 0:08:09 lr 0.000037	time 0.7380 (0.7526)	loss 2.8953 (2.6226)	grad_norm 2.9997 (3.0611)	mem 23874MB
[2022-11-14 08:09:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][650/1251]	eta 0:07:32 lr 0.000037	time 0.7470 (0.7525)	loss 2.0518 (2.6166)	grad_norm 2.8904 (3.0567)	mem 23874MB
[2022-11-14 08:10:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][700/1251]	eta 0:06:54 lr 0.000037	time 0.8343 (0.7522)	loss 2.2331 (2.6168)	grad_norm 5.1891 (3.0478)	mem 23874MB
[2022-11-14 08:10:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][750/1251]	eta 0:06:16 lr 0.000037	time 0.7430 (0.7521)	loss 2.9678 (2.6189)	grad_norm 3.4135 (3.0597)	mem 23874MB
[2022-11-14 08:11:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][800/1251]	eta 0:05:39 lr 0.000036	time 0.7417 (0.7520)	loss 2.9278 (2.6246)	grad_norm 2.9514 (3.0652)	mem 23874MB
[2022-11-14 08:12:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][850/1251]	eta 0:05:01 lr 0.000036	time 0.7429 (0.7518)	loss 2.7144 (2.6251)	grad_norm 2.7953 (3.0681)	mem 23874MB
[2022-11-14 08:12:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][900/1251]	eta 0:04:23 lr 0.000036	time 0.7431 (0.7516)	loss 2.7300 (2.6309)	grad_norm 2.4833 (3.0606)	mem 23874MB
[2022-11-14 08:13:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][950/1251]	eta 0:03:46 lr 0.000036	time 0.7443 (0.7515)	loss 2.5734 (2.6365)	grad_norm 2.6208 (inf)	mem 23874MB
[2022-11-14 08:14:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][1000/1251]	eta 0:03:08 lr 0.000036	time 0.8242 (0.7514)	loss 2.5214 (2.6326)	grad_norm 5.3175 (inf)	mem 23874MB
[2022-11-14 08:14:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][1050/1251]	eta 0:02:31 lr 0.000036	time 0.7401 (0.7514)	loss 1.8439 (2.6312)	grad_norm 2.4410 (inf)	mem 23874MB
[2022-11-14 08:15:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][1100/1251]	eta 0:01:53 lr 0.000036	time 0.8283 (0.7513)	loss 2.3333 (2.6306)	grad_norm 2.2567 (inf)	mem 23874MB
[2022-11-14 08:15:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][1150/1251]	eta 0:01:15 lr 0.000036	time 0.7395 (0.7513)	loss 2.4664 (2.6299)	grad_norm 3.8730 (inf)	mem 23874MB
[2022-11-14 08:16:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][1200/1251]	eta 0:00:38 lr 0.000036	time 0.7476 (0.7514)	loss 2.7403 (2.6294)	grad_norm 3.2188 (inf)	mem 23874MB
[2022-11-14 08:17:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [268/300][1250/1251]	eta 0:00:00 lr 0.000036	time 0.7280 (0.7511)	loss 2.6221 (2.6268)	grad_norm 4.6904 (inf)	mem 23874MB
[2022-11-14 08:17:11 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 268 training takes 0:15:39
[2022-11-14 08:17:11 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_268.pth saving......
[2022-11-14 08:17:12 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_268.pth saved !!!
[2022-11-14 08:17:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.736 (1.736)	Loss 0.7029 (0.7029)	Acc@1 83.398 (83.398)	Acc@5 96.875 (96.875)	Mem 23874MB
[2022-11-14 08:17:24 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.464 Acc@5 96.564
[2022-11-14 08:17:24 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.5%
[2022-11-14 08:17:26 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.880 (1.880)	Loss 0.7081 (0.7081)	Acc@1 84.961 (84.961)	Acc@5 96.387 (96.387)	Mem 23874MB
[2022-11-14 08:17:37 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.868 Acc@5 96.702
[2022-11-14 08:17:37 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 08:17:37 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 08:17:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][0/1251]	eta 0:53:30 lr 0.000036	time 2.5661 (2.5661)	loss 2.2786 (2.2786)	grad_norm 2.9274 (2.9274)	mem 23874MB
[2022-11-14 08:18:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][50/1251]	eta 0:15:44 lr 0.000036	time 0.8258 (0.7864)	loss 2.2558 (2.6190)	grad_norm 2.8449 (3.0192)	mem 23874MB
[2022-11-14 08:18:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][100/1251]	eta 0:14:44 lr 0.000036	time 0.7418 (0.7684)	loss 2.9091 (2.5765)	grad_norm 2.9833 (2.9462)	mem 23874MB
[2022-11-14 08:19:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][150/1251]	eta 0:13:59 lr 0.000036	time 0.7439 (0.7625)	loss 2.9259 (2.6151)	grad_norm 2.4938 (2.9881)	mem 23874MB
[2022-11-14 08:20:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][200/1251]	eta 0:13:18 lr 0.000036	time 0.7487 (0.7601)	loss 2.9884 (2.5998)	grad_norm 2.8931 (3.0757)	mem 23874MB
[2022-11-14 08:20:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][250/1251]	eta 0:12:38 lr 0.000036	time 0.7541 (0.7578)	loss 3.0964 (2.5930)	grad_norm 2.8291 (inf)	mem 23874MB
[2022-11-14 08:21:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][300/1251]	eta 0:11:59 lr 0.000035	time 0.7413 (0.7566)	loss 2.3767 (2.5979)	grad_norm 2.5062 (inf)	mem 23874MB
[2022-11-14 08:22:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][350/1251]	eta 0:11:20 lr 0.000035	time 0.7443 (0.7556)	loss 2.2380 (2.6082)	grad_norm 2.7474 (inf)	mem 23874MB
[2022-11-14 08:22:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][400/1251]	eta 0:10:42 lr 0.000035	time 0.7421 (0.7549)	loss 2.2961 (2.6087)	grad_norm 2.9392 (inf)	mem 23874MB
[2022-11-14 08:23:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][450/1251]	eta 0:10:04 lr 0.000035	time 0.7461 (0.7545)	loss 3.0058 (2.6099)	grad_norm 5.9294 (inf)	mem 23874MB
[2022-11-14 08:23:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][500/1251]	eta 0:09:26 lr 0.000035	time 0.7450 (0.7544)	loss 1.9841 (2.6159)	grad_norm 2.7894 (inf)	mem 23874MB
[2022-11-14 08:24:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][550/1251]	eta 0:08:48 lr 0.000035	time 0.8079 (0.7540)	loss 3.0665 (2.6207)	grad_norm 2.3842 (inf)	mem 23874MB
[2022-11-14 08:25:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][600/1251]	eta 0:08:10 lr 0.000035	time 0.7461 (0.7539)	loss 2.9371 (2.6216)	grad_norm 3.6737 (inf)	mem 23874MB
[2022-11-14 08:25:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][650/1251]	eta 0:07:32 lr 0.000035	time 0.7393 (0.7536)	loss 2.8622 (2.6255)	grad_norm 2.3038 (inf)	mem 23874MB
[2022-11-14 08:26:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][700/1251]	eta 0:06:55 lr 0.000035	time 0.7465 (0.7536)	loss 2.7132 (2.6311)	grad_norm 2.9309 (inf)	mem 23874MB
[2022-11-14 08:27:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][750/1251]	eta 0:06:17 lr 0.000035	time 0.7434 (0.7536)	loss 2.3041 (2.6378)	grad_norm 2.5820 (inf)	mem 23874MB
[2022-11-14 08:27:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][800/1251]	eta 0:05:39 lr 0.000035	time 0.7295 (0.7536)	loss 2.0635 (2.6416)	grad_norm 2.6725 (inf)	mem 23874MB
[2022-11-14 08:28:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][850/1251]	eta 0:05:02 lr 0.000035	time 0.7377 (0.7533)	loss 2.6036 (2.6381)	grad_norm 2.5667 (inf)	mem 23874MB
[2022-11-14 08:28:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][900/1251]	eta 0:04:24 lr 0.000035	time 0.7471 (0.7531)	loss 2.1830 (2.6377)	grad_norm 2.5223 (inf)	mem 23874MB
[2022-11-14 08:29:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][950/1251]	eta 0:03:46 lr 0.000035	time 0.7397 (0.7530)	loss 2.7226 (2.6358)	grad_norm 2.4694 (inf)	mem 23874MB
[2022-11-14 08:30:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][1000/1251]	eta 0:03:09 lr 0.000035	time 0.7419 (0.7531)	loss 1.6435 (2.6356)	grad_norm 2.5939 (inf)	mem 23874MB
[2022-11-14 08:30:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][1050/1251]	eta 0:02:31 lr 0.000034	time 0.7422 (0.7529)	loss 2.8707 (2.6327)	grad_norm 2.5553 (inf)	mem 23874MB
[2022-11-14 08:31:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][1100/1251]	eta 0:01:53 lr 0.000034	time 0.7437 (0.7529)	loss 2.7131 (2.6321)	grad_norm 2.4095 (inf)	mem 23874MB
[2022-11-14 08:32:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][1150/1251]	eta 0:01:16 lr 0.000034	time 0.7413 (0.7528)	loss 2.1542 (2.6341)	grad_norm 3.5605 (inf)	mem 23874MB
[2022-11-14 08:32:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][1200/1251]	eta 0:00:38 lr 0.000034	time 0.7393 (0.7526)	loss 3.0589 (2.6359)	grad_norm 3.2820 (inf)	mem 23874MB
[2022-11-14 08:33:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [269/300][1250/1251]	eta 0:00:00 lr 0.000034	time 0.7566 (0.7525)	loss 2.7043 (2.6365)	grad_norm 12.7455 (inf)	mem 23874MB
[2022-11-14 08:33:19 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 269 training takes 0:15:41
[2022-11-14 08:33:19 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_269.pth saving......
[2022-11-14 08:33:20 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_269.pth saved !!!
[2022-11-14 08:33:22 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.680 (1.680)	Loss 0.7628 (0.7628)	Acc@1 81.543 (81.543)	Acc@5 96.777 (96.777)	Mem 23874MB
[2022-11-14 08:33:32 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.596 Acc@5 96.524
[2022-11-14 08:33:32 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.6%
[2022-11-14 08:33:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.968 (1.968)	Loss 0.7011 (0.7011)	Acc@1 83.105 (83.105)	Acc@5 96.680 (96.680)	Mem 23874MB
[2022-11-14 08:33:45 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.874 Acc@5 96.684
[2022-11-14 08:33:45 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 08:33:45 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 08:33:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][0/1251]	eta 0:54:11 lr 0.000034	time 2.5990 (2.5990)	loss 2.3437 (2.3437)	grad_norm 2.2305 (2.2305)	mem 23874MB
[2022-11-14 08:34:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][50/1251]	eta 0:15:51 lr 0.000034	time 0.7506 (0.7924)	loss 2.7324 (2.6807)	grad_norm 9.6067 (3.3892)	mem 23874MB
[2022-11-14 08:35:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][100/1251]	eta 0:14:48 lr 0.000034	time 0.7410 (0.7720)	loss 2.3352 (2.6234)	grad_norm 2.7338 (3.2366)	mem 23874MB
[2022-11-14 08:35:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][150/1251]	eta 0:14:01 lr 0.000034	time 0.7358 (0.7642)	loss 3.1192 (2.6360)	grad_norm 2.6597 (3.1488)	mem 23874MB
[2022-11-14 08:36:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][200/1251]	eta 0:13:19 lr 0.000034	time 0.8189 (0.7611)	loss 2.2650 (2.6331)	grad_norm 2.9323 (3.0832)	mem 23874MB
[2022-11-14 08:36:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][250/1251]	eta 0:12:39 lr 0.000034	time 0.7419 (0.7586)	loss 2.1613 (2.6170)	grad_norm 3.1334 (3.0447)	mem 23874MB
[2022-11-14 08:37:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][300/1251]	eta 0:11:59 lr 0.000034	time 0.7383 (0.7568)	loss 2.8042 (2.6340)	grad_norm 2.6081 (3.0306)	mem 23874MB
[2022-11-14 08:38:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][350/1251]	eta 0:11:21 lr 0.000034	time 0.7467 (0.7559)	loss 1.9977 (2.6261)	grad_norm 2.4132 (2.9938)	mem 23874MB
[2022-11-14 08:38:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][400/1251]	eta 0:10:42 lr 0.000034	time 0.7419 (0.7551)	loss 2.9652 (2.6366)	grad_norm 2.4506 (2.9950)	mem 23874MB
[2022-11-14 08:39:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][450/1251]	eta 0:10:04 lr 0.000034	time 0.7470 (0.7550)	loss 2.2215 (2.6354)	grad_norm 2.8936 (3.0153)	mem 23874MB
[2022-11-14 08:40:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][500/1251]	eta 0:09:26 lr 0.000034	time 0.7435 (0.7542)	loss 2.9314 (2.6252)	grad_norm 2.7833 (3.0136)	mem 23874MB
[2022-11-14 08:40:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][550/1251]	eta 0:08:48 lr 0.000034	time 0.7374 (0.7539)	loss 2.7755 (2.6290)	grad_norm 3.8184 (3.0209)	mem 23874MB
[2022-11-14 08:41:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][600/1251]	eta 0:08:10 lr 0.000033	time 0.7430 (0.7537)	loss 2.7553 (2.6286)	grad_norm 3.3274 (3.0112)	mem 23874MB
[2022-11-14 08:41:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][650/1251]	eta 0:07:32 lr 0.000033	time 0.7413 (0.7535)	loss 3.2352 (2.6310)	grad_norm 2.6555 (3.0130)	mem 23874MB
[2022-11-14 08:42:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][700/1251]	eta 0:06:55 lr 0.000033	time 0.7423 (0.7533)	loss 2.3530 (2.6321)	grad_norm 2.0658 (3.0068)	mem 23874MB
[2022-11-14 08:43:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][750/1251]	eta 0:06:17 lr 0.000033	time 0.7368 (0.7531)	loss 2.8324 (2.6332)	grad_norm 2.7313 (2.9905)	mem 23874MB
[2022-11-14 08:43:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][800/1251]	eta 0:05:39 lr 0.000033	time 0.7561 (0.7529)	loss 2.9441 (2.6280)	grad_norm 2.5179 (2.9784)	mem 23874MB
[2022-11-14 08:44:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][850/1251]	eta 0:05:01 lr 0.000033	time 0.7454 (0.7526)	loss 2.8521 (2.6287)	grad_norm 2.8595 (2.9800)	mem 23874MB
[2022-11-14 08:45:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][900/1251]	eta 0:04:24 lr 0.000033	time 0.7476 (0.7525)	loss 2.0510 (2.6285)	grad_norm 2.6277 (2.9801)	mem 23874MB
[2022-11-14 08:45:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][950/1251]	eta 0:03:46 lr 0.000033	time 0.7425 (0.7524)	loss 2.1177 (2.6283)	grad_norm 2.8824 (2.9736)	mem 23874MB
[2022-11-14 08:46:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][1000/1251]	eta 0:03:08 lr 0.000033	time 0.7412 (0.7524)	loss 1.8579 (2.6260)	grad_norm 2.4340 (2.9796)	mem 23874MB
[2022-11-14 08:46:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][1050/1251]	eta 0:02:31 lr 0.000033	time 0.7409 (0.7522)	loss 2.4622 (2.6305)	grad_norm 2.5400 (2.9986)	mem 23874MB
[2022-11-14 08:47:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][1100/1251]	eta 0:01:53 lr 0.000033	time 0.7444 (0.7521)	loss 2.8451 (2.6311)	grad_norm 2.4024 (2.9915)	mem 23874MB
[2022-11-14 08:48:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][1150/1251]	eta 0:01:15 lr 0.000033	time 0.7424 (0.7522)	loss 3.1525 (2.6325)	grad_norm 3.0666 (2.9937)	mem 23874MB
[2022-11-14 08:48:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][1200/1251]	eta 0:00:38 lr 0.000033	time 0.7433 (0.7520)	loss 2.8310 (2.6328)	grad_norm 2.3860 (2.9914)	mem 23874MB
[2022-11-14 08:49:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [270/300][1250/1251]	eta 0:00:00 lr 0.000033	time 0.7306 (0.7520)	loss 2.9386 (2.6353)	grad_norm 3.0515 (2.9908)	mem 23874MB
[2022-11-14 08:49:26 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 270 training takes 0:15:40
[2022-11-14 08:49:26 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_270.pth saving......
[2022-11-14 08:49:27 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_270.pth saved !!!
[2022-11-14 08:49:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.869 (1.869)	Loss 0.7210 (0.7210)	Acc@1 82.520 (82.520)	Acc@5 96.582 (96.582)	Mem 23874MB
[2022-11-14 08:49:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.656 Acc@5 96.576
[2022-11-14 08:49:40 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.7%
[2022-11-14 08:49:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.933 (1.933)	Loss 0.6881 (0.6881)	Acc@1 83.203 (83.203)	Acc@5 96.680 (96.680)	Mem 23874MB
[2022-11-14 08:49:52 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.856 Acc@5 96.672
[2022-11-14 08:49:52 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 08:49:52 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 08:49:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][0/1251]	eta 0:52:22 lr 0.000033	time 2.5123 (2.5123)	loss 2.7310 (2.7310)	grad_norm 3.4991 (3.4991)	mem 23874MB
[2022-11-14 08:50:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][50/1251]	eta 0:15:46 lr 0.000033	time 0.7404 (0.7878)	loss 2.9840 (2.6048)	grad_norm 3.1513 (3.3344)	mem 23874MB
[2022-11-14 08:51:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][100/1251]	eta 0:14:44 lr 0.000033	time 0.7396 (0.7686)	loss 2.1787 (2.6021)	grad_norm 2.2241 (3.0846)	mem 23874MB
[2022-11-14 08:51:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][150/1251]	eta 0:14:00 lr 0.000032	time 0.7449 (0.7630)	loss 2.1818 (2.6055)	grad_norm 2.7956 (3.0368)	mem 23874MB
[2022-11-14 08:52:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][200/1251]	eta 0:13:18 lr 0.000032	time 0.7470 (0.7596)	loss 2.9734 (2.6137)	grad_norm 2.7188 (2.9895)	mem 23874MB
[2022-11-14 08:53:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][250/1251]	eta 0:12:38 lr 0.000032	time 0.7457 (0.7579)	loss 2.8103 (2.6084)	grad_norm 2.9734 (2.9754)	mem 23874MB
[2022-11-14 08:53:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][300/1251]	eta 0:11:59 lr 0.000032	time 0.7442 (0.7566)	loss 3.0243 (2.6168)	grad_norm 3.0810 (3.0617)	mem 23874MB
[2022-11-14 08:54:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][350/1251]	eta 0:11:20 lr 0.000032	time 0.7447 (0.7558)	loss 2.7070 (2.6147)	grad_norm 2.7883 (3.0634)	mem 23874MB
[2022-11-14 08:54:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][400/1251]	eta 0:10:42 lr 0.000032	time 0.7384 (0.7551)	loss 2.0361 (2.6082)	grad_norm 3.2299 (3.0895)	mem 23874MB
[2022-11-14 08:55:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][450/1251]	eta 0:10:04 lr 0.000032	time 0.7385 (0.7548)	loss 2.8897 (2.6131)	grad_norm 2.7339 (3.1104)	mem 23874MB
[2022-11-14 08:56:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][500/1251]	eta 0:09:26 lr 0.000032	time 0.7413 (0.7541)	loss 2.7721 (2.6169)	grad_norm 2.4337 (3.1180)	mem 23874MB
[2022-11-14 08:56:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][550/1251]	eta 0:08:48 lr 0.000032	time 0.7425 (0.7540)	loss 2.9684 (2.6265)	grad_norm 2.9744 (3.1361)	mem 23874MB
[2022-11-14 08:57:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][600/1251]	eta 0:08:10 lr 0.000032	time 0.8163 (0.7537)	loss 2.9639 (2.6223)	grad_norm 3.0413 (3.1425)	mem 23874MB
[2022-11-14 08:58:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][650/1251]	eta 0:07:32 lr 0.000032	time 0.7467 (0.7533)	loss 1.8602 (2.6197)	grad_norm 3.0341 (nan)	mem 23874MB
[2022-11-14 08:58:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][700/1251]	eta 0:06:54 lr 0.000032	time 0.7502 (0.7531)	loss 3.0143 (2.6203)	grad_norm 2.8245 (nan)	mem 23874MB
[2022-11-14 08:59:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][750/1251]	eta 0:06:17 lr 0.000032	time 0.7472 (0.7528)	loss 2.9990 (2.6174)	grad_norm 2.8023 (nan)	mem 23874MB
[2022-11-14 08:59:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][800/1251]	eta 0:05:39 lr 0.000032	time 0.7432 (0.7528)	loss 1.8305 (2.6190)	grad_norm 2.6685 (nan)	mem 23874MB
[2022-11-14 09:00:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][850/1251]	eta 0:05:01 lr 0.000032	time 0.7490 (0.7525)	loss 2.1221 (2.6230)	grad_norm 2.5856 (nan)	mem 23874MB
[2022-11-14 09:01:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][900/1251]	eta 0:04:24 lr 0.000032	time 0.7361 (0.7523)	loss 2.1291 (2.6279)	grad_norm 2.4419 (nan)	mem 23874MB
[2022-11-14 09:01:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][950/1251]	eta 0:03:46 lr 0.000031	time 0.7455 (0.7523)	loss 2.3677 (2.6255)	grad_norm 2.9169 (nan)	mem 23874MB
[2022-11-14 09:02:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][1000/1251]	eta 0:03:08 lr 0.000031	time 0.7446 (0.7520)	loss 2.4219 (2.6265)	grad_norm 3.5252 (nan)	mem 23874MB
[2022-11-14 09:03:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][1050/1251]	eta 0:02:31 lr 0.000031	time 0.7383 (0.7520)	loss 1.8938 (2.6287)	grad_norm 2.4056 (nan)	mem 23874MB
[2022-11-14 09:03:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][1100/1251]	eta 0:01:53 lr 0.000031	time 0.7373 (0.7520)	loss 2.8204 (2.6306)	grad_norm 2.7546 (nan)	mem 23874MB
[2022-11-14 09:04:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][1150/1251]	eta 0:01:15 lr 0.000031	time 0.7370 (0.7516)	loss 2.8333 (2.6334)	grad_norm 2.4649 (nan)	mem 23874MB
[2022-11-14 09:04:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][1200/1251]	eta 0:00:38 lr 0.000031	time 0.7392 (0.7517)	loss 2.7166 (2.6308)	grad_norm 2.6566 (nan)	mem 23874MB
[2022-11-14 09:05:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [271/300][1250/1251]	eta 0:00:00 lr 0.000031	time 0.7263 (0.7514)	loss 2.9008 (2.6289)	grad_norm 2.7465 (nan)	mem 23874MB
[2022-11-14 09:05:33 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 271 training takes 0:15:40
[2022-11-14 09:05:33 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_271.pth saving......
[2022-11-14 09:05:34 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_271.pth saved !!!
[2022-11-14 09:05:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.976 (1.976)	Loss 0.7763 (0.7763)	Acc@1 81.934 (81.934)	Acc@5 95.508 (95.508)	Mem 23874MB
[2022-11-14 09:05:46 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.584 Acc@5 96.566
[2022-11-14 09:05:46 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.6%
[2022-11-14 09:05:48 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.951 (1.951)	Loss 0.6823 (0.6823)	Acc@1 83.496 (83.496)	Acc@5 96.973 (96.973)	Mem 23874MB
[2022-11-14 09:05:59 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.854 Acc@5 96.684
[2022-11-14 09:05:59 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 09:05:59 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 09:06:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][0/1251]	eta 0:53:26 lr 0.000031	time 2.5632 (2.5632)	loss 3.0045 (3.0045)	grad_norm 2.8237 (2.8237)	mem 23874MB
[2022-11-14 09:06:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][50/1251]	eta 0:15:45 lr 0.000031	time 0.7472 (0.7871)	loss 2.8301 (2.6161)	grad_norm 3.2198 (3.4657)	mem 23874MB
[2022-11-14 09:07:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][100/1251]	eta 0:14:45 lr 0.000031	time 0.7419 (0.7697)	loss 1.7108 (2.6356)	grad_norm 2.5081 (3.1505)	mem 23874MB
[2022-11-14 09:07:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][150/1251]	eta 0:14:01 lr 0.000031	time 0.7397 (0.7639)	loss 2.3609 (2.6271)	grad_norm 2.7108 (3.1730)	mem 23874MB
[2022-11-14 09:08:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][200/1251]	eta 0:13:18 lr 0.000031	time 0.7375 (0.7596)	loss 1.6279 (2.6064)	grad_norm 3.0642 (3.3464)	mem 23874MB
[2022-11-14 09:09:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][250/1251]	eta 0:12:39 lr 0.000031	time 0.7383 (0.7583)	loss 2.7355 (2.6040)	grad_norm 2.5876 (3.3219)	mem 23874MB
[2022-11-14 09:09:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][300/1251]	eta 0:11:59 lr 0.000031	time 0.8301 (0.7566)	loss 2.9217 (2.6213)	grad_norm 2.9914 (3.2409)	mem 23874MB
[2022-11-14 09:10:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][350/1251]	eta 0:11:21 lr 0.000031	time 0.7421 (0.7561)	loss 3.1678 (2.6100)	grad_norm 2.9918 (3.2292)	mem 23874MB
[2022-11-14 09:11:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][400/1251]	eta 0:10:42 lr 0.000031	time 0.7375 (0.7554)	loss 3.0537 (2.6187)	grad_norm 2.9558 (3.2242)	mem 23874MB
[2022-11-14 09:11:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][450/1251]	eta 0:10:04 lr 0.000031	time 0.7349 (0.7544)	loss 3.0787 (2.6129)	grad_norm 2.9717 (3.2508)	mem 23874MB
[2022-11-14 09:12:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][500/1251]	eta 0:09:26 lr 0.000031	time 0.7392 (0.7539)	loss 2.7153 (2.6238)	grad_norm 2.8621 (3.2332)	mem 23874MB
[2022-11-14 09:12:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][550/1251]	eta 0:08:48 lr 0.000030	time 0.7460 (0.7537)	loss 1.7025 (2.6212)	grad_norm 2.1620 (3.1919)	mem 23874MB
[2022-11-14 09:13:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][600/1251]	eta 0:08:10 lr 0.000030	time 0.7380 (0.7533)	loss 2.3710 (2.6215)	grad_norm 2.4607 (3.1708)	mem 23874MB
[2022-11-14 09:14:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][650/1251]	eta 0:07:32 lr 0.000030	time 0.7393 (0.7531)	loss 2.8866 (2.6223)	grad_norm 3.0361 (3.1803)	mem 23874MB
[2022-11-14 09:14:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][700/1251]	eta 0:06:54 lr 0.000030	time 0.8359 (0.7527)	loss 2.8121 (2.6264)	grad_norm 2.8709 (3.2302)	mem 23874MB
[2022-11-14 09:15:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][750/1251]	eta 0:06:17 lr 0.000030	time 0.8352 (0.7527)	loss 2.3955 (2.6234)	grad_norm 3.0444 (3.2344)	mem 23874MB
[2022-11-14 09:16:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][800/1251]	eta 0:05:39 lr 0.000030	time 0.7420 (0.7523)	loss 2.0290 (2.6278)	grad_norm 3.4845 (3.2131)	mem 23874MB
[2022-11-14 09:16:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][850/1251]	eta 0:05:01 lr 0.000030	time 0.7386 (0.7522)	loss 2.9330 (2.6235)	grad_norm 2.4529 (3.1950)	mem 23874MB
[2022-11-14 09:17:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][900/1251]	eta 0:04:23 lr 0.000030	time 0.7441 (0.7520)	loss 2.4679 (2.6225)	grad_norm 2.3323 (3.1851)	mem 23874MB
[2022-11-14 09:17:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][950/1251]	eta 0:03:46 lr 0.000030	time 0.7536 (0.7520)	loss 2.8038 (2.6252)	grad_norm 2.9029 (3.1772)	mem 23874MB
[2022-11-14 09:18:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][1000/1251]	eta 0:03:08 lr 0.000030	time 0.7531 (0.7517)	loss 2.9642 (2.6195)	grad_norm 8.0830 (3.1821)	mem 23874MB
[2022-11-14 09:19:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][1050/1251]	eta 0:02:31 lr 0.000030	time 0.7483 (0.7517)	loss 3.0560 (2.6187)	grad_norm 3.3710 (3.1733)	mem 23874MB
[2022-11-14 09:19:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][1100/1251]	eta 0:01:53 lr 0.000030	time 0.8277 (0.7515)	loss 2.1184 (2.6164)	grad_norm 2.5637 (3.1900)	mem 23874MB
[2022-11-14 09:20:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][1150/1251]	eta 0:01:15 lr 0.000030	time 0.7466 (0.7514)	loss 2.8421 (2.6175)	grad_norm 3.1424 (3.1961)	mem 23874MB
[2022-11-14 09:21:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][1200/1251]	eta 0:00:38 lr 0.000030	time 0.7424 (0.7514)	loss 1.9540 (2.6143)	grad_norm 2.6507 (3.1875)	mem 23874MB
[2022-11-14 09:21:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [272/300][1250/1251]	eta 0:00:00 lr 0.000030	time 0.7285 (0.7511)	loss 1.9568 (2.6153)	grad_norm 2.4949 (3.1773)	mem 23874MB
[2022-11-14 09:21:39 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 272 training takes 0:15:39
[2022-11-14 09:21:39 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_272.pth saving......
[2022-11-14 09:21:40 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_272.pth saved !!!
[2022-11-14 09:21:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.759 (1.759)	Loss 0.6819 (0.6819)	Acc@1 84.570 (84.570)	Acc@5 96.777 (96.777)	Mem 23874MB
[2022-11-14 09:21:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.568 Acc@5 96.548
[2022-11-14 09:21:53 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.6%
[2022-11-14 09:21:55 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.024 (2.024)	Loss 0.6696 (0.6696)	Acc@1 84.863 (84.863)	Acc@5 97.070 (97.070)	Mem 23874MB
[2022-11-14 09:22:05 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.832 Acc@5 96.678
[2022-11-14 09:22:05 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.8%
[2022-11-14 09:22:05 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 09:22:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][0/1251]	eta 0:53:11 lr 0.000030	time 2.5509 (2.5509)	loss 2.5101 (2.5101)	grad_norm 3.1072 (3.1072)	mem 23874MB
[2022-11-14 09:22:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][50/1251]	eta 0:15:44 lr 0.000030	time 0.7422 (0.7867)	loss 2.9004 (2.5747)	grad_norm 3.3173 (3.1878)	mem 23874MB
[2022-11-14 09:23:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][100/1251]	eta 0:14:44 lr 0.000030	time 0.7463 (0.7689)	loss 2.8727 (2.5976)	grad_norm 3.1749 (3.2070)	mem 23874MB
[2022-11-14 09:24:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][150/1251]	eta 0:14:00 lr 0.000029	time 0.7476 (0.7632)	loss 2.3148 (2.6104)	grad_norm 2.6154 (3.1725)	mem 23874MB
[2022-11-14 09:24:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][200/1251]	eta 0:13:18 lr 0.000029	time 0.7447 (0.7596)	loss 2.8782 (2.5920)	grad_norm 3.0241 (inf)	mem 23874MB
[2022-11-14 09:25:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][250/1251]	eta 0:12:38 lr 0.000029	time 0.7418 (0.7576)	loss 2.4538 (2.5806)	grad_norm 2.6600 (inf)	mem 23874MB
[2022-11-14 09:25:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][300/1251]	eta 0:11:59 lr 0.000029	time 0.8186 (0.7568)	loss 2.7212 (2.5874)	grad_norm 2.8768 (inf)	mem 23874MB
[2022-11-14 09:26:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][350/1251]	eta 0:11:20 lr 0.000029	time 0.7305 (0.7554)	loss 2.6823 (2.5855)	grad_norm 2.6076 (inf)	mem 23874MB
[2022-11-14 09:27:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][400/1251]	eta 0:10:42 lr 0.000029	time 0.7462 (0.7551)	loss 2.7433 (2.5808)	grad_norm 2.3271 (inf)	mem 23874MB
[2022-11-14 09:27:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][450/1251]	eta 0:10:04 lr 0.000029	time 0.7424 (0.7544)	loss 2.0878 (2.5795)	grad_norm 3.1025 (inf)	mem 23874MB
[2022-11-14 09:28:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][500/1251]	eta 0:09:26 lr 0.000029	time 0.7382 (0.7538)	loss 2.6707 (2.5825)	grad_norm 2.6438 (inf)	mem 23874MB
[2022-11-14 09:29:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][550/1251]	eta 0:08:48 lr 0.000029	time 0.7413 (0.7533)	loss 3.0000 (2.5873)	grad_norm 4.0157 (inf)	mem 23874MB
[2022-11-14 09:29:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][600/1251]	eta 0:08:10 lr 0.000029	time 0.7372 (0.7530)	loss 3.0126 (2.5952)	grad_norm 3.0217 (inf)	mem 23874MB
[2022-11-14 09:30:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][650/1251]	eta 0:07:32 lr 0.000029	time 0.7466 (0.7529)	loss 2.5443 (2.5962)	grad_norm 3.2814 (inf)	mem 23874MB
[2022-11-14 09:30:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][700/1251]	eta 0:06:54 lr 0.000029	time 0.8182 (0.7530)	loss 2.7206 (2.5957)	grad_norm 2.6572 (inf)	mem 23874MB
[2022-11-14 09:31:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][750/1251]	eta 0:06:17 lr 0.000029	time 0.7420 (0.7526)	loss 2.2521 (2.5979)	grad_norm 2.2646 (inf)	mem 23874MB
[2022-11-14 09:32:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][800/1251]	eta 0:05:39 lr 0.000029	time 0.7392 (0.7526)	loss 2.8995 (2.5986)	grad_norm 2.5056 (inf)	mem 23874MB
[2022-11-14 09:32:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][850/1251]	eta 0:05:01 lr 0.000029	time 0.7390 (0.7525)	loss 2.2879 (2.5936)	grad_norm 3.9761 (inf)	mem 23874MB
[2022-11-14 09:33:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][900/1251]	eta 0:04:24 lr 0.000029	time 0.7381 (0.7523)	loss 2.1966 (2.5972)	grad_norm 3.0420 (inf)	mem 23874MB
[2022-11-14 09:34:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][950/1251]	eta 0:03:46 lr 0.000029	time 0.7403 (0.7524)	loss 2.0868 (2.6024)	grad_norm 2.8401 (inf)	mem 23874MB
[2022-11-14 09:34:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][1000/1251]	eta 0:03:08 lr 0.000029	time 0.7942 (0.7522)	loss 2.6796 (2.6047)	grad_norm 2.6577 (inf)	mem 23874MB
[2022-11-14 09:35:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][1050/1251]	eta 0:02:31 lr 0.000028	time 0.7374 (0.7522)	loss 2.6890 (2.6076)	grad_norm 3.1097 (inf)	mem 23874MB
[2022-11-14 09:35:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][1100/1251]	eta 0:01:53 lr 0.000028	time 0.8286 (0.7520)	loss 2.4362 (2.6061)	grad_norm 2.8561 (inf)	mem 23874MB
[2022-11-14 09:36:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][1150/1251]	eta 0:01:15 lr 0.000028	time 0.7420 (0.7518)	loss 1.9603 (2.6058)	grad_norm 3.1063 (inf)	mem 23874MB
[2022-11-14 09:37:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][1200/1251]	eta 0:00:38 lr 0.000028	time 0.7550 (0.7518)	loss 2.0189 (2.6019)	grad_norm 2.4691 (inf)	mem 23874MB
[2022-11-14 09:37:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [273/300][1250/1251]	eta 0:00:00 lr 0.000028	time 0.7307 (0.7517)	loss 2.7787 (2.6040)	grad_norm 2.7450 (inf)	mem 23874MB
[2022-11-14 09:37:46 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 273 training takes 0:15:40
[2022-11-14 09:37:46 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_273.pth saving......
[2022-11-14 09:37:47 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_273.pth saved !!!
[2022-11-14 09:37:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.643 (1.643)	Loss 0.7888 (0.7888)	Acc@1 81.934 (81.934)	Acc@5 95.703 (95.703)	Mem 23874MB
[2022-11-14 09:38:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.576 Acc@5 96.550
[2022-11-14 09:38:00 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.6%
[2022-11-14 09:38:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.932 (1.932)	Loss 0.6489 (0.6489)	Acc@1 84.277 (84.277)	Acc@5 97.559 (97.559)	Mem 23874MB
[2022-11-14 09:38:12 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.850 Acc@5 96.672
[2022-11-14 09:38:12 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 09:38:12 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 09:38:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][0/1251]	eta 0:54:08 lr 0.000028	time 2.5965 (2.5965)	loss 2.9437 (2.9437)	grad_norm 2.6562 (2.6562)	mem 23874MB
[2022-11-14 09:38:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][50/1251]	eta 0:15:49 lr 0.000028	time 0.8219 (0.7906)	loss 2.5363 (2.6245)	grad_norm 3.2637 (3.0037)	mem 23874MB
[2022-11-14 09:39:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][100/1251]	eta 0:14:47 lr 0.000028	time 0.7485 (0.7710)	loss 2.5370 (2.6152)	grad_norm 2.9715 (2.9773)	mem 23874MB
[2022-11-14 09:40:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][150/1251]	eta 0:14:00 lr 0.000028	time 0.7396 (0.7638)	loss 2.7502 (2.6176)	grad_norm 2.8868 (3.0872)	mem 23874MB
[2022-11-14 09:40:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][200/1251]	eta 0:13:18 lr 0.000028	time 0.7403 (0.7602)	loss 2.2703 (2.6261)	grad_norm 3.0979 (3.0807)	mem 23874MB
[2022-11-14 09:41:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][250/1251]	eta 0:12:39 lr 0.000028	time 0.7404 (0.7584)	loss 2.7046 (2.6236)	grad_norm 3.4440 (3.0687)	mem 23874MB
[2022-11-14 09:42:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][300/1251]	eta 0:11:59 lr 0.000028	time 0.7505 (0.7569)	loss 2.9183 (2.6174)	grad_norm 2.5498 (3.0472)	mem 23874MB
[2022-11-14 09:42:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][350/1251]	eta 0:11:21 lr 0.000028	time 0.7405 (0.7563)	loss 2.9258 (2.6091)	grad_norm 3.3405 (3.0355)	mem 23874MB
[2022-11-14 09:43:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][400/1251]	eta 0:10:42 lr 0.000028	time 0.7349 (0.7554)	loss 2.9794 (2.6121)	grad_norm 2.4627 (3.0302)	mem 23874MB
[2022-11-14 09:43:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][450/1251]	eta 0:10:04 lr 0.000028	time 0.7398 (0.7548)	loss 1.5806 (2.6142)	grad_norm 3.0879 (3.0321)	mem 23874MB
[2022-11-14 09:44:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][500/1251]	eta 0:09:26 lr 0.000028	time 0.7385 (0.7545)	loss 2.5179 (2.6138)	grad_norm 2.9457 (3.0364)	mem 23874MB
[2022-11-14 09:45:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][550/1251]	eta 0:08:48 lr 0.000028	time 0.7411 (0.7539)	loss 1.8962 (2.6072)	grad_norm 2.6776 (3.0744)	mem 23874MB
[2022-11-14 09:45:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][600/1251]	eta 0:08:10 lr 0.000028	time 0.7459 (0.7537)	loss 2.9199 (2.6028)	grad_norm 2.7217 (3.0671)	mem 23874MB
[2022-11-14 09:46:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][650/1251]	eta 0:07:32 lr 0.000028	time 0.7428 (0.7537)	loss 2.6329 (2.6072)	grad_norm 2.4519 (3.0905)	mem 23874MB
[2022-11-14 09:47:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][700/1251]	eta 0:06:55 lr 0.000027	time 0.7373 (0.7534)	loss 2.5469 (2.6020)	grad_norm 2.7499 (3.1076)	mem 23874MB
[2022-11-14 09:47:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][750/1251]	eta 0:06:17 lr 0.000027	time 0.7363 (0.7533)	loss 2.9982 (2.6020)	grad_norm 2.8033 (3.0939)	mem 23874MB
[2022-11-14 09:48:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][800/1251]	eta 0:05:39 lr 0.000027	time 0.7441 (0.7530)	loss 2.6867 (2.6035)	grad_norm 3.1093 (3.0843)	mem 23874MB
[2022-11-14 09:48:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][850/1251]	eta 0:05:01 lr 0.000027	time 0.7384 (0.7529)	loss 2.2079 (2.6078)	grad_norm 2.4570 (3.0854)	mem 23874MB
[2022-11-14 09:49:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][900/1251]	eta 0:04:24 lr 0.000027	time 0.7410 (0.7527)	loss 2.8896 (2.6104)	grad_norm 3.2773 (3.0991)	mem 23874MB
[2022-11-14 09:50:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][950/1251]	eta 0:03:46 lr 0.000027	time 0.8042 (0.7524)	loss 2.6679 (2.6114)	grad_norm 2.5544 (3.0914)	mem 23874MB
[2022-11-14 09:50:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][1000/1251]	eta 0:03:08 lr 0.000027	time 0.7455 (0.7525)	loss 2.7838 (2.6127)	grad_norm 2.8356 (3.1008)	mem 23874MB
[2022-11-14 09:51:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][1050/1251]	eta 0:02:31 lr 0.000027	time 0.7476 (0.7523)	loss 2.6308 (2.6167)	grad_norm 3.0565 (3.0893)	mem 23874MB
[2022-11-14 09:52:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][1100/1251]	eta 0:01:53 lr 0.000027	time 0.7429 (0.7523)	loss 3.0955 (2.6205)	grad_norm 3.2374 (3.0865)	mem 23874MB
[2022-11-14 09:52:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][1150/1251]	eta 0:01:15 lr 0.000027	time 0.7428 (0.7522)	loss 2.7340 (2.6205)	grad_norm 3.1610 (3.0794)	mem 23874MB
[2022-11-14 09:53:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][1200/1251]	eta 0:00:38 lr 0.000027	time 0.7403 (0.7521)	loss 2.4831 (2.6191)	grad_norm 2.9671 (3.0877)	mem 23874MB
[2022-11-14 09:53:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [274/300][1250/1251]	eta 0:00:00 lr 0.000027	time 0.7300 (0.7519)	loss 2.6973 (2.6165)	grad_norm 3.6720 (3.0876)	mem 23874MB
[2022-11-14 09:53:53 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 274 training takes 0:15:40
[2022-11-14 09:53:53 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_274.pth saving......
[2022-11-14 09:53:54 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_274.pth saved !!!
[2022-11-14 09:53:56 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.699 (1.699)	Loss 0.7220 (0.7220)	Acc@1 84.277 (84.277)	Acc@5 96.582 (96.582)	Mem 23874MB
[2022-11-14 09:54:07 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.660 Acc@5 96.588
[2022-11-14 09:54:07 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.7%
[2022-11-14 09:54:09 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.877 (1.877)	Loss 0.7499 (0.7499)	Acc@1 82.910 (82.910)	Acc@5 95.508 (95.508)	Mem 23874MB
[2022-11-14 09:54:19 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.836 Acc@5 96.670
[2022-11-14 09:54:19 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.8%
[2022-11-14 09:54:19 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 09:54:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][0/1251]	eta 0:53:17 lr 0.000027	time 2.5558 (2.5558)	loss 2.0968 (2.0968)	grad_norm 3.1567 (3.1567)	mem 23874MB
[2022-11-14 09:55:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][50/1251]	eta 0:15:51 lr 0.000027	time 0.7474 (0.7922)	loss 2.5033 (2.6063)	grad_norm 3.0198 (2.9752)	mem 23874MB
[2022-11-14 09:55:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][100/1251]	eta 0:14:47 lr 0.000027	time 0.7475 (0.7713)	loss 2.9210 (2.6149)	grad_norm 3.7938 (3.0176)	mem 23874MB
[2022-11-14 09:56:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][150/1251]	eta 0:14:00 lr 0.000027	time 0.7437 (0.7638)	loss 2.9416 (2.6124)	grad_norm 2.8649 (3.0238)	mem 23874MB
[2022-11-14 09:56:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][200/1251]	eta 0:13:19 lr 0.000027	time 0.7424 (0.7609)	loss 2.9824 (2.6033)	grad_norm 2.7499 (3.0232)	mem 23874MB
[2022-11-14 09:57:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][250/1251]	eta 0:12:39 lr 0.000027	time 0.7459 (0.7590)	loss 2.3056 (2.5961)	grad_norm 2.6631 (3.0335)	mem 23874MB
[2022-11-14 09:58:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][300/1251]	eta 0:12:00 lr 0.000027	time 0.7466 (0.7577)	loss 2.8777 (2.5855)	grad_norm 2.4959 (3.0786)	mem 23874MB
[2022-11-14 09:58:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][350/1251]	eta 0:11:21 lr 0.000026	time 0.7381 (0.7567)	loss 2.6815 (2.5776)	grad_norm 2.6246 (3.0722)	mem 23874MB
[2022-11-14 09:59:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][400/1251]	eta 0:10:43 lr 0.000026	time 0.7434 (0.7560)	loss 2.2153 (2.5724)	grad_norm 2.5048 (3.0523)	mem 23874MB
[2022-11-14 10:00:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][450/1251]	eta 0:10:05 lr 0.000026	time 0.7571 (0.7558)	loss 2.0580 (2.5688)	grad_norm 3.8757 (3.0668)	mem 23874MB
[2022-11-14 10:00:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][500/1251]	eta 0:09:27 lr 0.000026	time 0.7458 (0.7554)	loss 2.5786 (2.5762)	grad_norm 2.4875 (3.0689)	mem 23874MB
[2022-11-14 10:01:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][550/1251]	eta 0:08:49 lr 0.000026	time 0.7467 (0.7550)	loss 2.3808 (2.5800)	grad_norm 2.4967 (3.1021)	mem 23874MB
[2022-11-14 10:01:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][600/1251]	eta 0:08:11 lr 0.000026	time 0.8118 (0.7546)	loss 2.0320 (2.5820)	grad_norm 2.9938 (3.0936)	mem 23874MB
[2022-11-14 10:02:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][650/1251]	eta 0:07:33 lr 0.000026	time 0.7466 (0.7544)	loss 2.7254 (2.5917)	grad_norm 4.1496 (3.1076)	mem 23874MB
[2022-11-14 10:03:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][700/1251]	eta 0:06:55 lr 0.000026	time 0.7461 (0.7541)	loss 2.7145 (2.5964)	grad_norm 3.1674 (nan)	mem 23874MB
[2022-11-14 10:03:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][750/1251]	eta 0:06:17 lr 0.000026	time 0.8347 (0.7540)	loss 2.8352 (2.5934)	grad_norm 2.6913 (nan)	mem 23874MB
[2022-11-14 10:04:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][800/1251]	eta 0:05:39 lr 0.000026	time 0.7427 (0.7536)	loss 3.1703 (2.6051)	grad_norm 2.5708 (nan)	mem 23874MB
[2022-11-14 10:05:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][850/1251]	eta 0:05:02 lr 0.000026	time 0.7303 (0.7536)	loss 2.5906 (2.6065)	grad_norm 2.9196 (nan)	mem 23874MB
[2022-11-14 10:05:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][900/1251]	eta 0:04:24 lr 0.000026	time 0.7414 (0.7534)	loss 1.7164 (2.6042)	grad_norm 2.9671 (nan)	mem 23874MB
[2022-11-14 10:06:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][950/1251]	eta 0:03:46 lr 0.000026	time 0.7448 (0.7533)	loss 1.6202 (2.6038)	grad_norm 2.8938 (nan)	mem 23874MB
[2022-11-14 10:06:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][1000/1251]	eta 0:03:09 lr 0.000026	time 0.7429 (0.7532)	loss 2.6177 (2.6055)	grad_norm 2.6117 (nan)	mem 23874MB
[2022-11-14 10:07:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][1050/1251]	eta 0:02:31 lr 0.000026	time 0.7438 (0.7533)	loss 2.8254 (2.6034)	grad_norm 2.5517 (nan)	mem 23874MB
[2022-11-14 10:08:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][1100/1251]	eta 0:01:53 lr 0.000026	time 0.7442 (0.7530)	loss 2.6017 (2.6046)	grad_norm 2.6558 (nan)	mem 23874MB
[2022-11-14 10:08:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][1150/1251]	eta 0:01:16 lr 0.000026	time 0.7564 (0.7530)	loss 2.9085 (2.6061)	grad_norm 3.3860 (nan)	mem 23874MB
[2022-11-14 10:09:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][1200/1251]	eta 0:00:38 lr 0.000026	time 0.7469 (0.7529)	loss 2.7739 (2.6015)	grad_norm 2.7068 (nan)	mem 23874MB
[2022-11-14 10:10:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [275/300][1250/1251]	eta 0:00:00 lr 0.000026	time 0.7292 (0.7528)	loss 1.8593 (2.5996)	grad_norm 2.8699 (nan)	mem 23874MB
[2022-11-14 10:10:01 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 275 training takes 0:15:41
[2022-11-14 10:10:02 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_275.pth saving......
[2022-11-14 10:10:03 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_275.pth saved !!!
[2022-11-14 10:10:05 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.774 (1.774)	Loss 0.6915 (0.6915)	Acc@1 83.203 (83.203)	Acc@5 96.777 (96.777)	Mem 23874MB
[2022-11-14 10:10:15 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.634 Acc@5 96.532
[2022-11-14 10:10:15 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.6%
[2022-11-14 10:10:17 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.972 (1.972)	Loss 0.7223 (0.7223)	Acc@1 84.375 (84.375)	Acc@5 96.875 (96.875)	Mem 23874MB
[2022-11-14 10:10:28 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.846 Acc@5 96.666
[2022-11-14 10:10:28 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.8%
[2022-11-14 10:10:28 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 10:10:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][0/1251]	eta 0:53:12 lr 0.000026	time 2.5523 (2.5523)	loss 2.6564 (2.6564)	grad_norm 2.9438 (2.9438)	mem 23874MB
[2022-11-14 10:11:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][50/1251]	eta 0:15:50 lr 0.000025	time 0.7417 (0.7912)	loss 2.2473 (2.6272)	grad_norm 2.3995 (3.0336)	mem 23874MB
[2022-11-14 10:11:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][100/1251]	eta 0:14:47 lr 0.000025	time 0.7455 (0.7712)	loss 1.9964 (2.5615)	grad_norm 2.7722 (3.0882)	mem 23874MB
[2022-11-14 10:12:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][150/1251]	eta 0:14:01 lr 0.000025	time 0.7384 (0.7647)	loss 2.1079 (2.5671)	grad_norm 2.3930 (3.1237)	mem 23874MB
[2022-11-14 10:13:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][200/1251]	eta 0:13:19 lr 0.000025	time 0.7453 (0.7612)	loss 2.9153 (2.5811)	grad_norm 4.1803 (3.1248)	mem 23874MB
[2022-11-14 10:13:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][250/1251]	eta 0:12:39 lr 0.000025	time 0.7444 (0.7591)	loss 2.7908 (2.5741)	grad_norm 2.4508 (3.0860)	mem 23874MB
[2022-11-14 10:14:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][300/1251]	eta 0:12:00 lr 0.000025	time 0.7395 (0.7572)	loss 2.6801 (2.5761)	grad_norm 3.4228 (3.0669)	mem 23874MB
[2022-11-14 10:14:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][350/1251]	eta 0:11:21 lr 0.000025	time 0.7414 (0.7569)	loss 2.8654 (2.5919)	grad_norm 2.8353 (3.0673)	mem 23874MB
[2022-11-14 10:15:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][400/1251]	eta 0:10:43 lr 0.000025	time 0.7397 (0.7556)	loss 2.8731 (2.5899)	grad_norm 2.9344 (3.0451)	mem 23874MB
[2022-11-14 10:16:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][450/1251]	eta 0:10:05 lr 0.000025	time 0.7543 (0.7553)	loss 1.6458 (2.5918)	grad_norm 2.8346 (3.0475)	mem 23874MB
[2022-11-14 10:16:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][500/1251]	eta 0:09:26 lr 0.000025	time 0.7364 (0.7546)	loss 1.9165 (2.5940)	grad_norm 3.7452 (3.0603)	mem 23874MB
[2022-11-14 10:17:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][550/1251]	eta 0:08:48 lr 0.000025	time 0.8230 (0.7544)	loss 2.2591 (2.5874)	grad_norm 2.7532 (3.0567)	mem 23874MB
[2022-11-14 10:18:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][600/1251]	eta 0:08:10 lr 0.000025	time 0.7595 (0.7541)	loss 2.6002 (2.5817)	grad_norm 2.6901 (3.0617)	mem 23874MB
[2022-11-14 10:18:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][650/1251]	eta 0:07:33 lr 0.000025	time 0.7406 (0.7538)	loss 2.9692 (2.5901)	grad_norm 2.7758 (3.0707)	mem 23874MB
[2022-11-14 10:19:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][700/1251]	eta 0:06:55 lr 0.000025	time 0.8338 (0.7534)	loss 2.9072 (2.5876)	grad_norm 4.4669 (3.0792)	mem 23874MB
[2022-11-14 10:19:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][750/1251]	eta 0:06:17 lr 0.000025	time 0.7442 (0.7533)	loss 3.0639 (2.5878)	grad_norm 2.7068 (3.0755)	mem 23874MB
[2022-11-14 10:20:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][800/1251]	eta 0:05:39 lr 0.000025	time 0.7404 (0.7532)	loss 2.8317 (2.5926)	grad_norm 2.6727 (3.0709)	mem 23874MB
[2022-11-14 10:21:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][850/1251]	eta 0:05:01 lr 0.000025	time 0.7410 (0.7529)	loss 2.0495 (2.5969)	grad_norm 2.5975 (3.0712)	mem 23874MB
[2022-11-14 10:21:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][900/1251]	eta 0:04:24 lr 0.000025	time 0.7412 (0.7528)	loss 2.7147 (2.5984)	grad_norm 3.4541 (3.0705)	mem 23874MB
[2022-11-14 10:22:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][950/1251]	eta 0:03:46 lr 0.000025	time 0.7441 (0.7526)	loss 2.3531 (2.5935)	grad_norm 3.0902 (3.0723)	mem 23874MB
[2022-11-14 10:23:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][1000/1251]	eta 0:03:08 lr 0.000025	time 0.7446 (0.7526)	loss 2.6391 (2.5932)	grad_norm 3.0318 (3.0705)	mem 23874MB
[2022-11-14 10:23:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][1050/1251]	eta 0:02:31 lr 0.000024	time 0.7428 (0.7525)	loss 1.7645 (2.5929)	grad_norm 2.7327 (3.0652)	mem 23874MB
[2022-11-14 10:24:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][1100/1251]	eta 0:01:53 lr 0.000024	time 0.7397 (0.7523)	loss 2.9818 (2.5996)	grad_norm 4.0337 (3.0674)	mem 23874MB
[2022-11-14 10:24:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][1150/1251]	eta 0:01:15 lr 0.000024	time 0.7403 (0.7521)	loss 1.9257 (2.5948)	grad_norm 3.9395 (3.0722)	mem 23874MB
[2022-11-14 10:25:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][1200/1251]	eta 0:00:38 lr 0.000024	time 0.7380 (0.7522)	loss 2.2420 (2.5957)	grad_norm 2.4649 (3.0865)	mem 23874MB
[2022-11-14 10:26:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [276/300][1250/1251]	eta 0:00:00 lr 0.000024	time 0.7277 (0.7518)	loss 3.0568 (2.5971)	grad_norm 2.8886 (3.0998)	mem 23874MB
[2022-11-14 10:26:09 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 276 training takes 0:15:40
[2022-11-14 10:26:09 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_276.pth saving......
[2022-11-14 10:26:10 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_276.pth saved !!!
[2022-11-14 10:26:12 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.762 (1.762)	Loss 0.6932 (0.6932)	Acc@1 84.277 (84.277)	Acc@5 96.387 (96.387)	Mem 23874MB
[2022-11-14 10:26:22 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.728 Acc@5 96.520
[2022-11-14 10:26:22 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.7%
[2022-11-14 10:26:24 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.052 (2.052)	Loss 0.6531 (0.6531)	Acc@1 84.082 (84.082)	Acc@5 97.070 (97.070)	Mem 23874MB
[2022-11-14 10:26:35 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.868 Acc@5 96.656
[2022-11-14 10:26:35 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 10:26:35 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 10:26:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][0/1251]	eta 0:55:12 lr 0.000024	time 2.6480 (2.6480)	loss 2.5643 (2.5643)	grad_norm 2.6208 (2.6208)	mem 23874MB
[2022-11-14 10:27:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][50/1251]	eta 0:15:45 lr 0.000024	time 0.7455 (0.7874)	loss 2.6267 (2.4188)	grad_norm 4.0776 (3.0039)	mem 23874MB
[2022-11-14 10:27:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][100/1251]	eta 0:14:47 lr 0.000024	time 0.7498 (0.7706)	loss 2.3412 (2.5017)	grad_norm 2.9274 (3.0076)	mem 23874MB
[2022-11-14 10:28:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][150/1251]	eta 0:14:02 lr 0.000024	time 0.7442 (0.7650)	loss 2.7239 (2.5355)	grad_norm 2.5216 (3.0832)	mem 23874MB
[2022-11-14 10:29:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][200/1251]	eta 0:13:19 lr 0.000024	time 0.7378 (0.7608)	loss 2.1835 (2.5351)	grad_norm 2.9999 (3.0577)	mem 23874MB
[2022-11-14 10:29:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][250/1251]	eta 0:12:39 lr 0.000024	time 0.7452 (0.7588)	loss 2.8844 (2.5622)	grad_norm 2.6487 (3.0458)	mem 23874MB
[2022-11-14 10:30:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][300/1251]	eta 0:12:00 lr 0.000024	time 0.8142 (0.7575)	loss 2.4262 (2.5635)	grad_norm 3.0497 (3.1144)	mem 23874MB
[2022-11-14 10:31:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][350/1251]	eta 0:11:21 lr 0.000024	time 0.7498 (0.7569)	loss 2.5661 (2.5677)	grad_norm 2.8304 (3.1072)	mem 23874MB
[2022-11-14 10:31:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][400/1251]	eta 0:10:43 lr 0.000024	time 0.7405 (0.7558)	loss 2.5194 (2.5776)	grad_norm 2.4447 (inf)	mem 23874MB
[2022-11-14 10:32:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][450/1251]	eta 0:10:05 lr 0.000024	time 0.7571 (0.7555)	loss 2.6446 (2.5684)	grad_norm 3.1152 (inf)	mem 23874MB
[2022-11-14 10:32:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][500/1251]	eta 0:09:27 lr 0.000024	time 0.7329 (0.7552)	loss 2.4983 (2.5639)	grad_norm 4.2530 (inf)	mem 23874MB
[2022-11-14 10:33:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][550/1251]	eta 0:08:48 lr 0.000024	time 0.7414 (0.7545)	loss 2.0997 (2.5600)	grad_norm 3.1790 (inf)	mem 23874MB
[2022-11-14 10:34:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][600/1251]	eta 0:08:10 lr 0.000024	time 0.7458 (0.7541)	loss 2.6399 (2.5583)	grad_norm 2.7250 (inf)	mem 23874MB
[2022-11-14 10:34:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][650/1251]	eta 0:07:33 lr 0.000024	time 0.7440 (0.7538)	loss 2.9042 (2.5594)	grad_norm 3.3653 (inf)	mem 23874MB
[2022-11-14 10:35:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][700/1251]	eta 0:06:55 lr 0.000024	time 0.8190 (0.7537)	loss 2.9268 (2.5635)	grad_norm 2.5843 (inf)	mem 23874MB
[2022-11-14 10:36:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][750/1251]	eta 0:06:17 lr 0.000024	time 0.7422 (0.7533)	loss 1.9947 (2.5613)	grad_norm 2.3947 (inf)	mem 23874MB
[2022-11-14 10:36:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][800/1251]	eta 0:05:39 lr 0.000024	time 0.7436 (0.7533)	loss 3.0178 (2.5620)	grad_norm 3.3193 (inf)	mem 23874MB
[2022-11-14 10:37:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][850/1251]	eta 0:05:01 lr 0.000023	time 0.7411 (0.7528)	loss 2.7514 (2.5665)	grad_norm 3.4361 (inf)	mem 23874MB
[2022-11-14 10:37:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][900/1251]	eta 0:04:24 lr 0.000023	time 0.7382 (0.7527)	loss 2.9701 (2.5683)	grad_norm 3.3003 (inf)	mem 23874MB
[2022-11-14 10:38:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][950/1251]	eta 0:03:46 lr 0.000023	time 0.7416 (0.7526)	loss 2.4797 (2.5706)	grad_norm 2.9121 (inf)	mem 23874MB
[2022-11-14 10:39:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][1000/1251]	eta 0:03:08 lr 0.000023	time 0.8083 (0.7525)	loss 3.0367 (2.5714)	grad_norm 3.3303 (inf)	mem 23874MB
[2022-11-14 10:39:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][1050/1251]	eta 0:02:31 lr 0.000023	time 0.7391 (0.7524)	loss 3.0632 (2.5714)	grad_norm 4.7614 (inf)	mem 23874MB
[2022-11-14 10:40:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][1100/1251]	eta 0:01:53 lr 0.000023	time 0.8368 (0.7521)	loss 2.7679 (2.5705)	grad_norm 2.5747 (inf)	mem 23874MB
[2022-11-14 10:41:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][1150/1251]	eta 0:01:15 lr 0.000023	time 0.7378 (0.7520)	loss 2.6645 (2.5715)	grad_norm 3.0201 (inf)	mem 23874MB
[2022-11-14 10:41:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][1200/1251]	eta 0:00:38 lr 0.000023	time 0.7428 (0.7521)	loss 2.1434 (2.5709)	grad_norm 2.5844 (inf)	mem 23874MB
[2022-11-14 10:42:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [277/300][1250/1251]	eta 0:00:00 lr 0.000023	time 0.7314 (0.7518)	loss 2.3859 (2.5721)	grad_norm 2.6275 (inf)	mem 23874MB
[2022-11-14 10:42:16 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 277 training takes 0:15:40
[2022-11-14 10:42:16 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_277.pth saving......
[2022-11-14 10:42:17 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_277.pth saved !!!
[2022-11-14 10:42:19 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.816 (1.816)	Loss 0.7281 (0.7281)	Acc@1 83.789 (83.789)	Acc@5 96.094 (96.094)	Mem 23874MB
[2022-11-14 10:42:30 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.628 Acc@5 96.522
[2022-11-14 10:42:30 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.6%
[2022-11-14 10:42:32 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.004 (2.004)	Loss 0.7413 (0.7413)	Acc@1 84.180 (84.180)	Acc@5 96.289 (96.289)	Mem 23874MB
[2022-11-14 10:42:43 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.878 Acc@5 96.650
[2022-11-14 10:42:43 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 10:42:43 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 10:42:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][0/1251]	eta 0:51:46 lr 0.000023	time 2.4829 (2.4829)	loss 2.6146 (2.6146)	grad_norm 2.7386 (2.7386)	mem 23874MB
[2022-11-14 10:43:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][50/1251]	eta 0:15:41 lr 0.000023	time 0.8197 (0.7836)	loss 2.0379 (2.4928)	grad_norm 2.3528 (3.0722)	mem 23874MB
[2022-11-14 10:44:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][100/1251]	eta 0:14:43 lr 0.000023	time 0.7425 (0.7676)	loss 2.5562 (2.5836)	grad_norm 3.2341 (3.3129)	mem 23874MB
[2022-11-14 10:44:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][150/1251]	eta 0:13:58 lr 0.000023	time 0.7424 (0.7614)	loss 2.8543 (2.5827)	grad_norm 2.5616 (3.1763)	mem 23874MB
[2022-11-14 10:45:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][200/1251]	eta 0:13:17 lr 0.000023	time 0.7406 (0.7586)	loss 1.7125 (2.5914)	grad_norm 2.9837 (3.2072)	mem 23874MB
[2022-11-14 10:45:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][250/1251]	eta 0:12:37 lr 0.000023	time 0.7595 (0.7570)	loss 2.0054 (2.5734)	grad_norm 9.5817 (3.1889)	mem 23874MB
[2022-11-14 10:46:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][300/1251]	eta 0:11:58 lr 0.000023	time 0.8207 (0.7557)	loss 2.8935 (2.5719)	grad_norm 2.6103 (3.2522)	mem 23874MB
[2022-11-14 10:47:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][350/1251]	eta 0:11:19 lr 0.000023	time 0.7388 (0.7547)	loss 2.3902 (2.5783)	grad_norm 3.0587 (3.2515)	mem 23874MB
[2022-11-14 10:47:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][400/1251]	eta 0:10:41 lr 0.000023	time 0.7455 (0.7540)	loss 2.7802 (2.5912)	grad_norm 3.1050 (3.2286)	mem 23874MB
[2022-11-14 10:48:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][450/1251]	eta 0:10:03 lr 0.000023	time 0.7414 (0.7534)	loss 2.9025 (2.6040)	grad_norm 2.7370 (3.2427)	mem 23874MB
[2022-11-14 10:49:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][500/1251]	eta 0:09:25 lr 0.000023	time 0.8218 (0.7532)	loss 2.1230 (2.5921)	grad_norm 3.0078 (3.2280)	mem 23874MB
[2022-11-14 10:49:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][550/1251]	eta 0:08:47 lr 0.000023	time 0.7447 (0.7527)	loss 2.6688 (2.5980)	grad_norm 2.6059 (3.2229)	mem 23874MB
[2022-11-14 10:50:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][600/1251]	eta 0:08:09 lr 0.000023	time 0.7406 (0.7524)	loss 1.8567 (2.5973)	grad_norm 3.1428 (3.2469)	mem 23874MB
[2022-11-14 10:50:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][650/1251]	eta 0:07:32 lr 0.000022	time 0.7488 (0.7524)	loss 2.9512 (2.5929)	grad_norm 2.9990 (3.2273)	mem 23874MB
[2022-11-14 10:51:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][700/1251]	eta 0:06:54 lr 0.000022	time 0.8281 (0.7522)	loss 2.9153 (2.5932)	grad_norm 2.9098 (3.2477)	mem 23874MB
[2022-11-14 10:52:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][750/1251]	eta 0:06:16 lr 0.000022	time 0.7385 (0.7520)	loss 2.6603 (2.5874)	grad_norm 2.3633 (3.2377)	mem 23874MB
[2022-11-14 10:52:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][800/1251]	eta 0:05:39 lr 0.000022	time 0.7510 (0.7520)	loss 2.8636 (2.5877)	grad_norm 5.3438 (3.2371)	mem 23874MB
[2022-11-14 10:53:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][850/1251]	eta 0:05:01 lr 0.000022	time 0.7455 (0.7519)	loss 2.5733 (2.5852)	grad_norm 3.3787 (3.2340)	mem 23874MB
[2022-11-14 10:54:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][900/1251]	eta 0:04:23 lr 0.000022	time 0.7497 (0.7517)	loss 2.7781 (2.5911)	grad_norm 4.5195 (3.2193)	mem 23874MB
[2022-11-14 10:54:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][950/1251]	eta 0:03:46 lr 0.000022	time 0.7360 (0.7515)	loss 2.5028 (2.5958)	grad_norm 2.7427 (3.2119)	mem 23874MB
[2022-11-14 10:55:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][1000/1251]	eta 0:03:08 lr 0.000022	time 0.7399 (0.7515)	loss 2.8935 (2.5941)	grad_norm 2.5487 (3.2031)	mem 23874MB
[2022-11-14 10:55:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][1050/1251]	eta 0:02:31 lr 0.000022	time 0.7375 (0.7515)	loss 2.2903 (2.5922)	grad_norm 3.1578 (3.1911)	mem 23874MB
[2022-11-14 10:56:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][1100/1251]	eta 0:01:53 lr 0.000022	time 0.8340 (0.7515)	loss 2.7589 (2.5970)	grad_norm 3.0086 (3.1832)	mem 23874MB
[2022-11-14 10:57:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][1150/1251]	eta 0:01:15 lr 0.000022	time 0.7423 (0.7513)	loss 3.0582 (2.5987)	grad_norm 2.8553 (3.1691)	mem 23874MB
[2022-11-14 10:57:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][1200/1251]	eta 0:00:38 lr 0.000022	time 0.7389 (0.7513)	loss 2.1794 (2.5939)	grad_norm 3.0546 (3.1676)	mem 23874MB
[2022-11-14 10:58:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [278/300][1250/1251]	eta 0:00:00 lr 0.000022	time 0.7319 (0.7510)	loss 2.8856 (2.5960)	grad_norm 2.8503 (3.1731)	mem 23874MB
[2022-11-14 10:58:22 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 278 training takes 0:15:39
[2022-11-14 10:58:22 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_278.pth saving......
[2022-11-14 10:58:23 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_278.pth saved !!!
[2022-11-14 10:58:25 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.785 (1.785)	Loss 0.6768 (0.6768)	Acc@1 84.473 (84.473)	Acc@5 97.168 (97.168)	Mem 23874MB
[2022-11-14 10:58:36 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.698 Acc@5 96.570
[2022-11-14 10:58:36 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.7%
[2022-11-14 10:58:38 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.059 (2.059)	Loss 0.6664 (0.6664)	Acc@1 83.789 (83.789)	Acc@5 97.168 (97.168)	Mem 23874MB
[2022-11-14 10:58:49 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.904 Acc@5 96.650
[2022-11-14 10:58:49 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 10:58:49 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 10:58:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][0/1251]	eta 0:53:19 lr 0.000022	time 2.5573 (2.5573)	loss 2.9015 (2.9015)	grad_norm 3.4368 (3.4368)	mem 23874MB
[2022-11-14 10:59:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][50/1251]	eta 0:15:42 lr 0.000022	time 0.7374 (0.7844)	loss 2.7944 (2.6274)	grad_norm 2.6684 (3.1557)	mem 23874MB
[2022-11-14 11:00:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][100/1251]	eta 0:14:44 lr 0.000022	time 0.7420 (0.7683)	loss 2.7849 (2.5701)	grad_norm 2.6392 (3.2039)	mem 23874MB
[2022-11-14 11:00:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][150/1251]	eta 0:13:59 lr 0.000022	time 0.8369 (0.7628)	loss 2.1934 (2.6063)	grad_norm 2.8539 (3.3230)	mem 23874MB
[2022-11-14 11:01:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][200/1251]	eta 0:13:18 lr 0.000022	time 0.7436 (0.7598)	loss 2.7450 (2.6073)	grad_norm 3.1377 (inf)	mem 23874MB
[2022-11-14 11:01:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][250/1251]	eta 0:12:39 lr 0.000022	time 0.8342 (0.7582)	loss 2.6153 (2.5976)	grad_norm 2.7700 (inf)	mem 23874MB
[2022-11-14 11:02:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][300/1251]	eta 0:11:59 lr 0.000022	time 0.7425 (0.7567)	loss 2.6499 (2.6057)	grad_norm 2.6959 (inf)	mem 23874MB
[2022-11-14 11:03:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][350/1251]	eta 0:11:20 lr 0.000022	time 0.7395 (0.7556)	loss 2.8448 (2.5990)	grad_norm 2.2413 (inf)	mem 23874MB
[2022-11-14 11:03:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][400/1251]	eta 0:10:42 lr 0.000022	time 0.7386 (0.7550)	loss 2.9540 (2.6026)	grad_norm 3.0436 (inf)	mem 23874MB
[2022-11-14 11:04:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][450/1251]	eta 0:10:04 lr 0.000022	time 0.7460 (0.7543)	loss 2.1984 (2.6016)	grad_norm 2.9797 (inf)	mem 23874MB
[2022-11-14 11:05:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][500/1251]	eta 0:09:26 lr 0.000021	time 0.7393 (0.7543)	loss 2.5257 (2.6044)	grad_norm 2.7574 (inf)	mem 23874MB
[2022-11-14 11:05:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][550/1251]	eta 0:08:48 lr 0.000021	time 0.7408 (0.7536)	loss 2.4556 (2.6058)	grad_norm 3.2962 (inf)	mem 23874MB
[2022-11-14 11:06:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][600/1251]	eta 0:08:10 lr 0.000021	time 0.8243 (0.7533)	loss 2.3461 (2.6118)	grad_norm 2.5415 (inf)	mem 23874MB
[2022-11-14 11:06:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][650/1251]	eta 0:07:32 lr 0.000021	time 0.7396 (0.7528)	loss 2.6763 (2.6141)	grad_norm 2.4677 (inf)	mem 23874MB
[2022-11-14 11:07:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][700/1251]	eta 0:06:54 lr 0.000021	time 0.7429 (0.7527)	loss 2.6938 (2.6134)	grad_norm 3.3496 (inf)	mem 23874MB
[2022-11-14 11:08:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][750/1251]	eta 0:06:17 lr 0.000021	time 0.7516 (0.7526)	loss 2.8719 (2.6047)	grad_norm 3.6044 (inf)	mem 23874MB
[2022-11-14 11:08:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][800/1251]	eta 0:05:39 lr 0.000021	time 0.7349 (0.7525)	loss 1.9050 (2.6074)	grad_norm 3.3339 (inf)	mem 23874MB
[2022-11-14 11:09:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][850/1251]	eta 0:05:01 lr 0.000021	time 0.7407 (0.7523)	loss 2.9952 (2.6072)	grad_norm 2.5156 (inf)	mem 23874MB
[2022-11-14 11:10:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][900/1251]	eta 0:04:23 lr 0.000021	time 0.7407 (0.7520)	loss 2.6954 (2.6082)	grad_norm 2.9437 (inf)	mem 23874MB
[2022-11-14 11:10:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][950/1251]	eta 0:03:46 lr 0.000021	time 0.7494 (0.7519)	loss 2.8504 (2.6055)	grad_norm 2.6633 (inf)	mem 23874MB
[2022-11-14 11:11:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][1000/1251]	eta 0:03:08 lr 0.000021	time 0.7436 (0.7517)	loss 2.7780 (2.6059)	grad_norm 2.6250 (inf)	mem 23874MB
[2022-11-14 11:11:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][1050/1251]	eta 0:02:31 lr 0.000021	time 0.7386 (0.7518)	loss 2.8330 (2.6098)	grad_norm 3.5528 (inf)	mem 23874MB
[2022-11-14 11:12:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][1100/1251]	eta 0:01:53 lr 0.000021	time 0.7502 (0.7516)	loss 2.3896 (2.6066)	grad_norm 2.9277 (inf)	mem 23874MB
[2022-11-14 11:13:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][1150/1251]	eta 0:01:15 lr 0.000021	time 0.7400 (0.7516)	loss 2.3556 (2.6024)	grad_norm 3.8769 (inf)	mem 23874MB
[2022-11-14 11:13:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][1200/1251]	eta 0:00:38 lr 0.000021	time 0.7361 (0.7515)	loss 2.9218 (2.6008)	grad_norm 2.7747 (inf)	mem 23874MB
[2022-11-14 11:14:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [279/300][1250/1251]	eta 0:00:00 lr 0.000021	time 0.7294 (0.7512)	loss 2.9403 (2.5981)	grad_norm 2.8118 (inf)	mem 23874MB
[2022-11-14 11:14:29 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 279 training takes 0:15:39
[2022-11-14 11:14:29 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_279.pth saving......
[2022-11-14 11:14:30 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_279.pth saved !!!
[2022-11-14 11:14:32 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.719 (1.719)	Loss 0.6700 (0.6700)	Acc@1 84.082 (84.082)	Acc@5 96.875 (96.875)	Mem 23874MB
[2022-11-14 11:14:42 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.650 Acc@5 96.570
[2022-11-14 11:14:42 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.6%
[2022-11-14 11:14:44 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.946 (1.946)	Loss 0.6344 (0.6344)	Acc@1 85.059 (85.059)	Acc@5 97.168 (97.168)	Mem 23874MB
[2022-11-14 11:14:55 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.886 Acc@5 96.658
[2022-11-14 11:14:55 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 11:14:55 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 11:14:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][0/1251]	eta 0:50:43 lr 0.000021	time 2.4330 (2.4330)	loss 3.1029 (3.1029)	grad_norm 2.8119 (2.8119)	mem 23874MB
[2022-11-14 11:15:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][50/1251]	eta 0:15:43 lr 0.000021	time 0.7462 (0.7852)	loss 2.3258 (2.6139)	grad_norm 2.7631 (3.0918)	mem 23874MB
[2022-11-14 11:16:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][100/1251]	eta 0:14:43 lr 0.000021	time 0.7530 (0.7674)	loss 2.7244 (2.6162)	grad_norm 5.4780 (3.1457)	mem 23874MB
[2022-11-14 11:16:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][150/1251]	eta 0:13:57 lr 0.000021	time 0.7456 (0.7610)	loss 2.5814 (2.6213)	grad_norm 2.6474 (3.1404)	mem 23874MB
[2022-11-14 11:17:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][200/1251]	eta 0:13:16 lr 0.000021	time 0.7380 (0.7583)	loss 1.8467 (2.6095)	grad_norm 2.9261 (3.1688)	mem 23874MB
[2022-11-14 11:18:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][250/1251]	eta 0:12:37 lr 0.000021	time 0.7419 (0.7563)	loss 1.5638 (2.5926)	grad_norm 2.5531 (3.1651)	mem 23874MB
[2022-11-14 11:18:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][300/1251]	eta 0:11:59 lr 0.000021	time 0.7646 (0.7561)	loss 2.2901 (2.5821)	grad_norm 6.4263 (3.1861)	mem 23874MB
[2022-11-14 11:20:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][350/1251]	eta 0:14:31 lr 0.000021	time 2.5861 (0.9673)	loss 2.7856 (2.5822)	grad_norm 2.5029 (3.1561)	mem 23874MB
[2022-11-14 11:22:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][400/1251]	eta 0:16:32 lr 0.000020	time 2.6063 (1.1665)	loss 1.9770 (2.5672)	grad_norm 2.8201 (3.1854)	mem 23874MB
[2022-11-14 11:24:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][450/1251]	eta 0:17:38 lr 0.000020	time 2.5710 (1.3212)	loss 2.6019 (2.5713)	grad_norm 2.8246 (3.1827)	mem 23874MB
[2022-11-14 11:27:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][500/1251]	eta 0:18:05 lr 0.000020	time 2.5509 (1.4455)	loss 2.4136 (2.5777)	grad_norm 5.5511 (3.2124)	mem 23874MB
[2022-11-14 11:29:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][550/1251]	eta 0:18:04 lr 0.000020	time 2.5886 (1.5476)	loss 2.8259 (2.5808)	grad_norm 2.6120 (3.2274)	mem 23874MB
[2022-11-14 11:31:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][600/1251]	eta 0:17:42 lr 0.000020	time 2.4701 (1.6325)	loss 2.9410 (2.5764)	grad_norm 2.7787 (3.2248)	mem 23874MB
[2022-11-14 11:33:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][650/1251]	eta 0:16:49 lr 0.000020	time 0.7444 (1.6789)	loss 2.9899 (2.5830)	grad_norm 3.0871 (3.2080)	mem 23874MB
[2022-11-14 11:33:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][700/1251]	eta 0:14:48 lr 0.000020	time 0.7421 (1.6125)	loss 2.9353 (2.5886)	grad_norm 4.0972 (3.1999)	mem 23874MB
[2022-11-14 11:34:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][750/1251]	eta 0:12:59 lr 0.000020	time 0.7407 (1.5553)	loss 2.7331 (2.5914)	grad_norm 4.1134 (3.2018)	mem 23874MB
[2022-11-14 11:35:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][800/1251]	eta 0:11:18 lr 0.000020	time 0.7349 (1.5047)	loss 2.9825 (2.5906)	grad_norm 2.6299 (3.2033)	mem 23874MB
[2022-11-14 11:35:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][850/1251]	eta 0:09:45 lr 0.000020	time 0.7523 (1.4603)	loss 2.7641 (2.5952)	grad_norm 9.0128 (3.2025)	mem 23874MB
[2022-11-14 11:36:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][900/1251]	eta 0:08:18 lr 0.000020	time 0.7512 (1.4208)	loss 2.8747 (2.5939)	grad_norm 9.8423 (3.2158)	mem 23874MB
[2022-11-14 11:36:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][950/1251]	eta 0:06:57 lr 0.000020	time 0.7546 (1.3856)	loss 2.7264 (2.5872)	grad_norm 13.9870 (3.2248)	mem 23874MB
[2022-11-14 11:37:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][1000/1251]	eta 0:05:39 lr 0.000020	time 0.7398 (1.3539)	loss 3.0148 (2.5887)	grad_norm 2.5680 (3.2175)	mem 23874MB
[2022-11-14 11:38:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][1050/1251]	eta 0:04:26 lr 0.000020	time 0.7429 (1.3250)	loss 2.7358 (2.5899)	grad_norm 3.0488 (3.2111)	mem 23874MB
[2022-11-14 11:38:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][1100/1251]	eta 0:03:16 lr 0.000020	time 0.7425 (1.2988)	loss 2.6838 (2.5908)	grad_norm 2.9228 (3.2130)	mem 23874MB
[2022-11-14 11:39:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][1150/1251]	eta 0:02:08 lr 0.000020	time 0.7406 (1.2750)	loss 1.5148 (2.5841)	grad_norm 2.8972 (3.2108)	mem 23874MB
[2022-11-14 11:40:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][1200/1251]	eta 0:01:03 lr 0.000020	time 0.7454 (1.2531)	loss 2.7645 (2.5827)	grad_norm 3.2748 (3.2064)	mem 23874MB
[2022-11-14 11:40:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [280/300][1250/1251]	eta 0:00:01 lr 0.000020	time 0.7300 (1.2329)	loss 2.5112 (2.5797)	grad_norm 3.0965 (3.2026)	mem 23874MB
[2022-11-14 11:40:38 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 280 training takes 0:25:42
[2022-11-14 11:40:38 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_280.pth saving......
[2022-11-14 11:40:39 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_280.pth saved !!!
[2022-11-14 11:40:41 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.871 (1.871)	Loss 0.6378 (0.6378)	Acc@1 85.547 (85.547)	Acc@5 97.754 (97.754)	Mem 23874MB
[2022-11-14 11:40:52 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.700 Acc@5 96.582
[2022-11-14 11:40:52 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.7%
[2022-11-14 11:40:54 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.979 (1.979)	Loss 0.7169 (0.7169)	Acc@1 84.473 (84.473)	Acc@5 95.898 (95.898)	Mem 23874MB
[2022-11-14 11:41:05 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.896 Acc@5 96.662
[2022-11-14 11:41:05 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 11:41:05 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 11:41:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][0/1251]	eta 0:52:57 lr 0.000020	time 2.5402 (2.5402)	loss 1.8229 (1.8229)	grad_norm 2.5417 (2.5417)	mem 23874MB
[2022-11-14 11:41:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][50/1251]	eta 0:15:46 lr 0.000020	time 0.7405 (0.7883)	loss 2.2489 (2.5489)	grad_norm 2.6321 (3.3992)	mem 23874MB
[2022-11-14 11:42:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][100/1251]	eta 0:14:45 lr 0.000020	time 0.7438 (0.7695)	loss 2.4291 (2.6111)	grad_norm 2.7527 (3.3059)	mem 23874MB
[2022-11-14 11:43:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][150/1251]	eta 0:13:59 lr 0.000020	time 0.7404 (0.7628)	loss 2.2468 (2.6186)	grad_norm 2.6735 (3.3171)	mem 23874MB
[2022-11-14 11:43:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][200/1251]	eta 0:13:18 lr 0.000020	time 0.7479 (0.7598)	loss 1.6518 (2.6125)	grad_norm 9.6692 (3.3100)	mem 23874MB
[2022-11-14 11:44:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][250/1251]	eta 0:12:38 lr 0.000020	time 0.7468 (0.7581)	loss 2.5928 (2.6204)	grad_norm 2.5667 (3.3074)	mem 23874MB
[2022-11-14 11:44:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][300/1251]	eta 0:11:59 lr 0.000020	time 0.7447 (0.7568)	loss 2.7099 (2.6277)	grad_norm 2.7581 (3.3007)	mem 23874MB
[2022-11-14 11:45:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][350/1251]	eta 0:11:21 lr 0.000019	time 0.7431 (0.7560)	loss 2.6122 (2.6249)	grad_norm 3.4329 (3.2584)	mem 23874MB
[2022-11-14 11:46:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][400/1251]	eta 0:10:42 lr 0.000019	time 0.8132 (0.7547)	loss 2.8608 (2.6271)	grad_norm 2.4962 (3.2206)	mem 23874MB
[2022-11-14 11:46:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][450/1251]	eta 0:10:04 lr 0.000019	time 0.7486 (0.7546)	loss 3.0788 (2.6299)	grad_norm 3.0775 (3.2052)	mem 23874MB
[2022-11-14 11:47:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][500/1251]	eta 0:09:26 lr 0.000019	time 0.7403 (0.7541)	loss 1.8709 (2.6304)	grad_norm 2.6972 (3.2021)	mem 23874MB
[2022-11-14 11:48:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][550/1251]	eta 0:08:48 lr 0.000019	time 0.7426 (0.7537)	loss 3.0135 (2.6240)	grad_norm 3.7426 (3.1846)	mem 23874MB
[2022-11-14 11:48:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][600/1251]	eta 0:08:10 lr 0.000019	time 0.7397 (0.7532)	loss 2.8202 (2.6237)	grad_norm 3.0872 (3.1679)	mem 23874MB
[2022-11-14 11:49:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][650/1251]	eta 0:07:32 lr 0.000019	time 0.7478 (0.7532)	loss 2.6007 (2.6176)	grad_norm 3.0732 (3.1585)	mem 23874MB
[2022-11-14 11:49:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][700/1251]	eta 0:06:54 lr 0.000019	time 0.7407 (0.7527)	loss 3.0127 (2.6182)	grad_norm 2.9649 (3.1638)	mem 23874MB
[2022-11-14 11:50:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][750/1251]	eta 0:06:17 lr 0.000019	time 0.7440 (0.7527)	loss 3.0662 (2.6175)	grad_norm 2.3650 (3.1775)	mem 23874MB
[2022-11-14 11:51:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][800/1251]	eta 0:05:39 lr 0.000019	time 0.7475 (0.7525)	loss 1.9331 (2.6166)	grad_norm 2.5694 (3.1840)	mem 23874MB
[2022-11-14 11:51:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][850/1251]	eta 0:05:01 lr 0.000019	time 0.7419 (0.7524)	loss 2.5512 (2.6168)	grad_norm 3.2292 (3.1796)	mem 23874MB
[2022-11-14 11:52:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][900/1251]	eta 0:04:24 lr 0.000019	time 0.7384 (0.7523)	loss 1.8523 (2.6178)	grad_norm 2.9768 (3.1835)	mem 23874MB
[2022-11-14 11:53:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][950/1251]	eta 0:03:46 lr 0.000019	time 0.7494 (0.7521)	loss 2.4757 (2.6158)	grad_norm 2.5981 (3.1732)	mem 23874MB
[2022-11-14 11:53:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][1000/1251]	eta 0:03:08 lr 0.000019	time 0.7400 (0.7521)	loss 2.1959 (2.6149)	grad_norm 3.0679 (3.1650)	mem 23874MB
[2022-11-14 11:54:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][1050/1251]	eta 0:02:31 lr 0.000019	time 0.7395 (0.7520)	loss 2.9433 (2.6185)	grad_norm 2.9072 (3.1700)	mem 23874MB
[2022-11-14 11:54:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][1100/1251]	eta 0:01:53 lr 0.000019	time 0.7366 (0.7518)	loss 2.9047 (2.6158)	grad_norm 3.9928 (3.1785)	mem 23874MB
[2022-11-14 11:55:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][1150/1251]	eta 0:01:15 lr 0.000019	time 0.7464 (0.7517)	loss 2.6582 (2.6145)	grad_norm 2.9454 (3.2099)	mem 23874MB
[2022-11-14 11:56:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][1200/1251]	eta 0:00:38 lr 0.000019	time 0.7488 (0.7518)	loss 2.5076 (2.6082)	grad_norm 5.2696 (3.2010)	mem 23874MB
[2022-11-14 11:56:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [281/300][1250/1251]	eta 0:00:00 lr 0.000019	time 0.7293 (0.7514)	loss 2.6869 (2.6102)	grad_norm 2.7386 (3.2046)	mem 23874MB
[2022-11-14 11:56:45 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 281 training takes 0:15:40
[2022-11-14 11:56:45 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_281.pth saving......
[2022-11-14 11:56:46 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_281.pth saved !!!
[2022-11-14 11:56:48 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.828 (1.828)	Loss 0.7833 (0.7833)	Acc@1 81.543 (81.543)	Acc@5 95.801 (95.801)	Mem 23874MB
[2022-11-14 11:56:59 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.716 Acc@5 96.570
[2022-11-14 11:56:59 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.7%
[2022-11-14 11:57:01 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.053 (2.053)	Loss 0.7368 (0.7368)	Acc@1 82.812 (82.812)	Acc@5 96.191 (96.191)	Mem 23874MB
[2022-11-14 11:57:11 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.886 Acc@5 96.670
[2022-11-14 11:57:11 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 11:57:11 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 11:57:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][0/1251]	eta 0:54:20 lr 0.000019	time 2.6066 (2.6066)	loss 2.9424 (2.9424)	grad_norm 3.7421 (3.7421)	mem 23874MB
[2022-11-14 11:57:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][50/1251]	eta 0:15:42 lr 0.000019	time 0.7427 (0.7847)	loss 1.6222 (2.6137)	grad_norm 2.8938 (3.2446)	mem 23874MB
[2022-11-14 11:58:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][100/1251]	eta 0:14:45 lr 0.000019	time 0.7445 (0.7694)	loss 2.8974 (2.5868)	grad_norm 2.9407 (3.1942)	mem 23874MB
[2022-11-14 11:59:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][150/1251]	eta 0:14:00 lr 0.000019	time 0.8250 (0.7631)	loss 1.9428 (2.5734)	grad_norm 2.3509 (3.1642)	mem 23874MB
[2022-11-14 11:59:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][200/1251]	eta 0:13:18 lr 0.000019	time 0.7347 (0.7599)	loss 2.7815 (2.5528)	grad_norm 2.7796 (3.2787)	mem 23874MB
[2022-11-14 12:00:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][250/1251]	eta 0:12:39 lr 0.000019	time 0.7435 (0.7583)	loss 2.7624 (2.5519)	grad_norm 2.6197 (3.2453)	mem 23874MB
[2022-11-14 12:00:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][300/1251]	eta 0:11:59 lr 0.000019	time 0.7360 (0.7564)	loss 1.8277 (2.5538)	grad_norm 3.5447 (3.2149)	mem 23874MB
[2022-11-14 12:01:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][350/1251]	eta 0:11:20 lr 0.000018	time 0.7391 (0.7558)	loss 1.9709 (2.5506)	grad_norm 2.7750 (3.2529)	mem 23874MB
[2022-11-14 12:02:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][400/1251]	eta 0:10:42 lr 0.000018	time 0.7316 (0.7555)	loss 2.9476 (2.5575)	grad_norm 4.1809 (3.2307)	mem 23874MB
[2022-11-14 12:02:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][450/1251]	eta 0:10:04 lr 0.000018	time 0.7439 (0.7549)	loss 2.5547 (2.5634)	grad_norm 3.0906 (3.2161)	mem 23874MB
[2022-11-14 12:03:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][500/1251]	eta 0:09:26 lr 0.000018	time 0.7302 (0.7549)	loss 3.0068 (2.5673)	grad_norm 2.9952 (3.2094)	mem 23874MB
[2022-11-14 12:04:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][550/1251]	eta 0:08:48 lr 0.000018	time 0.8199 (0.7541)	loss 2.5440 (2.5639)	grad_norm 2.9774 (3.2232)	mem 23874MB
[2022-11-14 12:04:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][600/1251]	eta 0:08:10 lr 0.000018	time 0.7432 (0.7541)	loss 1.5442 (2.5615)	grad_norm 3.1105 (3.2122)	mem 23874MB
[2022-11-14 12:05:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][650/1251]	eta 0:07:33 lr 0.000018	time 0.7438 (0.7538)	loss 2.7824 (2.5578)	grad_norm 2.8431 (3.2045)	mem 23874MB
[2022-11-14 12:06:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][700/1251]	eta 0:06:55 lr 0.000018	time 0.7400 (0.7535)	loss 2.9398 (2.5621)	grad_norm 2.6085 (3.2146)	mem 23874MB
[2022-11-14 12:06:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][750/1251]	eta 0:06:17 lr 0.000018	time 0.7405 (0.7535)	loss 2.7198 (2.5652)	grad_norm 2.8108 (3.2141)	mem 23874MB
[2022-11-14 12:07:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][800/1251]	eta 0:05:39 lr 0.000018	time 0.8076 (0.7534)	loss 1.8138 (2.5695)	grad_norm 2.4705 (3.2204)	mem 23874MB
[2022-11-14 12:07:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][850/1251]	eta 0:05:01 lr 0.000018	time 0.7412 (0.7531)	loss 2.9157 (2.5654)	grad_norm 2.7517 (3.2173)	mem 23874MB
[2022-11-14 12:08:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][900/1251]	eta 0:04:24 lr 0.000018	time 0.7421 (0.7529)	loss 2.5153 (2.5684)	grad_norm 3.3103 (3.2073)	mem 23874MB
[2022-11-14 12:09:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][950/1251]	eta 0:03:46 lr 0.000018	time 0.8334 (0.7530)	loss 2.8738 (2.5719)	grad_norm 2.6584 (3.2065)	mem 23874MB
[2022-11-14 12:09:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][1000/1251]	eta 0:03:08 lr 0.000018	time 0.7395 (0.7528)	loss 1.7882 (2.5761)	grad_norm 3.2676 (3.1991)	mem 23874MB
[2022-11-14 12:10:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][1050/1251]	eta 0:02:31 lr 0.000018	time 0.7428 (0.7529)	loss 2.6786 (2.5789)	grad_norm 2.9848 (3.2016)	mem 23874MB
[2022-11-14 12:11:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][1100/1251]	eta 0:01:53 lr 0.000018	time 0.7436 (0.7526)	loss 1.8361 (2.5787)	grad_norm 2.6700 (3.2011)	mem 23874MB
[2022-11-14 12:11:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][1150/1251]	eta 0:01:16 lr 0.000018	time 0.7410 (0.7526)	loss 2.6049 (2.5800)	grad_norm 3.2242 (3.2014)	mem 23874MB
[2022-11-14 12:12:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][1200/1251]	eta 0:00:38 lr 0.000018	time 0.7418 (0.7526)	loss 2.9442 (2.5803)	grad_norm 3.3824 (3.2017)	mem 23874MB
[2022-11-14 12:12:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [282/300][1250/1251]	eta 0:00:00 lr 0.000018	time 0.7304 (0.7523)	loss 3.0767 (2.5818)	grad_norm 3.4260 (3.2004)	mem 23874MB
[2022-11-14 12:12:53 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 282 training takes 0:15:41
[2022-11-14 12:12:53 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_282.pth saving......
[2022-11-14 12:12:54 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_282.pth saved !!!
[2022-11-14 12:12:56 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.636 (1.636)	Loss 0.7449 (0.7449)	Acc@1 81.543 (81.543)	Acc@5 96.973 (96.973)	Mem 23874MB
[2022-11-14 12:13:06 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.724 Acc@5 96.592
[2022-11-14 12:13:06 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.7%
[2022-11-14 12:13:08 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.025 (2.025)	Loss 0.8126 (0.8126)	Acc@1 82.129 (82.129)	Acc@5 95.410 (95.410)	Mem 23874MB
[2022-11-14 12:13:19 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.914 Acc@5 96.666
[2022-11-14 12:13:19 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 12:13:19 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 12:13:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][0/1251]	eta 0:57:17 lr 0.000018	time 2.7480 (2.7480)	loss 2.0448 (2.0448)	grad_norm 2.7295 (2.7295)	mem 23874MB
[2022-11-14 12:13:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][50/1251]	eta 0:15:47 lr 0.000018	time 0.7486 (0.7892)	loss 1.7990 (2.5802)	grad_norm 2.6302 (3.0753)	mem 23874MB
[2022-11-14 12:14:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][100/1251]	eta 0:14:45 lr 0.000018	time 0.7446 (0.7697)	loss 2.4143 (2.5519)	grad_norm 2.5053 (3.1757)	mem 23874MB
[2022-11-14 12:15:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][150/1251]	eta 0:14:01 lr 0.000018	time 0.7429 (0.7643)	loss 2.8988 (2.5633)	grad_norm 2.9283 (3.1679)	mem 23874MB
[2022-11-14 12:15:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][200/1251]	eta 0:13:19 lr 0.000018	time 0.7420 (0.7610)	loss 2.6991 (2.5739)	grad_norm 2.6396 (3.1645)	mem 23874MB
[2022-11-14 12:16:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][250/1251]	eta 0:12:39 lr 0.000018	time 0.7451 (0.7589)	loss 1.7552 (2.5582)	grad_norm 2.6437 (3.1454)	mem 23874MB
[2022-11-14 12:17:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][300/1251]	eta 0:12:00 lr 0.000018	time 0.7486 (0.7572)	loss 2.5959 (2.5645)	grad_norm 3.0659 (3.1765)	mem 23874MB
[2022-11-14 12:17:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][350/1251]	eta 0:11:21 lr 0.000018	time 0.7465 (0.7563)	loss 2.6377 (2.5686)	grad_norm 2.7788 (inf)	mem 23874MB
[2022-11-14 12:18:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][400/1251]	eta 0:10:42 lr 0.000018	time 0.7383 (0.7550)	loss 1.9614 (2.5672)	grad_norm 3.0501 (inf)	mem 23874MB
[2022-11-14 12:19:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][450/1251]	eta 0:10:04 lr 0.000017	time 0.7393 (0.7548)	loss 2.9201 (2.5818)	grad_norm 3.2637 (inf)	mem 23874MB
[2022-11-14 12:19:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][500/1251]	eta 0:09:26 lr 0.000017	time 0.7412 (0.7542)	loss 2.2283 (2.5826)	grad_norm 2.8389 (inf)	mem 23874MB
[2022-11-14 12:20:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][550/1251]	eta 0:08:48 lr 0.000017	time 0.7395 (0.7538)	loss 3.0978 (2.5839)	grad_norm 2.8817 (inf)	mem 23874MB
[2022-11-14 12:20:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][600/1251]	eta 0:08:10 lr 0.000017	time 0.7428 (0.7533)	loss 2.3683 (2.5927)	grad_norm 3.1662 (inf)	mem 23874MB
[2022-11-14 12:21:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][650/1251]	eta 0:07:32 lr 0.000017	time 0.7392 (0.7532)	loss 2.2855 (2.5966)	grad_norm 2.7189 (inf)	mem 23874MB
[2022-11-14 12:22:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][700/1251]	eta 0:06:54 lr 0.000017	time 0.7386 (0.7527)	loss 2.7587 (2.5983)	grad_norm 4.1941 (inf)	mem 23874MB
[2022-11-14 12:22:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][750/1251]	eta 0:06:17 lr 0.000017	time 0.7374 (0.7527)	loss 2.3693 (2.5967)	grad_norm 3.1011 (inf)	mem 23874MB
[2022-11-14 12:23:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][800/1251]	eta 0:05:39 lr 0.000017	time 0.7414 (0.7524)	loss 2.8071 (2.5956)	grad_norm 3.2339 (inf)	mem 23874MB
[2022-11-14 12:23:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][850/1251]	eta 0:05:01 lr 0.000017	time 0.7438 (0.7522)	loss 2.6844 (2.5972)	grad_norm 2.7339 (nan)	mem 23874MB
[2022-11-14 12:24:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][900/1251]	eta 0:04:23 lr 0.000017	time 0.7388 (0.7519)	loss 2.6235 (2.5935)	grad_norm 3.0920 (nan)	mem 23874MB
[2022-11-14 12:25:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][950/1251]	eta 0:03:46 lr 0.000017	time 0.7571 (0.7518)	loss 2.8892 (2.5902)	grad_norm 3.0330 (nan)	mem 23874MB
[2022-11-14 12:25:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][1000/1251]	eta 0:03:08 lr 0.000017	time 0.7423 (0.7518)	loss 2.2414 (2.5910)	grad_norm 3.3653 (nan)	mem 23874MB
[2022-11-14 12:26:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][1050/1251]	eta 0:02:31 lr 0.000017	time 0.7405 (0.7518)	loss 2.8111 (2.5919)	grad_norm 2.4650 (nan)	mem 23874MB
[2022-11-14 12:27:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][1100/1251]	eta 0:01:53 lr 0.000017	time 0.7340 (0.7516)	loss 3.2089 (2.5929)	grad_norm 2.7128 (nan)	mem 23874MB
[2022-11-14 12:27:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][1150/1251]	eta 0:01:15 lr 0.000017	time 0.7387 (0.7515)	loss 2.9628 (2.5952)	grad_norm 2.7077 (nan)	mem 23874MB
[2022-11-14 12:28:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][1200/1251]	eta 0:00:38 lr 0.000017	time 0.7394 (0.7514)	loss 2.2007 (2.5932)	grad_norm 2.7725 (nan)	mem 23874MB
[2022-11-14 12:28:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [283/300][1250/1251]	eta 0:00:00 lr 0.000017	time 0.7336 (0.7511)	loss 2.6717 (2.5902)	grad_norm 3.5075 (nan)	mem 23874MB
[2022-11-14 12:28:59 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 283 training takes 0:15:39
[2022-11-14 12:28:59 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_283.pth saving......
[2022-11-14 12:29:00 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_283.pth saved !!!
[2022-11-14 12:29:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.849 (1.849)	Loss 0.6388 (0.6388)	Acc@1 85.840 (85.840)	Acc@5 96.582 (96.582)	Mem 23874MB
[2022-11-14 12:29:13 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.658 Acc@5 96.578
[2022-11-14 12:29:13 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.7%
[2022-11-14 12:29:15 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.935 (1.935)	Loss 0.5767 (0.5767)	Acc@1 87.402 (87.402)	Acc@5 97.852 (97.852)	Mem 23874MB
[2022-11-14 12:29:26 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.914 Acc@5 96.682
[2022-11-14 12:29:26 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 12:29:26 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 12:29:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][0/1251]	eta 0:53:36 lr 0.000017	time 2.5709 (2.5709)	loss 2.2200 (2.2200)	grad_norm 3.0287 (3.0287)	mem 23874MB
[2022-11-14 12:30:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][50/1251]	eta 0:15:47 lr 0.000017	time 0.7435 (0.7888)	loss 2.8051 (2.4906)	grad_norm 2.8701 (3.3138)	mem 23874MB
[2022-11-14 12:30:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][100/1251]	eta 0:14:45 lr 0.000017	time 0.7506 (0.7698)	loss 2.8534 (2.5571)	grad_norm 5.7509 (3.1693)	mem 23874MB
[2022-11-14 12:31:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][150/1251]	eta 0:14:02 lr 0.000017	time 0.7998 (0.7650)	loss 2.6251 (2.5627)	grad_norm 2.8095 (3.1918)	mem 23874MB
[2022-11-14 12:31:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][200/1251]	eta 0:13:19 lr 0.000017	time 0.8094 (0.7610)	loss 2.8594 (2.5806)	grad_norm 3.4845 (3.3153)	mem 23874MB
[2022-11-14 12:32:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][250/1251]	eta 0:12:40 lr 0.000017	time 0.8199 (0.7594)	loss 2.7482 (2.5904)	grad_norm 3.1865 (3.2946)	mem 23874MB
[2022-11-14 12:33:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][300/1251]	eta 0:12:00 lr 0.000017	time 0.7468 (0.7579)	loss 2.1423 (2.5848)	grad_norm 2.6739 (3.2540)	mem 23874MB
[2022-11-14 12:33:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][350/1251]	eta 0:11:21 lr 0.000017	time 0.7425 (0.7569)	loss 2.9899 (2.5797)	grad_norm 4.8454 (3.2405)	mem 23874MB
[2022-11-14 12:34:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][400/1251]	eta 0:10:43 lr 0.000017	time 0.7437 (0.7561)	loss 2.6001 (2.5712)	grad_norm 4.2993 (3.2563)	mem 23874MB
[2022-11-14 12:35:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][450/1251]	eta 0:10:05 lr 0.000017	time 0.7379 (0.7557)	loss 2.5458 (2.5758)	grad_norm 2.4489 (3.2861)	mem 23874MB
[2022-11-14 12:35:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][500/1251]	eta 0:09:27 lr 0.000017	time 0.7437 (0.7551)	loss 2.7166 (2.5682)	grad_norm 2.9626 (3.2768)	mem 23874MB
[2022-11-14 12:36:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][550/1251]	eta 0:08:49 lr 0.000017	time 0.8023 (0.7550)	loss 2.7379 (2.5666)	grad_norm 3.1947 (3.3043)	mem 23874MB
[2022-11-14 12:36:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][600/1251]	eta 0:08:11 lr 0.000017	time 0.7399 (0.7544)	loss 2.1125 (2.5659)	grad_norm 3.1722 (3.2796)	mem 23874MB
[2022-11-14 12:37:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][650/1251]	eta 0:07:33 lr 0.000016	time 0.7442 (0.7541)	loss 2.5257 (2.5676)	grad_norm 2.7017 (3.2589)	mem 23874MB
[2022-11-14 12:38:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][700/1251]	eta 0:06:55 lr 0.000016	time 0.7498 (0.7536)	loss 1.8787 (2.5699)	grad_norm 2.4740 (3.2427)	mem 23874MB
[2022-11-14 12:38:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][750/1251]	eta 0:06:17 lr 0.000016	time 0.7437 (0.7535)	loss 2.6132 (2.5708)	grad_norm 3.6653 (3.2456)	mem 23874MB
[2022-11-14 12:39:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][800/1251]	eta 0:05:39 lr 0.000016	time 0.7450 (0.7532)	loss 2.9323 (2.5723)	grad_norm 3.2603 (3.2566)	mem 23874MB
[2022-11-14 12:40:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][850/1251]	eta 0:05:02 lr 0.000016	time 0.7342 (0.7532)	loss 2.8296 (2.5681)	grad_norm 2.4720 (3.2755)	mem 23874MB
[2022-11-14 12:40:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][900/1251]	eta 0:04:24 lr 0.000016	time 0.7408 (0.7530)	loss 2.5161 (2.5651)	grad_norm 2.8783 (3.2627)	mem 23874MB
[2022-11-14 12:41:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][950/1251]	eta 0:03:46 lr 0.000016	time 0.8103 (0.7529)	loss 3.0658 (2.5669)	grad_norm 3.0544 (3.2666)	mem 23874MB
[2022-11-14 12:41:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][1000/1251]	eta 0:03:08 lr 0.000016	time 0.7419 (0.7527)	loss 2.2884 (2.5689)	grad_norm 3.0373 (3.2609)	mem 23874MB
[2022-11-14 12:42:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][1050/1251]	eta 0:02:31 lr 0.000016	time 0.7401 (0.7526)	loss 2.7810 (2.5690)	grad_norm 2.4892 (3.2509)	mem 23874MB
[2022-11-14 12:43:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][1100/1251]	eta 0:01:53 lr 0.000016	time 0.7388 (0.7524)	loss 1.7623 (2.5725)	grad_norm 2.8079 (3.2510)	mem 23874MB
[2022-11-14 12:43:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][1150/1251]	eta 0:01:15 lr 0.000016	time 0.7430 (0.7523)	loss 2.0103 (2.5691)	grad_norm 2.5107 (3.2444)	mem 23874MB
[2022-11-14 12:44:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][1200/1251]	eta 0:00:38 lr 0.000016	time 0.7450 (0.7521)	loss 2.8042 (2.5723)	grad_norm 2.8497 (3.2426)	mem 23874MB
[2022-11-14 12:45:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [284/300][1250/1251]	eta 0:00:00 lr 0.000016	time 0.7234 (0.7520)	loss 2.7971 (2.5794)	grad_norm 3.2017 (3.2473)	mem 23874MB
[2022-11-14 12:45:07 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 284 training takes 0:15:40
[2022-11-14 12:45:07 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_284.pth saving......
[2022-11-14 12:45:08 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_284.pth saved !!!
[2022-11-14 12:45:10 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.721 (1.721)	Loss 0.6921 (0.6921)	Acc@1 83.887 (83.887)	Acc@5 96.484 (96.484)	Mem 23874MB
[2022-11-14 12:45:20 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.770 Acc@5 96.586
[2022-11-14 12:45:20 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.8%
[2022-11-14 12:45:22 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.028 (2.028)	Loss 0.6749 (0.6749)	Acc@1 85.156 (85.156)	Acc@5 96.680 (96.680)	Mem 23874MB
[2022-11-14 12:45:33 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.918 Acc@5 96.684
[2022-11-14 12:45:33 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 12:45:33 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 264 epoch
[2022-11-14 12:45:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][0/1251]	eta 0:53:47 lr 0.000016	time 2.5801 (2.5801)	loss 2.0945 (2.0945)	grad_norm 3.2660 (3.2660)	mem 23874MB
[2022-11-14 12:46:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][50/1251]	eta 0:15:48 lr 0.000016	time 0.7437 (0.7895)	loss 3.1139 (2.5158)	grad_norm 2.7902 (3.2115)	mem 23874MB
[2022-11-14 12:46:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][100/1251]	eta 0:14:45 lr 0.000016	time 0.7442 (0.7696)	loss 2.9155 (2.5761)	grad_norm 2.9100 (3.1220)	mem 23874MB
[2022-11-14 12:47:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][150/1251]	eta 0:13:59 lr 0.000016	time 0.7438 (0.7629)	loss 2.8279 (2.5714)	grad_norm 3.0593 (3.1386)	mem 23874MB
[2022-11-14 12:48:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][200/1251]	eta 0:13:18 lr 0.000016	time 0.7462 (0.7596)	loss 2.3859 (2.5816)	grad_norm 2.9207 (3.1482)	mem 23874MB
[2022-11-14 12:48:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][250/1251]	eta 0:12:38 lr 0.000016	time 0.7441 (0.7578)	loss 2.7574 (2.5853)	grad_norm 2.8662 (3.1652)	mem 23874MB
[2022-11-14 12:49:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][300/1251]	eta 0:11:59 lr 0.000016	time 0.7396 (0.7568)	loss 2.7395 (2.5862)	grad_norm 2.9240 (3.1539)	mem 23874MB
[2022-11-14 12:49:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][350/1251]	eta 0:11:20 lr 0.000016	time 0.7402 (0.7553)	loss 2.3619 (2.5846)	grad_norm 2.6638 (3.1572)	mem 23874MB
[2022-11-14 12:50:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][400/1251]	eta 0:10:42 lr 0.000016	time 0.7389 (0.7551)	loss 2.8501 (2.5928)	grad_norm 3.1979 (3.1646)	mem 23874MB
[2022-11-14 12:51:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][450/1251]	eta 0:10:04 lr 0.000016	time 0.7438 (0.7543)	loss 2.2386 (2.5910)	grad_norm 3.3473 (3.1826)	mem 23874MB
[2022-11-14 12:51:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][500/1251]	eta 0:09:26 lr 0.000016	time 0.7450 (0.7540)	loss 2.9575 (2.5929)	grad_norm 4.6494 (3.1939)	mem 23874MB
[2022-11-14 12:52:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][550/1251]	eta 0:08:48 lr 0.000016	time 0.7408 (0.7537)	loss 3.0093 (2.5890)	grad_norm 3.2869 (3.1845)	mem 23874MB
[2022-11-14 12:53:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][600/1251]	eta 0:08:10 lr 0.000016	time 0.7437 (0.7533)	loss 2.7760 (2.5957)	grad_norm 2.6719 (3.1891)	mem 23874MB
[2022-11-14 12:53:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][650/1251]	eta 0:07:32 lr 0.000016	time 0.7467 (0.7531)	loss 2.3682 (2.5999)	grad_norm 3.3004 (3.1977)	mem 23874MB
[2022-11-14 12:54:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][700/1251]	eta 0:06:54 lr 0.000016	time 0.7453 (0.7530)	loss 3.1355 (2.5996)	grad_norm 2.6164 (3.1843)	mem 23874MB
[2022-11-14 12:54:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][750/1251]	eta 0:06:17 lr 0.000016	time 0.7408 (0.7529)	loss 2.5097 (2.5960)	grad_norm 3.3845 (3.1774)	mem 23874MB
[2022-11-14 12:55:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][800/1251]	eta 0:05:39 lr 0.000016	time 0.7417 (0.7526)	loss 2.7611 (2.5902)	grad_norm 2.7800 (3.1932)	mem 23874MB
[2022-11-14 12:56:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][850/1251]	eta 0:05:01 lr 0.000016	time 0.7426 (0.7526)	loss 2.9704 (2.5992)	grad_norm 3.7528 (3.2042)	mem 23874MB
[2022-11-14 12:56:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][900/1251]	eta 0:04:24 lr 0.000016	time 0.7415 (0.7526)	loss 2.7625 (2.5973)	grad_norm 4.7717 (3.2066)	mem 23874MB
[2022-11-14 12:57:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][950/1251]	eta 0:03:46 lr 0.000015	time 0.7389 (0.7524)	loss 3.0448 (2.5939)	grad_norm 2.9946 (3.2437)	mem 23874MB
[2022-11-14 12:58:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][1000/1251]	eta 0:03:08 lr 0.000015	time 0.7385 (0.7521)	loss 2.7107 (2.5943)	grad_norm 2.9571 (3.2378)	mem 23874MB
[2022-11-14 12:58:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][1050/1251]	eta 0:02:31 lr 0.000015	time 0.7447 (0.7521)	loss 2.8907 (2.5955)	grad_norm 3.2945 (3.2316)	mem 23874MB
[2022-11-14 12:59:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][1100/1251]	eta 0:01:53 lr 0.000015	time 0.7414 (0.7521)	loss 2.4448 (2.5908)	grad_norm 3.2554 (3.2196)	mem 23874MB
[2022-11-14 12:59:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][1150/1251]	eta 0:01:15 lr 0.000015	time 0.7420 (0.7520)	loss 3.1437 (2.5871)	grad_norm 4.1574 (3.2119)	mem 23874MB
[2022-11-14 13:00:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][1200/1251]	eta 0:00:38 lr 0.000015	time 0.7408 (0.7519)	loss 2.2305 (2.5883)	grad_norm 3.6168 (3.2131)	mem 23874MB
[2022-11-14 13:01:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [285/300][1250/1251]	eta 0:00:00 lr 0.000015	time 0.7349 (0.7517)	loss 3.1672 (2.5897)	grad_norm 3.3272 (3.2127)	mem 23874MB
[2022-11-14 13:01:14 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 285 training takes 0:15:40
[2022-11-14 13:01:14 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_285.pth saving......
[2022-11-14 13:01:15 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_285.pth saved !!!
[2022-11-14 13:01:17 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.714 (1.714)	Loss 0.6117 (0.6117)	Acc@1 86.328 (86.328)	Acc@5 96.973 (96.973)	Mem 23874MB
[2022-11-14 13:01:27 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.738 Acc@5 96.564
[2022-11-14 13:01:27 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.7%
[2022-11-14 13:01:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.931 (1.931)	Loss 0.7403 (0.7403)	Acc@1 84.668 (84.668)	Acc@5 95.410 (95.410)	Mem 23874MB
[2022-11-14 13:01:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.944 Acc@5 96.674
[2022-11-14 13:01:40 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 13:01:40 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.94% at 285 epoch
[2022-11-14 13:01:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][0/1251]	eta 0:54:12 lr 0.000015	time 2.5997 (2.5997)	loss 1.7074 (1.7074)	grad_norm 2.6611 (2.6611)	mem 23874MB
[2022-11-14 13:02:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][50/1251]	eta 0:15:47 lr 0.000015	time 0.7510 (0.7891)	loss 2.6811 (2.6014)	grad_norm 2.5482 (2.9698)	mem 23874MB
[2022-11-14 13:02:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][100/1251]	eta 0:14:46 lr 0.000015	time 0.7410 (0.7699)	loss 2.8832 (2.5837)	grad_norm 2.9418 (3.0860)	mem 23874MB
[2022-11-14 13:03:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][150/1251]	eta 0:14:01 lr 0.000015	time 0.7444 (0.7639)	loss 2.6529 (2.5979)	grad_norm 2.6812 (3.2929)	mem 23874MB
[2022-11-14 13:04:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][200/1251]	eta 0:13:19 lr 0.000015	time 0.7426 (0.7603)	loss 1.8250 (2.5954)	grad_norm 2.5874 (3.2950)	mem 23874MB
[2022-11-14 13:04:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][250/1251]	eta 0:12:39 lr 0.000015	time 0.7396 (0.7584)	loss 2.0472 (2.5888)	grad_norm 2.4580 (3.2728)	mem 23874MB
[2022-11-14 13:05:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][300/1251]	eta 0:11:59 lr 0.000015	time 0.7500 (0.7569)	loss 2.6554 (2.5930)	grad_norm 8.6027 (3.2825)	mem 23874MB
[2022-11-14 13:06:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][350/1251]	eta 0:11:21 lr 0.000015	time 0.7470 (0.7562)	loss 2.6571 (2.5935)	grad_norm 2.9293 (3.2539)	mem 23874MB
[2022-11-14 13:06:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][400/1251]	eta 0:10:42 lr 0.000015	time 0.8154 (0.7551)	loss 2.0865 (2.5937)	grad_norm 3.1579 (3.2700)	mem 23874MB
[2022-11-14 13:07:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][450/1251]	eta 0:10:04 lr 0.000015	time 0.7469 (0.7545)	loss 3.0553 (2.5932)	grad_norm 3.0320 (3.2458)	mem 23874MB
[2022-11-14 13:07:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][500/1251]	eta 0:09:26 lr 0.000015	time 0.7389 (0.7538)	loss 2.9029 (2.5885)	grad_norm 3.0067 (3.2625)	mem 23874MB
[2022-11-14 13:08:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][550/1251]	eta 0:08:48 lr 0.000015	time 0.7500 (0.7537)	loss 2.3900 (2.5860)	grad_norm 2.3190 (3.2773)	mem 23874MB
[2022-11-14 13:09:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][600/1251]	eta 0:08:10 lr 0.000015	time 0.7432 (0.7531)	loss 2.7935 (2.5820)	grad_norm 2.9676 (3.2580)	mem 23874MB
[2022-11-14 13:09:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][650/1251]	eta 0:07:32 lr 0.000015	time 0.7466 (0.7529)	loss 2.9609 (2.5864)	grad_norm 3.0749 (nan)	mem 23874MB
[2022-11-14 13:10:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][700/1251]	eta 0:06:54 lr 0.000015	time 0.7480 (0.7525)	loss 3.1805 (2.5872)	grad_norm 2.7156 (nan)	mem 23874MB
[2022-11-14 13:11:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][750/1251]	eta 0:06:16 lr 0.000015	time 0.7415 (0.7523)	loss 2.7579 (2.5842)	grad_norm 2.5798 (nan)	mem 23874MB
[2022-11-14 13:11:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][800/1251]	eta 0:05:39 lr 0.000015	time 0.7421 (0.7522)	loss 2.1276 (2.5835)	grad_norm 2.4078 (nan)	mem 23874MB
[2022-11-14 13:12:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][850/1251]	eta 0:05:01 lr 0.000015	time 0.7411 (0.7521)	loss 3.0178 (2.5915)	grad_norm 3.7971 (nan)	mem 23874MB
[2022-11-14 13:12:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][900/1251]	eta 0:04:23 lr 0.000015	time 0.7510 (0.7519)	loss 2.6997 (2.5869)	grad_norm 5.1054 (nan)	mem 23874MB
[2022-11-14 13:13:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][950/1251]	eta 0:03:46 lr 0.000015	time 0.7374 (0.7518)	loss 2.6078 (2.5883)	grad_norm 2.7624 (nan)	mem 23874MB
[2022-11-14 13:14:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][1000/1251]	eta 0:03:08 lr 0.000015	time 0.7415 (0.7516)	loss 2.1900 (2.5882)	grad_norm 2.8208 (nan)	mem 23874MB
[2022-11-14 13:14:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][1050/1251]	eta 0:02:31 lr 0.000015	time 0.7349 (0.7516)	loss 2.9850 (2.5840)	grad_norm 3.0866 (nan)	mem 23874MB
[2022-11-14 13:15:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][1100/1251]	eta 0:01:53 lr 0.000015	time 0.7432 (0.7514)	loss 2.7685 (2.5856)	grad_norm 2.8828 (nan)	mem 23874MB
[2022-11-14 13:16:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][1150/1251]	eta 0:01:15 lr 0.000015	time 0.7424 (0.7514)	loss 1.8201 (2.5852)	grad_norm 2.2160 (nan)	mem 23874MB
[2022-11-14 13:16:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][1200/1251]	eta 0:00:38 lr 0.000015	time 0.7388 (0.7513)	loss 2.9558 (2.5818)	grad_norm 4.2313 (nan)	mem 23874MB
[2022-11-14 13:17:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [286/300][1250/1251]	eta 0:00:00 lr 0.000015	time 0.7299 (0.7511)	loss 2.8893 (2.5824)	grad_norm 3.2312 (nan)	mem 23874MB
[2022-11-14 13:17:20 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 286 training takes 0:15:39
[2022-11-14 13:17:20 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_286.pth saving......
[2022-11-14 13:17:21 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_286.pth saved !!!
[2022-11-14 13:17:23 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.722 (1.722)	Loss 0.7650 (0.7650)	Acc@1 82.910 (82.910)	Acc@5 96.191 (96.191)	Mem 23874MB
[2022-11-14 13:17:34 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.804 Acc@5 96.632
[2022-11-14 13:17:34 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.8%
[2022-11-14 13:17:36 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.956 (1.956)	Loss 0.6375 (0.6375)	Acc@1 85.059 (85.059)	Acc@5 97.363 (97.363)	Mem 23874MB
[2022-11-14 13:17:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.956 Acc@5 96.666
[2022-11-14 13:17:47 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 84.0%
[2022-11-14 13:17:47 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 286 epoch
[2022-11-14 13:17:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][0/1251]	eta 0:54:01 lr 0.000015	time 2.5907 (2.5907)	loss 2.2928 (2.2928)	grad_norm 3.0199 (3.0199)	mem 23874MB
[2022-11-14 13:18:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][50/1251]	eta 0:15:47 lr 0.000015	time 0.7360 (0.7885)	loss 2.7709 (2.6006)	grad_norm 3.0324 (3.4659)	mem 23874MB
[2022-11-14 13:19:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][100/1251]	eta 0:14:44 lr 0.000015	time 0.7418 (0.7686)	loss 2.4771 (2.6449)	grad_norm 3.5980 (3.4003)	mem 23874MB
[2022-11-14 13:19:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][150/1251]	eta 0:13:59 lr 0.000014	time 0.7433 (0.7625)	loss 2.9666 (2.6254)	grad_norm 2.5606 (3.3452)	mem 23874MB
[2022-11-14 13:20:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][200/1251]	eta 0:13:18 lr 0.000014	time 0.8202 (0.7595)	loss 2.3783 (2.6123)	grad_norm 3.3889 (3.3445)	mem 23874MB
[2022-11-14 13:20:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][250/1251]	eta 0:12:38 lr 0.000014	time 0.7492 (0.7575)	loss 2.1708 (2.6080)	grad_norm 3.8724 (3.3461)	mem 23874MB
[2022-11-14 13:21:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][300/1251]	eta 0:11:59 lr 0.000014	time 0.7372 (0.7567)	loss 3.1351 (2.5994)	grad_norm 3.6866 (3.3491)	mem 23874MB
[2022-11-14 13:22:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][350/1251]	eta 0:11:20 lr 0.000014	time 0.7360 (0.7552)	loss 2.3684 (2.5852)	grad_norm 2.8634 (3.3162)	mem 23874MB
[2022-11-14 13:22:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][400/1251]	eta 0:10:42 lr 0.000014	time 0.7456 (0.7549)	loss 2.2004 (2.5807)	grad_norm 2.4613 (3.2971)	mem 23874MB
[2022-11-14 13:23:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][450/1251]	eta 0:10:04 lr 0.000014	time 0.7435 (0.7541)	loss 3.2037 (2.5770)	grad_norm 3.0711 (3.3169)	mem 23874MB
[2022-11-14 13:24:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][500/1251]	eta 0:09:26 lr 0.000014	time 0.7417 (0.7537)	loss 2.2153 (2.5739)	grad_norm 3.5016 (3.2973)	mem 23874MB
[2022-11-14 13:24:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][550/1251]	eta 0:08:48 lr 0.000014	time 0.7398 (0.7533)	loss 2.6480 (2.5737)	grad_norm 2.6095 (3.2992)	mem 23874MB
[2022-11-14 13:25:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][600/1251]	eta 0:08:10 lr 0.000014	time 0.8230 (0.7529)	loss 1.7845 (2.5706)	grad_norm 2.5415 (3.2779)	mem 23874MB
[2022-11-14 13:25:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][650/1251]	eta 0:07:32 lr 0.000014	time 0.7418 (0.7528)	loss 3.0781 (2.5744)	grad_norm 3.3461 (3.2734)	mem 23874MB
[2022-11-14 13:26:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][700/1251]	eta 0:06:54 lr 0.000014	time 0.7412 (0.7526)	loss 1.9130 (2.5735)	grad_norm 2.9896 (3.2671)	mem 23874MB
[2022-11-14 13:27:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][750/1251]	eta 0:06:16 lr 0.000014	time 0.7464 (0.7523)	loss 2.7043 (2.5730)	grad_norm 3.2945 (3.2606)	mem 23874MB
[2022-11-14 13:27:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][800/1251]	eta 0:05:39 lr 0.000014	time 0.7404 (0.7523)	loss 2.3589 (2.5694)	grad_norm 3.1212 (3.2631)	mem 23874MB
[2022-11-14 13:28:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][850/1251]	eta 0:05:01 lr 0.000014	time 0.7393 (0.7521)	loss 2.6577 (2.5656)	grad_norm 3.1635 (3.2501)	mem 23874MB
[2022-11-14 13:29:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][900/1251]	eta 0:04:23 lr 0.000014	time 0.7398 (0.7518)	loss 2.3890 (2.5671)	grad_norm 3.4655 (3.2399)	mem 23874MB
[2022-11-14 13:29:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][950/1251]	eta 0:03:46 lr 0.000014	time 0.7428 (0.7518)	loss 2.8319 (2.5716)	grad_norm 2.4613 (3.2612)	mem 23874MB
[2022-11-14 13:30:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][1000/1251]	eta 0:03:08 lr 0.000014	time 0.8206 (0.7517)	loss 2.9149 (2.5738)	grad_norm 2.8948 (3.2577)	mem 23874MB
[2022-11-14 13:30:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][1050/1251]	eta 0:02:31 lr 0.000014	time 0.7400 (0.7515)	loss 3.0142 (2.5680)	grad_norm 2.8263 (3.2492)	mem 23874MB
[2022-11-14 13:31:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][1100/1251]	eta 0:01:53 lr 0.000014	time 0.7432 (0.7515)	loss 2.3083 (2.5701)	grad_norm 2.8960 (3.2586)	mem 23874MB
[2022-11-14 13:32:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][1150/1251]	eta 0:01:15 lr 0.000014	time 0.7351 (0.7513)	loss 2.0722 (2.5702)	grad_norm 3.2516 (3.2741)	mem 23874MB
[2022-11-14 13:32:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][1200/1251]	eta 0:00:38 lr 0.000014	time 0.7356 (0.7513)	loss 2.3008 (2.5731)	grad_norm 3.6670 (3.2757)	mem 23874MB
[2022-11-14 13:33:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [287/300][1250/1251]	eta 0:00:00 lr 0.000014	time 0.7277 (0.7510)	loss 2.5857 (2.5714)	grad_norm 3.1250 (3.2707)	mem 23874MB
[2022-11-14 13:33:26 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 287 training takes 0:15:39
[2022-11-14 13:33:26 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_287.pth saving......
[2022-11-14 13:33:27 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_287.pth saved !!!
[2022-11-14 13:33:29 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.715 (1.715)	Loss 0.6872 (0.6872)	Acc@1 84.863 (84.863)	Acc@5 96.289 (96.289)	Mem 23874MB
[2022-11-14 13:33:40 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.826 Acc@5 96.590
[2022-11-14 13:33:40 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.8%
[2022-11-14 13:33:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.882 (1.882)	Loss 0.7426 (0.7426)	Acc@1 84.375 (84.375)	Acc@5 96.094 (96.094)	Mem 23874MB
[2022-11-14 13:33:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.962 Acc@5 96.652
[2022-11-14 13:33:53 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 84.0%
[2022-11-14 13:33:53 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 13:33:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][0/1251]	eta 0:53:24 lr 0.000014	time 2.5618 (2.5618)	loss 2.3655 (2.3655)	grad_norm 3.1591 (3.1591)	mem 23874MB
[2022-11-14 13:34:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][50/1251]	eta 0:15:44 lr 0.000014	time 0.7397 (0.7868)	loss 2.3530 (2.5806)	grad_norm 3.0202 (3.4658)	mem 23874MB
[2022-11-14 13:35:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][100/1251]	eta 0:14:44 lr 0.000014	time 0.7408 (0.7688)	loss 1.9064 (2.5622)	grad_norm 2.9214 (3.4379)	mem 23874MB
[2022-11-14 13:35:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][150/1251]	eta 0:13:59 lr 0.000014	time 0.7459 (0.7625)	loss 2.1136 (2.5853)	grad_norm 3.0729 (3.5277)	mem 23874MB
[2022-11-14 13:36:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][200/1251]	eta 0:13:19 lr 0.000014	time 0.7399 (0.7603)	loss 2.8862 (2.5836)	grad_norm 2.7049 (3.4364)	mem 23874MB
[2022-11-14 13:37:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][250/1251]	eta 0:12:39 lr 0.000014	time 0.7413 (0.7585)	loss 2.2407 (2.5620)	grad_norm 2.3029 (3.4111)	mem 23874MB
[2022-11-14 13:37:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][300/1251]	eta 0:12:00 lr 0.000014	time 0.7407 (0.7577)	loss 1.4476 (2.5634)	grad_norm 2.7740 (3.3647)	mem 23874MB
[2022-11-14 13:38:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][350/1251]	eta 0:11:21 lr 0.000014	time 0.7406 (0.7565)	loss 1.7196 (2.5722)	grad_norm 2.6106 (3.3200)	mem 23874MB
[2022-11-14 13:38:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][400/1251]	eta 0:10:43 lr 0.000014	time 0.8074 (0.7557)	loss 1.5083 (2.5771)	grad_norm 4.5955 (3.3198)	mem 23874MB
[2022-11-14 13:39:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][450/1251]	eta 0:10:04 lr 0.000014	time 0.7434 (0.7549)	loss 2.8282 (2.5785)	grad_norm 2.5255 (3.3047)	mem 23874MB
[2022-11-14 13:40:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][500/1251]	eta 0:09:26 lr 0.000014	time 0.7412 (0.7544)	loss 2.5372 (2.5843)	grad_norm 2.9934 (3.2808)	mem 23874MB
[2022-11-14 13:40:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][550/1251]	eta 0:08:48 lr 0.000014	time 0.7374 (0.7543)	loss 2.3971 (2.5812)	grad_norm 4.0532 (3.3270)	mem 23874MB
[2022-11-14 13:41:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][600/1251]	eta 0:08:10 lr 0.000014	time 0.7410 (0.7540)	loss 2.8005 (2.5736)	grad_norm 2.9671 (3.3039)	mem 23874MB
[2022-11-14 13:42:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][650/1251]	eta 0:07:32 lr 0.000014	time 0.7408 (0.7536)	loss 2.7753 (2.5735)	grad_norm 2.5096 (3.2914)	mem 23874MB
[2022-11-14 13:42:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][700/1251]	eta 0:06:55 lr 0.000014	time 0.7391 (0.7532)	loss 2.0931 (2.5795)	grad_norm 2.7853 (3.2673)	mem 23874MB
[2022-11-14 13:43:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][750/1251]	eta 0:06:17 lr 0.000014	time 0.7504 (0.7532)	loss 2.6822 (2.5812)	grad_norm 3.7265 (3.2574)	mem 23874MB
[2022-11-14 13:43:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][800/1251]	eta 0:05:39 lr 0.000013	time 0.7372 (0.7531)	loss 2.7976 (2.5814)	grad_norm 3.1178 (3.2565)	mem 23874MB
[2022-11-14 13:44:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][850/1251]	eta 0:05:01 lr 0.000013	time 0.7445 (0.7531)	loss 3.0346 (2.5865)	grad_norm 3.0201 (3.2452)	mem 23874MB
[2022-11-14 13:45:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][900/1251]	eta 0:04:24 lr 0.000013	time 0.7377 (0.7529)	loss 2.9363 (2.5860)	grad_norm 3.6225 (3.2416)	mem 23874MB
[2022-11-14 13:45:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][950/1251]	eta 0:03:46 lr 0.000013	time 0.8156 (0.7528)	loss 2.7802 (2.5871)	grad_norm 2.8292 (3.2725)	mem 23874MB
[2022-11-14 13:46:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][1000/1251]	eta 0:03:08 lr 0.000013	time 0.7460 (0.7526)	loss 2.0853 (2.5857)	grad_norm 3.3619 (3.2679)	mem 23874MB
[2022-11-14 13:47:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][1050/1251]	eta 0:02:31 lr 0.000013	time 0.7486 (0.7526)	loss 3.0545 (2.5842)	grad_norm 3.6244 (nan)	mem 23874MB
[2022-11-14 13:47:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][1100/1251]	eta 0:01:53 lr 0.000013	time 0.7377 (0.7525)	loss 1.7860 (2.5820)	grad_norm 2.6088 (nan)	mem 23874MB
[2022-11-14 13:48:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][1150/1251]	eta 0:01:15 lr 0.000013	time 0.7451 (0.7524)	loss 2.5567 (2.5823)	grad_norm 3.0119 (nan)	mem 23874MB
[2022-11-14 13:48:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][1200/1251]	eta 0:00:38 lr 0.000013	time 0.7401 (0.7523)	loss 2.7970 (2.5828)	grad_norm 3.3429 (nan)	mem 23874MB
[2022-11-14 13:49:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [288/300][1250/1251]	eta 0:00:00 lr 0.000013	time 0.7288 (0.7520)	loss 2.7452 (2.5819)	grad_norm 2.7922 (nan)	mem 23874MB
[2022-11-14 13:49:34 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 288 training takes 0:15:40
[2022-11-14 13:49:34 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_288.pth saving......
[2022-11-14 13:49:35 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_288.pth saved !!!
[2022-11-14 13:49:37 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.675 (1.675)	Loss 0.7991 (0.7991)	Acc@1 82.910 (82.910)	Acc@5 95.703 (95.703)	Mem 23874MB
[2022-11-14 13:49:47 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.748 Acc@5 96.550
[2022-11-14 13:49:47 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.7%
[2022-11-14 13:49:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.050 (2.050)	Loss 0.7495 (0.7495)	Acc@1 83.594 (83.594)	Acc@5 96.680 (96.680)	Mem 23874MB
[2022-11-14 13:50:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.934 Acc@5 96.654
[2022-11-14 13:50:00 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 13:50:00 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 13:50:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][0/1251]	eta 0:52:05 lr 0.000013	time 2.4981 (2.4981)	loss 1.6491 (1.6491)	grad_norm 2.9932 (2.9932)	mem 23874MB
[2022-11-14 13:50:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][50/1251]	eta 0:15:45 lr 0.000013	time 0.7595 (0.7869)	loss 2.8306 (2.6153)	grad_norm 9.4927 (3.5415)	mem 23874MB
[2022-11-14 13:51:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][100/1251]	eta 0:14:45 lr 0.000013	time 0.7380 (0.7693)	loss 1.7583 (2.6066)	grad_norm 2.4770 (3.3864)	mem 23874MB
[2022-11-14 13:51:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][150/1251]	eta 0:14:00 lr 0.000013	time 0.7498 (0.7630)	loss 2.6288 (2.5829)	grad_norm 6.6248 (3.2683)	mem 23874MB
[2022-11-14 13:52:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][200/1251]	eta 0:13:18 lr 0.000013	time 0.7504 (0.7601)	loss 2.8609 (2.5888)	grad_norm 2.6486 (3.3116)	mem 23874MB
[2022-11-14 13:53:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][250/1251]	eta 0:12:38 lr 0.000013	time 0.7357 (0.7580)	loss 2.8645 (2.5766)	grad_norm 2.7433 (3.3364)	mem 23874MB
[2022-11-14 13:53:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][300/1251]	eta 0:11:59 lr 0.000013	time 0.7375 (0.7561)	loss 2.8306 (2.5890)	grad_norm 3.3091 (3.3235)	mem 23874MB
[2022-11-14 13:54:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][350/1251]	eta 0:11:20 lr 0.000013	time 0.7477 (0.7549)	loss 2.7718 (2.5946)	grad_norm 6.5409 (3.2994)	mem 23874MB
[2022-11-14 13:55:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][400/1251]	eta 0:10:41 lr 0.000013	time 0.7372 (0.7541)	loss 2.8678 (2.5931)	grad_norm 2.8261 (3.2834)	mem 23874MB
[2022-11-14 13:55:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][450/1251]	eta 0:10:03 lr 0.000013	time 0.7373 (0.7536)	loss 2.5179 (2.5907)	grad_norm 3.4155 (3.2520)	mem 23874MB
[2022-11-14 13:56:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][500/1251]	eta 0:09:25 lr 0.000013	time 0.7367 (0.7529)	loss 2.6938 (2.5869)	grad_norm 2.8883 (3.2361)	mem 23874MB
[2022-11-14 13:56:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][550/1251]	eta 0:08:47 lr 0.000013	time 0.7413 (0.7524)	loss 2.5189 (2.5817)	grad_norm 3.1905 (3.2164)	mem 23874MB
[2022-11-14 13:57:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][600/1251]	eta 0:08:09 lr 0.000013	time 0.8124 (0.7522)	loss 2.1539 (2.5874)	grad_norm 3.6239 (3.2348)	mem 23874MB
[2022-11-14 13:58:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][650/1251]	eta 0:07:32 lr 0.000013	time 0.7390 (0.7522)	loss 2.3646 (2.5891)	grad_norm 2.5450 (3.2582)	mem 23874MB
[2022-11-14 13:58:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][700/1251]	eta 0:06:54 lr 0.000013	time 0.7408 (0.7518)	loss 2.8744 (2.5879)	grad_norm 3.0871 (3.2563)	mem 23874MB
[2022-11-14 13:59:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][750/1251]	eta 0:06:16 lr 0.000013	time 0.7373 (0.7516)	loss 2.7626 (2.5881)	grad_norm 2.8697 (3.2410)	mem 23874MB
[2022-11-14 14:00:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][800/1251]	eta 0:05:38 lr 0.000013	time 0.7323 (0.7513)	loss 2.7252 (2.5850)	grad_norm 3.0796 (3.2352)	mem 23874MB
[2022-11-14 14:00:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][850/1251]	eta 0:05:01 lr 0.000013	time 0.7556 (0.7514)	loss 2.6896 (2.5800)	grad_norm 2.5662 (3.2378)	mem 23874MB
[2022-11-14 14:01:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][900/1251]	eta 0:04:23 lr 0.000013	time 0.7372 (0.7510)	loss 2.7060 (2.5807)	grad_norm 2.6194 (3.2498)	mem 23874MB
[2022-11-14 14:01:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][950/1251]	eta 0:03:46 lr 0.000013	time 0.7374 (0.7510)	loss 2.6707 (2.5829)	grad_norm 2.6777 (3.2510)	mem 23874MB
[2022-11-14 14:02:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][1000/1251]	eta 0:03:08 lr 0.000013	time 0.7447 (0.7507)	loss 2.2342 (2.5829)	grad_norm 2.9891 (3.2490)	mem 23874MB
[2022-11-14 14:03:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][1050/1251]	eta 0:02:30 lr 0.000013	time 0.7482 (0.7507)	loss 2.9393 (2.5813)	grad_norm 7.0999 (3.2650)	mem 23874MB
[2022-11-14 14:03:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][1100/1251]	eta 0:01:53 lr 0.000013	time 0.7409 (0.7504)	loss 3.0138 (2.5787)	grad_norm 2.9725 (3.2616)	mem 23874MB
[2022-11-14 14:04:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][1150/1251]	eta 0:01:15 lr 0.000013	time 0.7437 (0.7505)	loss 3.1773 (2.5814)	grad_norm 4.0375 (3.2716)	mem 23874MB
[2022-11-14 14:05:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][1200/1251]	eta 0:00:38 lr 0.000013	time 0.7570 (0.7504)	loss 2.9349 (2.5788)	grad_norm 2.9182 (3.2637)	mem 23874MB
[2022-11-14 14:05:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [289/300][1250/1251]	eta 0:00:00 lr 0.000013	time 0.7282 (0.7502)	loss 2.9869 (2.5795)	grad_norm 3.4052 (3.2628)	mem 23874MB
[2022-11-14 14:05:39 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 289 training takes 0:15:38
[2022-11-14 14:05:39 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_289.pth saving......
[2022-11-14 14:05:40 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_289.pth saved !!!
[2022-11-14 14:05:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.830 (1.830)	Loss 0.6714 (0.6714)	Acc@1 84.473 (84.473)	Acc@5 96.875 (96.875)	Mem 23874MB
[2022-11-14 14:05:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.750 Acc@5 96.642
[2022-11-14 14:05:53 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.8%
[2022-11-14 14:05:55 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.955 (1.955)	Loss 0.7361 (0.7361)	Acc@1 83.691 (83.691)	Acc@5 96.484 (96.484)	Mem 23874MB
[2022-11-14 14:06:05 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.954 Acc@5 96.662
[2022-11-14 14:06:05 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 84.0%
[2022-11-14 14:06:05 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 14:06:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][0/1251]	eta 0:53:34 lr 0.000013	time 2.5698 (2.5698)	loss 3.1494 (3.1494)	grad_norm 3.4119 (3.4119)	mem 23874MB
[2022-11-14 14:06:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][50/1251]	eta 0:15:43 lr 0.000013	time 0.7381 (0.7856)	loss 2.0953 (2.6539)	grad_norm 3.7701 (3.2851)	mem 23874MB
[2022-11-14 14:07:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][100/1251]	eta 0:14:42 lr 0.000013	time 0.7433 (0.7666)	loss 2.6668 (2.6379)	grad_norm 3.2998 (3.2481)	mem 23874MB
[2022-11-14 14:08:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][150/1251]	eta 0:13:59 lr 0.000013	time 0.7447 (0.7623)	loss 2.2394 (2.6136)	grad_norm 3.4685 (3.3399)	mem 23874MB
[2022-11-14 14:08:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][200/1251]	eta 0:13:17 lr 0.000013	time 0.7437 (0.7588)	loss 2.5564 (2.6155)	grad_norm 2.8036 (3.2935)	mem 23874MB
[2022-11-14 14:09:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][250/1251]	eta 0:12:37 lr 0.000013	time 0.7416 (0.7567)	loss 2.9385 (2.6241)	grad_norm 2.7886 (3.2794)	mem 23874MB
[2022-11-14 14:09:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][300/1251]	eta 0:11:58 lr 0.000013	time 0.8362 (0.7557)	loss 2.9430 (2.6360)	grad_norm 3.0175 (3.2688)	mem 23874MB
[2022-11-14 14:10:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][350/1251]	eta 0:11:19 lr 0.000013	time 0.7432 (0.7546)	loss 2.5774 (2.6258)	grad_norm 2.9637 (3.2540)	mem 23874MB
[2022-11-14 14:11:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][400/1251]	eta 0:10:41 lr 0.000013	time 0.7381 (0.7541)	loss 2.9759 (2.6245)	grad_norm 3.3466 (3.2475)	mem 23874MB
[2022-11-14 14:11:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][450/1251]	eta 0:10:03 lr 0.000013	time 0.7397 (0.7535)	loss 2.8503 (2.6209)	grad_norm 3.0595 (3.2940)	mem 23874MB
[2022-11-14 14:12:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][500/1251]	eta 0:09:25 lr 0.000012	time 0.7380 (0.7528)	loss 2.7098 (2.6147)	grad_norm 2.8664 (3.2692)	mem 23874MB
[2022-11-14 14:13:00 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][550/1251]	eta 0:08:47 lr 0.000012	time 0.7365 (0.7525)	loss 1.8478 (2.6102)	grad_norm 2.6152 (3.2499)	mem 23874MB
[2022-11-14 14:13:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][600/1251]	eta 0:08:09 lr 0.000012	time 0.7450 (0.7521)	loss 3.0298 (2.6072)	grad_norm 2.7938 (3.2492)	mem 23874MB
[2022-11-14 14:14:15 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][650/1251]	eta 0:07:31 lr 0.000012	time 0.8194 (0.7519)	loss 3.0983 (2.6128)	grad_norm 2.5862 (3.2348)	mem 23874MB
[2022-11-14 14:14:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][700/1251]	eta 0:06:54 lr 0.000012	time 0.8348 (0.7517)	loss 2.0708 (2.6077)	grad_norm 2.3932 (3.2280)	mem 23874MB
[2022-11-14 14:15:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][750/1251]	eta 0:06:16 lr 0.000012	time 0.8219 (0.7516)	loss 1.7755 (2.6044)	grad_norm 3.0325 (3.2237)	mem 23874MB
[2022-11-14 14:16:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][800/1251]	eta 0:05:38 lr 0.000012	time 0.7482 (0.7514)	loss 2.7743 (2.6013)	grad_norm 3.6842 (3.2308)	mem 23874MB
[2022-11-14 14:16:45 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][850/1251]	eta 0:05:01 lr 0.000012	time 0.7364 (0.7512)	loss 2.9688 (2.6011)	grad_norm 3.0884 (3.2204)	mem 23874MB
[2022-11-14 14:17:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][900/1251]	eta 0:04:23 lr 0.000012	time 0.7462 (0.7509)	loss 3.1803 (2.5978)	grad_norm 3.5476 (3.2238)	mem 23874MB
[2022-11-14 14:17:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][950/1251]	eta 0:03:46 lr 0.000012	time 0.7407 (0.7509)	loss 2.7578 (2.5981)	grad_norm 2.4162 (3.2238)	mem 23874MB
[2022-11-14 14:18:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][1000/1251]	eta 0:03:08 lr 0.000012	time 0.7434 (0.7509)	loss 2.8991 (2.5902)	grad_norm 2.7291 (3.2220)	mem 23874MB
[2022-11-14 14:19:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][1050/1251]	eta 0:02:30 lr 0.000012	time 0.7370 (0.7507)	loss 2.2489 (2.5863)	grad_norm 4.0547 (3.2259)	mem 23874MB
[2022-11-14 14:19:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][1100/1251]	eta 0:01:53 lr 0.000012	time 0.8191 (0.7508)	loss 2.6348 (2.5869)	grad_norm 2.7624 (3.2233)	mem 23874MB
[2022-11-14 14:20:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][1150/1251]	eta 0:01:15 lr 0.000012	time 0.7416 (0.7505)	loss 2.8248 (2.5880)	grad_norm 3.4886 (3.2201)	mem 23874MB
[2022-11-14 14:21:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][1200/1251]	eta 0:00:38 lr 0.000012	time 0.7433 (0.7506)	loss 2.8819 (2.5825)	grad_norm 3.4893 (3.2274)	mem 23874MB
[2022-11-14 14:21:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [290/300][1250/1251]	eta 0:00:00 lr 0.000012	time 0.7296 (0.7504)	loss 2.8643 (2.5834)	grad_norm 2.7841 (3.2322)	mem 23874MB
[2022-11-14 14:21:44 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 290 training takes 0:15:38
[2022-11-14 14:21:44 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_290.pth saving......
[2022-11-14 14:21:46 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_290.pth saved !!!
[2022-11-14 14:21:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.712 (1.712)	Loss 0.7228 (0.7228)	Acc@1 83.887 (83.887)	Acc@5 96.387 (96.387)	Mem 23874MB
[2022-11-14 14:21:58 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.726 Acc@5 96.618
[2022-11-14 14:21:58 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.7%
[2022-11-14 14:22:00 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.948 (1.948)	Loss 0.7225 (0.7225)	Acc@1 83.691 (83.691)	Acc@5 96.387 (96.387)	Mem 23874MB
[2022-11-14 14:22:11 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.932 Acc@5 96.660
[2022-11-14 14:22:11 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 14:22:11 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 14:22:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][0/1251]	eta 0:52:31 lr 0.000012	time 2.5194 (2.5194)	loss 2.9879 (2.9879)	grad_norm 3.4507 (3.4507)	mem 23874MB
[2022-11-14 14:22:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][50/1251]	eta 0:15:48 lr 0.000012	time 0.7416 (0.7894)	loss 2.7851 (2.5825)	grad_norm 2.4333 (2.9831)	mem 23874MB
[2022-11-14 14:23:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][100/1251]	eta 0:14:46 lr 0.000012	time 0.7410 (0.7706)	loss 3.0272 (2.6101)	grad_norm 3.3958 (3.1940)	mem 23874MB
[2022-11-14 14:24:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][150/1251]	eta 0:14:01 lr 0.000012	time 0.7541 (0.7643)	loss 2.8438 (2.5398)	grad_norm 3.0361 (3.2037)	mem 23874MB
[2022-11-14 14:24:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][200/1251]	eta 0:13:19 lr 0.000012	time 0.7421 (0.7603)	loss 2.1849 (2.5315)	grad_norm 3.8426 (3.2424)	mem 23874MB
[2022-11-14 14:25:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][250/1251]	eta 0:12:39 lr 0.000012	time 0.7426 (0.7585)	loss 2.7474 (2.5496)	grad_norm 3.0582 (3.3253)	mem 23874MB
[2022-11-14 14:25:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][300/1251]	eta 0:12:00 lr 0.000012	time 0.7404 (0.7576)	loss 2.3764 (2.5524)	grad_norm 2.9512 (3.2781)	mem 23874MB
[2022-11-14 14:26:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][350/1251]	eta 0:11:21 lr 0.000012	time 0.7539 (0.7563)	loss 1.8808 (2.5476)	grad_norm 5.5915 (3.2542)	mem 23874MB
[2022-11-14 14:27:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][400/1251]	eta 0:10:43 lr 0.000012	time 0.7417 (0.7561)	loss 1.5977 (2.5505)	grad_norm 3.0131 (3.2617)	mem 23874MB
[2022-11-14 14:27:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][450/1251]	eta 0:10:04 lr 0.000012	time 0.7424 (0.7553)	loss 2.5964 (2.5452)	grad_norm 2.7251 (3.2772)	mem 23874MB
[2022-11-14 14:28:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][500/1251]	eta 0:09:27 lr 0.000012	time 0.7402 (0.7550)	loss 2.8471 (2.5567)	grad_norm 3.8191 (3.3061)	mem 23874MB
[2022-11-14 14:29:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][550/1251]	eta 0:08:48 lr 0.000012	time 0.7395 (0.7545)	loss 2.2514 (2.5574)	grad_norm 2.8749 (3.2971)	mem 23874MB
[2022-11-14 14:29:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][600/1251]	eta 0:08:10 lr 0.000012	time 0.7375 (0.7542)	loss 2.3833 (2.5653)	grad_norm 2.4803 (3.2971)	mem 23874MB
[2022-11-14 14:30:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][650/1251]	eta 0:07:33 lr 0.000012	time 0.7442 (0.7540)	loss 2.7848 (2.5674)	grad_norm 2.5499 (3.2829)	mem 23874MB
[2022-11-14 14:30:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][700/1251]	eta 0:06:55 lr 0.000012	time 0.7364 (0.7536)	loss 1.8650 (2.5657)	grad_norm 3.2830 (3.2838)	mem 23874MB
[2022-11-14 14:31:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][750/1251]	eta 0:06:17 lr 0.000012	time 0.8184 (0.7533)	loss 2.3107 (2.5699)	grad_norm 4.7061 (3.2656)	mem 23874MB
[2022-11-14 14:32:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][800/1251]	eta 0:05:39 lr 0.000012	time 0.7350 (0.7530)	loss 2.5999 (2.5703)	grad_norm 2.5424 (3.2642)	mem 23874MB
[2022-11-14 14:32:52 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][850/1251]	eta 0:05:01 lr 0.000012	time 0.7562 (0.7529)	loss 1.8488 (2.5626)	grad_norm 2.9472 (3.2665)	mem 23874MB
[2022-11-14 14:33:29 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][900/1251]	eta 0:04:24 lr 0.000012	time 0.7508 (0.7527)	loss 2.4570 (2.5622)	grad_norm 5.3955 (3.2779)	mem 23874MB
[2022-11-14 14:34:07 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][950/1251]	eta 0:03:46 lr 0.000012	time 0.8163 (0.7526)	loss 2.9085 (2.5626)	grad_norm 2.9622 (3.2696)	mem 23874MB
[2022-11-14 14:34:44 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][1000/1251]	eta 0:03:08 lr 0.000012	time 0.8214 (0.7526)	loss 1.5493 (2.5634)	grad_norm 2.4871 (3.2590)	mem 23874MB
[2022-11-14 14:35:22 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][1050/1251]	eta 0:02:31 lr 0.000012	time 0.7417 (0.7523)	loss 2.5912 (2.5678)	grad_norm 2.9816 (3.2530)	mem 23874MB
[2022-11-14 14:35:59 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][1100/1251]	eta 0:01:53 lr 0.000012	time 0.7404 (0.7522)	loss 2.8380 (2.5677)	grad_norm 4.5482 (3.2521)	mem 23874MB
[2022-11-14 14:36:37 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][1150/1251]	eta 0:01:15 lr 0.000012	time 0.7396 (0.7520)	loss 2.4867 (2.5698)	grad_norm 2.7270 (3.2499)	mem 23874MB
[2022-11-14 14:37:14 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][1200/1251]	eta 0:00:38 lr 0.000012	time 0.7386 (0.7519)	loss 2.7972 (2.5709)	grad_norm 2.8950 (3.2484)	mem 23874MB
[2022-11-14 14:37:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [291/300][1250/1251]	eta 0:00:00 lr 0.000012	time 0.7303 (0.7516)	loss 2.0052 (2.5696)	grad_norm 3.5736 (3.2700)	mem 23874MB
[2022-11-14 14:37:52 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 291 training takes 0:15:40
[2022-11-14 14:37:52 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_291.pth saving......
[2022-11-14 14:37:53 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_291.pth saved !!!
[2022-11-14 14:37:55 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.682 (1.682)	Loss 0.6744 (0.6744)	Acc@1 83.594 (83.594)	Acc@5 96.973 (96.973)	Mem 23874MB
[2022-11-14 14:38:05 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.772 Acc@5 96.622
[2022-11-14 14:38:05 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.8%
[2022-11-14 14:38:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.974 (1.974)	Loss 0.7142 (0.7142)	Acc@1 84.473 (84.473)	Acc@5 96.875 (96.875)	Mem 23874MB
[2022-11-14 14:38:18 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.930 Acc@5 96.666
[2022-11-14 14:38:18 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 14:38:18 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 14:38:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][0/1251]	eta 0:52:36 lr 0.000012	time 2.5229 (2.5229)	loss 2.9361 (2.9361)	grad_norm 2.4876 (2.4876)	mem 23874MB
[2022-11-14 14:38:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][50/1251]	eta 0:15:48 lr 0.000012	time 0.8242 (0.7896)	loss 1.8170 (2.5851)	grad_norm 2.8867 (3.8053)	mem 23874MB
[2022-11-14 14:39:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][100/1251]	eta 0:14:47 lr 0.000012	time 0.7426 (0.7708)	loss 2.7816 (2.5582)	grad_norm 3.2181 (3.6611)	mem 23874MB
[2022-11-14 14:40:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][150/1251]	eta 0:14:00 lr 0.000012	time 0.8115 (0.7634)	loss 2.7779 (2.5627)	grad_norm 3.0902 (3.5182)	mem 23874MB
[2022-11-14 14:40:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][200/1251]	eta 0:13:18 lr 0.000012	time 0.7416 (0.7601)	loss 3.0547 (2.5512)	grad_norm 3.0184 (3.4214)	mem 23874MB
[2022-11-14 14:41:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][250/1251]	eta 0:12:38 lr 0.000012	time 0.7404 (0.7580)	loss 3.1980 (2.5521)	grad_norm 2.9509 (3.3633)	mem 23874MB
[2022-11-14 14:42:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][300/1251]	eta 0:11:59 lr 0.000012	time 0.7393 (0.7565)	loss 2.9247 (2.5542)	grad_norm 2.8951 (3.3177)	mem 23874MB
[2022-11-14 14:42:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][350/1251]	eta 0:11:20 lr 0.000012	time 0.7395 (0.7556)	loss 1.9218 (2.5408)	grad_norm 2.6232 (3.3494)	mem 23874MB
[2022-11-14 14:43:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][400/1251]	eta 0:10:42 lr 0.000012	time 0.7392 (0.7552)	loss 2.3364 (2.5430)	grad_norm 3.0135 (3.3421)	mem 23874MB
[2022-11-14 14:43:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][450/1251]	eta 0:10:04 lr 0.000012	time 0.7517 (0.7544)	loss 2.7890 (2.5567)	grad_norm 6.0106 (3.3333)	mem 23874MB
[2022-11-14 14:44:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][500/1251]	eta 0:09:26 lr 0.000012	time 0.7413 (0.7543)	loss 2.0378 (2.5550)	grad_norm 3.4179 (3.3639)	mem 23874MB
[2022-11-14 14:45:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][550/1251]	eta 0:08:48 lr 0.000012	time 0.7385 (0.7537)	loss 2.2663 (2.5526)	grad_norm 2.6738 (3.4017)	mem 23874MB
[2022-11-14 14:45:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][600/1251]	eta 0:08:10 lr 0.000012	time 0.8126 (0.7537)	loss 2.8069 (2.5577)	grad_norm 2.9454 (3.3917)	mem 23874MB
[2022-11-14 14:46:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][650/1251]	eta 0:07:32 lr 0.000012	time 0.7371 (0.7533)	loss 2.7858 (2.5526)	grad_norm 2.8680 (3.3818)	mem 23874MB
[2022-11-14 14:47:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][700/1251]	eta 0:06:54 lr 0.000012	time 0.7531 (0.7529)	loss 3.0471 (2.5568)	grad_norm 5.0057 (3.3685)	mem 23874MB
[2022-11-14 14:47:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][750/1251]	eta 0:06:17 lr 0.000011	time 0.7388 (0.7528)	loss 1.7272 (2.5512)	grad_norm 2.5037 (3.3487)	mem 23874MB
[2022-11-14 14:48:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][800/1251]	eta 0:05:39 lr 0.000011	time 0.7403 (0.7528)	loss 1.6500 (2.5557)	grad_norm 2.7601 (3.3469)	mem 23874MB
[2022-11-14 14:48:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][850/1251]	eta 0:05:01 lr 0.000011	time 0.7461 (0.7525)	loss 2.7930 (2.5588)	grad_norm 3.2433 (3.3334)	mem 23874MB
[2022-11-14 14:49:36 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][900/1251]	eta 0:04:24 lr 0.000011	time 0.7438 (0.7525)	loss 2.2593 (2.5633)	grad_norm 3.3180 (3.3139)	mem 23874MB
[2022-11-14 14:50:13 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][950/1251]	eta 0:03:46 lr 0.000011	time 0.7487 (0.7524)	loss 2.7332 (2.5664)	grad_norm 5.1117 (3.2987)	mem 23874MB
[2022-11-14 14:50:51 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][1000/1251]	eta 0:03:08 lr 0.000011	time 0.7446 (0.7521)	loss 2.6502 (2.5718)	grad_norm 2.9331 (3.3194)	mem 23874MB
[2022-11-14 14:51:28 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][1050/1251]	eta 0:02:31 lr 0.000011	time 0.7631 (0.7521)	loss 2.5043 (2.5730)	grad_norm 2.6055 (3.3105)	mem 23874MB
[2022-11-14 14:52:06 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][1100/1251]	eta 0:01:53 lr 0.000011	time 0.7453 (0.7520)	loss 2.3443 (2.5779)	grad_norm 3.0277 (3.3098)	mem 23874MB
[2022-11-14 14:52:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][1150/1251]	eta 0:01:15 lr 0.000011	time 0.7403 (0.7519)	loss 2.8652 (2.5772)	grad_norm 4.2946 (3.2999)	mem 23874MB
[2022-11-14 14:53:21 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][1200/1251]	eta 0:00:38 lr 0.000011	time 0.7400 (0.7519)	loss 2.7449 (2.5759)	grad_norm 2.7312 (3.2958)	mem 23874MB
[2022-11-14 14:53:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [292/300][1250/1251]	eta 0:00:00 lr 0.000011	time 0.7294 (0.7516)	loss 3.0355 (2.5749)	grad_norm 4.3437 (3.2926)	mem 23874MB
[2022-11-14 14:53:58 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 292 training takes 0:15:40
[2022-11-14 14:53:59 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_292.pth saving......
[2022-11-14 14:54:00 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_292.pth saved !!!
[2022-11-14 14:54:01 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.753 (1.753)	Loss 0.6960 (0.6960)	Acc@1 84.180 (84.180)	Acc@5 96.680 (96.680)	Mem 23874MB
[2022-11-14 14:54:12 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.774 Acc@5 96.602
[2022-11-14 14:54:12 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.8%
[2022-11-14 14:54:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.973 (1.973)	Loss 0.6317 (0.6317)	Acc@1 86.035 (86.035)	Acc@5 97.266 (97.266)	Mem 23874MB
[2022-11-14 14:54:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.928 Acc@5 96.658
[2022-11-14 14:54:25 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 14:54:25 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 14:54:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][0/1251]	eta 0:53:24 lr 0.000011	time 2.5617 (2.5617)	loss 2.7239 (2.7239)	grad_norm 3.1879 (3.1879)	mem 23874MB
[2022-11-14 14:55:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][50/1251]	eta 0:15:49 lr 0.000011	time 0.7526 (0.7906)	loss 2.9901 (2.5366)	grad_norm 2.9903 (3.3018)	mem 23874MB
[2022-11-14 14:55:43 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][100/1251]	eta 0:14:47 lr 0.000011	time 0.7455 (0.7708)	loss 2.6581 (2.5331)	grad_norm 2.5651 (3.1794)	mem 23874MB
[2022-11-14 14:56:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][150/1251]	eta 0:14:00 lr 0.000011	time 0.7387 (0.7635)	loss 2.7505 (2.5456)	grad_norm 2.5909 (3.1966)	mem 23874MB
[2022-11-14 14:56:58 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][200/1251]	eta 0:13:19 lr 0.000011	time 0.7400 (0.7605)	loss 2.8992 (2.5579)	grad_norm 2.5160 (3.2169)	mem 23874MB
[2022-11-14 14:57:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][250/1251]	eta 0:12:38 lr 0.000011	time 0.7400 (0.7581)	loss 2.4730 (2.5627)	grad_norm 2.8493 (3.2076)	mem 23874MB
[2022-11-14 14:58:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][300/1251]	eta 0:11:59 lr 0.000011	time 0.7402 (0.7562)	loss 2.9180 (2.5685)	grad_norm 2.7494 (3.2217)	mem 23874MB
[2022-11-14 14:58:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][350/1251]	eta 0:11:20 lr 0.000011	time 0.7421 (0.7558)	loss 2.6116 (2.5703)	grad_norm 3.1498 (3.2008)	mem 23874MB
[2022-11-14 14:59:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][400/1251]	eta 0:10:41 lr 0.000011	time 0.7377 (0.7543)	loss 2.4344 (2.5625)	grad_norm 2.4529 (3.1859)	mem 23874MB
[2022-11-14 15:00:05 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][450/1251]	eta 0:10:03 lr 0.000011	time 0.7449 (0.7540)	loss 3.1340 (2.5567)	grad_norm 2.8618 (3.1908)	mem 23874MB
[2022-11-14 15:00:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][500/1251]	eta 0:09:25 lr 0.000011	time 0.7609 (0.7535)	loss 3.0337 (2.5586)	grad_norm 3.7477 (3.1934)	mem 23874MB
[2022-11-14 15:01:20 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][550/1251]	eta 0:08:47 lr 0.000011	time 0.7418 (0.7531)	loss 2.7365 (2.5552)	grad_norm 2.7120 (3.2118)	mem 23874MB
[2022-11-14 15:01:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][600/1251]	eta 0:08:10 lr 0.000011	time 0.7435 (0.7527)	loss 2.9050 (2.5592)	grad_norm 2.9295 (3.2323)	mem 23874MB
[2022-11-14 15:02:35 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][650/1251]	eta 0:07:32 lr 0.000011	time 0.7395 (0.7525)	loss 2.2390 (2.5597)	grad_norm 3.0194 (3.2893)	mem 23874MB
[2022-11-14 15:03:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][700/1251]	eta 0:06:54 lr 0.000011	time 0.7272 (0.7520)	loss 2.4608 (2.5654)	grad_norm 2.7796 (3.3025)	mem 23874MB
[2022-11-14 15:03:50 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][750/1251]	eta 0:06:16 lr 0.000011	time 0.7444 (0.7519)	loss 2.0449 (2.5646)	grad_norm 2.9995 (3.3064)	mem 23874MB
[2022-11-14 15:04:27 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][800/1251]	eta 0:05:38 lr 0.000011	time 0.7366 (0.7514)	loss 2.8112 (2.5699)	grad_norm 2.7918 (3.3175)	mem 23874MB
[2022-11-14 15:05:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][850/1251]	eta 0:05:01 lr 0.000011	time 0.7499 (0.7514)	loss 2.8299 (2.5734)	grad_norm 3.5688 (3.3170)	mem 23874MB
[2022-11-14 15:05:42 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][900/1251]	eta 0:04:23 lr 0.000011	time 0.7412 (0.7512)	loss 2.2320 (2.5749)	grad_norm 2.7177 (3.3127)	mem 23874MB
[2022-11-14 15:06:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][950/1251]	eta 0:03:46 lr 0.000011	time 0.7392 (0.7511)	loss 1.8239 (2.5798)	grad_norm 3.4290 (3.3031)	mem 23874MB
[2022-11-14 15:06:57 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][1000/1251]	eta 0:03:08 lr 0.000011	time 0.7468 (0.7510)	loss 2.0760 (2.5769)	grad_norm 2.8413 (3.3167)	mem 23874MB
[2022-11-14 15:07:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][1050/1251]	eta 0:02:30 lr 0.000011	time 0.7413 (0.7510)	loss 2.7947 (2.5747)	grad_norm 2.9240 (3.3324)	mem 23874MB
[2022-11-14 15:08:12 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][1100/1251]	eta 0:01:53 lr 0.000011	time 0.7411 (0.7509)	loss 2.8980 (2.5768)	grad_norm 2.8369 (3.3309)	mem 23874MB
[2022-11-14 15:08:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][1150/1251]	eta 0:01:15 lr 0.000011	time 0.7460 (0.7508)	loss 2.5015 (2.5734)	grad_norm 3.2932 (3.3307)	mem 23874MB
[2022-11-14 15:09:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][1200/1251]	eta 0:00:38 lr 0.000011	time 0.7342 (0.7506)	loss 2.8719 (2.5726)	grad_norm 3.3136 (3.3309)	mem 23874MB
[2022-11-14 15:10:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [293/300][1250/1251]	eta 0:00:00 lr 0.000011	time 0.7387 (0.7506)	loss 2.5584 (2.5692)	grad_norm 8.6939 (3.3231)	mem 23874MB
[2022-11-14 15:10:04 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 293 training takes 0:15:39
[2022-11-14 15:10:04 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_293.pth saving......
[2022-11-14 15:10:05 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_293.pth saved !!!
[2022-11-14 15:10:07 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.674 (1.674)	Loss 0.6813 (0.6813)	Acc@1 86.426 (86.426)	Acc@5 96.582 (96.582)	Mem 23874MB
[2022-11-14 15:10:18 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.790 Acc@5 96.580
[2022-11-14 15:10:18 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.8%
[2022-11-14 15:10:20 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.088 (2.088)	Loss 0.7246 (0.7246)	Acc@1 83.887 (83.887)	Acc@5 96.484 (96.484)	Mem 23874MB
[2022-11-14 15:10:31 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.920 Acc@5 96.656
[2022-11-14 15:10:31 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 15:10:31 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 15:10:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][0/1251]	eta 0:53:25 lr 0.000011	time 2.5626 (2.5626)	loss 2.7067 (2.7067)	grad_norm 3.0097 (3.0097)	mem 23874MB
[2022-11-14 15:11:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][50/1251]	eta 0:15:47 lr 0.000011	time 0.7437 (0.7890)	loss 2.1933 (2.6164)	grad_norm 2.9836 (3.1311)	mem 23874MB
[2022-11-14 15:11:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][100/1251]	eta 0:14:46 lr 0.000011	time 0.8307 (0.7699)	loss 2.8780 (2.6014)	grad_norm 3.1026 (3.3851)	mem 23874MB
[2022-11-14 15:12:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][150/1251]	eta 0:14:01 lr 0.000011	time 0.7444 (0.7642)	loss 2.3676 (2.5630)	grad_norm 3.7323 (3.3494)	mem 23874MB
[2022-11-14 15:13:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][200/1251]	eta 0:13:18 lr 0.000011	time 0.7394 (0.7601)	loss 2.7508 (2.5604)	grad_norm 3.7111 (3.3338)	mem 23874MB
[2022-11-14 15:13:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][250/1251]	eta 0:12:38 lr 0.000011	time 0.7149 (0.7579)	loss 2.6754 (2.5581)	grad_norm inf (inf)	mem 23874MB
[2022-11-14 15:14:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][300/1251]	eta 0:11:59 lr 0.000011	time 0.8169 (0.7566)	loss 2.9516 (2.5628)	grad_norm 3.1344 (inf)	mem 23874MB
[2022-11-14 15:14:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][350/1251]	eta 0:11:21 lr 0.000011	time 0.7428 (0.7563)	loss 2.6825 (2.5646)	grad_norm 3.2585 (inf)	mem 23874MB
[2022-11-14 15:15:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][400/1251]	eta 0:10:42 lr 0.000011	time 0.7466 (0.7554)	loss 1.9698 (2.5645)	grad_norm 3.4537 (inf)	mem 23874MB
[2022-11-14 15:16:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][450/1251]	eta 0:10:04 lr 0.000011	time 0.7437 (0.7549)	loss 2.6197 (2.5775)	grad_norm 2.8002 (inf)	mem 23874MB
[2022-11-14 15:16:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][500/1251]	eta 0:09:26 lr 0.000011	time 0.7429 (0.7540)	loss 2.9986 (2.5754)	grad_norm 3.2047 (inf)	mem 23874MB
[2022-11-14 15:17:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][550/1251]	eta 0:08:48 lr 0.000011	time 0.7413 (0.7539)	loss 2.9285 (2.5713)	grad_norm 2.7165 (inf)	mem 23874MB
[2022-11-14 15:18:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][600/1251]	eta 0:08:10 lr 0.000011	time 0.7422 (0.7536)	loss 2.4822 (2.5656)	grad_norm 2.7271 (inf)	mem 23874MB
[2022-11-14 15:18:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][650/1251]	eta 0:07:32 lr 0.000011	time 0.7451 (0.7535)	loss 2.5004 (2.5690)	grad_norm 3.2192 (inf)	mem 23874MB
[2022-11-14 15:19:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][700/1251]	eta 0:06:55 lr 0.000011	time 0.7950 (0.7534)	loss 2.6353 (2.5700)	grad_norm 2.9504 (inf)	mem 23874MB
[2022-11-14 15:19:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][750/1251]	eta 0:06:17 lr 0.000011	time 0.7408 (0.7530)	loss 2.6246 (2.5695)	grad_norm 3.2382 (inf)	mem 23874MB
[2022-11-14 15:20:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][800/1251]	eta 0:05:39 lr 0.000011	time 0.7356 (0.7528)	loss 2.7337 (2.5671)	grad_norm 3.2560 (inf)	mem 23874MB
[2022-11-14 15:21:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][850/1251]	eta 0:05:01 lr 0.000011	time 0.8255 (0.7528)	loss 2.5345 (2.5687)	grad_norm 2.8966 (inf)	mem 23874MB
[2022-11-14 15:21:49 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][900/1251]	eta 0:04:24 lr 0.000011	time 0.7401 (0.7526)	loss 2.7382 (2.5675)	grad_norm 3.3590 (inf)	mem 23874MB
[2022-11-14 15:22:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][950/1251]	eta 0:03:46 lr 0.000011	time 0.7516 (0.7526)	loss 2.1952 (2.5688)	grad_norm 2.7980 (inf)	mem 23874MB
[2022-11-14 15:23:04 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][1000/1251]	eta 0:03:08 lr 0.000011	time 0.8263 (0.7525)	loss 2.3388 (2.5666)	grad_norm 3.4629 (inf)	mem 23874MB
[2022-11-14 15:23:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][1050/1251]	eta 0:02:31 lr 0.000011	time 0.7513 (0.7523)	loss 2.8696 (2.5652)	grad_norm 2.7998 (inf)	mem 23874MB
[2022-11-14 15:24:19 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][1100/1251]	eta 0:01:53 lr 0.000011	time 0.8030 (0.7523)	loss 2.3063 (2.5662)	grad_norm 2.7355 (inf)	mem 23874MB
[2022-11-14 15:24:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][1150/1251]	eta 0:01:15 lr 0.000011	time 0.7375 (0.7522)	loss 1.8979 (2.5630)	grad_norm 2.9892 (inf)	mem 23874MB
[2022-11-14 15:25:34 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][1200/1251]	eta 0:00:38 lr 0.000011	time 0.7377 (0.7521)	loss 2.1137 (2.5615)	grad_norm 2.9405 (inf)	mem 23874MB
[2022-11-14 15:26:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [294/300][1250/1251]	eta 0:00:00 lr 0.000011	time 0.7311 (0.7519)	loss 2.4487 (2.5612)	grad_norm 3.1464 (inf)	mem 23874MB
[2022-11-14 15:26:11 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 294 training takes 0:15:40
[2022-11-14 15:26:12 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_294.pth saving......
[2022-11-14 15:26:13 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_294.pth saved !!!
[2022-11-14 15:26:14 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.791 (1.791)	Loss 0.7197 (0.7197)	Acc@1 83.496 (83.496)	Acc@5 95.801 (95.801)	Mem 23874MB
[2022-11-14 15:26:25 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.768 Acc@5 96.582
[2022-11-14 15:26:25 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.8%
[2022-11-14 15:26:27 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.105 (2.105)	Loss 0.6505 (0.6505)	Acc@1 84.961 (84.961)	Acc@5 97.363 (97.363)	Mem 23874MB
[2022-11-14 15:26:38 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.912 Acc@5 96.648
[2022-11-14 15:26:38 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 15:26:38 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 15:26:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][0/1251]	eta 0:52:21 lr 0.000011	time 2.5114 (2.5114)	loss 2.5728 (2.5728)	grad_norm 3.3555 (3.3555)	mem 23875MB
[2022-11-14 15:27:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][50/1251]	eta 0:15:45 lr 0.000011	time 0.8339 (0.7876)	loss 2.6700 (2.6004)	grad_norm 3.6381 (3.2015)	mem 23875MB
[2022-11-14 15:27:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][100/1251]	eta 0:14:46 lr 0.000011	time 0.7348 (0.7699)	loss 2.8436 (2.6120)	grad_norm 2.9486 (3.1688)	mem 23875MB
[2022-11-14 15:28:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][150/1251]	eta 0:14:00 lr 0.000011	time 0.8237 (0.7634)	loss 2.8522 (2.5987)	grad_norm 3.8751 (3.1654)	mem 23875MB
[2022-11-14 15:29:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][200/1251]	eta 0:13:18 lr 0.000011	time 0.8172 (0.7600)	loss 2.9047 (2.5988)	grad_norm 2.6218 (3.2708)	mem 23875MB
[2022-11-14 15:29:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][250/1251]	eta 0:12:38 lr 0.000011	time 0.7437 (0.7574)	loss 1.7358 (2.5765)	grad_norm 2.9954 (3.2193)	mem 23875MB
[2022-11-14 15:30:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][300/1251]	eta 0:11:59 lr 0.000011	time 0.8437 (0.7568)	loss 2.5818 (2.5758)	grad_norm 2.9884 (3.2395)	mem 23875MB
[2022-11-14 15:31:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][350/1251]	eta 0:11:20 lr 0.000011	time 0.7463 (0.7554)	loss 2.8530 (2.5827)	grad_norm 3.2826 (3.2509)	mem 23875MB
[2022-11-14 15:31:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][400/1251]	eta 0:10:42 lr 0.000011	time 0.7427 (0.7552)	loss 1.9392 (2.5920)	grad_norm 2.6711 (3.3098)	mem 23875MB
[2022-11-14 15:32:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][450/1251]	eta 0:10:04 lr 0.000011	time 0.7415 (0.7546)	loss 2.1867 (2.5763)	grad_norm 2.4738 (3.3116)	mem 23875MB
[2022-11-14 15:32:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][500/1251]	eta 0:09:26 lr 0.000011	time 0.7453 (0.7541)	loss 2.3136 (2.5806)	grad_norm 3.0990 (3.3094)	mem 23875MB
[2022-11-14 15:33:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][550/1251]	eta 0:08:48 lr 0.000011	time 0.7440 (0.7537)	loss 2.7001 (2.5876)	grad_norm 3.5132 (3.2963)	mem 23875MB
[2022-11-14 15:34:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][600/1251]	eta 0:08:10 lr 0.000011	time 0.7397 (0.7532)	loss 2.7174 (2.5870)	grad_norm 3.2215 (3.2835)	mem 23875MB
[2022-11-14 15:34:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][650/1251]	eta 0:07:32 lr 0.000011	time 0.7423 (0.7530)	loss 3.1238 (2.5915)	grad_norm 3.2910 (3.3108)	mem 23875MB
[2022-11-14 15:35:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][700/1251]	eta 0:06:54 lr 0.000011	time 0.8293 (0.7529)	loss 2.8988 (2.5957)	grad_norm 2.9726 (3.3419)	mem 23875MB
[2022-11-14 15:36:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][750/1251]	eta 0:06:17 lr 0.000011	time 0.8190 (0.7525)	loss 2.3530 (2.5935)	grad_norm 3.8731 (3.3480)	mem 23875MB
[2022-11-14 15:36:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][800/1251]	eta 0:05:39 lr 0.000011	time 0.7402 (0.7526)	loss 2.6447 (2.5981)	grad_norm 3.0486 (3.3389)	mem 23875MB
[2022-11-14 15:37:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][850/1251]	eta 0:05:01 lr 0.000011	time 0.7417 (0.7523)	loss 2.6471 (2.5994)	grad_norm 2.7191 (3.3368)	mem 23875MB
[2022-11-14 15:37:56 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][900/1251]	eta 0:04:24 lr 0.000010	time 0.7410 (0.7523)	loss 2.8419 (2.5993)	grad_norm 2.6210 (3.3401)	mem 23875MB
[2022-11-14 15:38:33 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][950/1251]	eta 0:03:46 lr 0.000010	time 0.7413 (0.7521)	loss 2.1369 (2.6000)	grad_norm 4.1574 (3.3385)	mem 23875MB
[2022-11-14 15:39:11 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][1000/1251]	eta 0:03:08 lr 0.000010	time 0.7435 (0.7521)	loss 2.8798 (2.6009)	grad_norm 2.8414 (3.3396)	mem 23875MB
[2022-11-14 15:39:48 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][1050/1251]	eta 0:02:31 lr 0.000010	time 0.8352 (0.7519)	loss 2.8653 (2.5993)	grad_norm 3.0235 (3.3305)	mem 23875MB
[2022-11-14 15:40:26 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][1100/1251]	eta 0:01:53 lr 0.000010	time 0.8376 (0.7519)	loss 2.2502 (2.5997)	grad_norm 3.1523 (3.3252)	mem 23875MB
[2022-11-14 15:41:03 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][1150/1251]	eta 0:01:15 lr 0.000010	time 0.7402 (0.7517)	loss 2.7363 (2.5995)	grad_norm 2.7077 (3.3105)	mem 23875MB
[2022-11-14 15:41:41 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][1200/1251]	eta 0:00:38 lr 0.000010	time 0.7386 (0.7517)	loss 1.7176 (2.5948)	grad_norm 2.7013 (3.3070)	mem 23875MB
[2022-11-14 15:42:18 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [295/300][1250/1251]	eta 0:00:00 lr 0.000010	time 0.7295 (0.7515)	loss 2.9981 (2.5950)	grad_norm 2.6035 (3.2961)	mem 23875MB
[2022-11-14 15:42:18 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 295 training takes 0:15:40
[2022-11-14 15:42:18 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_295.pth saving......
[2022-11-14 15:42:19 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_295.pth saved !!!
[2022-11-14 15:42:21 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.718 (1.718)	Loss 0.7354 (0.7354)	Acc@1 81.934 (81.934)	Acc@5 95.996 (95.996)	Mem 23875MB
[2022-11-14 15:42:32 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.824 Acc@5 96.632
[2022-11-14 15:42:32 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.8%
[2022-11-14 15:42:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.923 (1.923)	Loss 0.6868 (0.6868)	Acc@1 84.180 (84.180)	Acc@5 96.777 (96.777)	Mem 23875MB
[2022-11-14 15:42:44 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.908 Acc@5 96.656
[2022-11-14 15:42:44 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 15:42:44 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 15:42:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][0/1251]	eta 0:54:22 lr 0.000010	time 2.6076 (2.6076)	loss 2.5387 (2.5387)	grad_norm 2.4966 (2.4966)	mem 23875MB
[2022-11-14 15:43:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][50/1251]	eta 0:15:48 lr 0.000010	time 0.7451 (0.7895)	loss 2.8907 (2.5017)	grad_norm 4.0651 (3.0820)	mem 23875MB
[2022-11-14 15:44:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][100/1251]	eta 0:14:46 lr 0.000010	time 0.7466 (0.7699)	loss 2.6766 (2.5100)	grad_norm 2.7044 (3.1410)	mem 23875MB
[2022-11-14 15:44:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][150/1251]	eta 0:14:00 lr 0.000010	time 0.8401 (0.7638)	loss 2.6387 (2.5686)	grad_norm 3.0303 (3.2741)	mem 23875MB
[2022-11-14 15:45:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][200/1251]	eta 0:13:18 lr 0.000010	time 0.7545 (0.7601)	loss 2.2708 (2.5685)	grad_norm 4.4873 (3.3505)	mem 23875MB
[2022-11-14 15:45:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][250/1251]	eta 0:12:38 lr 0.000010	time 0.7442 (0.7580)	loss 2.0579 (2.5803)	grad_norm 2.9087 (3.3141)	mem 23875MB
[2022-11-14 15:46:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][300/1251]	eta 0:11:59 lr 0.000010	time 0.7412 (0.7569)	loss 2.1604 (2.5786)	grad_norm 3.2723 (3.2833)	mem 23875MB
[2022-11-14 15:47:10 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][350/1251]	eta 0:11:20 lr 0.000010	time 0.7425 (0.7556)	loss 2.5449 (2.5814)	grad_norm 2.6370 (3.3152)	mem 23875MB
[2022-11-14 15:47:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][400/1251]	eta 0:10:42 lr 0.000010	time 0.7363 (0.7550)	loss 2.4848 (2.5803)	grad_norm 3.7041 (3.3363)	mem 23875MB
[2022-11-14 15:48:25 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][450/1251]	eta 0:10:04 lr 0.000010	time 0.7367 (0.7544)	loss 2.1182 (2.5833)	grad_norm 2.6605 (3.3320)	mem 23875MB
[2022-11-14 15:49:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][500/1251]	eta 0:09:26 lr 0.000010	time 0.7444 (0.7540)	loss 1.8276 (2.5847)	grad_norm 3.4164 (3.3255)	mem 23875MB
[2022-11-14 15:49:40 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][550/1251]	eta 0:08:48 lr 0.000010	time 0.7452 (0.7533)	loss 2.9364 (2.5852)	grad_norm 2.5497 (3.3162)	mem 23875MB
[2022-11-14 15:50:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][600/1251]	eta 0:08:10 lr 0.000010	time 0.7456 (0.7529)	loss 2.8686 (2.5834)	grad_norm 3.2750 (3.3423)	mem 23875MB
[2022-11-14 15:50:55 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][650/1251]	eta 0:07:32 lr 0.000010	time 0.7431 (0.7527)	loss 2.8311 (2.5833)	grad_norm 2.6812 (3.3755)	mem 23875MB
[2022-11-14 15:51:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][700/1251]	eta 0:06:54 lr 0.000010	time 0.7480 (0.7523)	loss 2.6884 (2.5800)	grad_norm 5.6056 (3.3624)	mem 23875MB
[2022-11-14 15:52:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][750/1251]	eta 0:06:16 lr 0.000010	time 0.7413 (0.7522)	loss 3.0002 (2.5837)	grad_norm 3.7991 (3.3439)	mem 23875MB
[2022-11-14 15:52:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][800/1251]	eta 0:05:39 lr 0.000010	time 0.7399 (0.7521)	loss 2.9483 (2.5813)	grad_norm 2.9022 (3.3335)	mem 23875MB
[2022-11-14 15:53:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][850/1251]	eta 0:05:01 lr 0.000010	time 0.7535 (0.7519)	loss 2.6265 (2.5805)	grad_norm 5.3172 (3.3295)	mem 23875MB
[2022-11-14 15:54:02 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][900/1251]	eta 0:04:23 lr 0.000010	time 0.7536 (0.7519)	loss 2.6378 (2.5822)	grad_norm 7.0950 (3.3359)	mem 23875MB
[2022-11-14 15:54:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][950/1251]	eta 0:03:46 lr 0.000010	time 0.7393 (0.7517)	loss 1.8561 (2.5815)	grad_norm 2.4150 (3.3434)	mem 23875MB
[2022-11-14 15:55:17 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][1000/1251]	eta 0:03:08 lr 0.000010	time 0.7412 (0.7518)	loss 2.8145 (2.5788)	grad_norm 3.6093 (3.3394)	mem 23875MB
[2022-11-14 15:55:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][1050/1251]	eta 0:02:31 lr 0.000010	time 0.7382 (0.7516)	loss 2.5339 (2.5793)	grad_norm 3.6736 (3.3335)	mem 23875MB
[2022-11-14 15:56:32 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][1100/1251]	eta 0:01:53 lr 0.000010	time 0.7431 (0.7515)	loss 2.8923 (2.5770)	grad_norm 2.9992 (3.3285)	mem 23875MB
[2022-11-14 15:57:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][1150/1251]	eta 0:01:15 lr 0.000010	time 0.7380 (0.7514)	loss 2.9231 (2.5768)	grad_norm 2.7640 (3.3249)	mem 23875MB
[2022-11-14 15:57:47 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][1200/1251]	eta 0:00:38 lr 0.000010	time 0.7408 (0.7513)	loss 2.2936 (2.5759)	grad_norm 3.5781 (3.3209)	mem 23875MB
[2022-11-14 15:58:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [296/300][1250/1251]	eta 0:00:00 lr 0.000010	time 0.7345 (0.7511)	loss 2.4531 (2.5766)	grad_norm 3.4289 (3.3238)	mem 23875MB
[2022-11-14 15:58:24 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 296 training takes 0:15:39
[2022-11-14 15:58:25 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_296.pth saving......
[2022-11-14 15:58:26 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_296.pth saved !!!
[2022-11-14 15:58:27 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.673 (1.673)	Loss 0.7109 (0.7109)	Acc@1 82.812 (82.812)	Acc@5 96.582 (96.582)	Mem 23875MB
[2022-11-14 15:58:38 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.772 Acc@5 96.636
[2022-11-14 15:58:38 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.8%
[2022-11-14 15:58:40 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.996 (1.996)	Loss 0.7530 (0.7530)	Acc@1 81.934 (81.934)	Acc@5 96.387 (96.387)	Mem 23875MB
[2022-11-14 15:58:51 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.896 Acc@5 96.662
[2022-11-14 15:58:51 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 15:58:51 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 15:58:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][0/1251]	eta 0:54:06 lr 0.000010	time 2.5950 (2.5950)	loss 3.0026 (3.0026)	grad_norm 3.1834 (3.1834)	mem 23875MB
[2022-11-14 15:59:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][50/1251]	eta 0:15:46 lr 0.000010	time 0.7399 (0.7878)	loss 2.2657 (2.6050)	grad_norm 2.7226 (3.5274)	mem 23875MB
[2022-11-14 16:00:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][100/1251]	eta 0:14:45 lr 0.000010	time 0.7406 (0.7689)	loss 2.9577 (2.6238)	grad_norm 3.6225 (3.3708)	mem 23875MB
[2022-11-14 16:00:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][150/1251]	eta 0:13:59 lr 0.000010	time 0.7426 (0.7627)	loss 2.7813 (2.6180)	grad_norm 2.9104 (3.3444)	mem 23875MB
[2022-11-14 16:01:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][200/1251]	eta 0:13:18 lr 0.000010	time 0.7433 (0.7598)	loss 3.0656 (2.6058)	grad_norm 2.7723 (3.3627)	mem 23875MB
[2022-11-14 16:02:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][250/1251]	eta 0:12:39 lr 0.000010	time 0.7523 (0.7584)	loss 2.7746 (2.5940)	grad_norm 6.6419 (3.3415)	mem 23875MB
[2022-11-14 16:02:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][300/1251]	eta 0:11:59 lr 0.000010	time 0.7460 (0.7565)	loss 1.4013 (2.6065)	grad_norm 3.2594 (3.3598)	mem 23875MB
[2022-11-14 16:03:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][350/1251]	eta 0:11:20 lr 0.000010	time 0.7407 (0.7556)	loss 2.7482 (2.5986)	grad_norm 3.9675 (3.3691)	mem 23875MB
[2022-11-14 16:03:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][400/1251]	eta 0:10:42 lr 0.000010	time 0.7449 (0.7549)	loss 1.9857 (2.5905)	grad_norm 3.5606 (3.3464)	mem 23875MB
[2022-11-14 16:04:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][450/1251]	eta 0:10:04 lr 0.000010	time 0.7409 (0.7545)	loss 2.0093 (2.5908)	grad_norm 2.7908 (3.3351)	mem 23875MB
[2022-11-14 16:05:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][500/1251]	eta 0:09:26 lr 0.000010	time 0.7487 (0.7542)	loss 2.9831 (2.5944)	grad_norm 3.0025 (3.3380)	mem 23875MB
[2022-11-14 16:05:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][550/1251]	eta 0:08:48 lr 0.000010	time 0.7543 (0.7536)	loss 2.5694 (2.5837)	grad_norm 7.0618 (inf)	mem 23875MB
[2022-11-14 16:06:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][600/1251]	eta 0:08:10 lr 0.000010	time 0.7437 (0.7533)	loss 2.6649 (2.5857)	grad_norm 3.0348 (inf)	mem 23875MB
[2022-11-14 16:07:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][650/1251]	eta 0:07:32 lr 0.000010	time 0.7510 (0.7536)	loss 2.8428 (2.5882)	grad_norm 9.3911 (inf)	mem 23875MB
[2022-11-14 16:07:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][700/1251]	eta 0:06:54 lr 0.000010	time 0.7441 (0.7530)	loss 2.8769 (2.5857)	grad_norm 3.2509 (inf)	mem 23875MB
[2022-11-14 16:08:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][750/1251]	eta 0:06:17 lr 0.000010	time 0.7437 (0.7531)	loss 2.9301 (2.5872)	grad_norm 3.2313 (inf)	mem 23875MB
[2022-11-14 16:08:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][800/1251]	eta 0:05:39 lr 0.000010	time 0.7319 (0.7529)	loss 1.9153 (2.5872)	grad_norm 2.8283 (inf)	mem 23875MB
[2022-11-14 16:09:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][850/1251]	eta 0:05:01 lr 0.000010	time 0.7510 (0.7527)	loss 2.9369 (2.5851)	grad_norm 9.6846 (inf)	mem 23875MB
[2022-11-14 16:10:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][900/1251]	eta 0:04:24 lr 0.000010	time 0.7514 (0.7524)	loss 1.6046 (2.5846)	grad_norm 3.7965 (inf)	mem 23875MB
[2022-11-14 16:10:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][950/1251]	eta 0:03:46 lr 0.000010	time 0.7384 (0.7523)	loss 2.2843 (2.5830)	grad_norm 3.1802 (inf)	mem 23875MB
[2022-11-14 16:11:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][1000/1251]	eta 0:03:08 lr 0.000010	time 0.7517 (0.7522)	loss 2.7339 (2.5791)	grad_norm 6.2893 (inf)	mem 23875MB
[2022-11-14 16:12:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][1050/1251]	eta 0:02:31 lr 0.000010	time 0.7410 (0.7522)	loss 2.2735 (2.5759)	grad_norm 2.8545 (inf)	mem 23875MB
[2022-11-14 16:12:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][1100/1251]	eta 0:01:53 lr 0.000010	time 0.7398 (0.7520)	loss 2.6516 (2.5773)	grad_norm 3.3856 (inf)	mem 23875MB
[2022-11-14 16:13:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][1150/1251]	eta 0:01:15 lr 0.000010	time 0.7420 (0.7519)	loss 2.2070 (2.5731)	grad_norm 2.9892 (inf)	mem 23875MB
[2022-11-14 16:13:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][1200/1251]	eta 0:00:38 lr 0.000010	time 0.7411 (0.7519)	loss 2.2672 (2.5745)	grad_norm 2.7737 (inf)	mem 23875MB
[2022-11-14 16:14:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [297/300][1250/1251]	eta 0:00:00 lr 0.000010	time 0.7415 (0.7516)	loss 2.7100 (2.5772)	grad_norm 3.3661 (inf)	mem 23875MB
[2022-11-14 16:14:31 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 297 training takes 0:15:40
[2022-11-14 16:14:31 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_297.pth saving......
[2022-11-14 16:14:33 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_297.pth saved !!!
[2022-11-14 16:14:34 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.676 (1.676)	Loss 0.6750 (0.6750)	Acc@1 84.961 (84.961)	Acc@5 96.777 (96.777)	Mem 23875MB
[2022-11-14 16:14:45 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.758 Acc@5 96.616
[2022-11-14 16:14:45 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.8%
[2022-11-14 16:14:47 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.099 (2.099)	Loss 0.6846 (0.6846)	Acc@1 83.691 (83.691)	Acc@5 97.461 (97.461)	Mem 23875MB
[2022-11-14 16:14:58 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.898 Acc@5 96.670
[2022-11-14 16:14:58 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 16:14:58 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 16:15:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][0/1251]	eta 0:52:33 lr 0.000010	time 2.5205 (2.5205)	loss 2.3364 (2.3364)	grad_norm 2.9097 (2.9097)	mem 23875MB
[2022-11-14 16:15:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][50/1251]	eta 0:15:44 lr 0.000010	time 0.7392 (0.7867)	loss 2.7203 (2.5502)	grad_norm 3.0895 (3.2577)	mem 23875MB
[2022-11-14 16:16:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][100/1251]	eta 0:14:44 lr 0.000010	time 0.7432 (0.7686)	loss 2.9347 (2.5481)	grad_norm 3.7395 (3.3229)	mem 23875MB
[2022-11-14 16:16:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][150/1251]	eta 0:13:59 lr 0.000010	time 0.7416 (0.7623)	loss 2.7041 (2.5704)	grad_norm 3.1624 (3.2556)	mem 23875MB
[2022-11-14 16:17:30 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][200/1251]	eta 0:13:17 lr 0.000010	time 0.8166 (0.7586)	loss 1.6285 (2.5713)	grad_norm 3.5325 (3.2481)	mem 23875MB
[2022-11-14 16:18:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][250/1251]	eta 0:12:38 lr 0.000010	time 0.7433 (0.7573)	loss 2.6790 (2.5662)	grad_norm 2.9904 (3.2238)	mem 23875MB
[2022-11-14 16:18:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][300/1251]	eta 0:11:59 lr 0.000010	time 0.8147 (0.7562)	loss 2.2866 (2.5637)	grad_norm 3.5264 (3.2884)	mem 23875MB
[2022-11-14 16:19:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][350/1251]	eta 0:11:20 lr 0.000010	time 0.7519 (0.7553)	loss 2.6995 (2.5737)	grad_norm 2.9455 (3.2612)	mem 23875MB
[2022-11-14 16:20:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][400/1251]	eta 0:10:42 lr 0.000010	time 0.7411 (0.7547)	loss 2.5544 (2.5807)	grad_norm 4.2431 (3.2357)	mem 23875MB
[2022-11-14 16:20:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][450/1251]	eta 0:10:04 lr 0.000010	time 0.7418 (0.7545)	loss 2.7235 (2.5775)	grad_norm 3.5723 (3.2903)	mem 23875MB
[2022-11-14 16:21:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][500/1251]	eta 0:09:26 lr 0.000010	time 0.7416 (0.7539)	loss 3.1567 (2.5672)	grad_norm 3.2819 (3.3056)	mem 23875MB
[2022-11-14 16:21:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][550/1251]	eta 0:08:48 lr 0.000010	time 0.7491 (0.7539)	loss 2.8598 (2.5685)	grad_norm 3.0184 (3.3345)	mem 23875MB
[2022-11-14 16:22:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][600/1251]	eta 0:08:10 lr 0.000010	time 0.8238 (0.7536)	loss 1.9272 (2.5627)	grad_norm 2.6248 (3.3834)	mem 23875MB
[2022-11-14 16:23:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][650/1251]	eta 0:07:32 lr 0.000010	time 0.7474 (0.7535)	loss 2.9397 (2.5623)	grad_norm 3.0209 (3.3861)	mem 23875MB
[2022-11-14 16:23:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][700/1251]	eta 0:06:55 lr 0.000010	time 0.8249 (0.7534)	loss 2.4089 (2.5686)	grad_norm 2.9671 (3.3841)	mem 23875MB
[2022-11-14 16:24:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][750/1251]	eta 0:06:17 lr 0.000010	time 0.7515 (0.7530)	loss 2.6626 (2.5740)	grad_norm 36.0977 (3.4269)	mem 23875MB
[2022-11-14 16:25:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][800/1251]	eta 0:05:39 lr 0.000010	time 0.7465 (0.7530)	loss 2.5090 (2.5735)	grad_norm 2.5917 (3.4266)	mem 23875MB
[2022-11-14 16:25:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][850/1251]	eta 0:05:01 lr 0.000010	time 0.7388 (0.7529)	loss 2.8486 (2.5684)	grad_norm 2.9375 (3.4287)	mem 23875MB
[2022-11-14 16:26:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][900/1251]	eta 0:04:24 lr 0.000010	time 0.7423 (0.7526)	loss 2.8305 (2.5695)	grad_norm 3.0392 (3.4433)	mem 23875MB
[2022-11-14 16:26:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][950/1251]	eta 0:03:46 lr 0.000010	time 0.8085 (0.7527)	loss 1.7506 (2.5659)	grad_norm 2.5718 (3.4320)	mem 23875MB
[2022-11-14 16:27:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][1000/1251]	eta 0:03:08 lr 0.000010	time 0.8480 (0.7524)	loss 2.8717 (2.5676)	grad_norm 3.0929 (3.4268)	mem 23875MB
[2022-11-14 16:28:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][1050/1251]	eta 0:02:31 lr 0.000010	time 0.7522 (0.7524)	loss 2.8025 (2.5664)	grad_norm 3.3528 (3.4146)	mem 23875MB
[2022-11-14 16:28:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][1100/1251]	eta 0:01:53 lr 0.000010	time 0.8327 (0.7524)	loss 2.4443 (2.5647)	grad_norm 3.6569 (3.4179)	mem 23875MB
[2022-11-14 16:29:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][1150/1251]	eta 0:01:15 lr 0.000010	time 0.7374 (0.7522)	loss 2.4815 (2.5640)	grad_norm 3.1540 (3.4065)	mem 23875MB
[2022-11-14 16:30:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][1200/1251]	eta 0:00:38 lr 0.000010	time 0.7238 (0.7522)	loss 2.2101 (2.5599)	grad_norm 4.8330 (3.4210)	mem 23875MB
[2022-11-14 16:30:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [298/300][1250/1251]	eta 0:00:00 lr 0.000010	time 0.7285 (0.7520)	loss 3.0353 (2.5586)	grad_norm 3.3083 (3.4326)	mem 23875MB
[2022-11-14 16:30:39 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 298 training takes 0:15:40
[2022-11-14 16:30:39 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_298.pth saving......
[2022-11-14 16:30:40 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_298.pth saved !!!
[2022-11-14 16:30:42 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.793 (1.793)	Loss 0.7977 (0.7977)	Acc@1 82.227 (82.227)	Acc@5 95.215 (95.215)	Mem 23875MB
[2022-11-14 16:30:53 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.822 Acc@5 96.604
[2022-11-14 16:30:53 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.8%
[2022-11-14 16:30:55 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.973 (1.973)	Loss 0.7627 (0.7627)	Acc@1 82.715 (82.715)	Acc@5 96.387 (96.387)	Mem 23875MB
[2022-11-14 16:31:06 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.916 Acc@5 96.668
[2022-11-14 16:31:06 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 16:31:06 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 16:31:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][0/1251]	eta 0:52:04 lr 0.000010	time 2.4974 (2.4974)	loss 2.8847 (2.8847)	grad_norm 2.8510 (2.8510)	mem 23875MB
[2022-11-14 16:31:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][50/1251]	eta 0:15:45 lr 0.000010	time 0.7487 (0.7876)	loss 2.5042 (2.5352)	grad_norm 3.3611 (3.0977)	mem 23875MB
[2022-11-14 16:32:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][100/1251]	eta 0:14:45 lr 0.000010	time 0.7398 (0.7692)	loss 2.0084 (2.5745)	grad_norm 2.6991 (3.1167)	mem 23875MB
[2022-11-14 16:33:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][150/1251]	eta 0:14:00 lr 0.000010	time 0.7484 (0.7634)	loss 2.7157 (2.5660)	grad_norm 3.4631 (3.3316)	mem 23875MB
[2022-11-14 16:33:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][200/1251]	eta 0:13:19 lr 0.000010	time 0.8070 (0.7605)	loss 1.9810 (2.5455)	grad_norm 3.3284 (3.3174)	mem 23875MB
[2022-11-14 16:34:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][250/1251]	eta 0:12:39 lr 0.000010	time 0.7396 (0.7585)	loss 3.0888 (2.5650)	grad_norm 3.0003 (3.3291)	mem 23875MB
[2022-11-14 16:34:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][300/1251]	eta 0:12:00 lr 0.000010	time 0.7343 (0.7571)	loss 2.9364 (2.5776)	grad_norm 3.7832 (3.3508)	mem 23875MB
[2022-11-14 16:35:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][350/1251]	eta 0:11:20 lr 0.000010	time 0.7369 (0.7555)	loss 2.2858 (2.5856)	grad_norm 3.0609 (3.3520)	mem 23875MB
[2022-11-14 16:36:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][400/1251]	eta 0:10:42 lr 0.000010	time 0.7254 (0.7554)	loss 3.0812 (2.5880)	grad_norm 3.0403 (3.3669)	mem 23875MB
[2022-11-14 16:36:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][450/1251]	eta 0:10:04 lr 0.000010	time 0.7467 (0.7545)	loss 2.3975 (2.5778)	grad_norm 3.0755 (3.3666)	mem 23875MB
[2022-11-14 16:37:23 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][500/1251]	eta 0:09:26 lr 0.000010	time 0.7542 (0.7541)	loss 1.5747 (2.5753)	grad_norm 3.5847 (3.3608)	mem 23875MB
[2022-11-14 16:38:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][550/1251]	eta 0:08:48 lr 0.000010	time 0.7413 (0.7538)	loss 2.4859 (2.5776)	grad_norm 3.6796 (3.4213)	mem 23875MB
[2022-11-14 16:38:38 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][600/1251]	eta 0:08:10 lr 0.000010	time 0.8171 (0.7533)	loss 1.8518 (2.5774)	grad_norm 3.6720 (3.4122)	mem 23875MB
[2022-11-14 16:39:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][650/1251]	eta 0:07:32 lr 0.000010	time 0.7516 (0.7531)	loss 2.7184 (2.5741)	grad_norm 5.2358 (3.4049)	mem 23875MB
[2022-11-14 16:39:53 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][700/1251]	eta 0:06:54 lr 0.000010	time 0.7428 (0.7529)	loss 2.2003 (2.5752)	grad_norm 3.5921 (3.3929)	mem 23875MB
[2022-11-14 16:40:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][750/1251]	eta 0:06:17 lr 0.000010	time 0.7412 (0.7527)	loss 2.6550 (2.5723)	grad_norm 2.9004 (3.3858)	mem 23875MB
[2022-11-14 16:41:08 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][800/1251]	eta 0:05:39 lr 0.000010	time 0.7427 (0.7527)	loss 2.4492 (2.5766)	grad_norm 4.7754 (3.3724)	mem 23875MB
[2022-11-14 16:41:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][850/1251]	eta 0:05:01 lr 0.000010	time 0.7458 (0.7528)	loss 2.9095 (2.5769)	grad_norm 2.8031 (3.3705)	mem 23875MB
[2022-11-14 16:42:24 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][900/1251]	eta 0:04:24 lr 0.000010	time 0.7386 (0.7525)	loss 2.6897 (2.5750)	grad_norm 2.9618 (3.3627)	mem 23875MB
[2022-11-14 16:43:01 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][950/1251]	eta 0:03:46 lr 0.000010	time 0.7293 (0.7525)	loss 2.3795 (2.5775)	grad_norm 3.1000 (3.3617)	mem 23875MB
[2022-11-14 16:43:39 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][1000/1251]	eta 0:03:08 lr 0.000010	time 0.8194 (0.7523)	loss 2.2255 (2.5709)	grad_norm 2.7878 (3.3581)	mem 23875MB
[2022-11-14 16:44:16 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][1050/1251]	eta 0:02:31 lr 0.000010	time 0.7403 (0.7523)	loss 2.2636 (2.5719)	grad_norm 3.0469 (inf)	mem 23875MB
[2022-11-14 16:44:54 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][1100/1251]	eta 0:01:53 lr 0.000010	time 0.7406 (0.7521)	loss 2.8200 (2.5720)	grad_norm 2.8526 (inf)	mem 23875MB
[2022-11-14 16:45:31 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][1150/1251]	eta 0:01:15 lr 0.000010	time 0.7616 (0.7520)	loss 3.0860 (2.5705)	grad_norm 6.3365 (inf)	mem 23875MB
[2022-11-14 16:46:09 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][1200/1251]	eta 0:00:38 lr 0.000010	time 0.7393 (0.7520)	loss 2.9462 (2.5698)	grad_norm 3.1195 (inf)	mem 23875MB
[2022-11-14 16:46:46 QFormer_transformer_small_patch4_window7_224] (main.py 312): INFO Train: [299/300][1250/1251]	eta 0:00:00 lr 0.000010	time 0.7329 (0.7519)	loss 3.1476 (2.5711)	grad_norm 4.4740 (inf)	mem 23875MB
[2022-11-14 16:46:46 QFormer_transformer_small_patch4_window7_224] (main.py 320): INFO EPOCH 299 training takes 0:15:40
[2022-11-14 16:46:46 QFormer_transformer_small_patch4_window7_224] (utils.py 162): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_299.pth saving......
[2022-11-14 16:46:48 QFormer_transformer_small_patch4_window7_224] (utils.py 164): INFO output/QFormer_transformer_small_patch4_window7_224/1024-dpr30-coords_lambda1e-1/ckpt_epoch_299.pth saved !!!
[2022-11-14 16:46:49 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 1.700 (1.700)	Loss 0.7184 (0.7184)	Acc@1 83.594 (83.594)	Acc@5 96.875 (96.875)	Mem 23875MB
[2022-11-14 16:47:00 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.894 Acc@5 96.598
[2022-11-14 16:47:00 QFormer_transformer_small_patch4_window7_224] (main.py 211): INFO Accuracy of the network on the 50000 test images: 83.9%
[2022-11-14 16:47:02 QFormer_transformer_small_patch4_window7_224] (main.py 364): INFO Test: [0/49]	Time 2.038 (2.038)	Loss 0.7400 (0.7400)	Acc@1 83.887 (83.887)	Acc@5 95.898 (95.898)	Mem 23875MB
[2022-11-14 16:47:13 QFormer_transformer_small_patch4_window7_224] (main.py 371): INFO  * Acc@1 83.928 Acc@5 96.646
[2022-11-14 16:47:13 QFormer_transformer_small_patch4_window7_224] (main.py 224): INFO Accuracy of the ema network on the 50000 test images: 83.9%
[2022-11-14 16:47:13 QFormer_transformer_small_patch4_window7_224] (main.py 228): INFO Max accuracy: 83.96% at 287 epoch
[2022-11-14 16:47:13 QFormer_transformer_small_patch4_window7_224] (main.py 232): INFO Training time 1 day, 1:57:20
